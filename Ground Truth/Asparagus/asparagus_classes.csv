ClassName,Code,File
D3_dispersion,"class D3_dispersion(torch.nn.Module):
    """"""
    Torch implementation of Grimme's D3 method (only Becke-Johnson damping is
    implemented)

    Grimme, Stefan, et al. ""A consistent and accurate ab initio parametrization
    of density functional dispersion correction (DFT-D) for the 94 elements
    H-Pu."" The Journal of Chemical Physics 132, 15 (2010): 154104.

    Update of the implementation according with respect to the tad-dftd3 module
    on git: https://github.com/dftd3/tad-dftd3 (15.11.2024)

    Parameters
    ----------
    cutoff: float
        Upper cutoff distance
    cuton: float
        Lower cutoff distance starting switch-off function
    trainable: bool, optional, default True
        If True the dispersion parameters are trainable
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    unit_properties: dict, optional, default {}
        Dictionary with the units of the model properties to initialize correct
        conversion factors.
    truncation: str, optional, default 'force'
        Truncation method of the Dispersion potential at the cutoff range:
            None, 'None': 
                No Dispersion potential shift applied
            'potential':
                Apply shifted Dispersion potential method
                    V_shifted(r) = V_Coulomb(r) - V_Coulomb(r_cutoff)
            'force', 'forces':
                Apply shifted Dispersion force method
                    V_shifted(r) = V_Dispersion(r) - V_Dispersion(r_cutoff)
                        - (dV_Dispersion/dr)|r_cutoff  * (r - r_cutoff)
    d3_s6: float, optional, default 1.0000
        d3_s6 dispersion parameter
    d3_s8: float, optional, default 0.9171
        d3_s8 dispersion parameter
    d3_a1: float, optional, default 0.3385
        d3_a1 dispersion parameter
    d3_a2: float, optional, default 2.8830
        d3_a2 dispersion parameter

    """"""

    def __init__(
        self,
        cutoff: float,
        cuton: float,
        trainable: bool,
        device: str,
        dtype: 'dtype',
        unit_properties: Optional[Dict[str, str]] = None,
        truncation: Optional[str] = 'force',
        d3_s6: Optional[float] = None,
        d3_s8: Optional[float] = None,
        d3_a1: Optional[float] = None,
        d3_a2: Optional[float] = None,
        **kwargs
    ):
        """"""
        Initialize Grimme D3 dispersion model.
        
        """"""

        super(D3_dispersion, self).__init__()

        # Relative filepath to package folder
        package_directory = os.path.dirname(os.path.abspath(__file__))

        # Assign variables
        self.dtype = dtype
        self.device = device

        # Load tables with reference values
        self.d3_rcov = torch.from_numpy(
            np.load(os.path.join(package_directory, ""grimme_d3"", ""rcov.npy""))
            ).to(dtype).to(device)
        self.d3_rcn = torch.from_numpy(
            np.genfromtxt(
                os.path.join(package_directory, ""grimme_d3"", ""refcn.csv""),
                delimiter=',')
            ).to(dtype).to(device)
        self.d3_rcn_max = torch.max(self.d3_rcn, dim=-1, keepdim=True)[0]
        self.d3_rc6 = torch.from_numpy(
            np.load(os.path.join(package_directory, ""grimme_d3"", ""rc6.npy""))
            ).to(dtype).to(device)
        self.d3_r2r4 = torch.from_numpy(
            np.load(os.path.join(package_directory, ""grimme_d3"", ""r2r4.npy""))
            ).to(dtype).to(device)
        
        # Assign truncation method
        if truncation is None or truncation.lower() == 'none':
            self.potential_fn = self.dispersion_fn
        elif truncation.lower() == 'potential':
            self.potential_fn = self.dispersion_sp_fn
        elif truncation.lower() in ['force', 'forces']:
            self.potential_fn = self.dispersion_sf_fn
        else:
            raise SyntaxError(
                ""Truncation method of the Dispersion potential ""
                + f""'{truncation:}' is unknown!\n""
                + ""Available are 'None', 'potential', 'force'."")

        # Initialize global dispersion correction parameters 
        # (default values for HF)
        if d3_s6 is None:
            d3_s6 = 1.0000
        if d3_s8 is None:
            d3_s8 = 0.9171
        if d3_a1 is None:
            d3_a1 = 0.3385
        if d3_a2 is None:
            d3_a2 = 2.8830
        
        if trainable:
            self.d3_s6 = torch.nn.Parameter(
                torch.tensor([d3_s6], device=device, dtype=dtype))
            self.d3_s8 = torch.nn.Parameter(
                torch.tensor([d3_s8], device=device, dtype=dtype))
            self.d3_a1 = torch.nn.Parameter(
                torch.tensor([d3_a1], device=device, dtype=dtype))
            self.d3_a2 = torch.nn.Parameter(
                torch.tensor([d3_a2], device=device, dtype=dtype))
        else:
            self.register_buffer(
                ""d3_s6"", torch.tensor([d3_s6], device=device, dtype=dtype))
            self.register_buffer(
                ""d3_s8"", torch.tensor([d3_s8], device=device, dtype=dtype))
            self.register_buffer(
                ""d3_a1"", torch.tensor([d3_a1], device=device, dtype=dtype))
            self.register_buffer(
                ""d3_a2"", torch.tensor([d3_a2], device=device, dtype=dtype))
        self.d3_k1 = torch.tensor([16.000], device=device, dtype=dtype)
        self.d3_k2 = torch.tensor([4./3.], device=device, dtype=dtype)
        self.d3_k3 = torch.tensor([-4.000], device=device, dtype=dtype)

        # Assign cutoff radii
        self.cutoff = cutoff
        self.cuton = cuton

        # Unit conversion factors
        self.set_unit_properties(unit_properties)

        # Prepare interaction switch-off range
        self.set_switch_of_range(cutoff, cuton)

        # Auxiliary parameter
        self.zero_dtype = torch.tensor(0.0, device=device, dtype=dtype)
        self.zero_double = torch.tensor(
            0.0, device=device, dtype=torch.float64)
        self.small_double = torch.tensor(
            1e-300, device=device, dtype=torch.float64)
        self.one_dtype = torch.tensor(1.0, device=device, dtype=dtype)
        self.max_dtype = torch.tensor(
            torch.finfo(dtype).max, device=device, dtype=dtype)

        return
        
    def __str__(self):
        return ""D3 Dispersion""

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {}

    def set_unit_properties(
        self,
        unit_properties: Dict[str, str],
    ):
        """"""
        Set unit conversion factors for compatibility between requested
        property units and applied property units (for physical constants)
        of the module.
        
        Parameters
        ----------
        unit_properties: dict
            Dictionary with the units of the model properties to initialize 
            correct conversion factors.
        
        """"""
        
        # Get conversion factors
        if unit_properties is None:
            unit_energy = settings._default_units.get('energy')
            unit_positions = settings._default_units.get('positions')
            factor_energy, _ = utils.check_units(unit_energy, 'Hartree')
            factor_positions, _ = utils.check_units('Bohr', unit_positions)
        else:
            factor_energy, _ = utils.check_units(
                unit_properties.get('energy'), 'Hartree')
            factor_positions, _ = utils.check_units(
                'Bohr', unit_properties.get('positions'))

        # Convert
        # Distances: model to Bohr
        # Energies: Hartree to model
        self.register_buffer(
            ""distances_model2Bohr"", 
            torch.tensor(
                [factor_positions], device=self.device, dtype=self.dtype))
        self.register_buffer(
            ""energies_Hatree2model"", 
            torch.tensor(
                [factor_energy], device=self.device, dtype=self.dtype))

        # Update interaction switch-off range units
        self.set_switch_of_range(self.cutoff, self.cuton)

        return

    def set_switch_of_range(
        self,
        cutoff: float,
        cuton: float,
    ):
        """"""
        Prepare switch-off parameters

        """"""
        
        self.cutoff = (
            torch.tensor([cutoff], device=self.device, dtype=self.dtype)
            * self.distances_model2Bohr)
        if cuton is None or cuton == cutoff:
            self.cuton = None
            self.switchoff_range = None
            self.use_switch = False
        else:
            self.cuton = (
                torch.tensor([cuton], device=self.device, dtype=self.dtype)
                * self.distances_model2Bohr)
            self.switchoff_range = (
                torch.tensor(
                    [cutoff - cuton], device=self.device, dtype=self.dtype)
                * self.distances_model2Bohr)
            self.use_switch = True

        return

    def switch_fn(
        self,
        distances: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Computes a smooth switch factors from 1 to 0 in the range from 'cuton'
        to 'cutoff'.
        
        """"""
        
        x = (self.cutoff - distances) / self.switchoff_range
        
        return torch.where(
            distances < self.cuton,
            torch.ones_like(x),
            torch.where(
                distances >= self.cutoff,
                torch.zeros_like(x),
                ((6.0*x - 15.0)*x + 10.0)*x**3
                )
            )

    def get_cn(
        self,
        atomic_numbers: torch.Tensor,
        atomic_numbers_i: torch.Tensor,
        atomic_numbers_j: torch.Tensor,
        distances: torch.Tensor,
        switch_off: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Compute coordination numbers by adding an inverse damping function.
        
        """"""

        # Compute atom pairs covalent radii
        rcov_ij = (
            torch.gather(self.d3_rcov, 0, atomic_numbers_i) 
            + torch.gather(self.d3_rcov, 0, atomic_numbers_j))
        
        cn_ij = (
            1.0/(1.0 + torch.exp(-self.d3_k1 * (rcov_ij/distances - 1.0))))
        if self.use_switch:
            cn_ij = cn_ij*switch_off

        return utils.scatter_sum(
            cn_ij, idx_i, dim=0, shape=atomic_numbers.shape)

    def get_weights(
        self,
        atomic_numbers: torch.Tensor,
        cn: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute dispersion weights
        
        """"""

        # Get reference atomic coordination numbers of atom pairs ij
        rcn = self.d3_rcn[atomic_numbers]

        # Selection of non-zero reference coordination numbers
        mask_rcn = rcn >= 0

        # Compute deviation between reference coordination number and actual
        # coordination number
        dcn = (rcn - cn.unsqueeze(-1)).type(torch.double)

        # Compute and normalize coordination number Gaussian weights in double 
        # precision and convert back to dtype
        gaussian_weights = torch.where(
            mask_rcn,
            torch.exp(self.d3_k3*dcn**2),
            self.zero_double)
        norm = torch.where(
            mask_rcn,
            torch.sum(gaussian_weights, dim=-1, keepdim=True),
            self.small_double)
        mask_norm = norm == 0
        norm = torch.where(
            mask_norm,
            self.small_double,
            norm)
        gaussian_weights = (gaussian_weights/norm).type(self.dtype)

        # Prevent exceptional values in the gaussian weights, either because
        # the norm was zero or the weight is to large.
        exceptional = torch.logical_or(
            mask_norm, gaussian_weights > self.max_dtype)
        if torch.any(exceptional):
            rcn_max = self.d3_rcn_max[atomic_numbers]
            gaussian_weights = torch.where(
                exceptional,
                torch.where(rcn == rcn_max, self.one_dtype, self.zero_dtype),
                gaussian_weights)
        gaussian_weights = torch.where(
            mask_rcn,
            gaussian_weights,
            self.zero_dtype)

        return gaussian_weights

    def get_c6(
        self,
        atomic_numbers_i: torch.Tensor,
        atomic_numbers_j: torch.Tensor,
        weigths: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute atomic c6 dispersion coefficients
        
        """"""

        # Collect reference c6 dispersion coefficients of atom pairs ij
        rc6 = self.d3_rc6[atomic_numbers_i, atomic_numbers_j]

        # Collect atomic weights of atom pairs ij
        weights_i = weigths[idx_i]
        weights_j = weigths[idx_j]
        weights_ij = weights_i.unsqueeze(-1)*weights_j.unsqueeze(-2)

        # Compute atomic c6 dispersion coefficients
        c6 = torch.sum(torch.sum(torch.mul(weights_ij, rc6), dim=-1), dim=-1)

        return c6

    def dispersion_fn(
        self,
        distances: torch.Tensor,
        distances6: torch.Tensor,
        distances8: torch.Tensor,
        switch_off: torch.Tensor,
        c6: torch.Tensor,
        c8: torch.Tensor,
        fct6: torch.Tensor,
        fct8: torch.Tensor,
        damp_c6: torch.Tensor,
        damp_c8: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute atomic D3 dispersion energy

        """"""
        
        # Compute atomic dispersion energy contributions
        e6 = -0.5*self.d3_s6*c6*damp_c6
        e8 = -0.5*self.d3_s8*c8*damp_c8

        # Apply switch-off function
        edisp = switch_off*(e6 + e8)
        
        return edisp

    def dispersion_sp_fn(
        self,
        distances: torch.Tensor,
        distances6: torch.Tensor,
        distances8: torch.Tensor,
        switch_off: torch.Tensor,
        c6: torch.Tensor,
        c8: torch.Tensor,
        fct6: torch.Tensor,
        fct8: torch.Tensor,
        damp_c6: torch.Tensor,
        damp_c8: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute shifted potential atomic D3 dispersion energy

        """"""

        # Compute all required powers of the cutoff distance
        cutoff2 = self.cutoff**2
        cutoff6 = cutoff2**3
        cutoff8 = cutoff6*cutoff2
        denominator6 = cutoff6 + fct6
        denominator8 = cutoff8 + fct8
        
        # Compute force shifted atomic dispersion energy contributions
        e6 = -0.5*self.d3_s6*c6*(damp_c6 - 1.0/denominator6)
        e8 = -0.5*self.d3_s8*c8*(damp_c8 - 1.0/denominator8)

        # Apply switch-off function
        edisp = switch_off*(e6 + e8)
        
        return edisp

    def dispersion_sf_fn(
        self,
        distances: torch.Tensor,
        distances6: torch.Tensor,
        distances8: torch.Tensor,
        switch_off: torch.Tensor,
        c6: torch.Tensor,
        c8: torch.Tensor,
        fct6: torch.Tensor,
        fct8: torch.Tensor,
        damp_c6: torch.Tensor,
        damp_c8: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute shifted force  atomic D3 dispersion energy

        """"""

        # Compute all required powers of the cutoff distance
        cutoff2 = self.cutoff**2
        cutoff6 = cutoff2**3
        cutoff8 = cutoff6*cutoff2
        denominator6 = cutoff6 + fct6
        denominator8 = cutoff8 + fct8
        
        # Compute force shifted atomic dispersion energy contributions
        e6 = -0.5*self.d3_s6*c6*(
            damp_c6 - 1.0/denominator6 
            + 6.0*cutoff6/denominator6**2*(distances/self.cutoff - 1.0))
        e8 = -0.5*self.d3_s8*c8*(
            damp_c8 - 1.0/denominator8 
            + 8.0*cutoff8/denominator8**2*(distances/self.cutoff - 1.0))

        # Apply switch-off function
        edisp = switch_off*(e6 + e8)
        
        return edisp

    def forward(
        self,
        atomic_numbers: torch.Tensor,
        distances: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute Grimme's D3 dispersion energy in Hartree with atom pair 
        distances in Bohr.

        Parameters
        ----------
        atomic_numbers : torch.Tensor
            Atomic numbers of all atoms in the batch.
        distances : torch.Tensor
            Distances between all atom pairs in the batch.
        idx_i : torch.Tensor
            Indices of the first atom of each pair.
        idx_j : torch.Tensor
            Indices of the second atom of each pair.

        Returns
        -------
        torch.Tensor
            Dispersion atom energy contribution
        
        """"""
        
        # Convert distances from model unit to Bohr
        distances_d3 = distances*self.distances_model2Bohr

        # Compute switch-off function
        if self.use_switch:
            switch_off = self.switch_fn(distances_d3)
        else:
            switch_off = torch.where(
                distances_d3 < self.cutoff,
                torch.ones_like(distances_d3),
                torch.zeros_like(distances_d3),
            )

        # Gather atomic numbers of atom pairs ij
        atomic_numbers_i = torch.gather(atomic_numbers, 0, idx_i)
        atomic_numbers_j = torch.gather(atomic_numbers, 0, idx_j)

        # Compute coordination numbers and of atom pairs ij
        cn = self.get_cn(
            atomic_numbers,
            atomic_numbers_i,
            atomic_numbers_j,
            distances_d3,
            switch_off,
            idx_i,
            idx_j)
        
        # Compute atomic weights
        weights = self.get_weights(
            atomic_numbers,
            cn)
        
        # Compute atomic C6 and C8 coefficients
        c6 = self.get_c6(
            atomic_numbers_i,
            atomic_numbers_j,
            weights,
            idx_i,
            idx_j)
        qq = (
            3.0
            * torch.gather(self.d3_r2r4, 0, atomic_numbers_i)
            * torch.gather(self.d3_r2r4, 0, atomic_numbers_j))
        c8 = qq*c6

        # Compute the powers of the atom pair distances
        distances2 = distances_d3**2
        distances6 = distances2**3
        distances8 = distances6*distances2

        # Apply rational Becke-Johnson damping.
        fct = self.d3_a1*torch.sqrt(qq) + self.d3_a2
        fct2 = fct**2
        fct6 = fct2**3
        fct8 = fct6*fct2
        damp_c6 = 1.0/(distances6 + fct6)
        damp_c8 = 1.0/(distances8 + fct8)

        # Compute atomic dispersion energy contributions
        print("""")
        Edisp = self.dispersion_fn(
            distances_d3,
            distances6,
            distances8,
            switch_off,
            c6,
            c8,
            fct6,
            fct8,
            damp_c6,
            damp_c8)
        print(""Switch-off: "", Edisp.detach().numpy())
        Edisp = self.dispersion_sp_fn(
            distances_d3,
            distances6,
            distances8,
            switch_off,
            c6,
            c8,
            fct6,
            fct8,
            damp_c6,
            damp_c8)
        print(""Potential shifted switch-off: "", Edisp.detach().numpy())
        Edisp = self.dispersion_sf_fn(
            distances_d3,
            distances6,
            distances8,
            switch_off,
            c6,
            c8,
            fct6,
            fct8,
            damp_c6,
            damp_c8)
        print(""Force shifted switch-off: "", Edisp.detach().numpy())
        Edisp = self.potential_fn(
            distances_d3,
            distances6,
            distances8,
            switch_off,
            c6,
            c8,
            fct6,
            fct8,
            damp_c6,
            damp_c8)

        # Return system dispersion energies and convert to model energy unit
        return self.energies_Hatree2model*utils.scatter_sum(
            Edisp, idx_i, dim=0, shape=atomic_numbers.shape)",./Asparagus/verification/verify_dispersion.py
Asparagus,"class Asparagus():
    """"""
    Asparagus main class

    Parameters
    ----------
    config: (str, dict, settings.Configuration), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of model parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    kwargs: dict, optional, default {}
        Additional model keyword input parameter

    """"""

    name = f""{__name__:s}: {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    def __init__(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        **kwargs
    ):

        super().__init__()

        #############################
        # # # Check Model Input # # #
        #############################

        # Initialize model parameter configuration dictionary
        # Keyword arguments overwrite entries in the configuration dictionary
        config = settings.get_config(
            config, config_file, config_from=self, **kwargs)

        # Check model parameter configuration and set default
        config.check(
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Get configuration file path
        self.config_file = config.get('config_file')

        # Print Asparagus header
        self.logger.info(utils.get_header(self.config_file))

        ###########################
        # # # Class Parameter # # #
        ###########################

        # DataContainer of reference data
        self.data_container = None
        # Model calculator
        self.model_calculator = None

        return

    def __str__(self) -> str:
        """"""
        Return class descriptor
        """"""
        return ""Asparagus Main""

    def __getitem__(self, args: str) -> Any:
        """"""
        Return item(s) from configuration dictionary
        """"""
        config = settings.get_config(self.config)
        return config.get(args)

    def get(self, args: str) -> Any:
        """"""
        Return item(s) from configuration dictionary
        """"""
        config = settings.get_config(self.config)
        return config.get(args)

    def set_data_container(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        data_container: Optional[data.DataContainer] = None,
        **kwargs
    ):
        """"""
        Set and, eventually, initialize DataContainer as class variable.

        Parameter:
        ----------
        config: (str, dict, settings.Config), optional, default None
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters.
        config_file: str, optional, default None
            Path to json file (str)
        data_container: data.DataContainer, optional, default None
            DataContainer object to assign to the Asparagus object

        """"""

        ######################################
        # # # Check Data Container Input # # #
        ######################################

        if config is None:
            config = settings.get_config(
                self.config, config_file, config_from=self)
        else:
            config = settings.get_config(
                config, config_file, config_from=self)

        # Check model parameter configuration and set default
        config_update = config.set(
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Update configuration dictionary
        config.update(config_update)

        #################################
        # # # Assign Data Container # # #
        #################################

        # Check custom data container
        if data_container is not None:

            # Assign data container
            self.data_container = data_container

            # Add data container info to configuration dictionary
            if hasattr(data_container, ""get_info""):
                config.update(data_container.get_info())

        else:

            # Get data container
            data_container = self._get_data_container(
                config,
                data_container=data_container,
                **kwargs)

            # Assign data container
            self.data_container = data_container

        return

    def get_data_container(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        **kwargs
    ) -> data.DataContainer:
        """"""
        Initialize and return DataContainer.

        Parameter:
        ----------
        config: (str, dict, settings.Configuration), optional, default None
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters.
        config_file: str, optional, default None
            Path to json file (str)

        Returns
        -------
        data.DataContainer
            Asparagus data container object

        """"""

        ######################################
        # # # Check Data Container Input # # #
        ######################################

        if config is None:
            config = settings.get_config(
                self.config, config_file, config_from=self)
        else:
            config = settings.get_config(
                config, config_file, config_from=self)

        # Check model parameter configuration and set default
        config_update = config.set(
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Update configuration dictionary
        config.update(config_update)

        ##################################
        # # # Prepare Data Container # # #
        ##################################

        data_container = self._get_data_container(
            config,
            **kwargs)

        return data_container

    def _get_data_container(
        self,
        config: settings.Configuration,
        **kwargs
    ) -> data.DataContainer:
        """"""
        Initialize and set DataContainer as class variable

        Parameter:
        ----------
        config: settings.Configuration
            Asparagus parameter settings.config class object

        Returns
        -------
        data.DataContainer
            Asparagus data container object

        """"""

        ##################################
        # # # Prepare Data Container # # #
        ##################################

        data_container = data.DataContainer(
            config,
            **kwargs)

        return data_container

    def set_model_calculator(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        model_calculator:
            Optional[Union[List[torch.nn.Module], torch.nn.Module]] = None,
        model_type: Optional[str] = None,
        model_directory: Optional[str] = None,
        model_ensemble: Optional[bool] = None,
        model_ensemble_num: Optional[int] = None,
        model_checkpoint: Optional[Union[int, str]] = None,
        model_compile: Optional[bool] = None,
        **kwargs,
    ):
        """"""
        Set and, eventually, initialize the calculator model class object

        Parameters
        ----------
        config: (str, dict, object), optional, default 'self.config'
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to json file (str)
        model_calculator: (torch.nn.Module, list(torch.nn.Module)),
                optional, default None
            Model calculator object or list of model calculator objects.
        model_type: str, optional, default None
            Model calculator type to initialize, e.g. 'PhysNet'. The default
            model is defined in settings.default._default_calculator_model.
        model_directory: str, optional, default None
            Model directory that contains checkpoint and log files.
        model_ensemble: bool, optional, default None
            Expect a model calculator ensemble. If None, check config or
            assume as False.
        model_ensemble_num: int, optional, default None
            Number of model calculator in ensemble.
        model_checkpoint: (int, str), optional, default None
            If None or 'best', load best model checkpoint.
            Otherwise load latest checkpoint file with 'last' or define a
            checkpoint index number of the respective checkpoint file.
        model_compile: bool, optional, default None
            If True, the model calculator will get compiled at the first
            call to enhance the performance. Generally not applicable during
            training where multiple backwards calls are done (e.g. energy
            gradient and loss metrics gradient)

        """"""

        ########################################
        # # # Check Model Calculator Input # # #
        ########################################

        if config is None:
            config = settings.get_config(
                self.config, config_file, config_from=self)
        else:
            config = settings.get_config(
                config, config_file, config_from=self)

        # Check model parameter configuration and set default
        config_update = config.set(
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Update configuration dictionary
        config.update(config_update)

        ###################################
        # # # Assign Model Calculator # # #
        ###################################

        # Get model calculator
        model_calculator = self._get_model_calculator(
            config,
            model_calculator=model_calculator,
            model_type=model_type,
            model_directory=model_directory,
            model_ensemble=model_ensemble,
            model_ensemble_num=model_ensemble_num,
            model_checkpoint=model_checkpoint,
            model_compile=model_compile,
            **kwargs,
            )

        # Assign model calculator
        self.model_calculator = model_calculator

        return

    def get_model_calculator(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        model_calculator: 
            Optional[Union[List[torch.nn.Module], torch.nn.Module]] = None,
        model_type: Optional[str] = None,
        model_directory: Optional[str] = None,
        model_ensemble: Optional[bool] = None,
        model_ensemble_num: Optional[int] = None,
        model_checkpoint: Optional[Union[int, str]] = None,
        model_compile: Optional[bool] = None,
        **kwargs,
    ) -> torch.nn.Module:
        """"""
        Return calculator model class object

        Parameters
        ----------
        config: (str, dict, object), optional, default 'self.config'
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to json file (str)
        model_calculator: (torch.nn.Module, list(torch.nn.Module)),
                optional, default None
            Model calculator object or list of model calculator objects.
        model_type: str, optional, default None
            Model calculator type to initialize, e.g. 'PhysNet'. The default
            model is defined in settings.default._default_calculator_model.
        model_directory: str, optional, default None
            Model directory that contains checkpoint and log files.
        model_ensemble: bool, optional, default None
            Expect a model calculator ensemble. If None, check config or
            assume as False.
        model_ensemble_num: int, optional, default None
            Number of model calculator in ensemble.
        model_checkpoint: (int, str), optional, default None
            If None or 'best', load best model checkpoint.
            Otherwise load latest checkpoint file with 'last' or define a
            checkpoint index number of the respective checkpoint file.
        model_compile: bool, optional, default None
            If True, the model calculator will get compiled at the first
            call to enhance the performance. Generally not applicable during
            training where multiple backwards calls are done (e.g. energy
            gradient and loss metrics gradient)

        Returns
        -------
        torch.nn.Module
            Asparagus calculator model object

        """"""

        ########################################
        # # # Check Model Calculator Input # # #
        ########################################

        # Assign model parameter configuration library
        if config is None:
            config = settings.get_config(
                self.config, config_file, config_from=self)
        else:
            config = settings.get_config(
                config, config_file, config_from=self)

        # Check model parameter configuration and set default
        config_update = config.set(
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Update configuration dictionary
        config.update(config_update)

        ####################################
        # # # Prepare Model Calculator # # #
        ####################################

        # Get model calculator
        model_calculator = self._get_model_calculator(
            config,
            model_calculator=model_calculator,
            model_type=model_type,
            model_directory=model_directory,
            model_ensemble=model_ensemble,
            model_ensemble_num=model_ensemble_num,
            model_checkpoint=model_checkpoint,
            model_compile=model_compile,
            **kwargs,
            )

        return model_calculator

    def _get_model_calculator(
        self,
        config: settings.Configuration,
        model_calculator: 
            Optional[Union[List[torch.nn.Module], torch.nn.Module]] = None,
        model_type: Optional[str] = None,
        model_directory: Optional[str] = None,
        model_ensemble: Optional[bool] = None,
        model_ensemble_num: Optional[int] = None,
        model_checkpoint: Optional[Union[int, str]] = None,
        model_compile: Optional[bool] = False,
        **kwargs,
    ) -> torch.nn.Module:
        """"""
        Return calculator model class object.

        Parameters
        ----------
        config: settings.Configuration
            Asparagus parameter settings.config class object
        model_calculator: (torch.nn.Module, list(torch.nn.Module)),
                optional, default None
            Model calculator object or list of model calculator objects.
        model_type: str, optional, default None
            Model calculator type to initialize, e.g. 'PhysNet'. The default
            model is defined in settings.default._default_calculator_model.
        model_directory: str, optional, default None
            Model directory that contains checkpoint and log files.
        model_ensemble: bool, optional, default None
            Expect a model calculator ensemble. If None, check config or
            assume as False.
        model_ensemble_num: int, optional, default None
            Number of model calculator in ensemble.
        model_checkpoint: int, optional, default 'best'
            If None or 'best', load best model checkpoint.
            Otherwise load latest checkpoint file with 'last' or define a
            checkpoint index number of the respective checkpoint file.
        model_compile: bool, optional, default False
            If True, the model calculator will get compiled at the first
            call to enhance the performance. Generally not applicable during
            training where multiple backwards calls are done (e.g. energy
            gradient and loss metrics gradient)

        Returns
        -------
        torch.nn.Module
            Asparagus calculator model object

        """"""

        ####################################
        # # # Prepare Model Calculator # # #
        ####################################

        # Assign model calculator
        model_calculator, checkpoint_state, checkpoint_file = (
            model.get_model_calculator(
                config=config,
                model_calculator=model_calculator,
                model_type=model_type,
                model_directory=model_directory,
                model_ensemble=model_ensemble,
                model_ensemble_num=model_ensemble_num,
                model_checkpoint=model_checkpoint,
                **kwargs)
            )

        # Load model checkpoint file
        model_calculator.load(
            checkpoint_state,
            checkpoint_file=checkpoint_file)

        # Compile model calculator if requested
        if model_compile:
            model_calculator.compile()

        return model_calculator

    def get_trainer(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        **kwargs,
    ) -> training.Trainer:
        """"""
        Initialize and return model calculator trainer.

        Parameters
        ----------
        config: (str, dict, object)
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to config json file (str)

        Returns:
        --------
        train.Trainer
            Model calculator trainer object

        """"""

        ###############################
        # # # Check Trainer Input # # #
        ###############################

        # Assign model parameter configuration library
        if config is None:
            config = settings.get_config(
                self.config, config_file, config_from=self)
        else:
            config = settings.get_config(
                config, config_file, config_from=self)

        # Check model parameter configuration and set default
        config_update = config.set(
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Update configuration dictionary
        config.update(config_update)

        #################################
        # # # Assign Reference Data # # #
        #################################

        if self.data_container is None:
            data_container = self.get_data_container(
                config=config,
                **kwargs)
        else:
            data_container = self.data_container

        ###################################
        # # # Assign Model Calculator # # #
        ###################################

        if self.model_calculator is None:
            model_calculator = self.get_model_calculator(
                config=config,
                **kwargs)
        else:
            model_calculator = self.model_calculator

        ###########################################
        # # # Assign Model Calculator Trainer # # #
        ###########################################

        # Assign model calculator trainer
        trainer = self._get_trainer(
            config=config,
            data_container=data_container,
            model_calculator=model_calculator,
            **kwargs)

        return trainer

    def _get_trainer(
        self,
        config: settings.Configuration,
        data_container: Optional[data.DataContainer] = None,
        model_calculator:
            Optional[Union[model.BaseModel, model.EnsembleModel]] = None,
        **kwargs,
    ) -> training.Trainer:
        """"""
        Initialize and return model calculator trainer.

        Parameters
        ----------
        config: settings.Configuration
            Asparagus parameter settings.config class object
        data_container: data.DataContainer, optional, default None
            Reference data container object providing training, validation and
            test data for the model training.
        model_calculator: torch.nn.Module, optional, default None
            Model or ensemble model calculator for property predictions.

        Returns:
        --------
        train.Trainer
            Model calculator trainer object

        """"""

        ###########################################
        # # # Assign Model Calculator Trainer # # #
        ###########################################

        # Check for single model calculator or model ensemble calculator
        if model_calculator is None:
            if (
                kwargs.get('model_ensemble') is None
                and config.get('model_ensemble') is None
            ):
                model_ensemble = False
            elif kwargs.get('model_ensemble') is None:
                model_ensemble = config.get('model_ensemble')
            else:
                model_ensemble = kwargs.get('model_ensemble')
        else:
            if hasattr(model_calculator, 'model_ensemble'):
                model_ensemble = model_calculator.model_ensemble
            else:
                model_ensemble = False

        # Initialize single model or model ensemble trainer
        if model_ensemble:
            trainer = training.EnsembleTrainer(
                config=config,
                data_container=data_container,
                model_calculator=model_calculator,
                **kwargs)
        else:
            trainer = training.Trainer(
                config=config,
                data_container=data_container,
                model_calculator=model_calculator,
                **kwargs)

        return trainer

    def train(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        **kwargs,
    ):
        """"""
        Quick command to initialize and start model calculator training.

        Parameters
        ----------
        config: (str, dict, object)
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to config json file (str)

        """"""

        ###########################################
        # # # Assign Model Calculator Trainer # # #
        ###########################################

        trainer = self.get_trainer(
            config=config,
            config_file=config_file,
            **kwargs)

        ########################################
        # # # Run Model Calculator Trainer # # #
        ########################################

        trainer.run(**kwargs)

        return

    def get_tester(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        **kwargs,
    ) -> training.Tester:
        """"""
        Initialize and return model calculator tester.

        Parameters
        ----------
        config: (str, dict, object)
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to config json file (str)

        Returns:
        --------
        train.Tester
            Model calculator tester object

        """"""

        ##############################
        # # # Check Tester Input # # #
        ##############################

        # Assign model parameter configuration library
        if config is None:
            config = settings.get_config(
                self.config, config_file, config_from=self)
        else:
            config = settings.get_config(
                config, config_file, config_from=self)

        # Check model parameter configuration and set default
        config_update = config.set(
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))

        # Update configuration dictionary
        config.update(config_update)

        #################################
        # # # Assign Reference Data # # #
        #################################

        if self.data_container is None:
            data_container = self.get_data_container(
                config=config,
                **kwargs)
        else:
            data_container = self.data_container

        ##########################################
        # # # Assign Model Calculator Tester # # #
        ##########################################

        # Assign model calculator trainer
        tester = self._get_tester(
            config,
            data_container=data_container,
            **kwargs)

        return tester

    def _get_tester(
        self,
        config: settings.Configuration,
        data_container: Optional[data.DataContainer] = None,
        **kwargs,
    ) -> training.Tester:
        """"""
        Initialize and return model calculator tester.

        Parameters
        ----------
        config: settings.Configuration
            Asparagus parameter settings.config class object
        data_container: data.DataContainer, optional, default None
            Reference data container object providing training, validation and
            test data for the model training.


        Returns:
        --------
        train.Tester
            Model calculator tester object

        """"""

        ###########################################
        # # # Assign Model Calculator Trainer # # #
        ###########################################

        tester = training.Tester(
            config=config,
            data_container=data_container,
            **kwargs)

        return tester

    def test(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        **kwargs,
    ):
        """"""
        Quick command to initialize and start model calculator training.

        Parameters
        ----------
        config: (str, dict, object)
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to config json file (str)

        """"""

        ###################################
        # # # Assign Model Calculator # # #
        ###################################

        if self.model_calculator is None:
            model_calculator = self.get_model_calculator(
                config=config,
                config_file=config_file,
                **kwargs)
        else:
            model_calculator = self.model_calculator

        ##########################################
        # # # Assign Model Calculator Tester # # #
        ##########################################

        tester = self.get_tester(
            config=config,
            config_file=config_file,
            **kwargs)

        #######################################
        # # # Run Model Calculator Tester # # #
        #######################################

        tester.test(
            model_calculator,
            **kwargs)

        return

    def get_ase_calculator(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        model_checkpoint: Optional[Union[int, str]] = 'best',
        **kwargs,
    ) -> 'ase.Calculator':
        """"""
        Return ASE calculator class object of the model calculator

        Parameter
        ---------
        config: (str, dict, object), optional, default 'self.config'
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to json file (str)
        model_checkpoint: (int, str), optional, default 'best'
            If None or 'best', load best model checkpoint.
            Otherwise load latest checkpoint file with 'last' or define a
            checkpoint index number of the respective checkpoint file.

        Returns
        -------
        ase.Calculator
            ASE calculator instance of the model calculator

        """"""

        ###################################
        # # # Assign Model Calculator # # #
        ###################################

        model_calculator = self.get_model_calculator(
            config=config,
            config_file=config_file,
            model_checkpoint=model_checkpoint,
            **kwargs)

        ##################################
        # # # Prepare ASE Calculator # # #
        ##################################

        ase_calculator = interface.ASE_Calculator(
            model_calculator,
            **kwargs)

        return ase_calculator

    def get_pycharmm_calculator(
        self,
        config: Optional[
            Union[str, Dict[str, Any], settings.Configuration]] = None,
        config_file: Optional[str] = None,
        model_checkpoint: Optional[int] = None,
        **kwargs
    ) -> Callable:
        """"""
        Return PyCHARMM calculator class object of the initialized model
        calculator.

        Parameters
        ----------
        config: (str, dict, object), optional, default 'self.config'
            Either the path to json file (str), dictionary (dict) or
            settings.config class object of model parameters
        config_file: str, optional, default see settings.default['config_file']
            Path to json file (str)
        model_checkpoint: int, optional, default None
            If None, load best model checkpoint. Otherwise define a checkpoint
            index number of the respective checkpoint file.

        Returns
        -------
        callable object
            PyCHARMM calculator object
        """"""

        ###################################
        # # # Assign Model Calculator # # #
        ###################################

        model_calculator = self.get_model_calculator(
            config=config,
            config_file=config_file,
            model_checkpoint=model_checkpoint,
            **kwargs)

        #######################################
        # # # Prepare PyCHARMM Calculator # # #
        #######################################

        pycharmm_calculator = interface.PyCharmm_Calculator(
            model_calculator,
            **kwargs)

        return pycharmm_calculator

    @property
    def config(self):
        return self.config_file",./Asparagus/asparagus/asparagus.py
DataContainer,"class DataContainer():
    """"""
    DataContainer object that manage the distribution of the reference
    data from one or multiple databases into a DataSet object and provide
    DataSubSets for training, validation and test sets.

    Parameters
    ----------
    config: (str, dict, settings.Configuration), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of model parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    data_file: (str, tuple(str)), optional, default ('data.db', 'db.sql')
        Either a single string of the reference Asparagus database file name
        or a tuple of the filename first and the file format label second.
    data_source: (str, list(str)), optional, default None
        List (or string) of paths to reference data files. Each entry can be
        either a string for the file path or a tuple with the filename first
        and the file format label second.
    data_alt_property_labels: dict, optional, default
            'settings._alt_property_labels'
        Dictionary of alternative property labeling to replace
        non-valid property labels with the valid one if possible.
    data_properties: list(str), optional,
            default ['energy', 'forces', 'dipole']
        Set of properties to store in the DataSet
    data_unit_properties: dictionary, optional,
            default {'energy': 'eV', 'forces': 'eV/Ang', 'dipole': 'e*Ang'}
        Dictionary from properties (keys) to corresponding unit as a
        string (item), e.g.:
            {property: unit}: { 'energy', 'eV',
                                'forces', 'eV/Ang', ...}
    data_source_unit_properties: dictionary, optional, default None
        Dictionary from properties (keys) to corresponding unit as a
        string (item) in the source data files.
        If None, the property units as defined in 'data_unit_properties'
        are assumed.
     ^   This input is only regarded for data source format, where no property
        units are defined such as the Numpy npz files.
    #data_source_property_filter: dictionary, optional, default None
        #Property conditions to avoid/filter out data from data source which
        #fulfill the conditions, e.g., an 'energy' (key) larger than a threshold
        #value (item).
    data_keep_scaling: bool, optional, default False
        If True, keep or take over the property and atomic energies scaling
        parameter either from the database file 'data_file' or, if not defined
        in 'data_file', from the next available scaling parameter dictionary
        of the source database files 'data_source' in order of the list order.
    data_num_train: (int, float), optional, default 0.8 (80% of data)
        Number of training data points [absolute (>1) or relative
        (<= 1.0)].
    data_num_valid: (int, float), optional, default 0.1 (10% of data)
        Number of validation data points [absolute (>1) or relative
        (<= 1.0)].
    data_num_test: (int, float), optional, default None
        Number of test data points [absolute (>1) or relative (< 1.0)].
        If None, remaining data in the database are used.
    data_seed: (int, float), optional, default: np.random.randint(1E6)
        Define seed for random data splitting.
    data_train_batch_size: int, optional, default 128
        Training batch size
    data_valid_batch_size: int, optional, default 128
        Validation batch size
    data_test_batch_size:  int, optional, default 128
        Test batch size
    data_num_workers: int, optional, default 1
        Number of data loader workers
    data_overwrite: bool, optional, default False
        Overwrite database files with reference data from
        'data_source' if available.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for data modules
    _default_args = {
        'data_file':                    ('data.db', 'sql.db'),
        'data_source':                  None,
        'data_properties':              ['energy', 'forces', 'dipole'],
        'data_unit_properties':         {'energy': 'eV',
                                        'forces': 'eV/Ang',
                                        'dipole': 'e*Ang'},
        'data_source_unit_properties':  None,
        #'data_source_property_filter':  None,
        'data_keep_scaling':            False,
        'data_alt_property_labels':     {},
        'data_num_train':               0.8,
        'data_num_valid':               0.1,
        'data_num_test':                None,
        'data_seed':                    np.random.randint(1E6),
        'data_overwrite':               False,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'data_file':                    [
            utils.is_string, utils.is_string_array_inhomogeneous],
        'data_source':                  [
            utils.is_string, utils.is_string_array_inhomogeneous,
            utils.is_None],
        'data_properties':              [utils.is_array_like],
        'data_unit_properties':         [utils.is_dictionary],
        'data_source_unit_properties':  [utils.is_dictionary, utils.is_None],
        #'data_source_property_filter':  [utils.is_dictionary, utils.is_None],
        'data_keep_scaling':            [utils.is_bool],
        'data_alt_property_labels':     [utils.is_dictionary],
        'data_num_train':               [utils.is_numeric],
        'data_num_valid':               [utils.is_numeric],
        'data_num_test':                [utils.is_numeric, utils.is_None],
        'data_seed':                    [utils.is_numeric],
        'data_overwrite':               [utils.is_bool],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None,
        data_file: Optional[Union[str, Tuple[str, str]]] = None,
        data_source: Optional[Union[str, List[str]]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_source_unit_properties: Optional[Dict[str, str]] = None,
        data_keep_scaling: Optional[bool] = None,
        data_num_train: Optional[Union[int, float]] = None,
        data_num_valid: Optional[Union[int, float]] = None,
        data_num_test: Optional[Union[int, float]] = None,
        data_seed: Optional[int] = None,
        data_train_batch_size: Optional[int] = None,
        data_valid_batch_size: Optional[int] = None,
        data_test_batch_size: Optional[int] = None,
        data_num_workers: Optional[int] = None,
        data_overwrite: Optional[bool] = None,
        device: Optional[str] = None,
        dtype: Optional[object] = None,
        **kwargs,
    ):

        super().__init__()

        #####################################
        # # # Check DataContainer Input # # #
        #####################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self, **kwargs)

        # Get database reference file path
        if data_file is None:
            data_file = config.get('data_file')

        # Check 'data_file' input for file format information
        data_file = self.check_data_files(data_file)
            
        # If not to overwrite, get metadata from existing database
        if data_overwrite or config.get('data_overwrite') or data_file is None:

            metadata = {}

        else:

            # Get, eventually the metadata dictionary from the data file
            metadata = data.get_metadata(data_file)

            # Check input with existing database properties
            data_properties, data_unit_properties = (
                self.get_properties_from_metadata(
                    metadata,
                    config,
                    data_properties,
                    data_unit_properties,
                    )
                )

        # Check data source file and format input
        if data_source is None:
            data_source = config.get('data_source')
        data_source = self.check_data_files(data_source, is_source=True)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            argsskip=['metadata'],
            check_default=utils.get_default_args(self, data),
            check_dtype=utils.get_dtype_args(self, data)
        )

        # Update global configuration dictionary
        config.update(config_update, config_from=self)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        ######################################
        # # # Check Data Parameter Input # # #
        ######################################

        # Check data file format again, if updated
        if self.data_file != data_file:
            self.data_file = self.check_data_files(self.data_file)

        # Check and prepare data property input
        data_properties, data_unit_properties, data_alt_property_labels = (
            self.check_data_properties(
                self.data_properties,
                self.data_unit_properties,
                self.data_alt_property_labels,
                )
            )

        # Reassign data properties
        self.data_properties = data_properties
        self.data_unit_properties = data_unit_properties
        self.data_alt_property_labels = data_alt_property_labels

        # Check and reassign source property unit input
        if self.data_source_unit_properties is not None:
            _, data_source_unit_properties, _ = (
            self.check_data_properties(
                self.data_properties,
                self.data_source_unit_properties,
                self.data_alt_property_labels,
                )
            )
            self.data_source_unit_properties = data_source_unit_properties

        #########################
        # # # DataSet Setup # # #
        #########################

        # Initialize reference data set
        self.dataset = data.DataSet(
            self.data_file,
            data_properties=self.data_properties,
            data_unit_properties=self.data_unit_properties,
            data_alt_property_labels=self.data_alt_property_labels,
            data_overwrite=self.data_overwrite,
            **kwargs)

        # Reset dataset overwrite flag
        self.data_overwrite = False
        config['data_overwrite'] = False

        # Load source data
        self.data_source = self.load_data_source(
            self.data_source,
            **kwargs)

        # Split the dataset into data subsets
        self.split_dataset(
            data_num_train=self.data_num_train,
            data_num_valid=self.data_num_valid,
            data_num_test=self.data_num_test,
            data_seed=self.data_seed,
            )

        # Update global configuration dictionary
        config.update(
            {
                'data_file': self.data_file,
                'data_properties': self.data_properties,
                'data_unit_properties': self.data_unit_properties,
                'data_source': self.data_source,
                'data_source_unit_properties': self.data_source_unit_properties
            },
            config_from=self
        )

        return

    def __str__(self):
        """"""
        Return class descriptor
        """"""
        if hasattr(self, 'data_file'):
            return (
                f""DataContainer '{self.data_file[0]:s}' ""
                + f"" ({self.data_file[1]:s})"")
        else:
            return ""DataContainer""

    def __len__(
        self,
    ) -> int:
        """"""
        Size of the complete data set
        """"""
        return len(self.dataset)

    def __iter__(
        self
    ):
        return self.dataset.__iter__()

    def __next__(
        self
    ):
        return self.dataset.__next__()

    def __getitem__(
        self,
        idx: int,
    ) -> Dict:
        """"""
        Get DataSet entry idx
        """"""
        return self.dataset.get(idx)

    def get(
        self,
        idx: int,
    ) -> Dict:
        """"""
        Get DataSet entry idx
        """"""
        return self.dataset.get(idx)

    def check_data_files(
        self,
        files: Union[str, Tuple[str, str], List[str], List[Tuple[str, str]]],
        is_source: Optional[bool] = False,
    ):
        """"""
        Check files input for file format information.

        Parameters
        ----------
        files: (str, tuple(str), list(str), list(tuple(str)))
            Single or list of data files and, eventually, file formats
        is_source: bool, optional, default False
            If False, files should be a single data file path with format.
            Else, a list of data source files are expected.

        Returns
        -------
        tuple(str) or list(tuple(str))
            File name and format informations either as tuple of on file
            (is_source=False) or a list of tuples (is_source=True).

        """"""

        # Check files input
        if files is None:
            return None

        # Initialize (file, format) list
        files_formats = []

        # Initialize files input list
        files_input = []

        # Files input is string
        if utils.is_string(files):

            # Check file existense if source
            if is_source and not os.path.isfile(files):
                self.logger.warning(
                    f""Source data file name ('{files:s}') does not exist!"")

            # Get file format
            file_format = data.check_data_format(
                files, is_source_format=is_source)
            files_formats.append([files, file_format])
            files_input.append(f"" <- {files:s}"")

        # Files is string list
        elif utils.is_string_array(files, inhomogeneity=True):

            # If data file, only file path and format is expected
            if not is_source:

                # Check assigned file format
                file_format = data.check_data_format(
                    files[0], is_source_format=is_source)
                format_format = data.check_data_format(
                    files[1], is_source_format=is_source)
                if file_format != format_format:
                    self.logger.warning(
                        f""Data file name ('{files[0]:s}') and format ""
                        + f""definition ('{files[1]:s}') does not match ""
                        + f""('{file_format:s}' != '{format_format:s}')!"")

                files_formats.append([files[0], format_format])
                files_input.append(f"" <- ({files[0]:s}, {files[1]:s})"")

            # If source files, multiple definitions can be expected
            else:

                # Iterate over files (or files and format)
                for file_i in files:

                    # Check for (file, format) pair
                    if utils.is_string_array(file_i):

                        # Check assigned file format
                        file_format = data.check_data_format(
                            file_i[0], is_source_format=is_source)
                        format_format = data.check_data_format(
                            file_i[1], is_source_format=is_source)
                        if file_format != format_format:
                            self.logger.warning(
                                f""Data file name ('{file_i[0]:s}') and format ""
                                + f""definition ('{file_i[1]:s}') does not ""
                                + f""match ('{file_format:s}' != ""
                                + f""'{format_format:s}')!"")

                        # Check for existense, which is expected
                        if not os.path.isfile(file_i[0]):
                            self.logger.warning(
                                f""Source data file name ('{file_i[0]:s}') ""
                                + ""does not exist!"")

                        files_formats.append([file_i[0], format_format])
                        files_input.append(
                            f"" <- ({file_i[0]:s}, {file_i[1]:s})"")

                    elif utils.is_string(file_i):

                        # Get file format
                        file_format = data.check_data_format(
                            file_i, is_source_format=is_source)

                        # Check for existense, if not it is most likely the
                        # file format definition of the former input.
                        if not os.path.isfile(file_i):

                            self.logger.warning(
                                f""Source data file name ('{file_i:s}') does ""
                                + "" not exist!"")

                        else:

                            files_formats.append([file_i, file_format])
                            files_input.append(f"" <- {file_i:s}"")

        # Prepare check info
        msg = """"
        for file_output, file_input in zip(files_formats, files_input):
            msg += (
                f"" ({file_output[0]:s}, {file_output[1]:s}) {file_input:s}\n"")

        # Return either tuple or list of tuples
        if is_source:
            self.logger.info(
                f""Data source files and formats detected:\n{msg:s}"")
            return files_formats
        else:
            self.logger.info(
                f""Data file and format detected:\n{msg:s}"")
            if len(files_formats) > 1:
                self.logger.warning(
                    ""Multiple files were defined as data files, but only one ""
                    + ""is supported!\n List of files and file formats: ""
                    + f"" {str(files_formats):s}\n""
                    + f"" Returned file and format {str(files_formats[0]):s}"")
            return files_formats[0]

    def get_properties_from_metadata(
        self,
        metadata: Dict[str, Any],
        config: settings.Configuration,
        data_properties,
        data_unit_properties,
    ) -> (List[str], Dict[str, str]):
        """"""
        Return property data from top priority source. Priority:
            1. Keyword argument input
            2. Config input
            3. Metadata properties

        Parameters
        ----------
        metadata: dict
            Database file metadata deictionary.
        config: setting.Configuration
            Configuaration object with parameters
        data_properties: list(str)
            Set of properties to store in the DataSet
        data_unit_properties: dictionary
            Dictionary from properties (keys) to corresponding unit as a
            string (item).

        Returns
        -------
        list
            Updated property labels
        dict
            Updated property units

        """"""
        
        # Get properties
        if data_properties is None and config.get('data_properties') is None:
            properties = metadata.get('load_properties')
        elif data_properties is None:
            properties = config.get('data_properties')
        else:
            properties = data_properties

        # Get property units
        if (
            data_unit_properties is None 
            and config.get('data_unit_properties') is None
        ):
            unit_properties = metadata.get('unit_properties')
        elif data_unit_properties is None:
            unit_properties = config.get('data_unit_properties')
        else:
            unit_properties = data_unit_properties

        return properties, unit_properties

    def get_from_metadata(
        self,
        metadata: Dict[str, Any],
        config: settings.Configuration,
        **kwargs,
    ) -> List[Any]:
        """"""
        Return input in kwargs from top priority source. Priority:
            1. Keyword argument input
            2. Config input
            3. Metadata properties

        Parameters
        ----------
        metadata: dict
            Database file metadata deictionary.
        config: setting.Configuration
            Configuaration object with parameters
        kwargs: dict
            Parameter dictionary to check

        Returns
        -------
        list
            Updated parameter list in the order of kwargs

        """"""

        # Initialize top priority property list
        properties = []

        # Iterate over properties
        for key, item in kwargs.items():

            # If not defined by input or in config, use property from metadata
            if item is None and config.get(key) is None:
                properties.append(metadata.get(key))
            # Else if defined by input, use property from config
            elif item is None:
                properties.append(config.get(key))
            # Else take input property
            else:
                properties.append(item)

        return properties

    def check_data_properties(
        self,
        data_properties: Union[str, List[str]],
        data_unit_properties: Dict[str, str],
        data_alt_property_labels: Dict[str, List[str]],
    ) -> (List[str], Dict[str, str], Dict[str, List[str]]):
        """"""
        Check data property input.

        Parameters
        ----------
        data_properties: (str, list(str))
            Set of properties to store in the DataSet
        data_unit_properties: dict
            Dictionary from properties (keys) to corresponding unit as a
            string (item)
        data_alt_property_labels: dict
            Alternative property labels to detect common mismatches.

        Returns
        -------
        list
            Updated parameter list in the order of kwargs

        """"""

        # Combine alternative property label input 'data_alt_property_labels'
        # with default setting, check for repetitions.
        data_alt_property_labels = utils.merge_dictionary_lists(
            data_alt_property_labels, settings._alt_property_labels)

        # Check for unknown property labels in data_properties
        # and replace if possible with internally used property label in
        # *data_alt_property_labels'
        if utils.is_string(data_properties):
            data_properties = [data_properties]
        for ip, data_prop in enumerate(data_properties):
            # Check property label for prefix
            if 'std_' in data_prop:
                prop = data_prop[4:]
            else:
                prop = data_prop
            match, modified, new_prop = utils.check_property_label(
                prop,
                valid_property_labels=settings._valid_properties,
                alt_property_labels=data_alt_property_labels)
            if match and modified:
                if 'std_' in data_prop:
                    new_prop = f""std_{new_prop:s}""
                self.logger.warning(
                    f""Property key '{data_prop}' in ""
                    + ""'data_properties' is not a valid label!\nProperty key ""
                    + f""'{data_prop}' is replaced by '{new_prop}'."")
                data_properties[ip] = new_prop
            elif not match:
                raise ValueError(
                    f""Unknown property ('{data_prop}') in 'data_properties'!"")

        # Check for unknown property labels in 'data_unit_properties'
        # and replace if possible with internally used property label in
        # 'data_alt_property_labels'
        for data_prop in data_unit_properties:
            # Check property label for prefix
            if 'std_' in data_prop:
                prop = data_prop[4:]
            else:
                prop = data_prop
            match, modified, new_prop = utils.check_property_label(
                prop,
                valid_property_labels=settings._valid_properties,
                alt_property_labels=data_alt_property_labels)
            if match and modified:
                if 'std_' in data_prop:
                    new_prop = f""std_{new_prop:s}""
                self.logger.warning(
                    f""Property key '{data_prop}' in ""
                    + ""'data_unit_properties' is not a valid label!\nProperty ""
                    + f""key '{data_prop}' is replaced by '{new_prop}'."")
                data_unit_properties[new_prop] = (
                    data_unit_properties.pop(data_prop))
            elif not match:
                raise ValueError(
                    f""Unknown property ('{data_prop}') in ""
                    + ""'data_unit_properties'!"")

        # Initialize checked property units dictionary
        checked_data_unit_properties = {}

        # Check if positions and charge units are defined in
        # 'data_unit_properties'.
        for prop in ['positions', 'charge']:
            if prop not in data_unit_properties:
                checked_data_unit_properties[prop] = (
                    settings._default_units[prop])
            else:
                checked_data_unit_properties[prop] = (
                    data_unit_properties[prop])

        # Check if all units from 'data_properties' are defined in
        # 'data_unit_properties', if not assign default units.
        for data_prop in data_properties:
            if data_prop not in data_unit_properties:
                # Check property label for prefix
                if 'std_' in data_prop:
                    prop = data_prop[4:]
                else:
                    prop = data_prop
                self.logger.warning(
                    f""No unit defined for property '{data_prop}'!\n""
                    + f""Default unit of '{settings._default_units[prop]}' ""
                    + ""will be used."")
                checked_data_unit_properties[data_prop] = (
                    settings._default_units[prop])
            else:
                checked_data_unit_properties[data_prop] = (
                    data_unit_properties[data_prop])

        return (
            data_properties, checked_data_unit_properties,
            data_alt_property_labels)

    def load_data_source(
        self,
        data_source: Union[str, List[str]],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
        data_source_unit_properties: Optional[Dict[str, str]] = None,
        data_keep_scaling: Optional[bool] = None,
        **kwargs,
    ) -> List[str]:
        """"""
        Load source data to reference DataSet.

        Parameters
        ----------
        data_source: (str, list(str))
            List (or string) of paths to reference data files. Each entry can
            be either a string for the file path or a tuple with the filename
            first and the file format label second.
        data_properties: list(str), optional, default None
            Set of properties to store in the DataSet
        data_unit_properties: dictionary, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item).
        data_alt_property_labels: dict
            Alternative property labels to detect common mismatches.
        data_source_unit_properties: dictionary, optional, default None
            Dictionary from properties (keys) to corresponding unit as a 
            string (item) in the source data files.
        data_keep_scaling: bool, optional, default None
            If True, keep or take over the property and atomic energies scaling
            parameter either from the database file 'data_file' or, if not
            defined in 'data_file', from the  source database file.

        Returns
        -------
        list(tuple(str))
            File name and format informations as a list of tuples.

        """"""

        # Check data source input
        if data_source is None:
            data_source = []
        data_source = self.check_data_files(data_source, is_source=True)

        # Check property input
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels
        if data_source_unit_properties is None:
            data_source_unit_properties = self.data_source_unit_properties
        if data_keep_scaling is None:
            data_keep_scaling = self.data_keep_scaling

        # Load reference data set(s) from defined source data path(s)
        for source in data_source:
            self.dataset.load_data(
                source,
                data_properties=data_properties,
                data_unit_properties=data_unit_properties,
                data_alt_property_labels=data_alt_property_labels,
                data_source_unit_properties=data_source_unit_properties,
                data_keep_scaling=data_keep_scaling,
            )

        # Return data source information from metadata
        metadata = self.dataset.get_metadata()

        return metadata.get('data_source')

    def split_dataset(
        self,
        data_num_train: Optional[Union[int, float]] = None,
        data_num_valid: Optional[Union[int, float]] = None,
        data_num_test: Optional[Union[int, float]] = None,
        data_seed: Optional[int] = None,
    ):
        """"""
        Split dataset into data subsets
        
        Parameters
        ----------
        data_num_train: (int, float), optional, default 0.8 (80% of data)
            Number of training data points [absolute (>1) or relative
            (<= 1.0)].
        data_num_valid: (int, float), optional, default 0.1 (10% of data)
            Number of validation data points [absolute (>1) or relative
            (<= 1.0)].
        data_num_test: (int, float), optional, default None
            Number of test data points [absolute (>1) or relative (< 1.0)].
            If None, remaining data in the database are used.
        data_seed: (int, float), optional, default: np.random.randint(1E6)
            Define seed for random data splitting.

        """"""

        # Prepare data split into training, validation and test set
        data_num_all = len(self.dataset)

        # Stop further setup if no data are available
        if not data_num_all:
            msg = f""No data are available in '{self.data_file[0]:s}!""
            self.logger.error(msg)
            raise SyntaxError(msg)

        # Check data split parameter
        if data_num_train is None:
            data_num_train = self.data_num_train
        if data_num_valid is None:
            data_num_valid = self.data_num_valid
        if data_num_test is None:
            data_num_test = self.data_num_test
        if data_seed is None:
            data_seed = self.data_seed

        # Prepare split parameters
        num_train, num_valid, num_test = self.prepare_split_parameter(
            data_num_all,
            data_num_train,
            data_num_valid,
            data_num_test)

        # Select training, validation and test data indices randomly
        np.random.seed(data_seed)
        idx_data = np.random.permutation(np.arange(data_num_all))
        idx_train = idx_data[:num_train]
        idx_valid = idx_data[num_train:(num_train + num_valid)]
        idx_test = idx_data[
            (num_train + num_valid):(num_train + num_valid + num_test)]

        # Initialize training, validation and test subset
        self.train_dataset = data.DataSubSet(
            self.data_file,
            'test',
            idx_train)
        self.valid_dataset = data.DataSubSet(
            self.data_file,
            'valid',
            idx_valid)
        self.test_dataset = data.DataSubSet(
            self.data_file,
            'test',
            idx_test)

        # Prepare dataset and subset label to objects dictionary
        self.all_datasets = {
            'all': self.dataset,
            'train': self.train_dataset,
            'training': self.train_dataset,
            'valid': self.valid_dataset,
            'validation': self.valid_dataset,
            'test': self.test_dataset,
            'testing': self.test_dataset,
            }

        # Prepare header
        message = (
            ""Dataset and subset split information of database ""
            + f""'{self.data_file[0]:s}'!\n""
            + f"" {'Dataset':<17s} |""
            + f"" {'Abs. Number':<14s} |""
            + f"" {'Rel. Number':<14s}\n""
            + ""-""*(20 + 17*2)
            + ""\n"")

        for label in ['All', 'Training', 'Validation', 'Test']:
            
            # Get dataset and subset sizes
            num_abs = len(self.all_datasets[label.lower()])
            num_rel = float(num_abs)/float(data_num_all)

            # Prepare information
            message += (
                f"" {label + ' Data':<17s} |""
                + f"" {num_abs:>14d} |""
                + f"" {num_rel*100:>13.1f}%\n"")

        # Print information
        self.logger.info(message)

        return

    def prepare_split_parameter(
        self,
        data_num_all: int,
        data_num_train: Union[int, float],
        data_num_valid: Union[int, float],
        data_num_test: Union[int, float],
    ) -> (int, int, int):
        """"""
        Split dataset into data subsets
        
        Parameters
        ----------
        data_num_all: int
            Total number of data.
        data_num_train: (int, float)
            Number of training data points [absolute (>1) or relative
            (<= 1.0)].
        data_num_valid: (int, float)
            Number of validation data points [absolute (>1) or relative
            (<= 1.0)].
        data_num_test: (int, float)
            Number of test data points [absolute (>1) or relative (< 1.0)].
            If None, remaining data in the database are used.

        Returns
        -------
        int
            Number of training data
        int
            Number of validation data
        int
            Number of test data

        """"""

        # Training set size
        if data_num_train < 0.0:
            msg = (
                ""Number of training set samples 'data_num_train'""
                + f""({data_num_train}) is lower then zero and invalid!"")
            self.logger.error(msg)
            raise ValueError(msg)
        elif data_num_train <= 1.0:
            rel_train = float(self.data_num_train)
            data_num_train = int(data_num_all*rel_train)
            rel_train = float(data_num_train)/float(data_num_all)
        elif data_num_train <= data_num_all:
            rel_train = float(data_num_train)/float(data_num_all)
        else:
            msg = (
                ""Number of training set samples 'data_num_train' ""
                + f""({data_num_train}) is larger than the total number ""
                + f""of data samples ({data_num_all})!""
            )
            self.logger.error(msg)
            raise ValueError(msg)

        # Validation set size
        if data_num_valid < 0.0:
            raise ValueError(
                ""Number of validation set samples 'data_num_valid' "" +
                f""({data_num_valid}) is lower then zero and invalid!\n"")
        elif data_num_valid < 1.0:
            rel_valid = float(data_num_valid)
            if (rel_train + rel_valid) > 1.0:
                new_rel_valid = 1.0 - float(data_num_train)/float(data_num_all)
                self.logger.warning(
                    f""Ratio of training set ({rel_train})"" +
                    f""and validation set samples ({rel_valid}) "" +
                    ""are larger 1.0!\n"" +
                    ""Ratio of validation set samples is set to "" +
                    f""{new_rel_valid}."")
                rel_valid = new_rel_valid
            data_num_valid = int(round(data_num_all*rel_valid))
            rel_valid = float(data_num_valid)/float(data_num_all)
        elif data_num_valid <= (data_num_all - data_num_train):
            rel_valid = float(data_num_valid)/float(data_num_all)
        else:
            new_data_num_valid = int(data_num_all - data_num_train)
            self.logger.warning(
                f""Number of training set ({data_num_train})"" +
                ""and validation set samples "" +
                f""({data_num_valid}) are larger then number of "" +
                f""data samples ({data_num_all})!\n"" +
                ""Number of validation set samples is set to "" +
                f""{new_data_num_valid}"")
            data_num_valid = new_data_num_valid
            rel_valid = float(data_num_valid)/float(data_num_all)

        # Test set size
        if data_num_test is None:
            data_num_test = (
                data_num_all - data_num_train - data_num_valid)
            rel_test = float(data_num_test)/float(data_num_all)
        elif data_num_test < 0.0:
            raise ValueError(
                ""Number of test set samples 'data_num_test' "" +
                f""({data_num_test}) is lower then zero and invalid!\n"")
        elif data_num_test < 1.0:
            rel_test = float(data_num_test)
            if (rel_test + rel_train + rel_valid) > 1.0:
                new_rel_test = (
                    1.0 - float(data_num_train + data_num_valid)
                    / float(data_num_all))
                self.logger.warning(
                    f""Ratio of test set ({rel_test})"" +
                    ""with training and validation set samples "" +
                    f""({rel_train}, {rel_valid}) "" +
                    ""are larger 1.0!\n"" +
                    ""Ratio of test set samples is set to "" +
                    f""{new_rel_test}."")
                rel_test = new_rel_test
            data_num_test = int(round(data_num_all*rel_test))
        elif data_num_test <= (data_num_all - data_num_train - data_num_valid):
            rel_test = float(data_num_test)/float(data_num_all)
        else:
            new_data_num_test = int(
                data_num_all - data_num_train - data_num_valid)
            self.logger.warning(
                f""Number of training ({data_num_train}), "" +
                f""validation set ({data_num_valid}) and "" +
                f""test set samples ({data_num_test}) are larger "" +
                f""then number of data samples ({data_num_all})!\n"" +
                ""Number of test set samples is set to "" +
                f""{new_data_num_test}."")
            data_num_test = new_data_num_test
            rel_test = float(data_num_test)/float(data_num_all)

        return data_num_train, data_num_valid, data_num_test

    def init_dataloader(
        self,
        train_batch_size: int,
        valid_batch_size: int,
        test_batch_size: int,
        num_workers: Optional[int] = 1,
        apply_atomic_energies_shift: Optional[bool] = True,
        atomic_energies_shift_list: Optional[List[float]] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
    ):
        """"""
        Initialize the data subset loader
        
        Parameters:
        -----------
        train_batch_size: int
            Training dataloader batch size
        valid_batch_size: int
            Validation dataloader batch size
        test_batch_size: int
            Test dataloader batch size
        num_workers: int, optional, default 1
            Number of data loader workers
        apply_atomic_energies_shift: bool, optional, default True
            Whether to apply atomic energies shift to reference energy provided
            by the data loader
        atomic_energies_shift_list: list(float), optional, default None
            Atom type specific energy shift terms to shift the system energies.
        device: str, optional, default global setting
            Device type for model variable allocation
        dtype: dtype object, optional, default global setting
            Model variables data type

        """"""

        # Check atomic energies shift list
        metadata = self.get_metadata()
        if apply_atomic_energies_shift and atomic_energies_shift_list is None:
            atomic_energies_shift_list = self.get_atomic_energies_shift()
        elif not apply_atomic_energies_shift:
            atomic_energies_shift_list = None

        # Check module variable parameters from configuration
        if device is None:
            device = self.device
        if dtype is None:
            dtype = self.dtype

        # Prepare training, validation and test data loader
        self.train_dataloader = data.DataLoader(
            self.train_dataset,
            train_batch_size,
            True,
            num_workers,
            device,
            dtype,
            data_atomic_energies_shift=atomic_energies_shift_list)
        self.valid_dataloader = data.DataLoader(
            self.valid_dataset,
            valid_batch_size,
            False,
            num_workers,
            device,
            dtype,
            data_atomic_energies_shift=atomic_energies_shift_list)
        self.test_dataloader = data.DataLoader(
            self.test_dataset,
            test_batch_size,
            False,
            num_workers,
            device,
            dtype,
            data_atomic_energies_shift=atomic_energies_shift_list)

        # Prepare dataset and subset label to objects dictionary
        self.all_dataloader = {
            'train': self.train_dataloader,
            'training': self.train_dataloader,
            'valid': self.valid_dataloader,
            'validation': self.valid_dataloader,
            'test': self.test_dataloader,
            'testing': self.test_dataloader}

        return

    def get_property_scaling(
        self,
        data_label: Optional[str] = 'all',
        overwrite: Optional[bool] = False,
        property_atom_scaled: Optional[Dict[str, str]] = None,
    ) -> Dict[str, List[float]]:
        """"""
        Compute property statistics with average and standard deviation.

        Parameters
        ----------
        data_label: str, optional, default 'train'
            Dataset or subset label using for computing property scaling
            statistics.
        overwrite: bool, optional, default False
            If property statistics already available and up-to-date, recompute
            them. The up-to-date flag will be reset to False if any database
            manipulation is done.
        property_atom_scaled: dict(str, str), optional, default None
            Property statistics (key) will be scaled by the number of atoms
            per system and stored with new property label (item).
            e.g. {'energy': 'atomic_energies'}

        Return
        ------
        dict(str, list(float))
            Property statistics dictionary

        """"""

        # Get current metadata dictionary
        metadata = self.dataset.get_metadata()

        # Check atom scaled properties dictionary
        if property_atom_scaled is None:
            property_atom_scaled = {}

        # Check for property scaling results or Initialize 
        # scaling and shift parameter dictionary
        if (
            metadata.get('data_property_scaling_uptodate') is not None
            and metadata['data_property_scaling_uptodate']
            and metadata.get('data_property_scaling_label') is not None
            and metadata['data_property_scaling_label'] == data_label
            and not overwrite
        ):
            property_scaling = metadata.get('data_property_scaling')
        else:
            property_scaling = {}

        # Compute property statistics
        scaling_result = data.compute_property_scaling(
            self.get_dataset(data_label),
            metadata.get('load_properties'),
            property_scaling,
            property_atom_scaled)

        # Update property scaling dictionary
        for prop, result in scaling_result.items():
            property_scaling[prop] = result

        # Prepare header
        message = (
            f""Property statistics of '{data_label:s}' data ""
            + f""of the database '{self.data_file[0]:s}'!\n""
            + f"" {'Property Label':<17s} |""
            + f"" {'Average':<17s} |""
            + f"" {'Std. Deviation':<17s} |""
            + f"" {'Unit':<17s}\n""
            + ""-""*(20*4)
            + ""\n"")
        
        # Prepare property statistics information output
        for prop in metadata.get('load_properties'):
            
            # Get property unit
            if prop in self.data_unit_properties:
                unit = self.data_unit_properties[prop]
            else:
                unit = ""None""

            # Add property statistics
            message += (
                f"" {prop:<17s} |""
                + f"" {property_scaling[prop][0]:>17.3e} |""
                + f"" {property_scaling[prop][1]:>17.3e} |""
                + f"" {unit:<17s}\n"")
        
            if prop in property_atom_scaled:

                # Atom scaled property label
                atom_prop = property_atom_scaled[prop]

                # Add atom scaled property statistics
                message += (
                    f"" {atom_prop:<17s} |""
                    + f"" {property_scaling[atom_prop][0]:>17.3e} |""
                    + f"" {property_scaling[atom_prop][1]:>17.3e} |""
                    + f"" {unit:<17s}\n"")
        
        # Print property statistics information
        self.logger.info(message)

        # Update property scaling
        if metadata.get('data_property_scaling') is None:
            metadata['data_property_scaling'] = property_scaling
        else:
            metadata['data_property_scaling'].update(property_scaling)
        metadata['data_property_scaling_label'] = data_label
        metadata['data_property_scaling_uptodate'] = True
        self.dataset.set_metadata(metadata)

        return property_scaling

    def get_atomic_energies_scaling(
        self,
        data_label: Optional[str] = 'all',
        overwrite: Optional[bool] = False,
    ) -> Dict[int, List[float]]:
        """"""
        Compute property statistics with average and standard deviation.

        Parameters
        ----------
        data_label: str, optional, default 'all'
            Dataset or subset label using for computing property scaling
            statistics.
        overwrite: bool, optional, default False
            If property statistics already available and up-to-date, recompute
            them. The up-to-date flag will be reset to False if any database
            manipulation is done.

        Return
        ------
        dict(int, list(float))
            Atomic energies scaling dictionary

        """"""

        # Get current metadata dictionary
        metadata = self.dataset.get_metadata()

        # Check for atomic energies scaling results or compute atom energies
        # scaling factor and shift term
        if (
            metadata.get('data_atomic_energies_scaling_uptodate') is not None
            and metadata['data_atomic_energies_scaling_uptodate']
            and metadata.get('data_atomic_energies_scaling_label') is not None
            and metadata['data_atomic_energies_scaling_label'] == data_label
            and not overwrite
        ):

            # Load stored atomic energies scaling
            atomic_energies_scaling_str = metadata.get(
                'data_atomic_energies_scaling')

            # Convert string keys to integer keys
            atomic_energies_scaling = {}
            for key, item in atomic_energies_scaling_str.items():
                atomic_energies_scaling[int(key)] = item

            # Check for NaN values in stored values
            nan_error = any([
                np.any(np.isnan(values))
                for _, values in atomic_energies_scaling.items()]
            )

            # If NaN values are found, recompute atomic energies scaling 
            # parameter
            if nan_error:
                atomic_energies_scaling = self.get_atomic_energies_scaling(
                    data_label=data_label,
                    overwrite=True)

        else:

            # Get energy unit
            if 'energy' in self.data_unit_properties:
                energy_unit = self.data_unit_properties['energy']
            else:
                energy_unit = ""None""

            atomic_energies_scaling, computation_message = (
                data.compute_atomic_energies_scaling(
                    self.get_dataset(data_label),
                    energy_unit)
                )

            # Prepare atomic energies statistics information output
            message = (
                f""Atomic energies statistics of {data_label:s} data ""
                + f""of the database '{self.data_file[0]:s}'!\n"")
            message += computation_message
            message += (
                f"" {'Element':<17s} |""
                + f"" {'Energy Shift':<17s} |""
                + f"" {'Energy Scaling':<17s} |""
                + f"" {'Unit':<17s}\n""
                + ""-""*(20*4)
                + ""\n"")

            for atomic_number, scaling in (atomic_energies_scaling.items()):

                # Add atomic energies statistics
                message += (
                    f"" {utils.chemical_symbols[atomic_number]:<17s} |""
                    + f"" {scaling[0]:>17.3e} |""
                    + f"" {scaling[1]:>17.3e} |""
                    + f"" {energy_unit:<17s}\n"")

            # Print atomic energies statistics information
            self.logger.info(message)

            # Update atomic energies scaling
            if metadata.get('data_atomic_energies_scaling') is None:
                metadata['data_atomic_energies_scaling'] = (
                    atomic_energies_scaling)
            else:
                metadata['data_atomic_energies_scaling'].update(
                    atomic_energies_scaling)
            metadata['data_atomic_energies_scaling_label'] = data_label
            metadata['data_atomic_energies_scaling_uptodate'] = True
            self.dataset.set_metadata(metadata)

        return atomic_energies_scaling

    def get_atomic_energies_shift(
        self,
        data_label: Optional[str] = 'all',
    ) -> List[float]:
        """"""
        Compute reference atomic energies shift to center system energies
        around zero.
        
        Parameters
        ----------
        data_label: str, optional, default 'all'
            Reference dataset ('all') or subset (e.g. 'training', 'validation')
            used for the atomic energies shift computation.

        Returns
        -------
        dict(int, float)
            Reference data atomic energies shift list

        """"""
        
        # Get atomic energies scaling guess
        if 'energy' in self.data_properties:
            data_atomic_energies_scaling = (
                self.get_atomic_energies_scaling(data_label=data_label))
        else:
            data_atomic_energies_scaling = {}

        # Prepare atomic energies shift list
        max_atomic_number = max([
            int(atomic_number)
            for atomic_number in data_atomic_energies_scaling.keys()])
        atomic_energies_shift = np.zeros(max_atomic_number + 1, dtype=float)
        for atomic_number in range(max_atomic_number + 1):
            if atomic_number in data_atomic_energies_scaling:
                atomic_energies_shift[atomic_number] = (
                    data_atomic_energies_scaling[atomic_number][0])

        return atomic_energies_shift

    def get_data_properties_dtype(
        self,
    ) -> 'dtype':
        """"""
        Return database properties float dtype
        
        Returns:
        --------
        'dtype'
            Properties float dtype (most likely torch.float64)

        """"""
        with self.dataset.connect(self.data_file[0], mode='r') as db:
            return db.properties_torch_dtype

    def get_metadata(self) -> Dict[str, Any]:
        """"""
        Return metadata
        """"""
        return self.dataset.get_metadata()

    def get_datalabels(self) -> List[str]:
        """"""
        Return the list of all available data set labels which return
        DataSubSet or DataLoader objects with the respective function
        (get_dataset and get_dataloader).
        """"""
        if hasattr(self, 'all_datasets'):
            return ['train', 'valid', 'test']
        else:
            raise AttributeError(
                ""Dataset and subsets were not initialized yet."")

    def get_dataset(self, label: str) -> Callable:
        """"""
        Return as specific DataSubSet object ('train', 'valid' or 'test')
        """"""
        if hasattr(self, 'all_datasets'):
            return self.all_datasets.get(label)
        else:
            raise AttributeError(
                ""Dataset and subsets were not initialized yet."")

    def get_dataloader(self, label: str) -> Callable:
        """"""
        Return as specific DataLoader object ('train', 'valid' or 'test')
        """"""
        if hasattr(self, 'all_dataloader'):
            return self.all_dataloader.get(label)
        else:
            raise AttributeError(
                ""Dataloaders were not initialized yet."")

    def get_train(self, idx: int) -> Dict:
        """"""
        Get Training DataSubSet entry idx
        """"""
        return self.train_dataset.get(idx)

    def get_valid(self, idx: int) -> Dict:
        """"""
        Get Validation DataSubSet entry idx
        """"""
        return self.valid_dataset.get(idx)

    def get_test(self, idx: int) -> Dict:
        """"""
        Get Test DataSubSet entry idx
        """"""
        return self.test_dataset.get(idx)

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return data information
        """"""

        return {
            'data_file': self.data_file,
            'data_file_format': self.data_file_format,
            'data_properties': self.data_properties,
            'data_unit_properties': self.data_unit_properties,
            'data_num_train': self.data_num_train,
            'data_num_valid': self.data_num_valid,
            'data_num_test': self.data_num_test,
            'data_overwrite': self.data_overwrite,
            }",./Asparagus/asparagus/data/datacontainer.py
DataSet,"class DataSet():
    """"""
    DataSet class containing and loading reference data from files

    Parameters
    ----------
    data_file: (str, tuple(str)), optional, default ('data.db', 'db.sql')
        Either a single string of the reference Asparagus database file name
        or a tuple of the filename first and the file format label second.
    data_label: str, optional, default 'all'
        Dataset label.
    data_properties: List(str), optional, default None
        Subset of properties to load.
    data_unit_properties: dict, optional, default None
        Dictionary from properties (keys) to corresponding unit as a
        string (item), e.g.:
            {property: unit}: { 'positions': 'Ang',
                                'energy': 'eV',
                                'force': 'eV/Ang', ...}
    data_alt_property_labels: dict, optional, default
            'settings._alt_property_labels'
        Dictionary of alternative property labeling to replace
        non-valid property labels with the valid one if possible.
    data_overwrite: bool, optional, default 'False'
        Overwrite database file

    Return
    ------
    callable
        Asparagus DataSet object
    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    def __init__(
        self,
        data_file: Union[str, Tuple[str, str]],
        data_label: Optional[Union[str]] = None,
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
        data_overwrite: Optional[bool] = False,
        **kwargs
    ):
        """"""
        Initialize DataSet class

        """"""

        # Assign data file and format
        if utils.is_string(data_file):
            self.data_file = (
                data_file,
                data.check_data_format(
                    data_file, is_source_format=False)
                )
        else:
            self.data_file = tuple(data_file)

        # Assign dataset label
        if data_label is None:
            self.data_label = 'all'
        else:
            self.data_label = data_label

        # Get database connect function
        self.connect = data.get_connect(self.data_file[1])

        # Check for data path existence
        path, _ = os.path.split(self.data_file[0])
        if path and not os.path.isdir(path):
            os.makedirs(path)

        # If overwrite, remove old DataSet file
        if os.path.exists(self.data_file[0]) and data_overwrite:
            with self.connect(self.data_file[0], 'w') as db:
                db.delete_file()

        # Copy current metadata
        metadata = self.get_metadata()

        # Check data property compatibility
        metadata = self.check_data_compatibility(
            metadata,
            data_properties,
            data_unit_properties)

        # Set metadata
        self.set_metadata(metadata)

        # Assign property list and units
        self.data_properties = data_properties
        self.data_unit_properties = data_unit_properties

        # Check alternative property labels
        if data_alt_property_labels is None:
            self.data_alt_property_labels = settings._alt_property_labels
        else:
            self.data_alt_property_labels = data_alt_property_labels

        # Assign database property dtype
        self.data_dtype_property = self.get_data_properties_dtype()

        # Initialize DataReader variable
        self.datareader = None

        # Initialize database pointer
        self.db = None

        return

    def __len__(
        self,
    ) -> int:

        if os.path.isfile(self.data_file[0]):
            with self.connect(self.data_file[0], mode='r') as db:
                return db.count()
        else:
            return 0

    def __getitem__(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        return self._get_properties(idx)

    def __setitem__(
        self,
        idx: int,
        properties: Dict[str, torch.tensor],
    ):

        return self._set_properties([idx], [properties])

    def __iter__(
        self
    ):
        # Start data counter and set dataset length
        self.counter = 0
        self.Ndata = len(self)
        
        # Open database
        self.db = self.connect(self.data_file[0], mode='r')
        
        return self

    def __next__(
        self
    ):
        # Check counter within number of data range
        if self.counter < self.Ndata:
            data = self.db.get(self.counter + 1)[0]
            self.counter += 1
            return data
        else:
            self.db.close()
            self.db = None
            raise StopIteration

    def get(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        return self._get_properties(idx)

    def get_properties(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        return self._get_properties(idx)

    def _get_properties(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        if self.db is None:
            self.db = self.connect(self.data_file[0], mode='r')
        return self.db.get(idx + 1)[0]

    def set_properties(
        self,
        idx: Union[int, List[int]],
        properties: Union[Dict[str, torch.tensor], List[Dict]],
    ):

        if utils.is_integer(idx):
            idx = [idx]
        if utils.is_dictionary(properties):
            properties = [properties]

        return self._set_properties(idx, properties)

    def _set_properties(
        self,
        idcs: List[int],
        properties: List[Dict[str, torch.tensor]],
    ):

        # Close database if open
        if self.db is not None:
            self.db.close()
            self.db = None

        # Assign properties
        with self.connect(self.data_file[0], mode='a') as db:
            for idx, props in zip(idcs, properties):
                row_id = db.write(props, row_id=idx + 1)

        return row_id

    def update_properties(
        self,
        idx: Union[int, List[int]],
        properties: Union[Dict[str, torch.tensor], List[Dict]],
    ):

        if utils.is_integer(idx):
            idx = [idx]
        if utils.is_dictionary(properties):
            properties = [properties]

        return self._update_properties(idx, properties)

    def _update_properties(
        self,
        idcs: List[int],
        properties: List[Dict[str, torch.tensor]],
    ):

        with self.connect(self.data_file[0], mode='a') as db:
            for idx, props in zip(idcs, properties):
                row_id = db.update(row_id=idx + 1, properties=props)

        return row_id

    @property
    def metadata(self):
        """"""
        DataSet metadata dictionary
        """"""
        return self.get_metadata()

    def get_metadata(
        self,
    ) -> Dict[str, Any]:
        """"""
        Get metadata from database

        Returns
        -------
        dict
            Metadata of the database
        """"""

        # Read metadata from database file
        with self.connect(self.data_file[0], mode='r') as db:
            return db.get_metadata()

    def set_metadata(
        self,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """"""
        Add metadata to the ASE database file
        """"""

        # Check for custom metadata
        if metadata is None:
            metadata = self.metadata

        # Set metadata
        with self.connect(self.data_file[0], mode='a') as db:
            db.set_metadata(metadata)

    def reset_database(
        self,
    ):
        with self.connect(self.data_file[0], mode='a') as db:
            db.reset()

    def load_data(
        self,
        data_source: Union[str, List[str]],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, str]] = None,
        data_source_unit_properties: Optional[Dict[str, str]] = None,
        data_keep_scaling: Optional[bool] = False,
    ):
        """"""
        Load properties from data source.

        Parameters:
        -----------
        data_source: (str, list(str))
            File path or a tuple of file path and file format label of data
            source to file.
        data_properties: list(str), optional, default None
            Set of properties to store in the DataSet
        data_unit_properties: dictionary, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item).
        data_alt_property_labels: dict
            Alternative property labels to detect common mismatches.
        data_source_unit_properties: dictionary, optional, default None
            Dictionary from properties (keys) to corresponding unit as a 
            string (item) in the source data files.
        data_keep_scaling: bool, optional, default False
            If True, keep previous property and atomic energies scaling
            parameter if available or take over from source database file.
            Else, set flag for recomputation.

        """"""

        # Get metadata from database file
        metadata = self.get_metadata()

        # Check data source file and format
        if utils.is_string(data_source):
            data_source = [
                data_source, data.check_data_format(
                    data_source, is_source_format=True)]
        else:
            data_source = list(data_source)

        # Check if data source already loaded
        if metadata.get('data_source') is None:
            metadata['data_source'] = []
        elif (
            tuple(data_source)
            in [tuple(source_i) for source_i in metadata['data_source']]
        ):
            self.logger.warning(
                f""Data source '{data_source[0]:s}' already ""
                + f""written to dataset '{self.data_file[0]:s}'! ""
                + ""Loading data source is skipped."")
            return

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Initialize DataReader
        datareader = data.DataReader(
            data_file=self.data_file,
            data_properties=data_properties,
            data_unit_properties=data_unit_properties,
            data_alt_property_labels=data_alt_property_labels)

        # Get metadata from source database file
        metadata_source = datareader.get_metadata(data_source)

        # Take over property scaling or reset scaling flag
        if (
            data_keep_scaling
            and metadata.get('data_property_scaling') is None
            and metadata_source.get('data_property_scaling') is not None
        ):
            metadata['data_property_scaling'] = (
                metadata_source['data_property_scaling'])
            metadata['data_property_scaling_label'] = (
                metadata_source['data_property_scaling_label'])
            metadata['data_property_scaling_uptodate'] = True
        elif (
            data_keep_scaling
            and metadata.get('data_property_scaling') is not None
        ):
            metadata['data_property_scaling_uptodate'] = True
        else:
            metadata['data_property_scaling_uptodate'] = False

        # Take over atomic energies scaling or reset scaling flag
        if (
            data_keep_scaling
            and metadata.get('data_atomic_energies_scaling') is None
            and metadata_source.get('data_atomic_energies_scaling') is not None
        ):
            metadata['data_atomic_energies_scaling'] = (
                metadata_source['data_atomic_energies_scaling'])
            metadata['data_atomic_energies_scaling_label'] = (
                metadata_source['data_atomic_energies_scaling_label'])
            metadata['data_atomic_energies_scaling_uptodate'] = True
        elif (
            data_keep_scaling
            and metadata.get('data_atomic_energies_scaling') is not None
        ):
            metadata['data_atomic_energies_scaling_uptodate'] = True
        else:
            metadata['data_atomic_energies_scaling_uptodate'] = False

        # Load data file
        datareader.load(
            data_source,
            data_source_unit_properties=data_source_unit_properties)

        # Append data source information
        metadata['data_source'].append(data_source)

        # If metadata properties is empty, initialize database
        if (
            metadata.get('load_properties') is None
            or metadata.get('unit_properties') is None
        ):
            metadata['load_properties'] = data_properties
            metadata['unit_properties'] = data_unit_properties

        # Set updated metadata
        self.set_metadata(metadata)

        return

    def add_atoms(
        self,
        atoms: ase.Atoms,
        properties: Dict[str, Any],
    ):
        """"""
        Add ASE Atoms system and properties

        """"""

        # In case, initialize DataReader with default properties
        if self.datareader is None:
            self.datareader = data.DataReader(
                data_file=self.data_file,
                data_properties=self.data_properties,
                data_unit_properties=self.data_unit_properties,
                data_alt_property_labels=self.data_alt_property_labels,
                )

        # Get dataset metadata
        metadata = self.get_metadata()

        # Load from ASE atoms object and property list
        self.datareader.load_atoms(
            atoms,
            properties,
            data_properties=metadata['load_properties'],
            data_unit_properties=metadata['unit_properties'],
            )

        return

    def check_data_compatibility(
        self,
        metadata: Dict[str, Any],
        data_properties: List[str],
        data_unit_properties: Dict[str, str],
    ):
        """"""
        Check compatibility between input for 'data_properties' and
        'data_unit_properties' with metadata.

        """"""

        if (
            metadata.get('load_properties') is None
            and data_properties is None
        ):
            metadata['load_properties'] = []
        elif metadata.get('load_properties') is None:
            metadata['load_properties'] = data_properties
        elif data_properties is None:
            data_properties = metadata['load_properties']
        else:
            mismatch_metadata = []
            mismatch_input = []
            # Check property match except for default properties always stored
            # in database
            for prop in metadata.get('load_properties'):
                if (
                    prop not in data_properties
                    and prop not in settings._default_property_labels
                ):
                    mismatch_metadata.append(prop)
            for prop in data_properties:
                if (
                    prop not in metadata.get('load_properties')
                    and prop not in settings._default_property_labels
                ):
                    mismatch_input.append(prop)
            if len(mismatch_metadata) or len(mismatch_input):
                message = (
                    ""Mismatch between DataSet 'load_properties' ""
                    + f""input and metadata in '{self.data_file[0]:s}'!\n"")
                for prop in mismatch_metadata:
                    message += f""Property '{prop:s}' in metadata not in input.""
                for prop in mismatch_input:
                    message += f""Property '{prop:s}' in input not in metadata.""
                self.logger.error(message)
                raise SyntaxError(message)

        # Check compatibility between 'data_unit_properties' in metadata
        # and input
        if (
            metadata.get('unit_properties') is None
            and data_unit_properties is None
        ):
            metadata['unit_properties'] = {}
            data_unit_properties = {}
        elif metadata.get('unit_properties') is None:
            metadata['unit_properties'] = {}
            message = """"
            for prop in data_properties:
                if prop in data_unit_properties:
                    metadata['unit_properties'][prop] = (
                        data_unit_properties[prop])
                else:
                    message += f""No Property unit defined for '{prop:s}'.\n""
            if len(message):
                raise SyntaxError(
                    ""DataSet 'load_properties' input contains properties with""
                    + ""unkown property units in 'unit_properties'!\n""
                    + message)
        elif data_unit_properties is None:
            data_unit_properties = metadata['unit_properties']
        else:
            mismatch = []
            # Check property unit match
            for prop, ui in data_unit_properties.items():
                um = metadata.get('unit_properties').get(ui)
                if um is not None and um.lower() != ui.lower():
                    mismatch.append((prop, um, ui))
            if len(mismatch):
                message = (
                    ""Mismatch between DataContainer 'data_unit_properties' ""
                    + f""input and metadata in '{self.data_file[0]:s}'!\n"")
                for (prop, um, ui) in mismatch:
                    message += (
                        f""Unit for property for '{prop:s}' in metadata ""
                        + f""'{um:s}' does not match with inout '{ui:s}'!"")
                self.logger.error(message)
                raise SyntaxError(message)

        # Check for position and charge entry in 'unit_properties' in
        # metadata
        for prop in ['positions', 'charge']:
            if prop not in metadata['unit_properties']:
                if prop in data_unit_properties:
                    metadata['unit_properties'][prop] = (
                        data_unit_properties[prop])
                else:
                    metadata['unit_properties'][prop] = (
                        settings._default_units[prop])

        return metadata

    def get_data_properties_dtype(
        self,
    ) -> 'dtype':
        """"""
        Return database properties float dtype
        
        Returns:
        --------
        'dtype'
            Properties float dtype (most likely torch.float64)

        """"""
        with self.connect(self.data_file[0], mode='r') as db:
            return db.properties_torch_dtype",./Asparagus/asparagus/data/dataset.py
DataSubSet,"class DataSubSet(DataSet):
    """"""
    DataSubSet class iterating and returning over a subset of DataSet.

    Parameters
    ----------
    data_file: (str, tuple(str)), optional, default ('data.db', 'db.sql')
        Either a single string of the reference Asparagus database file name
        or a tuple of the filename first and the file format label second.
    data_label: str
        Data subset label
    subset_idx: List(int)
        List of reference data indices of this subset.

    Returns
    -------
    object
        DataSubSet to present training, validation or testing
    """"""

    def __init__(
        self,
        data_file: Union[str, Tuple[str, str]],
        data_label: str,
        subset_idx: List[int],
    ):
        """"""
        DataSubSet class
        """"""

        # Inherit from DataSet base class
        super().__init__(
            data_file,
            data_label,
            )

        # Assign arguments
        self.subset_idx = np.array(subset_idx, dtype=int)

        # Iterate over args
        for arg, item in locals().items():
            match = utils.check_input_dtype(
                arg, item, settings._dtypes_args, raise_error=True)

        # Check database file
        if not os.path.exists(data_file[0]):
            raise ValueError(
                f""File {data_file[0]:s} does not exists!\n"")

        # Number of subset data points
        self.Nidx = len(self.subset_idx)

        # Get parent dataset parameters from metadata
        metadata = self.get_metadata()
        self.data_properties = metadata.get('load_properties')
        self.data_unit_properties = metadata.get('unit_properties')

        return

    def __len__(
        self,
    ) -> int:

        return self.Nidx

    def __getitem__(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        return self._get_properties(self.subset_idx[idx])

    def __setitem__(
        self,
        idx: int,
        properties: Dict[str, torch.tensor],
    ):

        return self._set_properties([self.subset_idx[idx]], [properties])

    def get(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        return self._get_properties(self.subset_idx[idx])

    def get_properties(
        self,
        idx: int,
    ) -> Dict[str, torch.tensor]:

        # Load property data
        properties = self._get_properties(self.subset_idx[idx])

        return properties

    def set_properties(
        self,
        idx: Union[int, List[int]],
        properties: Union[Dict[str, torch.tensor], List[Dict]],
    ):

        if utils.is_integer(idx):
            idx = [idx]
        if utils.is_dictionary(properties):
            properties = [properties]

        return self._set_properties(
            [self.subset_idx[idxi] for idxi in idx], properties)

    def update_properties(
        self,
        idx: Union[int, List[int]],
        properties: Union[Dict[str, torch.tensor], List[Dict]],
    ):

        if utils.is_integer(idx):
            idx = [idx]
        if utils.is_dictionary(properties):
            properties = [properties]

        return self._update_properties(
            [self.subset_idx[idxi] for idxi in idx], properties)",./Asparagus/asparagus/data/dataset.py
DataLoader,"class DataLoader(torch.utils.data.DataLoader):
    """"""
    Data loader class from a dataset
    
    Parameters
    ----------
    dataset: (data.DataSet, data.DataSubSet)
        DataSet or DataSubSet instance of reference data
    batch_size: int
        Number of atomic systems per batch
    data_shuffle: bool
        Shuffle batch compilation after each epoch
    num_workers: int
        Number of parallel workers for collecting data
    data_collate_fn: callable, optional, default None
        Callable function that prepare and return batch data
    data_pin_memory: bool, optional, default False
        If True data are loaded to GPU
    data_atomic_energies_shift: list(float), optional, default None
        Atom type specific energy shift terms to shift the system energies.
    device: str, optional, default 'cpu'
        Device type for data allocation
    dtype: dtype object, optional, default 'torch.float64'
        Reference data type to convert to

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    def __init__(
        self,
        dataset: Union[data.DataSet, data.DataSubSet],
        batch_size: int,
        data_shuffle: bool,
        num_workers: int,
        device: str,
        dtype: object,
        data_collate_fn: Optional[object] = None,
        data_pin_memory: Optional[bool] = False,
        data_atomic_energies_shift: Optional[List[float]] = None,
        **kwargs
    ):
        """"""
        Initialize data loader.
        """"""

        # Check collate function
        if data_collate_fn is None:
            data_collate_fn = self.data_collate_fn

        # Assign reference dataset as class parameter for additions by the
        # neighbor list functions
        self.dataset = dataset
        self.batch_size = batch_size

        #if device == 'cpu':
            #self.data_num_workers = 0
        #else:
        if num_workers is None:
            self.num_workers = 0
        else:
            self.num_workers = num_workers

        # Initiate DataLoader
        super(DataLoader, self).__init__(
            dataset=dataset,
            batch_size=batch_size,
            shuffle=data_shuffle,
            num_workers=self.num_workers,
            collate_fn=self.data_collate_fn,
            pin_memory=data_pin_memory,
            **kwargs
        )

        # Initialize neighbor list function class parameter
        self.neighbor_list = None

        # Assign reference data conversion parameter
        #self.device = utils.check_device_option(device, config)
        #self.dtype = utils.check_dtype_option(dtype, config)
        self.device = device
        self.dtype = dtype

        # Assign atomic energies shift
        self.set_data_atomic_energies_shift(
            data_atomic_energies_shift,
            self.dataset.get_data_properties_dtype())

        return

    def set_data_atomic_energies_shift(
        self,
        atomic_energies_shift: List[float],
        atomic_energies_dtype: 'dtype',
    ):
        """"""
        Assign atomic energies shift list per atom type
        
        Parameters
        ----------
        atomic_energies_shift: list(float)
            Atom type specific energy shift terms to shift the system energies.
        atomic_energies_dtype: dtype
            Database properties dtype
        """"""

        if atomic_energies_shift is None:
            self.atomic_energies_shift = None
        else:
            self.atomic_energies_shift = torch.tensor(
                atomic_energies_shift, dtype=atomic_energies_dtype)

        return

    def init_neighbor_list(
        self,
        cutoff: Optional[Union[float, List[float]]] = np.inf,
        store: Optional[bool] = False,
        device: Optional[str] = None,
        dtype: Optional[object] = None,
    ):
        """"""
        Initialize neighbor list function

        Parameters
        ----------
        cutoff: float, optional, default infinity
            Neighbor list cutoff range equal to max interaction range
        store: bool, optional, default False
            Pre-compute neighbor list and store in the reference dataset

        """"""

        # Check input parameter
        if device is None:
            device = self.device
        if dtype is None:
            dtype = self.dtype

        # Initialize neighbor list creator
        self.neighbor_list = module.TorchNeighborListRangeSeparated(
            cutoff, device, dtype)

        return

    def data_collate_fn(
        self,
        batch: Dict[str, torch.Tensor],
    ) -> Dict[str, torch.Tensor]:
        """"""
        Prepare batch properties from a dataset such as pair indices and
        return with system properties

        Parameters
        ----------
        batch: dict
            Data batch

        Returns
        -------
        dict
            Collated data batch with additional properties
            Properties:
                atoms_number: (Natoms,) torch.Tensor
                atomic_numbers: (Natoms,) torch.Tensor
                positions: (Natoms, 3) torch.Tensor
                forces: (Natoms, 3) torch.Tensor
                energy: (Natoms,) torch.Tensor
                charge: (Natoms,) torch.Tensor
                idx_i: (Npairs,) torch.Tensor
                idx_j: (Npairs,) torch.Tensor
                pbc_offset_ij: (Npairs, 3) torch.Tensor
                idx_u: (Npairs,) torch.Tensor
                idx_v: (Npairs,) torch.Tensor
                pbc_offset_uv: (Npairs, 3) torch.Tensor
                sys_i: (Natoms,) torch.Tensor

        """"""

        # Collected batch system properties
        coll_batch = {}

        # Get batch size a.k.a. number of systems
        Nsys = len(batch)

        # Get atoms number per system segment
        coll_batch['atoms_number'] = torch.tensor(
            [b['atoms_number'] for b in batch],
            device=self.device, dtype=torch.int64)

        # System segment index
        coll_batch['sys_i'] = torch.repeat_interleave(
            torch.arange(Nsys, device=self.device, dtype=torch.int64),
            repeats=coll_batch['atoms_number'], dim=0).to(
                device=self.device, dtype=torch.int64)

        # System fragment index
        if batch[0].get('fragments') is not None:
            coll_batch['fragments'] = torch.cat(
                [b['fragments'] for b in batch], 0).to(
                    device=self.device, dtype=torch.int64)

        # Atomic numbers properties
        coll_batch['atomic_numbers'] = torch.cat(
            [b['atomic_numbers'] for b in batch], 0).to(
                device=self.device, dtype=torch.int64)

        # Atom positions
        coll_batch['positions'] = torch.cat(
            [b['positions'] for b in batch], 0).to(
                device=self.device, dtype=self.dtype)

        # Periodic boundary conditions
        coll_batch['pbc'] = torch.cat(
            [b['pbc'] for b in batch], 0).to(
                device=self.device, dtype=torch.bool
                ).reshape(Nsys, 3)

        # Unit cell sizes
        coll_batch['cell'] = torch.cat(
            [b['cell'] for b in batch], 0).to(
                device=self.device, dtype=self.dtype
                ).reshape(Nsys, -1)

        # Compute the cumulative segment size number
        atomic_numbers_cumsum = torch.cat(
            [
                torch.zeros(
                    (1,), device=self.device, dtype=coll_batch['sys_i'].dtype),
                torch.cumsum(coll_batch[""atoms_number""][:-1], dim=0)
            ],
            dim=0).to(
                device=self.device, dtype=torch.int64)

        # Iterate over batch properties
        skip_props = [
            'atoms_number', 'atomic_numbers', 'positions', 'pbc', 'cell',
            'atom_types', 'fragments']
        for prop_i in batch[0]:

            # Skip previous parameter and None
            if prop_i in skip_props or batch[0].get(prop_i) is None:

                continue

            # Special property: energy
            elif prop_i == 'energy':
                
                if self.atomic_energies_shift is None:
                    
                    coll_batch[prop_i] = torch.tensor(
                        [b[prop_i] for b in batch],
                        device=self.device, dtype=self.dtype)
                    

                else:

                    shifted_energy = [
                        b[prop_i]
                        - torch.sum(
                            self.atomic_energies_shift[b['atomic_numbers']])
                        for b in batch]
                    coll_batch[prop_i] = torch.tensor(
                        shifted_energy,
                        device=self.device, dtype=self.dtype)
            
            # Special property: atomic energies
            elif prop_i == 'atomic_energies':
                
                if self.atomic_energies_shift is None:
                    
                    coll_batch[prop_i] = torch.cat(
                        [b[prop_i] for b in batch], 0).to(
                            device=self.device, dtype=self.dtype)

                else:
                    
                    shifted_atomic_energies = [
                        b[prop_i]
                        - torch.sum(
                            self.atomic_energies_shift[b['atomic_numbers']])
                        for b in batch]
                    coll_batch[prop_i] = torch.cat(
                        shifted_atomic_energies, 0).to(
                            device=self.device, dtype=self.dtype)
            
            # Properties (float data)
            else:

                # Concatenate tensor data
                if batch[0][prop_i].size():
                    coll_batch[prop_i] = torch.cat(
                        [b[prop_i] for b in batch], 0).to(
                            device=self.device, dtype=self.dtype)

                # Concatenate numeric data
                else:
                    coll_batch[prop_i] = torch.tensor(
                        [b[prop_i] for b in batch],
                        device=self.device, dtype=self.dtype)

        # Compute pair indices and position offsets
        if self.neighbor_list is None:
            self.init_neighbor_list()
        coll_batch = self.neighbor_list(
            coll_batch,
            atomic_numbers_cumsum=atomic_numbers_cumsum)

        return coll_batch

    @property
    def data_properties(self):
        return self.dataset.data_properties",./Asparagus/asparagus/data/dataloader.py
DataBase_npz,"class DataBase_npz(data.DataBase):
    """"""
    Numpy npz data base class
    """"""

    _metadata = {}

    # Structural and reference property dtypes
    properties_numpy_dtype = np.float64
    properties_torch_dtype = torch.float64

    def __init__(
        self,
        data_file: str,
        lock_file: bool,
        memory_limit: Optional[float] = 1024.0,
    ):
        """"""
        Numpy Database object that contain reference data.

        Parameters
        ----------
        data_file: str
            Reference database file
        lock_file: bool
            Use a lock file when manipulating the database to prevent
            parallel manipulation by multiple processes.
        memory_limit: float, optional, default 1024.
            File size limit in MB of the database file which is loaded to the
            memory when smaller. Otherwise keep database just connection
            (slower). Default value is 1GB or 1024MB.

        """"""

        # Inherit from DataBase base class
        super().__init__(data_file)

        # Check for .npz suffix
        if self.data_file.split('.')[-1].lower() != 'npz':
            self.data_file = f""{self.data_file:s}.npz""

        # Prepare data locker
        if lock_file and utils.is_string(data_file):
            self.lock = Lock(self.data_file + '.lock', world=DummyMPI())
        else:
            self.lock = None

        # Prepare metadata file path
        self.metadata_label = 'metadata:'
        self.metadata_nlabel = len(self.metadata_label)

        # Initialize class variables
        self.connected = False
        self.data = None
        self.data_new = {}
        self.metadata_new = False

        # Assign file size limit
        self.memory_limit = memory_limit

        return

    def _load(self):
        if os.path.exists(self.data_file):
            # Get file size in MB
            size = os.path.getsize(self.data_file)/(1024.**2)
            # If file size smaller than limit, load data to memory, else only
            # open file
            if size < self.memory_limit:
                self.data = dict(np.load(self.data_file))
            else:
                self.data = np.load(self.data_file)
        else:
            self.data = None
        self.connected = True
        return

    def _save(self):
        
        # Check for changed data
        if self.data_new or self.metadata_new:
            
            # Check if data are loaded
            if not self.connected:
                self._load()

            # Store new data
            data_merged = self._merge()
            np.savez(self.data_file, **data_merged)

            # Reset new data flags
            self.data_new = {}
            self.metadata_new = False

        return

    def __enter__(self):
        self._load()
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if exc_type is not None:
            raise exc_type
        else:
            self._save()
        self.connected = False
        self.data = None
        self.data_new = {}
        return

    def close(self):
        self._save()
        self.connected = False
        self.data = None
        self.data_new = {}
        return

    def _merge(self) -> Dict[str, Any]:
        """"""
        Check if stored data (self.data) are to merge with loaded data
        (self.data_new) to a data dictionary that will be store in a file.
        Also check for metadata updates.

        Returns
        -------
        dict(str, any)
            Data dictionary including metadata

        """"""

        # If no new data or metadata are available return current data
        if not (bool(self.data_new) or self.metadata_new):
            return self.data

        # Update data if new data are available,
        # otherwise set as current data dictionary
        if self.data_new:
            data_merged = self._merge_data()
        else:
            data_merged = {}
            for prop in self.data:
                data_merged[prop] = self.data[prop]

        # Update metadata if changed
        if self.metadata_new:
        
            # Convert metadata to npz compatible dictionary
            md = self._convert_metadata()
        
            # Update data with metadata
            data_merged.update(md)

        return data_merged

    def _merge_data(self) -> Dict[str, Any]:
        """"""
        Merge stored data (self.data) with loaded data (self.data_new) to a
        data dictionary.

        Returns
        -------
        dict(str, any)
            Data dictionary

        """"""

        # Initialize merged data dictionary
        data_merged = {}
        
        # Iterate over data properties and ids
        for prop in self.data:

            # Skip system and atoms id
            if prop in ['system:id', 'atoms:id']:
                continue

            # Skip metadata keys
            if (
                len(prop) > self.metadata_nlabel
                and prop[:self.metadata_nlabel] == self.metadata_label
            ):
                continue

            # Generate row id and system id lists from certain property
            if prop == system_id_property:
                
                next_sys_id = self.data['system:id'][-1]
                new_sys_id = next_sys_id + np.cumsum(
                    [data_i.shape[0] for data_i in self.data_new[prop]])
                data_merged['system:id'] = np.concatenate(
                    (self.data['system:id'], new_sys_id),
                    axis=0)
                
                last_row_id = self.get_last_id()
                new_row_id = last_row_id + 1 + np.arange(
                    len(self.data_new[prop]), dtype=integer_numpy_dtype)
                data_merged['id'] = np.concatenate(
                    (self.data['id'], new_row_id),
                    axis=0)

            # Generate atoms id lists from certain property
            if prop == atoms_id_property:
                next_sys_id = self.data['atoms:id'][-1]
                new_sys_id = next_sys_id + np.cumsum(
                    [data_i.shape[0] for data_i in self.data_new[prop]])
                data_merged['atoms:id'] = np.concatenate(
                    (self.data['atoms:id'], new_sys_id),
                    axis=0)

            # Each other property must be found in the new data
            if self.data_new.get(prop) is None:
                raise SyntaxError(
                    ""New data do not contain property information for ""
                    + f""'{prop:s}'!"")

            # Finalize property id lists and merge
            if len(prop) > 3 and prop[-3:] == ':id':

                next_prop_id = self.data[prop][-1]
                new_prop_id = next_prop_id + np.cumsum(
                    [data_i for data_i in self.data_new[prop]])
                data_merged[prop] = np.concatenate(
                    (self.data[prop], new_prop_id),
                    axis=0)

            # Merge property lists
            else:

                data_merged[prop] = np.concatenate(
                    (self.data[prop], *self.data_new[prop]),
                    axis=0)

        return data_merged

    def split_data(self):
        """"""
        Split the stored data dictionary (self.data) to lists of data entries
        (self.data_new) that allow individual data manipulation.

        """"""

        # Store recent changes and reload data
        self._save()
        self._load()

        # Initialize empty data dictionary self.data_new
        self._init_data_new()

        # Split system id
        self.data_new['id'] = np.array(
            self.data['id'], dtype=integer_numpy_dtype)

        # Split data time and user name
        self.data_new['mtime'] = np.array(
            self.data['mtime'], dtype=string_numpy_dtype).reshape(-1, 1)
        self.data_new['username'] = np.array(
            self.data['username'], dtype=string_numpy_dtype).reshape(-1, 1)

        # Iterate over data
        for idx in self.data_new['id']:

            # Split from structural properties
            for prop_i in structure_properties_dtype:

                # Get system ids
                id_start = self.data[structure_properties_ids[prop_i]][idx - 1]
                id_end = self.data[structure_properties_ids[prop_i]][idx]

                # Append property
                self.data_new[prop_i].append(
                    self.data[prop_i][id_start:id_end])

            # Split from reference properties
            for prop_i in self.metadata.get('load_properties'):

                # Get system ids
                id_start = self.data[prop_i + ':id'][idx - 1]
                id_end = self.data[prop_i + ':id'][idx]

                # Append property and property ids
                self.data_new[prop_i].append(
                    self.data[prop_i][id_start:id_end])
                self.data_new[prop_i + ':id'].append(
                    np.array(id_end - id_start, dtype=integer_numpy_dtype))

        return

    @lock
    def _set_metadata(self, metadata):

        # Set new metadata flag
        self.metadata_new = True

        # Store metadata as class variable
        self._metadata = metadata

        # Add version
        self._metadata['version'] = VERSION

        # Initialize data system
        self._init_systems()

        return self._metadata

    def _get_metadata(self):

        # Load metadata from data if not stored
        if not len(self._metadata):
            self._metadata = self._load_metadata()

        return self._metadata

    def _load_metadata(self):

        # Check if data are loaded
        if not self.connected:
            self._load()

        # Initialize metadata dictionary
        metadata = {}

        # Check data dictionary
        if self.data is None:
            return metadata

        # Iterate over data keys
        for key in self.data:

            # Skip if not a metadata label
            if (
                len(key) < self.metadata_nlabel
                or key[:self.metadata_nlabel] != self.metadata_label
            ):
                continue

            # Extract metadata key
            metadata_key = key[self.metadata_nlabel:]

            # Check metadata item
            if utils.is_array_like(self.data[key]):
                data_item = self.data[key].tolist()
            elif utils.is_string(self.data[key]):
                data_item = str(self.data[key])
            else:
                data_item = self.data[key]

            # Check for converted subdictionary keys or items
            if ':' in metadata_key:
                
                # Get metadata key, as well as subdictionary key and item
                metadata_keys = metadata_key.split(':')
                metadata_key = metadata_keys[0]
                dict_key = metadata_keys[1]

                # Set subdictionary key and items
                if metadata_key not in metadata:
                    metadata[metadata_key] = {}
                metadata[metadata_key][dict_key] = data_item
            
            else:
                
                # Set metadata property
                metadata[metadata_key] = data_item

        return metadata

    def _convert_metadata(self):

        # Initialize npz compatible metadata dictionary
        metadata_npz = {}
        
        for key, metadata_item in self._metadata.items():
            
            # Create metadata key
            metadata_key = self.metadata_label + key

            # Check for dictionary conversion
            if utils.is_dictionary(metadata_item):
                
                # Set subdictionary keys and items separately
                for dict_key, dict_item in metadata_item.items():

                    # Check metadata item
                    if utils.is_array_like(dict_item):
                        dict_item = np.array(dict_item)

                    # Modify metadata key and add to metadata
                    metadata_dict_key = metadata_key + ':' + str(dict_key)
                    metadata_npz[metadata_dict_key] = dict_item

            # Else, store directly
            else:
                
                # Check metadata item
                if utils.is_array_like(metadata_item):
                    metadata_item = np.array(metadata_item)

                metadata_npz[metadata_key] = metadata_item

        return metadata_npz

    def _init_systems(self):
        
        # Get metadata
        metadata = self._get_metadata()
        
        # Init data dictionary
        if self.data is None and metadata.get('load_properties') is not None:
            self._init_data()
        
        # Get version
        if metadata.get('version') is None:
            self.version = VERSION
        else:
            self.version = metadata.get('version')

        # Check version compatibility
        if self.version > VERSION:
            raise IOError(
                f""Can not read newer version of the database format ""
                f""(version {self.version})."")

        return

    def _get(self, selection, **kwargs):

        # Check if data are loaded
        if not self.connected:
            self._load()

        # Reference Data list
        rows = []

        # Iterate over selection
        for idx in selection:

            # Initialize reference data dictionary
            row = {}

            # Get structural properties
            for prop_i in structure_properties_dtype:

                # Get system ids
                id_start = self.data[structure_properties_ids[prop_i]][idx - 1]
                id_end = self.data[structure_properties_ids[prop_i]][idx]

                # Get property
                row[prop_i] = torch.tensor(self.data[prop_i][id_start:id_end])

            # Get reference properties
            for prop_i in self.metadata.get('load_properties'):

                # Get system ids
                id_start = self.data[prop_i + ':id'][idx - 1]
                id_end = self.data[prop_i + ':id'][idx]

                # Get property
                row[prop_i] = torch.tensor(
                    self.data[prop_i][id_start:id_end],
                    dtype=self.properties_torch_dtype)

            # Append data
            rows.append(row)

        return rows

    def _init_data(self):

        # Initialize data dictionary
        self.data = {}

        # Initialize data id and system id list
        self.data['id'] = np.array([], dtype=integer_numpy_dtype)
        self.data['system:id'] = np.array([0], dtype=integer_numpy_dtype)
        self.data['atoms:id'] = np.array([0], dtype=integer_numpy_dtype)

        # Initialize lists for current data time and User name
        self.data['mtime'] = np.array([], dtype=string_numpy_dtype)
        self.data['username'] = np.array([], dtype=string_numpy_dtype)

        # Initialize lists for structural properties
        for prop_i, dtype_i in structure_properties_dtype.items():
            self.data[prop_i] = np.array([], dtype=dtype_i).reshape(
                structure_properties_shape[prop_i])

        # Initialize lists for reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype:
                self.data[prop_i] = np.array(
                    [], dtype=self.properties_numpy_dtype).reshape(
                        reference_properties_shape[prop_i])
                self.data[prop_i + ':id'] = np.array(
                    [0], dtype=integer_numpy_dtype)
            elif 'std_' in prop_i and prop_i[4:] in reference_properties_shape:
                self.data[prop_i] = np.array(
                    [], dtype=self.properties_numpy_dtype).reshape(
                        reference_properties_shape[prop_i[4:]])
                self.data[prop_i + ':id'] = np.array(
                    [0], dtype=integer_numpy_dtype)

        return

    def _init_data_new(self):

        # Initialize data dictionary
        self.data_new = {}

        # Initialize data id list
        self.data_new['id'] = []

        # Initialize lists for current data time and User name
        self.data_new['mtime'] = []
        self.data_new['username'] = []

        # Initialize lists for structural properties
        for prop_i in structure_properties_dtype:
            self.data_new[prop_i] = []

        # Initialize lists for reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype:
                self.data_new[prop_i] = []
                self.data_new[prop_i + ':id'] = []

        return

    def _reset(self):

        # Reset stored metadata dictionary
        self._metadata = {}
        return

    @lock
    def _write(self, properties, row_id):

        # Check if data are loaded
        if not self.connected:
            self._load()

        # Check data dictionary
        if self.data is None:
            self._init_data()

        # Check if 'row_id' already already occupied
        if row_id is not None and row_id in self.data['id']:
            row_id = self._update(row_id, properties)
            return row_id

        # Initialize new data dictionary
        if not self.data_new:
            self._init_data_new()

        # Assign new id
        if row_id is None:
            row_id = self.get_last_id() + 1
        self.data_new['id'].append(
            np.array([row_id], dtype=integer_numpy_dtype))

        # Current data time and username
        self.data_new['mtime'].append(
            np.array([time.ctime()], dtype=string_numpy_dtype))
        self.data_new['username'].append(
            np.array([os.getenv('USER')], dtype=string_numpy_dtype))

        # Structural properties
        for prop_i, dtype_i in structure_properties_dtype.items():
            if properties.get(prop_i) is None:
                self.data_new[prop_i].append(None)
            else:
                self.data_new[prop_i].append(
                    np.array(properties.get(prop_i), dtype=dtype_i).reshape(
                        structure_properties_shape[prop_i]))

        # Reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype:
                if properties.get(prop_i) is None:
                    self.data_new[prop_i].append(None)
                else:
                    data_i = np.array(
                        properties.get(prop_i),
                        dtype=self.properties_numpy_dtype)
                    if prop_i in reference_properties_shape:
                        data_i = data_i.reshape(
                            reference_properties_shape[prop_i])
                    elif (
                        'std_' in prop_i
                        and prop_i[4:] in reference_properties_shape
                    ):
                        data_i = data_i.reshape(
                            reference_properties_shape[prop_i[4:]])
                    self.data_new[prop_i].append(data_i)
                    self.data_new[prop_i + ':id'].append(
                        np.array(
                            data_i.shape[0],
                            dtype=integer_numpy_dtype)
                        )

        return row_id

    def _update(self, row_id, properties):

        # Check if data are loaded
        if not self.connected:
            self._load()

        # Check data dictionary
        if self.data is None:
            self._init_data()

        # Check if 'row_id' already already occupied
        if row_id is None or row_id not in self.data['id']:
            row_id = self._write(properties, row_id)
            return row_id

        # Split data for individual row manipulation
        self.split_data()

        # Update data time and username
        self.data_new['mtime'][row_id] = np.array(
            [time.ctime()], dtype=string_numpy_dtype)
        self.data_new['username'][row_id] = np.array(
            [os.getenv('USER')], dtype=string_numpy_dtype)

        # Update structural properties
        for prop_i, dtype_i in structure_properties_dtype.items():
            if properties.get(prop_i) is None:
                self.data_new[prop_i][row_id] = None
            else:
                self.data_new[prop_i][row_id] = np.array(
                    properties.get(prop_i), dtype=dtype_i
                    ).reshape(structure_properties_shape[prop_i])

        # Update reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype:
                if properties.get(prop_i) is None:
                    self.data_new[prop_i][row_id] = None
                else:
                    data_i = np.array(
                        properties.get(prop_i),
                        dtype=self.properties_numpy_dtype)
                    if prop_i in reference_properties_shape:
                        data_i = data_i.reshape(
                            reference_properties_shape[prop_i])
                    elif (
                        'std_' in prop_i
                        and prop_i[4:] in reference_properties_shape
                    ):
                        data_i = data_i.reshape(
                            reference_properties_shape[prop_i[4:]])
                    self.data_new[prop_i][row_id] = data_i
                    self.data_new[prop_i + ':id'][row_id] = np.array(
                        data_i.shape[0], dtype=integer_numpy_dtype)

        return row_id

    def get_last_id(self):

        # Check if data are loaded
        if not self.connected:
            self._load()

        # Get last row id
        if 'id' in self.data and len(self.data['id']):
            row_id = self.data['id'][-1]
        else:
            row_id = 0

        return row_id

    def _count(self, cmps):

        # Check if data are loaded or new data are in the queue
        if not self.connected:
            self._load()
        if self.data_new:
            self._merge()

        if self.data is None:
            return 0
        elif 'id' in self.data and len(self.data['id']):
            return self.data['id'][-1]
        else:
            return 0

    def _delete(self, row_id):

        # Check if data are loaded
        if not self.connected:
            self._load()

        # Check data dictionary
        if self.data is None:
            self._init_data()

        # Check if 'row_id' is found and can be deleted
        if row_id is None or row_id not in self.data['id']:
            return row_id

        # Split data for individual row manipulation
        self.split_data()

        # Delete data id list
        del data_new['id'][row_id]

        # Delete data time and username
        del self.data_new['mtime'][row_id]
        del self.data_new['username'][row_id]

        # Delete structural properties
        for prop_i, dtype_i in structure_properties_dtype.items():
            del self.data_new[prop_i][row_id]

        # Delete reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype:
                del self.data_new[prop_i][row_id]

        return row_id

    def _delete_file(self):
        """"""
        Delete database and metadata file
        """"""
        if os.path.exists(self.data_file):
            os.remove(self.data_file)
        return

    @property
    def metadata(self):
        return self._get_metadata()",./Asparagus/asparagus/data/database_npz.py
DataBase,"class DataBase:
    """"""
    Base class for the database
    """"""

    def __init__(
        self,
        data_file: str,
    ):
        """"""
        DataBase object that contain reference data.
        This is a condensed version of the ASE Database class:
        https://gitlab.com/ase/ase/-/blob/master/ase/db/core.py

        Parameters
        ----------
        data_file: str
            Reference database file

        """"""

        # DataBase file name
        if utils.is_string(data_file):
            data_file = os.path.expanduser(data_file)
        self.data_file = data_file

    def set_metadata(self, metadata):
        """"""
        Set database metadata.

        Parameters
        ----------
        metadata: dict
            Database metadata

        Returns
        -------
        dict
            Metadata stored in Database
            If Database is new output is same as input.
        """"""

        return self._set_metadata(metadata)

    def __len__(self):
        return self.count()

    def __delitem__(self, rwo_id):
        self._delete([rwo_id])

    def __getitem__(self, selection):
        return self.get(selection)

    def __exit__(self, exc_type, exc_value, tb):
        if exc_type is not None:
            raise exc_type
        return

    def _set_metadata(self, metadata):
        raise NotImplementedError

    @property
    def metadata(self) -> Dict[str, Any]:
        self._get_metadata()

    def get_metadata(self):
        """"""
        Get the database metadata dictionary

        Returns
        -------
            dict
                Metadata stored in Database
                If Database is new output is same as input.
        """"""

        return self._get_metadata()

    def _get_metadata(self):
        raise NotImplementedError

    def init_systems(self):
        """"""
        Initialize systems column in database according to metadata
        """"""
        return self._init_systems()

    def _init_systems(self):
        raise NotImplementedError

    def reset(self):
        self._reset()

    def _reset(self):
        raise NotImplementedError

    def write(self, properties={}, row_id=None, **kwargs):
        """"""
        Write reference data to database.

        Parameters
        ----------
            properties: dict
                Reference data
            row_id: int
                Overwrite existing row.

        Returns
        -------
            int
                Returns integer id of the new row.
        """"""

        row_id = self._write(properties, row_id)

        return row_id

    def _write(self, properties, row_id=None):
        return 1

    @parallel_function
    def update(self, row_id, properties={}, **kwargs):
        """"""
        Update reference data of certain properties in database.

        Parameters
        ----------
        properties: dict
            Reference data
        row_id: int
            Overwrite existing row.

        Returns
        -------
        int
            Returns integer id of the updated row.
        """"""

        row_id = self._update(row_id, properties)

        return row_id

    def _update(self, row_id, properties):
        return 1

    def get(self, selection=None, **kwargs):
        """"""
        Select a single or multiple rows and return it as a dictionary.
        
        Parameters
        ----------
        selection: (int, list(int))
            See the select() method.
        
        Returns
        -------
        dict
            Returns entry of the selection.
        """"""

        if utils.is_integer(selection):
            selection = [selection]
        return self._get(selection, **kwargs)

    def _get(self, selection, **kwargs):
        raise NotImplementedError

    def count(self, selection=None, **kwargs):
        """"""
        Count rows in DataBase
        """"""
        return self._count(selection, **kwargs)
        
    def _count(self, selection, **kwargs):
        raise NotImplementedError

    def delete(self, row_ids):
        """"""
        Delete entry from the database.
        """"""
        if utils.is_integer(row_ids):
            row_ids = [row_ids]
        self._delete(row_ids)
        
        return

    def _delete(self, row_ids):
        raise NotImplementedError

    @parallel_function
    def reserve(self):
        """"""
        Write empty row if not already present.
        """"""

        # Write empty row
        row_id = self._write({}, None)

        return row_id

    def _get_metadata(self):
        raise NotImplementedError

    def delete_file(self):
        """"""
        Delete DataBase and related files
        """"""
        return self._delete_file()

    def _delete_file(self):
        raise NotImplementedError",./Asparagus/asparagus/data/database.py
DataBase_SQLite3,"class DataBase_SQLite3(data.DataBase):
    """"""
    SQL lite 3 data base class
    """"""

    # Initialize connection interface
    connection = None
    _metadata = {}

    # Used for autoincrement id
    default = 'NULL'

    # Structural and reference property dtypes
    properties_numpy_dtype = np.float64
    properties_torch_dtype = torch.float64

    def __init__(
        self,
        data_file: str,
        lock_file: bool,
    ):
        """"""
        SQLite3 dataBase object that contain reference data.
        This is a condensed version of the ASE Database class:
        https://gitlab.com/ase/ase/-/blob/master/ase/db/sqlite.py

        Parameters
        ----------
        data_file: str
            Reference database file
        lock_file: bool
            Use a lock file when manipulating the database to prevent
            parallel manipulation by multiple processes.

        """"""

        # Inherit from DataBase base class
        super().__init__(data_file)

        # Prepare data locker
        self.lock_file = self.data_file + '.lock'
        if lock_file and utils.is_string(self.data_file):
            self.lock = Lock(self.data_file + '.lock', world=DummyMPI())
        else:
            self.lock = None

        with self.managed_connection() as con:

            # Select any data in the database
            cur = con.execute(
                ""SELECT COUNT(*) FROM sqlite_master WHERE name='systems'"")
            data_fetched = cur.fetchone()[0]

            # Get and assign version number
            if data_fetched == 0:
                self.version = VERSION
            else:
                cur = con.execute(
                    'SELECT value FROM information WHERE name=""version""')
                self.version = int(cur.fetchone()[0])

            # Initialize version compatible parameters. If not compatible
            # parameter are defined, take the latest version smaller than the
            # requested one
            if self.version in init_systems_version:
                self.init_systems_execute = [
                    init_system.strip()
                    for init_system in init_systems_version[self.version]]
            else:
                version = self.latest_version(init_systems_version)
                self.init_systems_execute = (init_systems_version[version][:])
            if self.version in structure_properties_dtype_version:
                self.structure_properties_dtype = (
                    structure_properties_dtype_version[self.version])
            else:
                version = self.latest_version(
                    structure_properties_dtype_version)
                self.structure_properties_dtype = (
                    structure_properties_dtype_version[version])
            if self.version in structure_properties_shape_version:
                self.structure_properties_shape = (
                    structure_properties_shape_version[self.version])
            else:
                version = self.latest_version(
                    structure_properties_shape_version)
                self.structure_properties_shape = (
                    structure_properties_shape_version[version])

        # Check version compatibility
        if self.version > VERSION:
            raise IOError(
                f""Can not read newer version of the database format ""
                f""(version {self.version})."")

        return

    def _connect(self):
        return sqlite3.connect(self.data_file, timeout=20)

    def __enter__(self):
        assert self.connection is None
        self.change_count = 0
        self.connection = self._connect()
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if exc_type is None:
            self.connection.commit()
        else:
            self.connection.rollback()
        self.connection.close()
        self.connection = None
        return

    def close(self):
        if self.connection is not None:
            self.connection.commit()
            self.connection.close()
        self.connection = None
        return

    @contextmanager
    def managed_connection(
        self,
        commit_frequency: Optional[int] = 5000,
    ):
        try:
            con = self.connection or self._connect()
            yield con
        except ValueError as exc:
            if self.connection is None:
                con.close()
            raise exc
        else:
            if self.connection is None:
                con.commit()
                con.close()
            else:
                self.change_count += 1
                if self.change_count % commit_frequency == 0:
                    con.commit()

    @lock
    def _set_metadata(
        self,
        metadata: Dict[str, Any],
    ):

        # Convert metadata dictionary
        md = json.dumps(metadata)

        with self.managed_connection() as con:

            # Select any data in the database
            cur = con.execute(
                ""SELECT COUNT(*) FROM sqlite_master WHERE name='information'"")

            if cur.fetchone()[0]:

                # Update metadata if existing
                cur.execute(
                    ""UPDATE information SET value=? WHERE name='metadata'"",
                    [md])
                con.commit()

            else:

                # Initialize data columns
                for statement in init_information:
                    con.execute(statement)
                con.commit()

                # Write metadata
                cur.execute(
                    ""INSERT INTO information VALUES (?, ?)"", ('metadata', md))
                con.commit()

        # Store metadata
        self._metadata = metadata

        # Initialize data system
        self._init_systems()

        return

    def _get_metadata(self) -> Dict[str, Any]:

        # Read metadata if not in memory
        if not len(self._metadata):

            with self.managed_connection() as con:

                # Check if table 'information' exists
                cur = con.execute(
                    'SELECT name FROM sqlite_master ""'
                    + '""WHERE type=""table"" AND name=""information""')
                result = cur.fetchone()

                if result is None:
                    self._metadata = {}
                    return self._metadata

                # Check if metadata exist
                cur = con.execute(
                    'SELECT count(name) FROM information '
                    + 'WHERE name=""metadata""')
                result = cur.fetchone()[0]

                # Read metadata if exist
                if result:
                    cur = con.execute(
                        'SELECT value FROM information WHERE name=""metadata""')
                    results = cur.fetchall()
                    if results:
                        self._metadata = json.loads(results[0][0])
                else:
                    self._metadata = {}

        return self._metadata

    def latest_version(self, parameter_version):
        version_available = [
            int(version) for version in parameter_version.keys()
            if int(version) <= self.version]
        return max(version_available)

    def _init_systems(self):

        # Get metadata
        metadata = self._get_metadata()

        with self.managed_connection() as con:

            # Select any data in the database
            cur = con.execute(
                ""SELECT COUNT(*) FROM sqlite_master WHERE name='systems'"")
            data_fetched = cur.fetchone()[0]

            # If no system in database, initialize
            if data_fetched == 0:

                # Update initial statements with properties to load
                for prop_i in metadata.get('load_properties'):
                    if prop_i not in self.structure_properties_dtype.keys():
                        self.init_systems_execute[0] += f""{prop_i} BLOB,\n""
                self.init_systems_execute[0] = (
                    self.init_systems_execute[0].strip()[:-1] + "")"")

                # Initialize data columns
                for statement in self.init_systems_execute:
                    con.execute(statement)
                con.commit()

        # # Check version compatibility
        # if self.version > VERSION:
        #     raise IOError(
        #         f""Can not read newer version of the database format ""
        #         f""(version {self.version})."")

        return

    def _reset(self):

        # Reset stored metadata dictionary
        self._metadata = {}
        return

    def _get(
        self,
        selection: Union[int, List[int]],
        **kwargs,
    ) -> Dict[str, Any]:

        # Get row of selection
        row = list(self.select(selection, **kwargs))

        # Check selection results
        if row is None:
            raise KeyError('no match')

        return row

    def parse_selection(
        self,
        selection: Union[int, List[int]],
        **kwargs,
    ) -> List[Tuple[str]]:
        """"""
        Interpret the row selection
        """"""

        if selection is None or selection == '':
            cmps = []
        elif utils.is_integer(selection):
            cmps = [('id', '=', int(selection))]
        elif utils.is_integer_array(selection):
            cmps = [('id', '=', int(selection_i)) for selection_i in selection]
        else:
            raise ValueError(
                f""Database selection '{selection}' is not a valid input!\n"" +
                ""Provide either an index or list of indices."")

        return cmps

    @parallel_generator
    def select(
        self,
        selection: Optional[Union[int, List[int]]] = None,
        selection_filter: Optional[Callable] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """"""
        Select rows.

        Return AtomsRow iterator with results.  Selection is done
        using key-value pairs.

        Parameters
        ----------
        selection: (int, list(int)), optional, default None
            Row index or list of indices
        selection_filter: callable, optional, default None
            A function that takes as input a row and returns True or False.

        Returns
        -------
        dict
            Returns entry of the selection.
        """"""

        # Check and interpret selection
        cmps = self.parse_selection(selection)

        # Iterate over selection
        for row in self._select(cmps):

            # Apply potential reference data filter or not
            if selection_filter is None or selection_filter(row):

                yield row

    def encode(
        self,
        obj: Any,
    ) -> Any:
        return object_to_bytes(obj)

    def decode(
        self,
        txt: str,
    ) -> Any:
        return bytes_to_object(txt)

    def blob(
        self,
        item: Any,
    ) -> Any:
        """"""
        Convert an item to blob/buffer object if it is an array.
        """"""

        if item is None:
            return None
        elif utils.is_integer(item):
            return item
        elif utils.is_numeric(item):
            return item

        if item.dtype == np.int64:
            item = item.astype(np.int32)
        if item.dtype == torch.int64:
            item = item.astype(torch.int32)
        if not np.little_endian:
            item = item.byteswap()
        return memoryview(np.ascontiguousarray(item))

    def deblob(
        self,
        buf,
        dtype=None,
        shape=None
    ) -> Any:
        """"""
        Convert blob/buffer object to ndarray of correct dtype and shape.
        (without creating an extra view).
        """"""

        if buf is None:
            return None

        if dtype is None:
            dtype = self.properties_numpy_dtype

        if len(buf):
            item = np.frombuffer(buf, dtype)
            if not np.little_endian:
                item = item.byteswap()
        else:
            item = np.zeros(0, dtype)

        if shape is not None:
            item = item.reshape(shape)

        return item

    @lock
    def _write(
        self,
        properties: Dict[str, Any],
        row_id: int,
    ) -> int:

        # Reference data list
        columns = []
        values = []

        # Current datatime and User name
        columns += ['mtime', 'username']
        values += [time.ctime(), os.getenv('USER')]

        # Structural properties
        for prop_i, dtype_i in self.structure_properties_dtype.items():

            columns += [prop_i]
            if properties.get(prop_i) is None:
                values += [None]
            elif utils.is_array_like(properties.get(prop_i)):
                values += [self.blob(
                    np.array(properties.get(prop_i), dtype=dtype_i))]
            else:
                values += [dtype_i(properties.get(prop_i))]

        # Reference properties
        for prop_i in self.metadata.get('load_properties'):

            if prop_i not in self.structure_properties_dtype:

                columns += [prop_i]
                if properties.get(prop_i) is None:
                    values += [None]
                elif utils.is_array_like(properties.get(prop_i)):
                    values += [self.blob(
                        np.array(
                            properties.get(prop_i),
                            dtype=self.properties_numpy_dtype))]
                else:
                    values += [self.properties_numpy_dtype(
                        properties.get(prop_i))]

        # Convert values to tuple
        columns = tuple(columns)
        values = tuple(values)

        # Add or update database values
        with self.managed_connection() as con:

            # Add values to database
            if row_id is None:

                # Get SQL cursor
                cur = con.cursor()

                # Add to database
                q = self.default + ', ' + ', '.join('?' * len(values))
                cur.execute(
                    f""INSERT INTO systems VALUES ({q})"", values)
                row_id = self.get_last_id(cur)

            else:

                row_id = self._update(row_id, values=values, columns=columns)

        return row_id

    def update(
        self,
        row_id: int,
        properties: Dict[str, Any],
    ) -> int:

        # Reference data list
        columns = []
        values = []

        # Current datatime and User name
        columns += ['mtime', 'username']
        values += [time.ctime(), os.getenv('USER')]

        # Structural properties
        for prop_i, dtype_i in self.structure_properties_dtype.items():

            if prop_i in properties:
                columns += [prop_i]
                if properties.get(prop_i) is None:
                    values += [None]
                elif utils.is_array_like(properties.get(prop_i)):
                    values += [self.blob(
                        np.array(properties.get(prop_i), dtype=dtype_i))]
                else:
                    values += [dtype_i(properties.get(prop_i))]

        # Reference properties
        for prop_i in self.metadata.get('load_properties'):

            if (
                    prop_i in properties
                    and prop_i not in self.structure_properties_dtype
            ):

                columns += [prop_i]
                if properties.get(prop_i) is None:
                    values += [None]
                elif utils.is_array_like(properties.get(prop_i)):
                    values += [self.blob(
                        np.array(
                            properties.get(prop_i),
                            dtype=self.properties_numpy_dtype))]
                else:
                    values += [self.properties_numpy_dtype(
                        properties.get(prop_i))]

        # Convert values to tuple
        columns = tuple(columns)
        values = tuple(values)

        # Add or update database values
        row_id = self._update(row_id, values=values, columns=columns)

        return row_id

    @lock
    def _update(
        self,
        row_id: int,
        values: Optional[Any] = None,
        columns: Optional[List[str]] = None,
        properties: Optional[Any] = None,
    ) -> int:

        if values is None and properties is None:

            raise SyntaxError(
                ""At least one input 'values' or 'properties' should ""
                + ""contain reference data!"")

        elif values is None:

            row_id = self._write(properties, row_id)

        else:

            # Add or update database values
            with self.managed_connection() as con:

                # Get SQL cursor
                cur = con.cursor()

                # Update values in database
                q = ', '.join([f'{column:s} = ?' for column in columns])
                cur.execute(
                    f""UPDATE systems SET {q} WHERE id=?"",
                    values + (row_id,))

        return row_id

    def get_last_id(
        self,
        cur: object,
    ) -> int:

        # Select last seqeuence  number from database
        cur.execute('SELECT seq FROM sqlite_sequence WHERE name=""systems""')

        # Get next row id
        result = cur.fetchone()
        if result is not None:
            row_id = result[0]
            return row_id
        else:
            return 0

    def _select(
        self,
        cmps: List[Tuple[str]],
        verbose=False,
    ) -> Dict[str, Any]:

        sql, args = self.create_select_statement(cmps)
        metadata = self._get_metadata()
        with self.managed_connection() as con:

            # Execute SQL request
            cur = con.cursor()
            cur.execute(sql, args)

            for row in cur.fetchall():

                yield self.convert_row(row, metadata, verbose=verbose)

    def create_select_statement(
        self,
        cmps: List[Tuple[str]],
        what: Optional[str] = 'systems.*',
    ) -> (str, List[int]):
        """"""
        Translate selection to SQL statement.
        """"""

        tables = ['systems']
        where = []
        args = []

        # Prepare SQL statement
        for key, op, value in cmps:
            where.append('systems.{}{}?'.format(key, op))
            args.append(value)

        # Create SQL statement
        sql = ""SELECT {} FROM\n  "".format(what) + "", "".join(tables)
        if where:
            sql += ""\n  WHERE\n  "" + "" AND\n  "".join(where)

        return sql, args

    def convert_row(
        self,
        row: List[Any],
        metadata: Dict[str, Any],
        verbose: Optional[bool] = False,
    ) -> Dict[str, Any]:

        # Convert reference properties to a dictionary
        properties = {}

        # Add database information
        if verbose:

            # Get row id
            properties[""row_id""] = row[0]

            # Get modification date
            properties[""mtime""] = row[1]

            # Get username
            properties[""user""] = row[2]

        # Structural properties
        Np = 3
        for prop_i, dtype_i in self.structure_properties_dtype.items():
            if row[Np] is None:
                properties[prop_i] = None
            elif isinstance(row[Np], bytes):
                properties[prop_i] = torch.from_numpy(
                    self.deblob(
                        row[Np],
                        dtype=dtype_i,
                        shape=self.structure_properties_shape[prop_i]
                    ).copy())
            else:
                properties[prop_i] = torch.reshape(
                    torch.tensor(row[Np], dtype=dtype_i),
                    self.structure_properties_shape[prop_i])

            Np += 1

        for prop_i in metadata.get('load_properties'):

            if prop_i in self.structure_properties_dtype:
                continue

            if row[Np] is None:
                properties[prop_i] = None
            elif isinstance(row[Np], bytes):
                properties[prop_i] = torch.from_numpy(
                    self.deblob(
                        row[Np], dtype=self.properties_numpy_dtype).copy()
                    ).to(self.properties_torch_dtype)
            else:
                properties[prop_i] = torch.tensor(
                    row[Np], dtype=self.properties_torch_dtype)

            if prop_i in reference_properties_shape:
                properties[prop_i] = torch.reshape(
                    properties[prop_i],
                    reference_properties_shape[prop_i])
            elif 'std_' in prop_i and prop_i[4:] in reference_properties_shape:
                properties[prop_i] = torch.reshape(
                    properties[prop_i],
                    reference_properties_shape[prop_i[4:]])

            Np += 1

        return properties

    def _count(self, selection, **kwargs):

        # Check selection
        cmps = self.parse_selection(selection)

        sql, args = self.create_select_statement(cmps, what='COUNT(*)')

        with self.managed_connection() as con:
            cur = con.cursor()
            try:
                cur.execute(sql, args)
                return cur.fetchone()[0]
            except sqlite3.OperationalError:
                return 0

    @parallel_function
    @lock
    def delete(
        self,
        row_ids: Union[int, List[int]],
    ):
        """"""
        Delete rows.

        Parameters
        ----------
        row_ids: int or list of int
            Row index or list of indices to delete
        """"""

        if not len(row_ids):
            return

        self._delete(row_ids)
        self.vacuum()

        return

    def _delete(
        self,
        row_ids: Union[int, List[int]]
    ):

        with self.managed_connection() as con:
            cur = con.cursor()
            selection = ', '.join([str(row_id) for row_id in row_ids])
            for table in all_tables[::-1]:
                cur.execute(
                    f""DELETE FROM {table} WHERE id in ({selection});"")

        return

    def vacuum(self):
        """"""
        Execute SQL command 'Vacuum' (?)
        """"""

        with self.managed_connection() as con:
            con.commit()
            con.cursor().execute(""VACUUM"")

        return

    def _delete_file(self):
        """"""
        Delete database file
        """"""
        if os.path.exists(self.data_file):
            os.remove(self.data_file)
        if os.path.exists(self.lock_file):
            os.remove(self.lock_file)
        return

    @property
    def metadata(self):
        return self._get_metadata()",./Asparagus/asparagus/data/database_sqlite3.py
DataReader,"class DataReader():
    """"""
    DataReader class that contain functions to read and adapt data.

    Parameters
    ----------
    data_file: (str, list(str))
        File path or a tuple of file path and file format label of data
        source to file.
    data_properties: List(str), optional, default None
        Subset of properties to load to dataset file
    data_unit_properties: dict, optional, default None
        Dictionary from properties (keys) to corresponding unit as a string
        (item).
    data_alt_property_labels: dict, optional, default None
        Dictionary of alternative property labeling to replace non-valid
        property labels with the valid one if possible.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    def __init__(
        self,
        data_file: Optional[str] = None,
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
    ):
        """"""
        Initialize DataReader.

        """"""

        # Assign data file and format
        if utils.is_string(data_file):
            self.data_file = (
                data_file, data.check_data_format(
                    data_file, is_source_format=False))
        else:
            self.data_file = data_file

        # Get reference database connect function
        self.connect = data.get_connect(self.data_file[1])

        # Assign property list and units
        self.data_properties = data_properties
        self.data_unit_properties = data_unit_properties

        # Check alternative property labels
        if data_alt_property_labels is None:
            self.data_alt_property_labels = settings._alt_property_labels
        else:
            self.data_alt_property_labels = data_alt_property_labels

        # Default property labels
        self.default_property_labels = [
            'atoms_number', 'atomic_numbers', 'cell', 'pbc']

        # Assign dataset format with respective load function
        self.data_file_format_load = {
            'db.sql':   self.load_db,
            'db.npz':   self.load_db,
            'db.h5':    self.load_db,
            'npz':      self.load_npz,
            'ase.db':   self.load_ase,
            'ase.traj': self.load_traj,
            }

        # Assign dataset format with respective metadata load function
        self.data_file_format_get_metadata = {
            'db.sql':   self.get_db_metadata,
            'db.npz':   self.get_db_metadata,
            'db.h5':    self.get_db_metadata,
            }

        return

    def load(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
        data_source_unit_properties: Optional[Dict[str, str]] = None,
        **kwargs,
    ):
        """"""
        Load data from respective dataset format.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'positions': 'Ang',
                                    'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.
        data_source_unit_properties: dictionary, optional, default None
            Dictionary from properties (keys) to corresponding unit as a 
            string (item) in the source data files.

        """"""

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Load data from source of respective format
        if self.data_file_format_load.get(data_source[1]) is None:
            raise SyntaxError(
                f""Data format '{data_source[1]:s}' of data source file ""
                + f""'{data_source[0]:s}' is unknown!\n""
                + ""Supported data formats are: ""
                + f""{self.data_file_format_load.keys()}""
                )
        else:
            _ = self.data_file_format_load[data_source[1]](
                data_source,
                data_properties=data_properties,
                data_unit_properties=data_unit_properties,
                data_alt_property_labels=data_alt_property_labels,
                data_source_unit_properties=data_source_unit_properties,
                **kwargs)

        return

    def get_metadata(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """"""
        Load data from respective dataset format.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'positions': 'Ang',
                                    'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        dict(str, any)
            Source database file metadata

        """"""

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Load data from source of respective format
        if self.data_file_format_get_metadata.get(data_source[1]) is None:
            metadata = {}
        else:
            metadata = self.data_file_format_get_metadata[data_source[1]](
                data_source,
                data_properties=data_properties,
                data_unit_properties=data_unit_properties,
                data_alt_property_labels=data_alt_property_labels)

        return metadata

    def load_db(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_skip_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, str]] = None,
        **kwargs,
    ) -> Union[str, List[Dict[str, Any]]]:
        """"""
        Load data from asparagus dataset format.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file.
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_skip_properties: List(str), optional, default None
            Properties not to load even if in data source.
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        (str, list(dict(str, any)))
            Either data file path or list of source properties if data file is
            not defined.

        """"""

        # Check if data source is empty
        if os.path.isfile(data_source[0]):
            with data.connect(data_source[0], data_source[1], mode='r') as db:
                Ndata = db.count()
        else:
            Ndata = 0
        if Ndata == 0:
            self.logger.warning(
                f""Data source '{data_source[0]:s}' is empty!"")
            return

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels
        if data_skip_properties is None:
            data_skip_properties = []

        # Get data sample property labels for label to comparison
        with data.connect(data_source[0], data_source[1], mode='r') as db:
            source_properties = db.get(1)[0].keys()

        # Assign data source property labels to valid property labels.
        assigned_properties = self.assign_property_labels(
            data_source,
            source_properties,
            data_properties,
            data_skip_properties,
            data_alt_property_labels)

        # Get source property units
        with data.connect(data_source[0], data_source[1], mode='r') as db:
            source_unit_properties = db.get_metadata()['unit_properties']

        # Get property unit conversion
        unit_conversion = self.get_unit_conversion(
            assigned_properties,
            data_unit_properties,
            source_unit_properties,
        )

        # Property match summary
        self.print_property_summary(
            data_source,
            assigned_properties,
            unit_conversion,
            data_unit_properties,
            source_unit_properties,
        )

        # If not dataset file is given, load source data to memory
        if self.data_file is None:

            # Add atoms systems to list
            all_atoms_properties = []
            self.logger.info(
                f""Load {Ndata} data point from '{data_source[0]:s}'!"")

            # Open source dataset
            with data.connect(
                data_source[0], data_source[1], mode='r'
            ) as db_source:

                # Iterate over source data
                for idx in range(Ndata):

                    # Get property source data
                    source = db_source.get(idx + 1)[0]

                    # Collect system data
                    atoms_properties = self.collect_from_source(
                        source,
                        unit_conversion,
                        data_properties,
                        assigned_properties)

                    # Add system data to database
                    all_atoms_properties.append(atoms_properties)

        # If dataset file is given, write to dataset
        else:

            # Add atom systems to database
            all_atoms_properties = self.data_file[0]
            with self.connect(self.data_file[0], mode='a') as db:

                self.logger.info(
                    f""Writing '{data_source[0]:s}' to database "" +
                    f""'{self.data_file[0]:s}'!\n"" +
                    f""{Ndata} data point will be added."")

                # Open source dataset
                with data.connect(
                    data_source[0], data_source[1], mode='r'
                ) as db_source:

                    # Iterate over source data
                    for idx in range(Ndata):

                        # Get atoms object and property data
                        source = db_source.get(idx + 1)[0]

                        # Collect system data
                        atoms_properties = self.collect_from_source(
                            source,
                            unit_conversion,
                            data_properties,
                            assigned_properties)

                        # Write to reference database file
                        db.write(properties=atoms_properties)

        # Print completion message
        self.logger.info(
            f""Loading from Asparagus dataset '{data_source[0]:s}' ""
            + ""complete!"")

        return all_atoms_properties

    def load_ase(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_skip_properties: Optional[List[str]] = None,
        data_alt_property_labels: Optional[Dict[str, str]] = None,
        **kwargs,
    ) -> Union[str, List[Dict[str, Any]]]:
        """"""
        Load data from ASE database formats.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file.
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_skip_properties: List(str), optional, default None
            Properties not to load even if in data source.
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        (str, list(dict(str, any)))
            Either data file path or list of source properties if data file is
            not defined.

        """"""

        # Check if data source is empty
        if os.path.isfile(data_source[0]):
            with data.connect(data_source[0], data_source[1], mode='r') as db:
                Ndata = db.count()
        else:
            Ndata = 0
        if Ndata == 0:
            self.logger.warning(
                f""Data source '{data_source[0]:s}' is empty!"")
            return

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Check list of properties to skip
        default_skip_properties = [
            'atoms_number', 'atomic_numbers', 'cell', 'pbc']
        if data_skip_properties is None:
            data_skip_properties = default_skip_properties
        else:
            for prop in default_skip_properties:
                if prop not in data_skip_properties:
                    data_skip_properties.append(prop)

        # Get data sample property labels for label to comparison
        with data.connect(data_source[0], data_source[1], mode='r') as db:
            source_properties = db.get(1)[0].keys()

        # Assign data source property labels to valid property labels.
        assigned_properties = self.assign_property_labels(
            data_source,
            source_properties,
            data_properties,
            data_skip_properties,
            data_alt_property_labels)

        # Get source property units - default ASE units
        source_unit_properties = settings._ase_units

        # Get property unit conversion
        unit_conversion = self.get_unit_conversion(
            assigned_properties,
            data_unit_properties,
            source_unit_properties,
        )

        # Property match summary
        self.print_property_summary(
            data_source,
            assigned_properties,
            unit_conversion,
            data_unit_properties,
            source_unit_properties,
        )

        # If not dataset file is given, load source data to memory
        if self.data_file is None:

            # Add atoms systems to list
            all_atoms_properties = []
            self.logger.info(
                f""Load {Ndata:d} data point from '{data_source[0]:s}'!"")

            # Open source dataset
            with ase_db.connect(data_source[0]) as db_source:

                # Iterate over source data
                for idx in range(Ndata):

                    # Get atoms object and property data
                    atoms = db_source.get_atoms(idx + 1)
                    source = db_source.get(idx + 1)

                    # Collect system data
                    atoms_properties = self.collect_from_atoms_source(
                        atoms,
                        source,
                        unit_conversion,
                        data_properties,
                        assigned_properties)

                    # Add atoms system data
                    all_atoms_properties.append(atoms_properties)

        # If dataset file is given, write to dataset
        else:

            # Add atom systems to database
            all_atoms_properties = self.data_file[0]
            with self.connect(self.data_file[0], mode='a') as db:

                self.logger.info(
                    f""Writing '{data_source[0]}' to database "" +
                    f""'{self.data_file[0]}'!\n"" +
                    f""{Ndata} data point will be added."")

                # Open source dataset
                with ase_db.connect(data_source[0]) as db_source:

                    # Iterate over source data
                    for idx in range(Ndata):

                        # Get atoms object and property data
                        atoms = db_source.get_atoms(idx + 1)
                        source = db_source.get(idx + 1)

                        # Collect system data
                        atoms_properties = self.collect_from_atoms_source(
                            atoms,
                            source,
                            unit_conversion,
                            data_properties,
                            assigned_properties)

                        # Write to reference database file
                        db.write(properties=atoms_properties)

        # Print completion message
        self.logger.info(
            f""Loading from ASE database '{data_source[0]}' complete!"")

        return all_atoms_properties

    def load_npz(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_skip_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, str]] = None,
        data_source_unit_properties: Optional[Dict[str, str]] = None,
        **kwargs,
    ):
        """"""
        Load data from npz dataset format.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file.
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_skip_properties: List(str), optional, default None
            Properties not to load even if in data source.
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.
        data_source_unit_properties: dictionary, optional, default None
            Dictionary from properties (keys) to corresponding unit as a 
            string (item) in the source data files.

        Returns
        -------
        (str, list(dict(str, any)))
            Either data file path or list of source properties if data file is
            not defined.

        """"""

        # Check if data source is empty
        if os.path.isfile(data_source[0]):
            source = np.load(data_source[0])
            Ndata = len(source.keys())
        else:
            source = None
            Ndata = 0
        if Ndata == 0:
            self.logger.warning(
                f""Data source '{data_source[0]:s}' is empty!"")
            return

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels
        if data_skip_properties is None:
            data_skip_properties = []

        # Get data sample property labels for label to comparison
        source_properties = source.keys()

        # Assign data source property labels to valid property labels.
        assigned_properties = self.assign_property_labels(
            data_source,
            source_properties,
            data_properties,
            data_skip_properties,
            data_alt_property_labels)

        # Initialize source data dictionary and assign properties
        source_dict = {}

        # Atom numbers
        if 'atoms_number' in assigned_properties:
            source_dict['atoms_number'] = (
                source[assigned_properties['atoms_number']])
            Ndata = len(source_dict['atoms_number'])
        else:
            raise ValueError(
                ""Property 'atoms_number' not found in npz dataset ""
                + f""'{self.data_file[0]}'!\n"")

        # Atomic number
        if 'atomic_numbers' in assigned_properties:
            source_dict['atomic_numbers'] = (
                source[assigned_properties['atomic_numbers']])
        else:
            raise ValueError(
                ""Property 'atomic_numbers' not found in npz dataset ""
                + f""'{self.data_file[0]}'!\n"")

        # Atom positions
        if 'positions' in assigned_properties:
            source_dict['positions'] = source[assigned_properties['positions']]
        else:
            raise ValueError(
                ""Property 'positions' not found in npz dataset ""
                + f""'{self.data_file[0]}'!\n"")

        # Total atoms charge
        if 'charge' in assigned_properties:
            source_dict['charge'] = source[assigned_properties['charge']]
        else:
            source_dict['charge'] = np.zeros(Ndata, dtype=float)
            self.logger.warning(
                ""Property 'charge' not found in npz dataset ""
                + f""'{self.data_file[0]}'!\nCharges are assumed to be zero."")

        # Cell information
        if 'cell' in assigned_properties:
            source_dict['cell'] = source[assigned_properties['cell']]
        else:
            source_dict['cell'] = np.zeros((Ndata, 3), dtype=float)
            self.logger.info(
                ""No cell information in npz dataset ""
                + f""'{self.data_file[0]}'!"")

        # PBC information
        if 'pbc' in assigned_properties:
            source_dict['pbc'] = source[assigned_properties['pbc']]
        else:
            source_dict['pbc'] = np.zeros((Ndata, 3), dtype=bool)
            self.logger.info(
                ""No pbc information in npz dataset ""
                + f""'{self.data_file[0]}'!"")

        # Check if all properties in 'data_properties' are found
        found_properties = [
            prop in assigned_properties.keys()
            for prop in data_properties]
        for ip, prop in enumerate(data_properties):
            if not found_properties[ip]:
                self.logger.error(
                    f""Requested property '{prop:s}' in ""
                    + ""'data_properties' is not found in Numpy ""
                    + f""dataset '{data_source[0]}'!"")
        if not all(found_properties):
            raise ValueError(
                ""Not all properties in 'data_properties' are found ""
                + f""in Numpy dataset '{data_source[0]}'!\n"")

        # Get property unit conversion
        unit_conversion = self.get_unit_conversion(
            assigned_properties,
            data_unit_properties,
            data_source_unit_properties,
            )

        # Property match summary
        self.print_property_summary(
            data_source,
            assigned_properties,
            unit_conversion,
            data_unit_properties,
            data_source_unit_properties,
        )

        # Collect properties from source
        for prop, item in assigned_properties.items():
            if prop in self.default_property_labels:
                continue
            if prop in data_properties:
                try:
                    source_dict[prop] = source[item]
                except ValueError:
                    raise ValueError(
                        f""Property '{prop:s}' from the npz entry ""
                        + f""'{item:s}' could not be loaded!"")

        # If not dataset file is given, load source data to memory
        if self.data_file is None:

            # Add atoms systems to list
            self.logger.info(
                f""Load {Ndata:d} data point from '{data_source[0]:s}'!"")
            all_atoms_properties = self.collect_from_source_list(
                source_dict,
                unit_conversion,
                data_properties,
                assigned_properties)

        # If dataset file is given, write to dataset
        else:

            # Add atom systems to database
            self.logger.info(
                f""Writing '{data_source[0]:s}' to database ""
                + f""'{self.data_file[0]:s}'!\n""
                + f""{Ndata:d} data point will be added."")
            all_atoms_properties = self.collect_from_source_list(
                source_dict,
                unit_conversion,
                data_properties,
                assigned_properties)

            # Write to reference database file
            with self.connect(self.data_file[0], mode='a') as db:

                for idx in range(Ndata):
                    db.write(properties=all_atoms_properties[idx])

            # Reassign data file path as usual when written to database
            all_atoms_properties = self.data_file[0]

        # Print completion message
        self.logger.info(
            f""Loading from npz database '{data_source[0]}' complete!"")

        return all_atoms_properties

    def load_traj(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_skip_properties: Optional[List[str]] = None,
        data_alt_property_labels: Optional[Dict[str, str]] = None,
        **kwargs,
    ):
        """"""
        Load data from ASE trajectory file.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file.
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_skip_properties: List(str), optional, default None
            Properties not to load even if in data source.
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        (str, list(dict(str, any)))
            Either data file path or list of source properties if data file is
            not defined.

        """"""

        # Check if data source is empty
        if os.path.isfile(data_source[0]):
            source = ase.io.Trajectory(data_source[0])
            Ndata = len(source)
        else:
            source = None
            Ndata = 0
        if Ndata == 0:
            self.logger.warning(
                f""Data source '{data_source[0]:s}' is empty!"")
            return

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Check list of properties to skip
        default_skip_properties = [
            'atoms_number', 'atomic_numbers', 'cell', 'pbc']
        if data_skip_properties is None:
            data_skip_properties = default_skip_properties
        else:
            for prop in default_skip_properties:
                if prop not in data_skip_properties:
                    data_skip_properties.append(prop)

        # Get data sample to compare property labels
        data_sample = source[0]

        # Check if data source has properties
        if data_sample.calc is None:
            self.logger.warning(
                f""Data source '{data_source[0]:s}' has no properties!"")
            return

        # Check if data source has properties
        if source[0].calc is None:
            self.logger.warning(
                f""Data source '{data_source[0]:s}' has no properties!"")
            return

        # Get data sample property labels for label to comparison
        source_properties = ['positions', 'charge']
        source_properties += list(data_sample.calc.results)

        # Assign data source property labels to valid property labels.
        assigned_properties = self.assign_property_labels(
            data_source,
            source_properties,
            data_properties,
            data_skip_properties,
            data_alt_property_labels)

        # Get source property units - default ASE units
        source_unit_properties = settings._ase_units

        # Get property unit conversion
        unit_conversion = self.get_unit_conversion(
            assigned_properties,
            data_unit_properties,
            source_unit_properties,
        )

        # Property match summary
        self.print_property_summary(
            data_source,
            assigned_properties,
            unit_conversion,
            data_unit_properties,
            source_unit_properties,
        )

        # If not dataset file is given, load source data to memory
        if self.data_file is None:

            # Add atoms systems to list
            all_atoms_properties = []
            self.logger.info(
                f""Load {Ndata:d} data point from '{data_source[0]:s}'!"")

            # Iterate over source data
            for idx in range(Ndata):

                # Atoms system data
                atoms_properties = {}

                # Get atoms object and property data
                atoms = source[idx]

                # Collect system data
                atoms_properties = self.collect_from_atoms(
                    atoms,
                    unit_conversion,
                    data_properties,
                    assigned_properties)

                # Add atoms system data
                all_atoms_properties.append(atoms_properties)

        # If dataset file is given, write to dataset
        else:

            # Add atom systems to database
            all_atoms_properties = self.data_file
            with self.connect(self.data_file[0], mode='a') as db:

                self.logger.info(
                    f""Writing '{data_source[0]}' to database "" +
                    f""'{self.data_file[0]}'!\n"" +
                    f""{Ndata} data point will be added."")

                # Iterate over source data
                for idx in range(Ndata):

                    # Atoms system data
                    atoms_properties = {}

                    # Get atoms object and property data
                    atoms = source[idx]

                    # Collect system data
                    atoms_properties = self.collect_from_atoms(
                        atoms,
                        unit_conversion,
                        data_properties,
                        assigned_properties)

                    # Write to reference database file
                    db.write(properties=atoms_properties)

        # Print completion message
        self.logger.info(
            f""Loading from ASE trajectory '{data_source[0]:s}' complete!"")

        return all_atoms_properties

    def load_atoms(
        self,
        atoms: object,
        atoms_properties: Dict[str, Any],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_skip_properties: Optional[List[str]] = None,
        data_alt_property_labels: Optional[Dict[str, str]] = None,
    ) -> Union[str, List[Dict[str, Any]]]:
        """"""
        Load atoms object with properties to dataset format.

        Parameters
        ----------
        atoms: ASE Atoms object
            ASE Atoms object with conformation belonging to the properties.
        atoms_properties: dict
            Atoms object properties
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_skip_properties: List(str), optional, default None
            Properties not to load even if in data source.
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        (str, list(dict(str, any)))
            Either data file path or list of source properties if data file is
            not defined.

        """"""

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Check list of properties to skip
        default_skip_properties = [
            'atoms_number', 'atomic_numbers', 'cell', 'pbc']
        if data_skip_properties is None:
            data_skip_properties = default_skip_properties
        else:
            for prop in default_skip_properties:
                if prop not in data_skip_properties:
                    data_skip_properties.append(prop)

        # Get data sample property labels for label to comparison
        source_properties = atoms_properties.keys()

        # Assign data source property labels to valid property labels.
        assigned_properties = self.assign_property_labels(
            ('ase.Atoms', None),
            source_properties,
            data_properties,
            data_skip_properties,
            data_alt_property_labels)

        # Get source property units - default ASE units
        source_unit_properties = settings._ase_units

        # Get property unit conversion
        unit_conversion = self.get_unit_conversion(
            assigned_properties,
            data_unit_properties,
            source_unit_properties,
        )

        # If not dataset file is given, load data to memory
        if self.data_file is None:

            # Atoms system data
            load_properties = {}

            # Fundamental properties
            load_properties['atoms_number'] = (
                atoms.get_global_number_of_atoms())
            load_properties['atomic_numbers'] = (
                atoms.get_atomic_numbers())
            load_properties['positions'] = (
                unit_conversion['positions']*atoms.get_positions())
            load_properties['cell'] = (
                unit_conversion['positions']*self.convert_cell(
                    atoms.get_cell()[:]))
            load_properties['pbc'] = atoms.get_pbc()
            if atoms_properties.get('charge') is None:
                load_properties['charge'] = 0.0
            else:
                load_properties['charge'] = atoms_properties['charge']

            # Collect properties
            for prop, item in atoms_properties.items():
                load_properties[prop] = (
                    unit_conversion[prop]*item)

        # If dataset file is given, write to dataset
        else:

            # Atoms system data
            load_properties = {}
            with self.connect(self.data_file[0], mode='a') as db:

                # Fundamental properties
                load_properties['atoms_number'] = (
                    atoms.get_global_number_of_atoms())
                load_properties['atomic_numbers'] = (
                    atoms.get_atomic_numbers())
                load_properties['positions'] = (
                    unit_conversion['positions']*atoms.get_positions())
                load_properties['cell'] = (
                    unit_conversion['positions']*self.convert_cell(
                        atoms.get_cell()[:]))
                load_properties['pbc'] = atoms.get_pbc()
                if atoms_properties.get('charge') is None:
                    load_properties['charge'] = 0.0
                else:
                    load_properties['charge'] = atoms_properties['charge']

                # Collect properties
                for prop, item in atoms_properties.items():
                    if prop in data_properties:
                        load_properties[prop] = (
                            unit_conversion[prop]*item)

                # Write to ASE database file
                db.write(properties=load_properties)

        return load_properties

    def assign_property_labels(
        self,
        data_source: List[str],
        source_properties: List[str],
        data_properties: List[str],
        skip_properties: List[str],
        data_alt_property_labels: Dict[str, List[str]],
    ) -> Dict[str, str]:
        """"""
        Assign data source property labels to valid property labels.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file
        source_properties: List(str)
            Properties list of source data
        data_properties: List(str)
            Subset of properties to load
        skip_properties: List(str)
            Properties not to load even if in source.
        data_alt_property_labels: dict
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        dict(str, str)
            Assigned data property label (key) to source label (item)

        """"""

        # Assign data source property labels to valid labels.
        assigned_properties = {}
        for source_label in source_properties:

            # Check source label for prefix
            if 'std_' in source_label:
                label = source_label[4:]
            else:
                label = source_label

            # Skip system properties, which are read otherwise
            if label in skip_properties:
                continue

            match, modified, valid_label = utils.check_property_label(
                label,
                valid_property_labels=settings._valid_properties,
                alt_property_labels=data_alt_property_labels)
            if match:
                if 'std_' in source_label:
                    valid_label = f""std_{valid_label:s}""
                if modified:
                    self.logger.warning(
                        f""Property key '{source_label:s}' in database ""
                        + f""'{data_source[0]:s}' is not a valid label!\n""
                        + f""Property key '{source_label:s}' is assigned as ""
                        + f""'{valid_label:s}'."")
                assigned_properties[valid_label] = label
            else:
                self.logger.warning(
                    f""Unknown property '{source_label:s}' in ""
                    + f""database '{data_source[0]:s}'!\nProperty ignored."")

        # Check if all properties in 'data_properties' are found
        found_properties = [
            prop in assigned_properties
            for prop in data_properties]
        for ip, prop in enumerate(data_properties):
            if not found_properties[ip]:
                self.logger.error(
                    f""Requested property '{prop}' in ""
                    + ""'data_properties' is not found in ""
                    + f""database '{data_source[0]:s}'!"")
        if not all(found_properties):
            raise ValueError(
                ""Not all properties in 'data_properties' are found ""
                + f""in database '{data_source[0]:s}'!"")

        return assigned_properties

    def get_unit_conversion(
        self,
        data_properties: Dict[str, str],
        data_unit_properties: Dict[str, float],
        source_unit_properties: Dict[str, float],
    ) -> Dict[str, float]:
        """"""
        Assign source property to data property unit conversion factors.

        Parameters
        ----------
        data_properties: dict(str, str)
            Property labels dictionary from data file (key) and data source
            (item)
        data_unit_properties: dict
            Dictionary from data properties (keys) to corresponding unit as a
            string (item)
        source_unit_properties: dict
            Dictionary from source properties (keys) to corresponding unit as a
            string (item)

        Returns
        -------
        dict(str, str)
            Assigned source to data property unit conversion factor

        """"""

        # Property match summary and unit conversion
        if data_unit_properties is None or source_unit_properties is None:

            # Set default conversion factor dictionary
            unit_conversion = {}
            for prop in data_properties.keys():
                unit_conversion[prop] = 1.0

        else:

            # Check units of positions and properties
            unit_conversion = {}
            for data_prop in data_properties.keys():

                # Check for property prefix
                if 'std_' in data_prop:
                    prop = data_prop[4:]
                else:
                    prop = data_prop

                if source_unit_properties.get(prop) is None:
                    source_unit_property = None
                else:
                    source_unit_property = source_unit_properties.get(prop)
                if (
                    data_unit_properties.get(data_prop) is None
                    and data_unit_properties.get(prop) is None
                ):
                    data_unit_property = None
                elif data_unit_properties.get(prop) is None:
                    data_unit_property = data_unit_properties.get(data_prop)
                else:
                    data_unit_property = data_unit_properties.get(prop)

                unit_conversion[data_prop], _ = (
                    utils.check_units(
                        data_unit_property,
                        source_unit_property)
                    )

        return unit_conversion

    def print_property_summary(
        self,
        data_source,
        assigned_properties,
        unit_conversion,
        data_unit_properties,
        source_unit_properties,
    ):
        """"""
        Print property match summary

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file
        assigned_properties: dict
            Assigned data property label (key) to source label (item)
        unit_conversion: dict
            Assigned source to data property unit conversion factor
        data_unit_properties: dict
            Dictionary from data properties (keys) to corresponding unit as a
            string (item)
        source_unit_properties: dict
            Dictionary from source properties (keys) to corresponding unit as a
            string (item)

        """"""

        # Prepare header
        message = (
            ""Property assignment from database ""
            + f""'{data_source[0]:s}'!\n""
            + f"" {'Load':4s} |""
            + f"" {'Property Label':<14s} |""
            + f"" {'Data Unit':<14s} |""
            + f"" {'Source Label':<14s} |""
            + f"" {'Source Unit':<14s} |""
            + f"" {'Conversion Fac.':<14s}\n""
            + ""-""*(7 + 17*5)
            + ""\n"")

        # Iterate over properties
        for data_prop, source_prop in assigned_properties.items():

            # # Skip default properties
            # if data_prop in self.default_property_labels:
            #     continue

            # Check property labels
            if source_unit_properties is None:
                source_unit_property = ""None""
            elif (
                source_unit_properties.get(data_prop) is None
                and source_unit_properties.get(source_prop) is None
            ):
                source_unit_property = ""None""
            elif source_unit_properties.get(data_prop) is not None:
                source_unit_property = source_unit_properties.get(data_prop)
            elif source_unit_properties.get(source_prop) is not None:
                source_unit_property = source_unit_properties.get(source_prop)
            else:
                source_unit_property = ""None""

            if (
                data_unit_properties is None or
                data_unit_properties.get(data_prop) is None
            ):
                data_unit_property = ""None""
            else:
                data_unit_property = data_unit_properties.get(data_prop)

            if (
                data_prop in unit_conversion
                or data_prop in self.default_property_labels
                or data_prop in ['positions', 'charge']
            ):
                load_label = "" x  ""
            else:
                load_label = ""    ""

            message += (
                f"" {load_label:4s} |""
                + f"" {data_prop:<14s} |""
                + f"" {data_unit_property:<14s} |""
                + f"" {source_prop:<14s} |""
                + f"" {source_unit_property:<14s} |""
                + f"" {unit_conversion[data_prop]:11.9e}\n""
                )

        # Print property information
        self.logger.info(message)

        return

    def collect_from_source(
        self,
        source: Dict[str, Any],
        conversion: Dict[str, float],
        load_properties: List[str],
        property_labels: Dict[str, str],
    ) -> Dict[str, Any]:
        """"""
        Collect properties from database entry to property dictionary and
        apply unit conversion.

        Parameters
        ----------
        source: dict
            Database source dictionary
        conversion: dict
            Unit conversion factors dictionary.
        load_properties: list(str)
            Properties to load from source
        property_labels: dict(str, str), optional, default None
            List of additional source properties (key) to add from property
            source label (item). If None, all properties in source are added.

        Returns
        -------
        dict(str, any)
            System property dictionary

        """"""

        # Atoms system data
        atoms_properties = {}

        # Fundamental properties
        atoms_properties['atoms_number'] = source['atoms_number']
        atoms_properties['atomic_numbers'] = (
            source['atomic_numbers'])
        atoms_properties['positions'] = (
            conversion['positions']*source['positions'])
        atoms_properties['cell'] = (
            conversion['positions']*self.convert_cell(source['cell']))
        atoms_properties['pbc'] = source['pbc']
        if source.get('charge') is None:
            atoms_properties['charge'] = 0.0
        else:
            atoms_properties['charge'] = source['charge']
        if source.get('fragments') is not None:
            atoms_properties['fragments'] = source['fragments']

        # Collect properties
        for prop, item in property_labels.items():
            if prop in load_properties:
                atoms_properties[prop] = (
                    conversion[prop]*source[item])

        return atoms_properties

    def collect_from_source_list(
        self,
        source: Dict[str, List[Any]],
        conversion: Dict[str, float],
        load_properties: List[str],
        property_labels: Dict[str, str],
    ) -> Dict[str, Any]:
        """"""
        Collect properties from complete data dictionary to property
        dictionaries and apply unit conversion.

        Parameters
        ----------
        source: dict
            Database source dictionary
        conversion: dict
            Unit conversion factors dictionary.
        load_properties: list(str)
            Properties to load from source
        property_labels: dict(str, str), optional, default None
            List of additional source properties (key) to add from property
            source label (item). If None, all properties in source are added.

        Returns
        -------
        dict(str, any)
            System property dictionary

        """"""

        # Initialize data list
        all_atoms_properties = []

        # Get Number of data entries and max atom number
        Ndata = source['atoms_number'].shape[0]
        max_atoms_number = source['atomic_numbers'].shape[1]

        # Iterate over all data entries
        for idx in range(Ndata):

            # Atoms system data
            atoms_properties = {}

            # Fundamental properties
            atoms_properties['atoms_number'] = source['atoms_number'][idx]
            atoms_properties['atomic_numbers'] = (
                source['atomic_numbers'][idx][:source['atoms_number'][idx]])
            atoms_properties['positions'] = (
                conversion['positions']
                * source['positions'][idx][:source['atoms_number'][idx]])
            atoms_properties['cell'] = (
                conversion['positions']*self.convert_cell(source['cell'][idx]))
            atoms_properties['pbc'] = source['pbc'][idx]
            if source.get('charge') is None:
                atoms_properties['charge'] = 0.0
            else:
                atoms_properties['charge'] = source['charge'][idx]
            if source.get('fragments') is not None:
                atoms_properties['fragments'] = (
                    source['fragments'][idx][:source['atoms_number'][idx]])

            # Collect properties
            for prop, item in source.items():
                if (
                    prop in load_properties
                    and item[idx].shape
                    and source['atoms_number'][idx] != max_atoms_number
                    and item[idx].shape[0] == max_atoms_number
                    and np.all(item[idx][source['atoms_number'][idx]:] == 0.0)
                ):
                    atoms_properties[prop] = (
                        conversion[prop]
                        * item[idx][:source['atoms_number'][idx]])
                elif prop in load_properties:
                    atoms_properties[prop] = conversion[prop]*item[idx]

            # Add atoms system data
            all_atoms_properties.append(atoms_properties)

        return all_atoms_properties

    def collect_from_atoms_source(
        self,
        atoms: ase.Atoms,
        source: Dict[str, Any],
        conversion: Dict[str, float],
        load_properties: List[str],
        property_labels: Dict[str, str] = None,
    ) -> Dict[str, Any]:
        """"""
        Collect properties from database entry to property dictionary and
        apply unit conversion.

        Parameters
        ----------
        atoms: ase.Atoms
            Database atoms object
        source: dict
            Database source dictionary
        conversion: dict
            Unit conversion factors dictionary.
        load_properties: list(str)
            Properties to load from source
        property_labels: dict(str, str), optional, default None
            List of additional source properties (key) to add from property
            source label (item). If None, all properties in source are added.

        Returns
        -------
        dict(str, any)
            System property dictionary

        """"""

        # Atoms system data
        atoms_properties = {}

        # Fundamental properties
        atoms_properties['atoms_number'] = (
            atoms.get_global_number_of_atoms())
        atoms_properties['atomic_numbers'] = (
            atoms.get_atomic_numbers())
        atoms_properties['positions'] = (
            conversion['positions']*atoms.get_positions())
        atoms_properties['cell'] = (
            conversion['positions']*self.convert_cell(atoms.get_cell()[:]))
        atoms_properties['pbc'] = atoms.get_pbc()
        if 'charge' in source:
            atoms_properties['charge'] = source['charge']
        elif 'charges' in source:
            atoms_properties['charge'] = sum(source['charges'])
        elif 'initial_charges' in source:
            atoms_properties['charge'] = sum(
                source['initial_charges'])
        else:
            atoms_properties['charge'] = 0.0
        if source.get('fragments') is not None:
            atoms_properties['fragments'] = source['fragments']

        # Collect properties
        for prop, item in property_labels.items():
            if prop in load_properties:
                atoms_properties[prop] = (
                    conversion[prop]*source[item])

        return atoms_properties

    def collect_from_atoms(
        self,
        atoms: ase.Atoms,
        conversion: Dict[str, float],
        load_properties: List[str],
        property_labels: Dict[str, str] = None,
    ) -> Dict[str, Any]:
        """"""
        Collect properties from database entry to property dictionary and
        apply unit conversion.

        Parameters
        ----------
        atoms: ase.Atoms
            Database atoms object including calculator object
        conversion: dict
            Unit conversion factors dictionary.
        load_properties: list(str)
            Properties to load from source
        property_labels: dict(str, str), optional, default None
            List of additional source properties (key) to add from property
            source label (item). If None, all properties in source are added.

        Returns
        -------
        dict(str, any)
            System property dictionary

        """"""

        # Atoms system data
        atoms_properties = {}

        # Get atoms property data
        properties = atoms.calc

        # Fundamental properties
        atoms_properties['atoms_number'] = (
            atoms.get_global_number_of_atoms())
        atoms_properties['atomic_numbers'] = (
            atoms.get_atomic_numbers())
        atoms_properties['positions'] = (
            conversion['positions']*atoms.get_positions())
        atoms_properties['cell'] = (
            conversion['positions']*self.convert_cell(atoms.get_cell()[:]))
        atoms_properties['pbc'] = atoms.get_pbc()
        if 'charge' in properties.parameters:
            atoms_properties['charge'] = (
                properties.parameters['charge'])
        elif 'charges' in properties.results:
            atoms_properties['charge'] = sum(
                properties.results['charges'])
        else:
            atoms_properties['charge'] = 0.0

        # Collect properties
        for prop, item in property_labels.items():
            if prop in properties.results:
                atoms_properties[prop] = (
                    conversion[prop]*properties.results[prop])

        return atoms_properties

    def convert_cell(
        self,
        cell,
        shape_out: Tuple[int] = (3, 3),
    ) -> np.ndarray:
        """"""
        Convert system cell information to 3x3 array

        Parameters
        ----------
        cell: list(float)
            Cell information in various shape
        shape_out: tuple(int)
            Returned cell shape

        Returns
        -------
        np.ndarray
            Cell information in requested shape

        """"""

        # Check input data type
        if cell is None:
            return np.zeros(shape_out, dtype=float)
        else:
            cell = np.array(cell, dtype=float)

        # Check if requested shape is correct otherwise flatten array for
        # further checks
        if cell.shape == shape_out:
            return cell
        else:
            cell = cell.reshape(-1)

        # Check if data shape is flatten
        shape_prod = np.prod(shape_out)
        if cell.shape == (shape_prod,):
            return cell.reshape(shape_out)

        # Check if data is diagonalized
        if len(shape_out) > 1 and np.diag(cell).shape == shape_out:
            return np.diag(cell)

        return SyntaxError(f""Invalid cell shape '{cell.shape:}'."")

    def get_db_metadata(
        self,
        data_source: List[str],
        data_properties: Optional[List[str]] = None,
        data_unit_properties: Optional[Dict[str, str]] = None,
        data_alt_property_labels: Optional[Dict[str, List[str]]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """"""
        Load data from asparagus dataset format.

        Parameters
        ----------
        data_source: list(str)
            Tuple of file path and file format label of data source to file.
        data_properties: List(str), optional, default None
            Subset of properties to load
        data_unit_properties: dict, optional, default None
            Dictionary from properties (keys) to corresponding unit as a
            string (item), e.g.:
                {property: unit}: { 'positions': 'Ang',
                                    'energy': 'eV',
                                    'force': 'eV/Ang', ...}
        data_alt_property_labels: dict, optional, default None
            Dictionary of alternative property labeling to replace
            non-valid property labels with the valid one if possible.

        Returns
        -------
        dict(str, any)
            Source database file metadata

        """"""

        # Check property list, units and alternative labels
        if data_properties is None:
            data_properties = self.data_properties
        if data_unit_properties is None:
            data_unit_properties = self.data_unit_properties
        if data_alt_property_labels is None:
            data_alt_property_labels = self.data_alt_property_labels

        # Get source data metadata
        with data.connect(data_source[0], data_source[1], mode='r') as db:
            source_metadata = db.get_metadata()
            source_properties = db.get(1)[0].keys()
            source_unit_properties = source_metadata['unit_properties']

        # Assign data source property labels to valid property labels.
        assigned_properties = self.assign_property_labels(
            data_source,
            source_properties,
            data_properties,
            [],
            data_alt_property_labels)

        # Get property unit conversion
        unit_conversion = self.get_unit_conversion(
            assigned_properties,
            data_unit_properties,
            source_unit_properties,
        )

        # Convert property scaling
        if source_metadata.get('data_property_scaling') is not None:
            for prop in source_metadata['data_property_scaling']:
                if unit_conversion[prop] is None:
                    conversion = 1.0
                else:
                    conversion = unit_conversion[prop]
                source_metadata['data_property_scaling'][prop] = [
                    conversion*item
                    for item in source_metadata['data_property_scaling'][prop]]

        # Convert atomic energies scaling
        if source_metadata.get('data_atomic_energies_scaling') is not None:
            if unit_conversion['energy'] is None:
                conversion = 1.0
            else:
                conversion = unit_conversion['energy']
            for atom in source_metadata['data_atomic_energies_scaling']:
                source_metadata['data_atomic_energies_scaling'][atom] = [
                    conversion*item for item in
                    source_metadata['data_atomic_energies_scaling'][atom]]

        return source_metadata",./Asparagus/asparagus/data/datareader.py
DataBase_hdf5,"class DataBase_hdf5(data.DataBase):
    """"""
    HDF5 data base class
    """"""

    # Initialize metadata parameter
    _metadata = {}

    # Structural and reference property dtypes
    properties_numpy_dtype = np.float64
    properties_torch_dtype = torch.float64

    def __init__(
        self,
        data_file: str,
        mode: str,
    ):
        """"""
        HDF5 Database object that contain reference data.

        Parameters
        ----------
        data_file: str
            Reference database file
        mode: str,
            Conection mode such as read(r), write(w) or append(a).

        Returns
        -------
        data.DataBase
            h5py object for data storing

        """"""

        # Inherit from DataBase base class
        super().__init__(data_file)

        self.data = None
        self.mode = mode

        return

    def __enter__(self):
        self.data = h5py.File(self.data_file, self.mode)
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if self.data.name:
            self.data.close()
            self.data = None
        if exc_type is not None:
            raise exc_type
        return

    def close(self):
        if self.data.name:
            self.data.close()
            self.data = None
        return

    def _set_metadata(self, metadata):

        # Convert metadata dictionary
        md = json.dumps(metadata)

        # Update or set metadata
        if self.data.get('metadata') is not None:
            del self.data['metadata']
        self.data['metadata'] = md

        # Store metadata
        self._metadata = metadata

        return

    def _get_metadata(self):

        # Read metadata
        if not len(self._metadata):

            if self.data.get('metadata') is None:
                self._metadata = {}
            else:
                md = self.data['metadata']
                self._metadata = json.loads(str(np.array(md, dtype=str)))

        return self._metadata

    def _init_systems(self):

        # Set or check version
        if self.data.get('version') is None:
            self.data['version'] = VERSION
        else:
            self.version = np.array(self.data['version'], dtype=np.int32)
            if self.version > VERSION:
                raise IOError(
                    f""Can not read newer version of the database format ""
                    f""(version {self.version})."")

        if self.data.get('systems') is None:
            self.data.create_group('systems')
        if self.data.get('last_id') is None:
            self.data['last_id'] = 0

        return

    def _reset(self):

        # Reset stored metadata dictionary
        self._metadata = {}
        return

    def _get(self, selection, **kwargs):
        rows = []
        metadata = self._get_metadata()
        for idx in selection:
            row = {}
            for prop_i, dtype_i in structure_properties_dtype.items():
                item = self.data['systems'][f""{selection[0]:d}""].get(prop_i)
                if item is not None:
                    row[prop_i] = torch.tensor(np.array(item, dtype=dtype_i))
            for prop_i in metadata.get('load_properties'):
                item = self.data['systems'][f""{selection[0]:d}""].get(prop_i)
                if item is not None:
                    row[prop_i] = torch.tensor(
                        np.array(item, dtype=properties_numpy_dtype))
            rows.append(row)
        return rows

    def get_last_id(self):

        # Get last row id
        if self.data.get('last_id') is not None:
            row_id = np.array(self.data['last_id'], dtype=np.int32)
        elif (
            self.data.get('last_id') is None
            and self.data.get('systems') is None
        ):
            row_id = 0
        elif (
            self.data.get('last_id') is None
            and self.data.get('systems') is not None
        ):
            row_id = 0
            for key in self.data['systems']:
                try:
                    key_id = int(key)
                except ValueError:
                    pass
                else:
                    if key_id > row_id:
                        row_id = key_id
        else:
            row_id = 0

        return row_id

    def _write(self, properties, row_id):

        # Reference data list
        ref_data = {}

        # Current datatime and User name
        ref_data['mtime'] = time.ctime()
        ref_data['username'] = os.getenv('USER')

        # Structural properties
        for prop_i, dtype_i in structure_properties_dtype.items():
            if properties.get(prop_i) is None:
                ref_data[prop_i] = None
            elif utils.is_array_like(properties.get(prop_i)):
                ref_data[prop_i] = np.array(
                    properties.get(prop_i), dtype=dtype_i)
            else:
                ref_data[prop_i] = dtype_i(properties.get(prop_i))

        # Reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype.keys():
                if properties.get(prop_i) is None:
                    ref_data[prop_i] = None
                elif utils.is_array_like(properties.get(prop_i)):
                    ref_data[prop_i] = np.array(
                        properties.get(prop_i), dtype=dtype_i)
                else:
                    ref_data[prop_i] = dtype_i(properties.get(prop_i))

        # Add or update database values
        if row_id is None:
            row_id = self.get_last_id() + 1
        row_id = self._update(row_id, ref_data)
    
        return row_id

    def update(self, row_id, properties):

        # Reference data list
        ref_data = {}
        
        # Current datatime and User name
        ref_data['mtime'] = time.ctime()
        ref_data['username'] = os.getenv('USER')
        
        # Structural properties
        for prop_i, dtype_i in structure_properties_dtype.items():
            if properties.get(prop_i) is None:
                ref_data[prop_i] = None
            elif utils.is_array_like(properties.get(prop_i)):
                ref_data[prop_i] = np.array(
                    properties.get(prop_i), dtype=dtype_i)
            else:
                ref_data[prop_i] = dtype_i(properties.get(prop_i))

        # Reference properties
        for prop_i in self.metadata.get('load_properties'):
            if prop_i not in structure_properties_dtype.keys():
                if properties.get(prop_i) is None:
                    ref_data[prop_i] = None
                elif utils.is_array_like(properties.get(prop_i)):
                    ref_data[prop_i] = np.array(
                        properties.get(prop_i), dtype=dtype_i)
                else:
                    ref_data[prop_i] = dtype_i(properties.get(prop_i))

        # Add or update database values
        row_id = self._update(row_id, ref_data)

        return row_id

    def _update(self, row_id, ref_data=None, properties=None):
        
        if ref_data is None and properties is None:
            
            raise SyntaxError(
                ""At least one input 'ref_data' or 'properties' should ""
                + ""contain reference data!"")
        
        elif ref_data is None:
            
            row_id = self._write(properties, row_id)
            
        else:
        
            # Add or update database values
            key_id = f""{row_id:d}""
            if self.data['systems'].get(key_id) is None:
                self.data['systems'].create_group(key_id)
                for key, item in ref_data.items():
                    if item is not None:
                        self.data['systems'][key_id][key] = item
                self.data['last_id'][...] = row_id
            else:
                del self.data['systems'][key_id]
                for key, item in ref_data.items():
                    self.data['systems'][key] = item

        return row_id

    def _count(self, selection, **kwargs):
        if self.data.get('systems') is None:
            return 0
        else:
            return len(self.data['systems'])

    def _delete(self, row_ids):
        raise NotImplementedError()

    def _delete_file(self):
        """"""
        Delete database file
        """"""
        if os.path.exists(self.data_file):
            os.remove(self.data_file)
        return

    @property
    def metadata(self):
        return self._get_metadata()",./Asparagus/asparagus/data/database_hdf5.py
Poly6Cutoff,"class Poly6Cutoff(torch.nn.Module):
    """"""
    2nd derivative smooth polynomial cutoff function of 6th order.

    $$f(x) = 1 - 6x^{5} + 15x^{4} - 10x^{3}$$
    with $$x = distance/cutoff$$

    Parameters
    ----------
    cutoff: float
        Cutoff distance
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        cutoff: float,
        device: Optional[str] = 'cpu',
        dtype: Optional['dtype'] = torch.float64,
    ):
        """"""
        Initialize cutoff function.
        """"""

        super(Poly6Cutoff, self).__init__()

        # Set cutoff value in the register for model parameters
        self.register_buffer(
            ""cutoff"", torch.tensor([cutoff], device=device, dtype=dtype))

        return

    def forward(
        self,
        distance: torch.Tensor
    ) -> torch.Tensor:

        """"""
        Forward pass of the cutoff function.

        Parameters
        ----------
        distance : torch.Tensor
            Atom pair distance tensor

        Returns
        -------
        torch.Tensor
            Cutoff values of input distance tensor

        """"""

        x = distance/self.cutoff

        return torch.where(
            x < 1.0,
            1.0 + ((-6.0*x + 15.0)*x - 10.0)*x**3,
            torch.zeros_like(distance))",./Asparagus/asparagus/layer/cutoff.py
Poly6Cutoff_range,"class Poly6Cutoff_range(torch.nn.Module):
    """"""
    2nd derivative smooth polynomial cutoff function of 6th order,
    within the range cuton < x < cutoff

    $$f(x) = 1 - 6x^{5} + 15x^{4} - 10x^{3}$$
    with $$(x - cutoff - width) = distance/width$$

    **Note**: This function is for CHARMM potential.

    Parameters
    ----------
    cutoff: float
        Upper Cutoff distance of cutoff range
    cuton: float
        Lower Cutoff distance of cutoff range
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        cutoff: float,
        cuton: float,
        device: Optional[str] = 'cpu',
        dtype: Optional['dtype'] = torch.float64,
    ):
        """"""
        Initialize cutoff function.
        """"""

        super(Poly6Cutoff_range, self).__init__()

        # Set cutoff value in the register for model parameters
        self.register_buffer(
            ""cutoff"", torch.tensor([cutoff], device=device, dtype=dtype))
        self.register_buffer(
            ""cuton"", torch.tensor([cuton], device=device, dtype=dtype))
        self.width = cutoff - cuton

        return

    def forward(
        self,
        distance: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Forward pass of the cutoff function.

        Parameters
        ----------
        distance : torch.Tensor
            Atom pair distance tensor

        Returns
        -------
        torch.Tensor
            Cutoff function values of input distance tensor

        """"""

        x = (distance - self.cuton)/self.width

        switch = torch.where(
            distance < self.cutoff,
            torch.ones_like(distance),
            torch.zeros_like(distance)
            )

        switch = torch.where(
            torch.logical_and(
                (distance > self.cuton),
                (distance < self.cutoff)
            ),
            1.0 + ((-6.0*x + 15.0)*x - 10.0)*x**3,
            switch)

        return switch",./Asparagus/asparagus/layer/cutoff.py
CosineCutoff,"class CosineCutoff(torch.nn.Module):
    """"""
    Behler-style cosine cutoff module, 
    within the range 0 < x < cutoff
    [source https://github.com/atomistic-machine-learning/schnetpack/blob/
        master/src/schnetpack/nn/cutoff.py]

    Parameters
    ----------
    cutoff: float
        Cutoff distance
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        cutoff: float,
        device: Optional[str] = 'cpu',
        dtype: Optional['dtype'] = torch.float64,
    ):
        """"""
        Initialize cutoff function.
        """"""

        super(CosineCutoff, self).__init__()

        # Set cutoff value in the register for model parameters
        self.register_buffer(
            ""cutoff"", torch.tensor([cutoff], device=device, dtype=dtype))

        return

    def forward(
        self,
        distance: torch.Tensor
    ) -> torch.Tensor:

        """"""
        Forward pass of the cutoff function.

        Parameters
        ----------
        distance : torch.Tensor
            Atom pair distance tensor

        Returns
        -------
        torch.Tensor
            Cutoff values of input distance tensor

        """"""

        x = distance/self.cutoff

        return torch.where(
            x < 1.0,
            0.5*(torch.cos(x*math.pi) + 1.0),
            torch.zeros_like(distance))",./Asparagus/asparagus/layer/cutoff.py
CosineCutoff_range,"class CosineCutoff_range(torch.nn.Module):
    """"""
    Behler-style cosine cutoff module.
    [source https://github.com/atomistic-machine-learning/schnetpack/blob/
        master/src/schnetpack/nn/cutoff.py]

    .. math::
       f(r) = \begin{cases}
        0.5 \times \left[1 + \cos\left(\frac{\pi r}{r_\text{cutoff}}\right)
          \right] & r < r_\text{cutoff} \\
        0 & r \geqslant r_\text{cutoff} \\
        \end{cases}

    Parameters
    ----------
    cutoff: float
        Upper cutoff distance of cutoff range
    cuton: float
        Lower cutoff distance of cutoff range
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        cuton: float,
        cutoff: float,
        device: Optional[str] = 'cpu',
        dtype: Optional['dtype'] = torch.float64,
    ):
        """"""
        Initialize cutoff function.
        """"""

        super(CosineCutoff_range, self).__init__()

        # Set cutoff value in the register for model parameters
        self.register_buffer(
            ""cutoff"", torch.tensor([cutoff], device=device, dtype=dtype))
        self.register_buffer(
            ""cuton"", torch.tensor([cuton], device=device, dtype=dtype))
        self.width = cutoff - cuton

        return
        
    def forward(
        self,
        distance: torch.Tensor
    ) -> torch.Tensor:

        """"""
        Forward pass of the cutoff function.

        Parameters
        ----------
        distance : torch.Tensor
            Atom pair distance tensor

        Returns
        -------
        torch.Tensor
            Cutoff function values of input distance tensor

        """"""
        
        x = (distance - self.cuton)/self.width
        
        switch = torch.where(
            distance < self.cutoff,
            torch.ones_like(distance),
            torch.zeros_like(distance)
            )

        switch = torch.where(
            torch.logical_and(
                (distance > self.cuton),
                (distance < self.cutoff)
            ),
            0.5*(torch.cos(x*math.pi) + 1.0),
            switch)

        return switch",./Asparagus/asparagus/layer/cutoff.py
DenseLayer,"class DenseLayer(torch.nn.Linear):
    """"""
    Dense layer: wrapper for torch.nn.Linear

    Parameters
    ----------
    n_input: int
        Number of input features.
    n_output: int
        Number of output features.
    activation_fn: callable, optional, default None
        Activation function. If None, identity is used.
    bias: bool, optional, default True
        If True, apply bias shift on neuron output.
    weight_init: callable, optional, default 'torch.nn.init.xavier_normal_'
        By Default, use Xavier initialization for neuron weight else zero
        weights are used.
    bias_init: callable, optional, default 'torch.nn.init.zeros_'
        By Default, zero bias values are initialized.
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        n_input: int,
        n_output: int,
        activation_fn: Union[Callable, torch.nn.Module],
        bias: bool,
        device: str,
        dtype: 'dtype',
        weight_init: Optional[Callable] = torch.nn.init.xavier_normal_,
        bias_init: Optional[Callable] = torch.nn.init.zeros_,
        
    ):
        """"""
        Initialize dense layer.

        """"""

        self.weight_init = weight_init
        self.bias_init = bias_init
        super(DenseLayer, self).__init__(
            n_input, n_output, bias=bias, device=device, dtype=dtype)

        # Assign activation function
        if activation_fn is None:
            self.activation = torch.nn.Identity()
        else:
            self.activation = activation_fn

        return

    def reset_parameters(
        self
    ):
        """"""
        Initialize dense layer variables.
        """"""
        self.weight_init(self.weight)
        if self.bias is not None:
            self.bias_init(self.bias)
        
        return

    def forward(
        self,
        input: torch.Tensor,
    ) -> torch.Tensor:

        output = torch.nn.functional.linear(input, self.weight, self.bias)
        output = self.activation(output)

        return output",./Asparagus/asparagus/layer/base.py
ResidualLayer,"class ResidualLayer(torch.nn.Module):
    """"""
    Residual layer

    Parameters
    ----------
    n_input: int
        Number of input features.
    activation_fn: callable
        Activation function passed to dense layer
    bias: bool
        If True, apply bias shift for dense layers.
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type
    weight_1_init: callable, optional, default 'torch.nn.init.orthogonal_'
        By Default, use orthogonal initialization for first dense layer 
        weights. If None, use zero initialization.
    weight_2_init: callable, optional, default 'torch.nn.init.zeros_'
        By Default, use zero initialization for second dense layer weights.

    """"""

    def __init__(
        self,
        n_input: int,
        activation_fn: Union[Callable, torch.nn.Module],
        bias: bool,
        device: str,
        dtype: 'dtype',
        weight_1_init: Optional[Callable] = torch.nn.init.orthogonal_,
        weight_2_init: Optional[Callable] = torch.nn.init.zeros_,
    ):

        super(ResidualLayer, self).__init__()

        # Assign initial activation function
        if activation_fn is None:
            self.activation = torch.nn.Identity()
        else:
            self.activation = activation_fn

        # Assign first dense layer
        self.dense1 = DenseLayer(
            n_input,
            n_input,
            activation_fn,
            bias,
            device,
            dtype,
            weight_init=weight_1_init)
        
        # Assign second dense layer
        self.dense2 = DenseLayer(
            n_input,
            n_input,
            None,
            bias,
            device,
            dtype,
            weight_init=weight_2_init)

        return

    def forward(
        self,
        input: torch.Tensor,
    ) -> torch.Tensor:

        output = self.activation(input)     # Activation
        output = self.dense1(output)        # Linear + Activation
        output = self.dense2(output)        # Linear 

        return input + output",./Asparagus/asparagus/layer/base.py
PaiNNInteraction,"class PaiNNInteraction(torch.nn.Module):
    """"""
    Interaction block for PaiNN.

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    activation_fn: callable, optional, default None
        Residual layer activation function.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type

    """"""

    def __init__(
        self,
        n_atombasis: int,
        activation_fn: Callable,
        device: str,
        dtype: 'dtype',
    ):
        """"""
        Initialize PaiNN interaction block.
        
        """"""

        super(PaiNNInteraction, self).__init__()

        # Initialize context layer
        self.context = torch.nn.Sequential(
            DenseLayer(
                n_atombasis, 
                n_atombasis,
                activation_fn,
                True,
                device,
                dtype,
                ),
            DenseLayer(
                n_atombasis, 
                3*n_atombasis,
                None,
                True,
                device,
                dtype,
                ),
            )

        return

    def forward(
        self,
        sfeatures: torch.Tensor,
        efeatures: torch.Tensor,
        descriptors: torch.Tensor,
        vectors: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
        n_atoms: int,
        n_features: int
    ) -> (torch.Tensor, torch.Tensor):

        # Apply context layer on scalar features
        cfea = self.context(sfeatures)

        # Apply conformational information on context of scalar features
        cfea = descriptors*cfea[idx_j]

        # Split update information
        ds, dev, dee = torch.split(
            cfea, n_features, dim=-1)

        # Reduce from pair information to atom i
        ds = torch.zeros(
            (n_atoms, 1, n_features),
            device=ds.device, dtype=ds.dtype
            ).index_add(0, idx_i, ds)

        # Compute equivariant feature vector update and reduce to atom i
        de = dev*vectors[..., None] + dee*efeatures[idx_j]
        de = torch.zeros(
            (n_atoms, 3, n_features),
            device=de.device, dtype=de.dtype
            ).index_add(0, idx_i, de)

        # Update scalar and equivariant feature vector
        sfeatures = sfeatures + ds
        efeatures = efeatures + de

        return sfeatures, efeatures",./Asparagus/asparagus/layer/layers_painn.py
PaiNNMixing,"class PaiNNMixing(torch.nn.Module):
    """"""
    Mixing block of scalar and equivariant feature vectors in PaiNN.

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    activation_fn: callable
        Residual layer activation function.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    stability_constant: float, optional, default 1e-8
        Numerical stability added constant
    
    """"""
    
    def __init__(
        self,
        n_atombasis: int,
        activation_fn: Callable,
        device: str,
        dtype: 'dtype',
        stability_constant: Optional[float] = 1.e-8,
    ):
        """"""
        Initialize PaiNN mixing block.
        
        """"""

        super(PaiNNMixing, self).__init__()

        # Assign class parameter
        self.stability_constant = stability_constant

        # Initialize context and mixing layer
        self.context = DenseLayer(
            n_atombasis, 
            2*n_atombasis,
            activation_fn,
            False,
            device,
            dtype,
            )
        self.mixing = torch.nn.Sequential(
            DenseLayer(
                2*n_atombasis, 
                n_atombasis,
                activation_fn,
                True,
                device,
                dtype,
                ),
            DenseLayer(
                n_atombasis, 
                3*n_atombasis,
                None,
                False,
                device,
                dtype,
                ),
            )

        return

    def forward(
        self,
        sfeatures: torch.Tensor,
        vfeatures: torch.Tensor,
        n_features: int,
    ) -> (torch.Tensor, torch.Tensor):
        
        # Apply context layer on vectorial feature vector and split 
        # information
        cfea = self.context(vfeatures)
        U, V = torch.split(cfea, n_features, dim=-1)
        
        # Mix scalar and vectorial feature vector
        U = torch.sqrt(
            torch.sum(U**2, dim=-2, keepdim=True) 
            + self.stability_constant)
        mix = torch.cat([sfeatures, U], dim=-1)
        mix = self.mixing(mix)
        
        # Split update information
        ds, dv, dsv = torch.split(mix, n_features, dim=-1)
        
        # Compute scalar and vectorial feature vector update
        ds = ds + dsv*torch.sum(U*V, dim=1, keepdim=True)
        dv = dv*V
        
        # Update feature and message vector 
        sfeatures = sfeatures + ds
        vfeatures = vfeatures + dv
        
        return sfeatures, vfeatures",./Asparagus/asparagus/layer/layers_painn.py
PaiNNGatedEquivarience,"class PaiNNGatedEquivarience(torch.nn.Module):
    """"""
    Gated equivariant block of the PaiNN model.
    
    Parameters
    ----------
    scalar_n_input: int
        Number of input scalar representation features.
    vector_n_input: int
        Number of input vector representation features.
    scalar_n_output: int
        Number of output scalar representation features.
    vector_n_output: int
        Number of output vector representation features.
    hidden_n_neurons: int
        Number of hidden neurons
    scalar_activation_fn: callable
        Activation function on scalar features. If None, identity is used.
    hidden_activation_fn: callable
        Activation function on hidden layer . If None, identity is used.
    bias: bool
        If True, apply bias shift on neuron output.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    weight_init: callable, optional, default 'torch.nn.init.xavier_normal_'
        By Default, use Xavier initialization for neuron weight else zero
        weights are used.
    bias_init: callable, optional, default 'torch.nn.init.zeros_'
        By Default, zero bias values are initialized.

    """"""
    def __init__(
        self,
        scalar_n_input: int,
        vector_n_input: int,
        scalar_n_output: int,
        vector_n_output: int,
        hidden_n_neurons: int,
        scalar_activation_fn: Union[Callable, torch.nn.Module],
        hidden_activation_fn: Union[Callable, torch.nn.Module],
        bias: bool,
        device: str,
        dtype: 'dtype',
        weight_init: Optional[Callable] = torch.nn.init.xavier_normal_,
        bias_init: Optional[Callable] = torch.nn.init.zeros_,
    ):
        """"""
        Initialize gated equivariant block.
        
        """"""
        
        super(PaiNNGatedEquivarience, self).__init__()
        
        # Input and output feature vector dimensions
        self.scalar_n_input = scalar_n_input
        self.vector_n_input = vector_n_input
        self.scalar_n_output = scalar_n_output
        self.vector_n_output = vector_n_output
        
        # Number of hidden layer
        self.hidden_n_neurons = hidden_n_neurons
        
        # Dense module to branch vector features for normalization to a scalar
        # feature vector and keeping a vector representation for scaling
        self.branch_vector = DenseLayer(
            vector_n_input,
            2*vector_n_output,
            None,
            False,
            device,
            dtype,
            weight_init=weight_init)

        # Mixed scalar/norm(vector) representation network
        self.mixed_scalars = torch.nn.Sequential(
            DenseLayer(
                scalar_n_input + vector_n_output,
                hidden_n_neurons,
                hidden_activation_fn,
                bias,
                device,
                dtype,
                weight_init=weight_init,
                bias_init=bias_init,
                ),
            DenseLayer(
                hidden_n_neurons,
                scalar_n_output + vector_n_output,
                None,
                bias,
                device,
                dtype,
                weight_init=weight_init,
                bias_init=bias_init,
                ),                
        )

        # Scalar feature activation function
        self.scalar_activation_fn = scalar_activation_fn

        return

    def forward(
        self,
        features: List[torch.Tensor],
    ) -> List[torch.Tensor]:
        """"""
        Forward pass of the gated equivariant block.
        
        Parameter
        ---------
        features: list(torch.tensor)
            List of scalar and vector feature vectors:
            sfeatures: torch.tensor(N_atoms, n_atombasis)
                Scalar atomic feature vectors
            vfeatures: torch.tensor(N_atoms, 3, n_atombasis)
                Vector atomic feature vectors
        
        Returns
        -------
        sfeatures: torch.tensor(N_atoms, n_atombasis)
            Modified scalar atomic feature vectors
        vfeatures: torch.tensor(N_atoms, 3, n_atombasis)
            Modified vector atomic feature vectors
        
        """"""
        
        # Extract scalar and vector features
        sfeatures, vfeatures = features
        
        # Branch vector features
        vmix = self.branch_vector(vfeatures)
        vfeatures_norm, vfeatures_keep = torch.split(
            vmix, self.vector_n_output, dim=-1)
        
        # Normalize vector features
        vfeatures_norm = torch.norm(vfeatures_norm, dim=-2)
        
        # Pass mixed features to network
        output = self.mixed_scalars(
            torch.cat([sfeatures, vfeatures_norm], dim=-1))
        
        # Split output into scalar features and vector scaling features
        sfeatures_out, vfeatures_scale = torch.split(
            output, [self.scalar_n_output, self.vector_n_output], dim=-1)
        
        # Scale vector features
        vfeatures_out = vfeatures_keep*vfeatures_scale[:, None, :]

        # Apply scalar activation function
        sfeatures_out = self.scalar_activation_fn(sfeatures_out)

        return (sfeatures_out, vfeatures_out)",./Asparagus/asparagus/layer/layers_painn.py
PaiNNOutput_scalar,"class PaiNNOutput_scalar(torch.nn.Module):
    """"""
    Deep neural network output block for scalar property predictions from 
    atom-wise representations in PaiNN.

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    n_property: int
        Dimension of the predicted property.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    n_layer: int, optional, default 2
        Number of hidden layer in the output block.
    n_neurons: (int list(int)), optional, default None
        Number of neurons of the hidden layers (int) or per hidden layer (list)
    activation_fn: callable, optional, default None
        Residual layer activation function.
    
    """"""
    
    _default_output_scalar = {
        'n_layer':              2,
        'n_neurons':            None,
        'activation_fn':        None,
        'bias_layer':           True,
        'bias_last':            True,
        'weight_init_layer':    torch.nn.init.zeros_,
        'weight_init_last':     torch.nn.init.zeros_,
        'bias_init_layer':      torch.nn.init.zeros_,
        'bias_init_last':       torch.nn.init.zeros_,
        }
    
    def __init__(
        self,
        n_atombasis: int,
        n_property: int,
        device: str,
        dtype: 'dtype',
        n_layer: Optional[int] = None,
        n_neurons: Optional[Union[int, List[int]]] = None,
        activation_fn: Optional[Callable] = None,
        bias_layer: Optional[bool] = None,
        bias_last: Optional[bool] = None,
        weight_init_layer: Optional[Callable] = None,
        weight_init_last: Optional[Callable] = None,
        bias_init_layer: Optional[Callable] = None,
        bias_init_last: Optional[Callable] = None,
    ):
        """"""
        Initialize PaiNN scalar output block.
        
        """"""
        
        super(PaiNNOutput_scalar, self).__init__()

        # Check input
        _ = utils.check_input_args(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=self._default_output_scalar,
            check_dtype=None)

        # Check hidden layer neuron options
        if self.n_layer:
            n_neurons_list = [self.n_atombasis]
            if utils.is_integer(self.n_neurons):
                n_neurons_list += [self.n_neurons]*(self.n_layer - 1)
            elif utils.is_integer_array(self.n_neurons):
                n_neurons_list += list(self.n_neurons)[:(self.n_layer - 1)]
            else:
                # Half number of hidden layer neurons with each layer
                ni = self.n_atombasis
                for ii in range(self.n_layer - 1):
                    ni = max(self.n_property, ni//2)
                    n_neurons_list.append(ni)
            n_neurons_list.append(self.n_property)
        else:
            # If no hidden layer, set hidden neurons to property neuron number
            n_neurons_list = [self.n_atombasis, self.n_property]

        # Initialize output module
        self.output = torch.nn.Sequential()

        # Append hidden layers
        for ii in range(n_layer - 1):
            self.output.append(
                DenseLayer(
                    n_neurons_list[ii], 
                    n_neurons_list[ii + 1],
                    self.activation_fn,
                    self.bias_layer,
                    device,
                    dtype,
                    weight_init=self.weight_init_layer,
                    bias_init=self.bias_init_layer,
                    ),
                )
        
        # Append output layer
        self.output.append(
            DenseLayer(
                n_neurons_list[-2], 
                n_neurons_list[-1],
                None,
                self.bias_last,
                device,
                dtype,
                weight_init=self.weight_init_last,
                bias_init=self.bias_init_last,
                ),
            )
        
        return

    def forward(
        self,
        features: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """"""
        Apply interaction block.
        
        Parameters
        ----------
        features: torch.Tensor(N_atoms, n_atombasis)
            Atomic feature vectors

        Returns
        -------
        torch.Tensor(N_atoms, n_property)
            Transformed atomic feature vector to result property vector
        
        """"""
        
        # Transform to result properties
        result = self.output(features)

        return result",./Asparagus/asparagus/layer/layers_painn.py
PaiNNOutput_tensor,"class PaiNNOutput_tensor(torch.nn.Module):
    """"""
    Gated equivariant output block for tensor like property predictions from 
    atom-wise representations in PaiNN.

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    n_property: int
        Dimension of the predicted property.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    n_layer: int, optional, default 2
        Number of hidden layer in the output block.
    n_neurons: (int list(int)), optional, default None
        Number of neurons of the input and output neurons of the layer in the
        gated equivariant block.
    hidden_n_neurons: (int list(int)), optional, default None
        Number of neurons of the hidden layers (int) or per tensor
        hidden layer (list)
    scalar_activation_fn: callable, optional, default None
        Scalar feature activation function.
    hidden_activation_fn: callable, optional, default None
        Hidden layer activation function.
    bias_layer: bool, optional, default True
        Add bias parameter for hidden layer neurons
    bias_last: bool, optional, default True
        Add bias parameter for last layer neuron(s)
    weight_init_layer: callable, optional, default 'torch.nn.init.zeros_'
        Weight parameter initialization function of the hidden layer
    weight_init_last: callable, optional, default 'torch.nn.init.zeros_'
        Weight parameter initialization function of the last layer
    bias_init_layer: callable, optional, default 'torch.nn.init.zeros_'
        Bias parameter initialization function of the hidden layer
    bias_init_last: callable, optional, default 'torch.nn.init.zeros_'
        Bias parameter initialization function of the last layer
    
    """"""
    
    _default_output_tensor = {
        'n_layer':              2,
        'n_neurons':            None,
        'hidden_n_neurons':     None,
        'scalar_activation_fn': None,
        'hidden_activation_fn': None,
        'bias_layer':           True,
        'bias_last':            True,
        'weight_init_layer':    torch.nn.init.zeros_,
        'weight_init_last':     torch.nn.init.zeros_,
        'bias_init_layer':      torch.nn.init.zeros_,
        'bias_init_last':       torch.nn.init.zeros_,
        }
    
    def __init__(
        self,
        n_atombasis: int,
        n_property: int,
        device: str,
        dtype: 'dtype',
        n_layer: Optional[int] = None,
        n_neurons: Optional[Union[int, List[int]]] = None,
        hidden_n_neurons: Optional[Union[int, List[int]]] = None,
        scalar_activation_fn: Optional[Callable] = None,
        hidden_activation_fn: Optional[Callable] = None,
        bias_layer: Optional[bool] = True,
        bias_last: Optional[bool] = True,
        weight_init_layer: Optional[Callable] = torch.nn.init.zeros_,
        weight_init_last: Optional[Callable] = torch.nn.init.zeros_,
        bias_init_layer: Optional[Callable] = torch.nn.init.zeros_,
        bias_init_last: Optional[Callable] = torch.nn.init.zeros_,
    ):
        """"""
        Initialize PaiNN tensor output block.
        
        """"""
        
        super(PaiNNOutput_tensor, self).__init__()

        # Check input
        _ = utils.check_input_args(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=self._default_output_tensor,
            check_dtype=None)

        # Check hidden scalar layer neuron options
        if self.n_layer:
            n_neurons_list = [self.n_atombasis]
            if utils.is_integer(self.n_neurons):
                n_neurons_list += [self.n_neurons]*(
                    self.n_layer - 1)
            elif utils.is_integer_array(self.n_neurons):
                n_neurons_list += list(self.n_neurons)[
                    :(self.n_layer - 1)]
            else:
                # Half number of hidden layer neurons with each layer
                ni = self.n_atombasis
                for ii in range(self.n_layer - 1):
                    ni = max(self.n_property, ni//2)
                    n_neurons_list.append(ni)
            n_neurons_list.append(self.n_property)
        else:
            # If no hidden layer, set hidden neurons to property neuron number
            n_neurons_list = [self.n_atombasis, self.n_property]

        # Check hidden layer neuron options
        if self.hidden_n_neurons is None:
            hidden_n_neurons_list = n_neurons_list[:-1]
        elif utils.is_integer(self.hidden_n_neurons):
            hidden_n_neurons_list = [self.hidden_n_neurons]*self.n_layer
        else:
            hidden_n_neurons_list = list(self.hidden_n_neurons)[:self.n_layer]

        # Initialize output module
        self.output = torch.nn.Sequential()

        # Append hidden layers
        for ii in range(n_layer - 1):
            self.output.append(
                PaiNNGatedEquivarience(
                    n_neurons_list[ii],
                    n_neurons_list[ii],
                    n_neurons_list[ii + 1],
                    n_neurons_list[ii + 1],
                    hidden_n_neurons_list[ii],
                    self.scalar_activation_fn,
                    self.hidden_activation_fn,
                    self.bias_layer,
                    device,
                    dtype,
                    weight_init=self.weight_init_layer,
                    bias_init=self.bias_init_layer,
                    ),
                )
        
        # Append output layer
        self.output.append(
            PaiNNGatedEquivarience(
                n_neurons_list[-2],
                n_neurons_list[-2],
                n_neurons_list[-1],
                n_neurons_list[-1],
                hidden_n_neurons_list[-1],
                self.scalar_activation_fn,
                None,
                self.bias_last,
                device,
                dtype,
                weight_init=self.weight_init_last,
                bias_init=self.bias_init_last,
                ),
            )

        return

    def forward(
        self,
        sfeatures: torch.Tensor,
        vfeatures: torch.Tensor,
    ) -> List[torch.Tensor]:
        """"""
        Apply scalar output block.
        
        Parameters
        ----------
        sfeatures: torch.tensor(N_atoms, n_atombasis)
            Scalar atomic feature vectors
        vfeatures: torch.tensor(N_atoms, 3, n_atombasis)
            Vector atomic feature vectors
        
        Returns
        -------
        torch.Tensor(N_atoms, n_property)
            Resulting atomic scalar property vector
        torch.Tensor(N_atoms, 3, n_property)
            Resulting atomic vector properties.
        
        Parameter
        ---------
        
        Returns
        -------
        scalar_result: torch.tensor(N_atoms, n_atombasis)
            Modified scalar atomic feature vectors
        vector_result: torch.tensor(N_atoms, 3, n_atombasis)
            Modified vector atomic feature vectors
        
        """"""
        
        # Transform to scalar and vector results
        scalar_result, vector_result = self.output(
            (sfeatures, vfeatures)
            )

        return scalar_result, vector_result",./Asparagus/asparagus/layer/layers_painn.py
GaussianRBF,"class GaussianRBF(torch.nn.Module):
    """"""
    Gaussian type radial basis functions.

    Parameters
    ----------
    rbf_n_basis: int
        Number of RBF center
    rbf_center_start: float
        Initial lower RBF center range
    rbf_center_end: float
        Initial upper RBF center range
    rbf_trainable: bool
        Trainable RBF center positions
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        rbf_n_basis: int,
        rbf_center_start: float,
        rbf_center_end: float,
        rbf_trainable: bool,
        device: str,
        dtype: 'dtype',
    ):
        """"""
        Initialize Gaussian Radial Basis Function.
        """"""

        super(GaussianRBF, self).__init__()

        self.rbf_n_basis = rbf_n_basis
        self.rbf_trainable = rbf_trainable

        # Initialize RBF centers and widths
        centers = torch.linspace(
            rbf_center_start,
            rbf_center_end,
            rbf_n_basis,
            device=device,
            dtype=dtype)
        widths = torch.ones_like(centers, device=device, dtype=dtype)

        if rbf_trainable:
            self.centers = torch.nn.Parameter(centers)
            self.widths = torch.nn.Parameter(widths)
        else:
            self.register_buffer(""centers"", centers)
            self.register_buffer(""widths"", widths)

        return

    def __str__(self):
        return 'GaussianRBF'        

    def forward(
        self,
        d: torch.Tensor
    ) -> torch.Tensor:

        x = torch.unsqueeze(d, -1)
        rbf = torch.exp(-0.5*((x - self.centers)/self.widths)**2)

        return rbf",./Asparagus/asparagus/layer/radial.py
GaussianRBF_PhysNet,"class GaussianRBF_PhysNet(torch.nn.Module):
    """"""
    Original PhysNet type radial basis functions (RBFs) with double exponential
    expression.

    Parameters
    ----------
    rbf_n_basis: int
        Number of RBFs
    rbf_center_start: float
        Initial lower RBF center range
    rbf_center_end: float
        Initial upper RBF center range
    rbf_trainable: bool
        Trainable RBF center positions
    device: str, optional, default 'cpu'
        Device type for model variable allocation
    dtype: dtype object, optional, default 'torch.float64'
        Model variables data type

    """"""

    def __init__(
        self,
        rbf_n_basis: int,
        rbf_center_start: float,
        rbf_center_end: float,
        rbf_trainable: bool,
        device: str,
        dtype: 'dtype',
    ):
        """"""
        Initialize original PhysNet type Gaussian Radial Basis Function.
        """"""

        super(GaussianRBF_PhysNet, self).__init__()

        self.rbf_n_basis = rbf_n_basis
        self.rbf_cutoff_fn = rbf_cutoff_fn
        self.rbf_trainable = rbf_trainable

        # Initialize RBF centers and widths
        centers = torch.linspace(
            rbf_center_start, np.exp(-rbf_center_end), rbf_n_basis)
        softp = torch.nn.Softplus()
        width_val = self.softplus_inverse(
            (0.5 / ((1.0 - np.exp(-rbf_center_end)) / rbf_n_basis)) ** 2)
        widths = torch.empty(
            rbf_n_basis, dtype=torch.float64).new_full(
                (rbf_n_basis,), softp(width_val * rbf_n_basis))

        if rbf_trainable:
            self.centers = torch.nn.Parameter(centers)
            self.widths = torch.nn.Parameter(widths)
        else:
            self.register_buffer(""centers"", centers)
            self.register_buffer(""widths"", widths)

        return

    def __str__(self):
        return 'GaussianRBF_PhysNet'        

    @staticmethod
    def softplus_inverse(x):
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x)
        return torch.log(torch.exp(x) - 1)

    def forward(
        self,
        d: torch.Tensor
    ) -> torch.Tensor:

        x = torch.unsqueeze(d, -1)
        rbf = torch.exp(-self.widths*(torch.exp(-x) - self.centers)**2)

        return rbf",./Asparagus/asparagus/layer/radial.py
InteractionBlock,"class InteractionBlock(torch.nn.Module):
    """"""
    Interaction block for PhysNet

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    n_radialbasis: int
        Number of input radial basis centers
    n_residual_interaction: int
        Number of residual layers for atomic feature and radial basis vector
        interaction.
    n_residual_features: int
        Number of residual layers for atomic feature interactions.
    activation_fn: callable
        Residual layer activation function.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type

    """"""

    def __init__(
        self,
        n_atombasis: int,
        n_radialbasis: int,
        n_residual_interaction: int,
        n_residual_features: int,
        activation_fn: Callable,
        device: str,
        dtype: 'dtype',
    ):
        """"""
        Initialize PhysNet interaction block.
        """"""

        super(InteractionBlock, self).__init__()

        # Atomic features and radial basis vector interaction layer
        self.interaction = InteractionLayer(
            n_atombasis,
            n_radialbasis,
            n_residual_interaction,
            activation_fn,
            device,
            dtype)

        # Atomic feature interaction layers
        self.residuals = torch.nn.ModuleList([
            ResidualLayer(
                n_atombasis,
                activation_fn,
                True,
                device,
                dtype)
            for _ in range(n_residual_features)])

        return

    def forward(
        self,
        features: torch.Tensor,
        descriptors: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Apply interaction block.
        
        Parameters
        ----------
        features: torch.Tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        descriptors: torch.Tensor(N_atoms, n_atombasis, n_radialbasis)
            Atom pair radial distribution vectors
        idx_i: torch.Tensor(N_pairs)
            Atom i pair index
        idx_j: torch.Tensor(N_pairs)
            Atom j pair index

        Returns
        -------
        torch.Tensor(N_atoms, n_atombasis)
            Modified atom feature vectors

        """"""

        # Apply interaction layer
        x = self.interaction(features, descriptors, idx_i, idx_j)

        # Iterate through atomic feature interaction layers
        for residual in self.residuals:
            x = residual(x)

        return x",./Asparagus/asparagus/layer/layers_physnet.py
InteractionLayer,"class InteractionLayer(torch.nn.Module):
    """"""
    Atomic features and radial basis vector interaction layer for PhysNet

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    n_radialbasis: int
        Number of input radial basis centers
    n_residual_interaction: int
        Number of residual layers for atomic feature and radial basis vector
        interaction.
    activation_fn: callable
        Residual layer activation function.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type

    """"""

    def __init__(
        self,
        n_atombasis: int,
        n_radialbasis: int,
        n_residual_interaction: int,
        activation_fn: Callable,
        device: str,
        dtype: 'dtype',
    ):

        super(InteractionLayer, self).__init__()

        # Assign activation function
        if activation_fn is None:
            self.activation = torch.nn.Identity()
        else:
            self.activation = activation_fn

        # Dense layer for the conversion from radial basis vector to atomic 
        # feature vector length
        self.radial2atom = DenseLayer(
            n_radialbasis,
            n_atombasis,
            None,
            False,
            device,
            dtype,
            weight_init=torch.nn.init.xavier_normal_)
            

        # Dense layer for atomic feature vector for atom i
        self.dense_i = DenseLayer(
            n_atombasis,
            n_atombasis,
            activation_fn,
            True,
            device,
            dtype)
        
        # Dense layer for atomic feature vector for atom j
        self.dense_j = DenseLayer(
            n_atombasis,
            n_atombasis,
            activation_fn,
            True,
            device,
            dtype)

        # Residual layers for atomic feature vector pair interaction modifying 
        # the message vector
        self.residuals_ij = torch.nn.ModuleList([
            ResidualLayer(
                n_atombasis,
                activation_fn,
                True,
                device,
                dtype)
            for _ in range(n_residual_interaction)])

        # Dense layer for message vector interaction
        self.dense_out = DenseLayer(
            n_atombasis,
            n_atombasis,
            None,
            True,
            device,
            dtype)
        
        # Scaling vector for mixing of initial atomic feature vector with
        # message vector
        self.scaling = torch.nn.Parameter(
            torch.ones([n_atombasis], device=device, dtype=dtype))
        
        # Special case flag for variable assignment on CPU's
        if device.lower() == 'cpu':
            self.cpu = True
        else:
            self.cpu = False

        return

    def forward(
        self,
        features: torch.Tensor,
        descriptors: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Apply interaction layer.
        
        Parameters
        ----------
        features: torch.Tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        descriptors: torch.Tensor(N_atoms, n_atombasis, n_radialbasis)
            Atom pair radial distribution vectors
        idx_i: torch.Tensor(N_pairs)
            Atom i pair index
        idx_j: torch.Tensor(N_pairs)
            Atom j pair index

        Returns
        -------
        torch.Tensor(N_atoms, n_atombasis)
            Modified atom feature vectors
        
        """"""

        # Apply initial activation function on atomic features
        x = self.activation(features)

        # Apply radial basis (descriptor) to feature vector layer
        g = self.radial2atom(descriptors)

        # Calculate contribution of central atom i and neighbor atoms j
        xi = self.dense_i(x)
        if self.cpu:
            gxj = g*self.dense_j(x)[idx_j]
        else:
            j = idx_j.view(-1, 1).expand(-1, features.shape[-1])
            gxj = g * torch.gather(self.dense_j(x), 0, j)

        # Combine descriptor weighted neighbor atoms feature vector for each
        # central atom i
        xj = utils.scatter_sum(gxj, idx_i, dim=0, shape=xi.shape)

        # Combine features to message vector
        message = xi + xj

        # Apply residual layers and activation function for message vector
        # interaction
        for residual in self.residuals_ij:
            message = residual(message)
        message = self.activation(message)

        # Mix initial atomic feature vector with message vector
        x = self.scaling*x + self.dense_out(message)

        # Normalize feature vectors
        #x = torch.nn.functional.normalize(x)
        
        return x",./Asparagus/asparagus/layer/layers_physnet.py
OutputBlock,"class OutputBlock(torch.nn.Module):
    """"""
    Output block for PhysNet

    Parameters
    ----------
    n_atombasis: int
        Number of atomic features (length of the atomic feature vector)
    n_results: int
        Number of output vector features.
    n_residual: int
        Number of residual layers for transformation from atomic feature vector
        to output results.
    activation_fn: callable
        Residual layer activation function.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type

    """"""

    def __init__(
        self,
        n_atombasis: int,
        n_results: int,
        n_residual: int,
        activation_fn: Callable,
        device: str,
        dtype: 'dtype',
    ):

        super(OutputBlock, self).__init__()

        # Assign activation function
        if activation_fn is None:
            self.activation_fn = torch.nn.Identity()
        else:
            self.activation_fn = activation_fn

        # Residual layer for atomic feature modification
        self.residuals = torch.nn.ModuleList([
            ResidualLayer(
                n_atombasis,
                activation_fn,
                True,
                device,
                dtype)
            for _ in range(n_residual)])

        # Dense layer for transforming atomic feature vector to result vector
        self.output = DenseLayer(
            n_atombasis,
            n_results,
            activation_fn,
            False,
            device,
            dtype,
            weight_init=torch.nn.init.xavier_normal_)

        return

    def forward(
        self,
        features: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Apply output block.
        
        Parameters
        ----------
        features: torch.Tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        
        Returns
        -------
        torch.Tensor(N_atoms, n_results)
            Transformed atomic feature vector to result vector
        
        """"""

        # Apply residual layers on atomic features
        for ir, residual in enumerate(self.residuals):
            features = residual(features)
        
        # Apply last activation function
        features = self.activation_fn(features)
        #features = torch.nn.functional.normalize(features)

        # Transform to result vector
        result = self.output(features)

        return result",./Asparagus/asparagus/layer/layers_physnet.py
ASE_Calculator,"class ASE_Calculator(ase_calc.Calculator):
    """"""
    ASE calculator interface for a Asparagus model potential.

    Parameters
    ----------
    model_calculator: (callable object, list of callable objects)
        Model calculator(s) to predict model properties. If an ensemble
        is given in form of a list of model calculators, the average value
        is returned as model prediction.
    atoms: ASE Atoms object, optional, default None
        ASE Atoms object to which the calculator will be attached.
    charge: float, optional, default 0.0
        Total charge of the respective ASE Atoms object.
    implemented_properties: (str, list(str)), optional, default None
        Properties predicted by the model calculator. If None, then
        all model properties (of the first model if ensemble) are
        available.
    label: str, optional, default 'asparagus'
        Label for the ASE calculator

    """"""

    # ASE specific calculator information
    default_parameters = {
        ""method"": ""Asparagus""}

    def __init__(
        self,
        model_calculator: Union[Callable, List[Callable]],
        atoms: Optional[ase.Atoms] = None,
        charge: Optional[float] = None,
        implemented_properties: Optional[List[str]] = None,
        label: Optional[str] = 'asparagus',
        **kwargs
    ):
        """"""
        Initialize ASE Calculator class.

        """"""

        # Initialize parent Calculator class
        ase_calc.Calculator.__init__(self, atoms=atoms, **kwargs)

        ###################################
        # # # Check NNP Calculator(s) # # #
        ###################################

        # Assign NNP calculator model(s)
        self.model_calculator = model_calculator
        if hasattr(self.model_calculator, 'model_ensemble'):
            self.model_ensemble = self.model_calculator.model_ensemble
        else:
            self.model_ensemble = False

        # Assign model calculator variables
        self.model_device = self.model_calculator.device
        self.model_dtype = self.model_calculator.dtype

        # Set implemented properties
        if implemented_properties is None:
            self.implemented_properties = (
                self.model_calculator.model_properties)
        else:
            if utils.is_string(implemented_properties):
                self.implemented_properties = [implemented_properties]
            else:
                self.implemented_properties = implemented_properties

        # Check if model calculator has loaded a checkpoint file
        if not self.model_calculator.checkpoint_loaded:
            raise SyntaxError(
                ""The model calculator does not seem to have a ""
                + ""proper parameter set loaded from a checkpoint file.""
                + ""\nMake sure parameters are loaded otherwise ""
                + ""model predictions are random."")

        ##################################
        # # # Set Calculator Options # # #
        ##################################

        # Initialize neighbor list function
        cutoffs = self.model_calculator.get_cutoff_ranges()
        self.neighbor_list = module.TorchNeighborListRangeSeparated(
            cutoffs,
            self.model_device,
            self.model_dtype)

        # Get unit conversion dictionary
        self.model_conversion = self.check_model_units(
            self.model_calculator.model_unit_properties)

        # Initialize atoms object batch
        if atoms is None:
            self.atoms_batch = {}
        else:
            self.atoms_batch = self.model_calculator.create_batch(
                atoms,
                charge=charge,
                conversion=self.model_conversion)

        # Initialize result dictionary
        self.results = {}

        # Initialize convergence flag
        self.converged = True

        # Set flag to use stored results if atoms property did not change
        self.use_cache = True

        return

    def check_model_units(
        self,
        model_units: Dict[str, str]
    ) -> Dict[str, float]:
        """"""
        Check model units with respect to ASE units and return conversion
        factors dictionary

        Parameter
        ---------
        model_units: dict(str, str)
            Dictionary of model property units.

        Returns
        -------
        dict(str, float)
            Dictionary of model to data property unit conversion factors

        """"""

        # Initialize model conversion factor dictionary
        model_conversion = {}

        # Check positions unit (None = ASE units by default)
        conversion, _ = utils.check_units(None, model_units['positions'])
        model_conversion['positions'] = conversion

        # Check implemented property units (None = ASE units by default)
        for prop in self.implemented_properties:
            conversion, _ = utils.check_units(None, model_units[prop])
            model_conversion[prop] = conversion

        return model_conversion

    def update_model_input(
        self, 
        batch: Dict[str, torch.Tensor],
        atoms: Union[ase.Atoms, List[ase.Atoms]],
        charge: Optional[Union[float, List[float]]] = None,
    ) -> Dict[str, torch.Tensor]:
        """"""
        Update atoms data bach by the changes in the assigned ASE atoms system.
        
        Parameter
        ---------
        batch: dict(str, torch.Tensor)
            ASE atoms data batch
        atoms: (ase.Atoms, list(ase.Atoms))
            ASE Atoms object or list of ASE Atoms objects to update the data
            batch.
        charge: (float, list(float)), optional, default None
            Optional total charge of the respective ASE Atoms object or
            objects.
        
        Returns
        -------
        dict(str, torch.Tensor)
            Updated atoms data batch

        """"""

        # Check atoms input
        if utils.is_ase_atoms(atoms):
            atoms = [atoms]
        elif not utils.is_ase_atoms_array(atoms):
            raise ValueError(
                ""Input 'atoms' is not an ASE Atoms object or list of ASE ""
                + ""atoms objects!"")

        # If model input is not initialized
        if not batch:
            batch = self.model_calculator.create_batch(
                atoms,
                charge=charge,
                conversion=self.model_conversion)
            return batch

        # Update atom positions and cell parameters
        fconv = self.model_conversion['positions']
        batch['positions'] = torch.cat(
            [
                torch.tensor(
                    atms.get_positions()*fconv, dtype=self.model_dtype)
                for atms in atoms
            ], 0).to(
                device=self.model_device, dtype=self.model_dtype)
        batch['cell'] = torch.tensor(
            np.array([atms.get_cell()[:]*fconv for atms in atoms]),
            dtype=self.model_dtype, device=self.model_device)

        # Create and assign atom pair indices and periodic offsets
        batch = self.neighbor_list(batch)
        
        return batch

    def calculate(
        self,
        atoms: Optional[Union[ase.Atoms, List[ase.Atoms]]] = None,
        charge: Optional[Union[float, List[float]]] = None,
        properties: List[str] = None,
        system_changes: List[str] = ase_calc.all_changes,
        verbose_results: Optional[bool] = False
    ) -> Dict[str, Any]:
        """"""
        Calculate model properties

        Parameter
        ---------
        atoms: (ase.Atoms, list(ase.Atoms)), optional, default None
            Optional ASE Atoms object or list of ASE Atoms objects to which the
            properties will be calculated. If given, atoms setup to prepare
            model calculator input will be run again.
        charge: (float, list(float)), optional, default None
            Optional total charge of the respective ASE Atoms object or
            objects. If the atoms charge is given as a float, the charge is
            assumed for all ASE atoms objects given.
        properties: list(str), optional, default None
            List of properties to be calculated. If None, all implemented
            properties will be calculated (will be anyways ...).
        verbose_results: bool, optional, default False
            If True, store extended model property contributions in the result
            dictionary.

        Results
        -------
        dict(str, any)
            ASE atoms property predictions

        """"""

        # Collect atoms object
        if atoms is None and self.atoms is None:
            raise ase_calc.CalculatorSetupError(
                ""ASE atoms object is not defined!"")
        elif atoms is None:
            atoms = self.atoms
        elif self.atoms is None:
            self.atoms = atoms
            # Potential different atom system needs a reset of the atom batch
            self.atoms_batch = {}
        elif not (
            len(atoms) == len(self.atoms)
            and np.all(atoms.numbers == self.atoms.numbers)
        ):
            # Different atoms system needs a reset of the atom batch
            self.atoms_batch = {}

        # Prepare or update atoms data batch
        self.atoms_batch = self.update_model_input(
                self.atoms_batch,
                atoms,
                charge)

        # Compute model properties
        prediction = self.model_calculator(
            self.atoms_batch,
            verbose_results=verbose_results)

        # Convert model properties
        self.assign_prediction(prediction, self.atoms_batch)

        return self.results

    def assign_prediction(
        self,
        prediction: Dict[str, torch.Tensor],
        atoms_batch: Dict[str, torch.Tensor],
    ):
        """"""
        Convert and assign and model prediction to the results dictionary.
        
        Parameter
        ---------
        prediction: list(float)
            Model prediction dictionary for ASE atoms system in batch
        atoms_batch: dict(str, torch.Tensor)
            ASE atoms systems data batch

        """"""

        # Check for multiple system prediction
        multi_sys = len(atoms_batch['atoms_number']) > 1
        if multi_sys:
            Nsys = atoms_batch['atoms_number'].shape[0]
            Natoms = atoms_batch['atomic_numbers'].shape[0]
            Npairs = atoms_batch['idx_i'].shape[0]

        # Iterate over model properties
        for prop in self.implemented_properties:

            # Convert property prediction
            pred = (
                prediction[prop].cpu().detach().numpy()
                *self.model_conversion[prop])

            # Resolve prediction system-wise
            if multi_sys:

                if not pred.shape:
                    pass
                elif pred.shape[0] == Nsys:
                    pred = [pred_i for pred_i in pred]
                elif pred.shape[0] == Natoms:
                    pred = [
                        pred[atoms_batch['sys_i'] == i_sys]
                        for i_sys in range(Nsys)]
                elif pred.shape[0] == Npairs:
                    pred = [
                        pred[
                            atoms_batch['sys_i'][atoms_batch['idx_i']] == i_sys
                        ] for i_sys in range(Nsys)]

            # Assign to results
            self.results[prop] = pred

        return",./Asparagus/asparagus/interface/model_ase.py
TagReplacement,"class TagReplacement:
    """"""
    Class for pre-defined functions that return input strings with
    respect to the respective atoms object.

    Parameters
    ----------
    calculator: ase.calculators.calculator.Calculator
        ASE compatible calculator class object

    """"""
    
    def __init__(
        self,
        calculator: ase.calculators.calculator.Calculator,
        **kwargs
    ):
        """"""
        Initialize class object.

        """"""

        # Assign calculator class
        self.calculator = calculator
        
        # Initialize function library
        self.functions = {
            '$xyz': self.get_xyz,
            '$charge': self.get_charge,
            '$mult': self.get_multiplicity,
            '$multiplicity': self.get_multiplicity,
            '$spin2': self.get_spin_times_2,
            '$dir': self.calculator.directory,
            '$directory': self.calculator.directory,
            }

        return
    
    def __getitem__(
        self,
        item: str,
    ) -> Callable:
        """"""
        Get replacement function from function dictionary.
        
        Parameters
        ----------
        item: str
            Dummy label 'item' which becomes replaced by the output of the
            returned replacement function.

        Returns
        -------
        callable
            Replacement function for the dummy label 'item'.

        """"""
        return self.functions.get(item)

    def get_xyz(
        self,
        atoms: ase.Atoms,
        parameters: Optional[Dict[str, Any]] = {},
    ) -> str:
        """"""
        Return lines of atoms element symbols and its Cartesian coordinates

        Parameters
        ----------
        atoms: ase.Atoms
            Reference ASE Atoms object to read positions from.
        parameters: dict, optional, default {}
            Additional system parameters

        Returns
        -------
        str
            Cartesian coordinated of the atoms in xyz format

        """"""
        
        out = """"
        for ia, atom in enumerate(atoms):
            if ia:
                out += ""\n""
            out += f""{atom.symbol:<3s} ""
            for i in range(3):
                out += f""{atom.position[i]: 12.8f} ""

        return out

    def get_charge(
        self,
        atoms: ase.Atoms,
        parameters: Optional[Dict[str, Any]] = {},
    ) -> str:
        """"""
        Return system charge as string of an integer.

        Parameters
        ----------
        atoms: ase.Atoms
            Reference ASE Atoms object to read positions from.
        parameters: dict, optional, default {}
            Additional system parameters

        Returns
        -------
        str
            System charge

        """"""

        if 'charge' in parameters:
            out = f""{parameters['charge']:d}""
        elif 'charge' in self.calculator.parameters:
            out = f""{self.calculator.parameters['charge']:d}""
        else:
            out = ""0""

        return out

    def get_multiplicity(
        self,
        atoms: ase.Atoms,
        parameters: Optional[Dict[str, Any]] = {},
    ) -> str:
        """"""
        Return system multiplicity as string of an integer.

        Parameters
        ----------
        atoms: ase.Atoms
            Reference ASE Atoms object
        parameters: dict, optional, default {}
            Additional system parameters

        Returns
        -------
        str
            System spin multiplicity 2*S + 1 with S as sum of unpaired spin
            quantum numbers 1/2

        """"""

        if 'multiplicity' in parameters:
            out = f""{parameters['multiplicity']:d}""
        elif 'multiplicity' in self.calculator.parameters:
            out = f""{self.calculator.parameters['multiplicity']:d}""
        else:
            # Get number of electrons
            number_protons = np.sum(atoms.get_atomic_numbers())
            number_electrons = (
                number_protons + int(self.get_charge(atoms, parameters)))
            # Singlet for even electron number, else dublett
            if number_electrons%2:
                out = ""2""
            else:
                out = ""1""

        return out

    def get_spin_times_2(
        self,
        atoms: ase.Atoms,
        parameters: Optional[Dict[str, Any]] = {},
    ) -> str:
        """"""
        Return system multiplicity in form of 2*S = 2*(n*1/2) equal the number
        of unpaired spins n as string of an integer.

        Parameters
        ----------
        atoms: ase.Atoms
            Reference ASE Atoms object
        parameters: dict, optional, default {}
            Additional system parameters

        Returns
        -------
        str
            System number of unpaired spins or sum of unpaired spin
            quantum numbers 1/2 but times 2 (2*S)

        """"""

        if 'multiplicity' in parameters:
            out = f""{parameters['multiplicity'] - 1:d}""
        elif 'multiplicity' in self.calculator.parameters:
            out = f""{self.calculator.parameters['multiplicity'] - 1:d}""
        else:
            out = ""0""

        return out

    def get_directory(
        self,
        atoms: ase.Atoms,
        parameters: Optional[Dict[str, Any]] = {},
    ) -> str:
        """"""
        Return calculator working directory.

        Parameters
        ----------
        atoms: ase.Atoms
            Reference ASE Atoms object
        parameters: dict, optional, default {}
            Additional system parameters

        Returns
        -------
        str
            Working directory of the calculator

        """"""
        if 'directory' in parameters:
            out = f""{parameters['directory']:s}""
        elif 'directory' in self.calculator:
            out = f""{self.calculator.directory:s}""
        else:
            out = "".""

        return out",./Asparagus/asparagus/interface/shell_ase.py
ShellCalculator,"class ShellCalculator(FileIOCalculator):
    """"""
    ASE Calculator class modifying and executing a template shell file which
    computes atoms properties and provide the results as compatible ASE format.

    Parameters
    ----------
    case: str, optional, default None
        Predefined case using template files in asparagus/template/shell with
        additional keyword parameter or default settings.
    files: (str, list(str)), optional, default None
        Template input files to copy into working directory and regarding for
        tag replacement.
    files_replace: dict(str, any) or list(dict(str, any)), optional, 
            default None
        Template file tag replacement commands in the form of a dictionary or
        a list of dictionaries. The keys of the dictionary is the tag in the
        template files which will be replaced by the respective item or
        its output if the item is a callable function. 
        
        If one dictionary is defined, the instructions are applied to all
        template files and if a list of dictionaries is given, each dictionary
        is applied on the template file of the same list index.
        
        The item of the dictionaries can be either a self defined callable
        function in form of 'func(ase.Atoms, **kwargs)' that returns a single
        string, a fix string itself or one of the following strings that will
        order the execution of one the pre-defined functions with the
        respective outputs:
            item        output
            '$xyz'      Lines of element symbols and Cartesian coordinates
            '$charge'   Integer of the ase.Atoms system charge
            '$dir'      Path of the working directory
            ...
    execute_file: str, optional, default files[0]
        Template file, which will be executed by the shell command.
        If not defined, the (first) template file in 'files' will be assumed
        as executable.
    result_properties: (str, list(str)), optional, default ['energy']
        List of system properties of the respective atoms object which are
        expected to be stored in the result file.
    result_file: str, optional, default 'result.json'
        Result file path where the calculation results are stored.
    result_file_path: str, optional, default 'json'
        Result file format to define the way of reading the results.
    atoms: ase.Atoms, optional, default None
        Optional Atoms object to which the calculator will be
        attached.  When restarting, atoms will get its positions and
        unit-cell updated from file.
    charge: int, optional, default 0
        Default atoms charge
    multiplicity: int, optional, default 1
        Default system spin multiplicity (2*S + 1)
    command: str, optional, default 'bash'
        Command to start the calculation.
    label: str, optional, default 'shell'
        Name used for all files.  Not supported by all calculators.
        May contain a directory, but please use the directory parameter
        for that instead.
        Asparagus: May be used as 'calculator_tag'.
    directory: str or PurePath
        Working directory in which to read and write files and
        perform calculations.

    """"""

    # Default parameters dictionary for initialization
    default_parameters = dict(
        files=[],
        files_replace={},
        execute_file=None,
        result_properties=['energy'],
        result_file='results.json',
        result_file_format='json',
        command='bash')

    # Discard any results if parameters were changed
    discard_results_on_any_change = True

    def __init__(
        self,
        case: Optional[str] = None,
        files: Optional[Union[str, List[str]]] = None,
        files_replace: 
            Optional[Union[List[Dict[str, Any]], Dict[str, Any]]] = None,
        execute_file: Optional[str] = None,
        result_properties: Optional[Union[str, List[str]]] = ['energy'],
        result_file: Optional[str] = 'results.json',
        result_file_format: Optional[str] = 'json',
        atoms: Optional[ase.Atoms] = None,
        charge: Optional[int] = None,
        multiplicity: Optional[int] = None,
        command: Optional[str] = 'bash',
        restart: Optional[bool] = None,
        label: Optional[str] = 'shell',
        directory: Optional[str] = 'calc',
        **kwargs
    ):
        """"""
        Initialize Shell Calculator class.
        """"""

        # Valid result file formats
        self._valid_result_file_format = {
            'npz': self.load_results_npz,
            'json': self.load_results_json,
            }

        # Initialize parent class
        FileIOCalculator.__init__(
            self,
            atoms=atoms,
            label=label,
            directory=directory,
            restart=restart,
            command=command,
            **kwargs)

        # Set calculator parameters
        self.set_shell(
            case,
            files,
            files_replace,
            execute_file,
            result_properties,
            result_file,
            result_file_format,
            charge,
            multiplicity,
            **kwargs)
        
        # Initialize implemented properties list
        self.implemented_properties = self.result_properties
        
        # Initialize tag replacement class
        self.tag_replacer = TagReplacement(self)
        
        # Initialize a point charge object
        self.pcpot = None
        
        # Set convergence flag
        self.converged = False

        return

    def __str__(self):
        return f""ShellCalculator {self.execute_file:s}""

    def set_shell(
        self,
        case: str,
        files: Union[str, List[str]],
        files_replace: Union[List[Dict[str, Any]], Dict[str, Any]],
        execute_file: str,
        result_properties: Union[str, List[str]],
        result_file: str,
        result_file_format: str,
        charge: int,
        multiplicity: int,
        **kwargs,
    ):
        """"""
        Check and set calculator parameters.

        """"""
        
        # Check template files and executable file
        files, files_replace, execute_file = self.set_input_files(
            case,
            files,
            files_replace, 
            execute_file,
            **kwargs)

        # Check result files and properties
        result_properties, result_file, result_file_format = (
            self.set_result_files(
                result_properties,
                result_file, 
                result_file_format)
            )

        # Check default system properties
        charge, multiplicity = self.set_system_properties(
            charge,
            multiplicity)
        
        # Set parameters
        self.case = case
        self.files = files
        self.files_replace = files_replace
        self.execute_file = execute_file
        self.result_properties = result_properties
        self.result_file = result_file
        self.result_file_format = result_file_format
        self.charge = charge
        self.multiplicity = multiplicity
        
        # Run parent file setup
        changed_parameters = FileIOCalculator.set(
            self, 
            files=self.files,
            files_replace=self.files_replace,
            execute_file=self.execute_file,
            result_properties=self.result_properties,
            result_file=self.result_file,
            result_file_format=self.result_file_format,
            charge=self.charge,
            multiplicity=self.multiplicity,
            **kwargs)
        if changed_parameters:
            self.reset()

        return

    def set_input_files(
        self,
        case: str,
        files: Union[str, List[str]],
        files_replace: Union[List[Dict[str, Any]], Dict[str, Any]],
        execute_file: str,
        **kwargs,
    ):
        """"""
        Check template files and executable file.

        """"""
        
        # Check for predefined cases
        if case is not None and case in templates.shell.shell_cases_available:
            
            files, case_files_replace, execute_file = (
                templates.shell.shell_cases_available[case](**kwargs))
            
            # Set or update files keyword replacement instructions
            if files_replace is None:
                files_replace = case_files_replace
            elif utils.is_dictionary(files_replace):
                files_replace.update({
                    key: item for key, item in case_files_replace.items()
                    if key not in files_replace})
            elif utils.is_dictionary_array(files_replace):
                [
                    files_replace_i.update({
                        key: item for key, item in case_files_replace.items()
                        if key not in files_replace})
                    for files_replace_i in files_replace
                ]

        # Check if template file exists
        if utils.is_string(files):
            files = [files]
        elif not utils.is_array_like(files):
            raise SyntaxError(
                ""Template files is neither a valid file path (str) or ""
                ""a list of file paths!"")
        for file_i in files:
            if not utils.is_string(file_i):
                raise SyntaxError(
                    f""Template file path '{file_i}' is not a valid file path!"")
            if not os.path.exists(file_i):
                raise SyntaxError(
                    f""Template file '{file_i:s}' does not exists!"")

        # Check for directory conflict
        for file_i in files:
            if (
                os.path.abspath(self.directory)
                == os.path.abspath(os.path.split(file_i)[0])
            ):
                raise SyntaxError(
                    f""Working directory '{os.path.abspath(self.directory):s}' ""
                    + ""for the calculation is the same path as for at least ""
                    + ""one of the template files ""
                    + f""'{os.path.abspath(file_i):s}'!\n""
                    + ""Avoid such conflict and the potential overwriting of ""
                    + ""any template file."")

        # Check executable file
        if execute_file is None:
            execute_file = os.path.split(files[0])[1]
        elif utils.is_string(execute_file):
            if (not os.path.exists(execute_file) and not any([
                execute_file in os.path.split(file_i)[1] for file_i in files])
            ):
                raise SyntaxError(
                    f""Executable file '{execute_file:s}' does not exists!"")
            else:
                execute_file = os.path.split(execute_file)[1]
        else:
            raise SyntaxError(
                f""Executable file '{execute_file}' is not a valid file path!"")

        # Check if template file replacement input
        if utils.is_dictionary(files_replace):
            files_replace = [files_replace]
        elif not utils.is_array_like(files_replace):
            raise SyntaxError(
                ""Template file replacement input is not of type dictionary ""
                + ""or a list of dictionaries!"")
        if len(files_replace) == 1:
            files_replace = files_replace*len(files)
        elif not len(files_replace) == len(files):
            raise SyntaxError(
                ""Mismatch between number of template file replacement "" 
                + f"" dictionaries ({len(files_replace)}) and numer of ""
                + f""template files ({len(files)})!"")
        for files_replace_i in files_replace:
            if not utils.is_dictionary(files_replace_i):
                raise SyntaxError(
                    f""Template file replacement input '{files_replace_i}' ""
                    + ""is not of type dictionary!"")
            
        return files, files_replace, execute_file

    def set_result_files(
        self,
        result_properties: Union[str, List[str]],
        result_file: str,
        result_file_format: str,
    ):
        """"""
        Check result files and properties.

        """"""

        # Check result properties
        if utils.is_string(result_properties):
            result_properties = [result_properties]
        elif not utils.is_array_like(result_properties):
            raise SyntaxError(
                ""System properties 'result_properties' is neither a property ""
                ""label (str) or a list of property labels!"")
        for prop_i in result_properties:
            if not utils.is_string(prop_i):
                raise SyntaxError(
                    f""System property '{prop_i}' is not a valid property ""
                    + ""label (str)!"")

        # Check result file path and format
        if not utils.is_string(result_file):
            raise SyntaxError(
                f""Result file '{result_file}' is not a valid file path!"")
        if result_file_format[0] == '.':
            result_file_format = result_file_format[1:]
        if result_file_format.lower() not in self._valid_result_file_format:
            raise SyntaxError(
                f""Result file format '{result_file_format}' is not a valid ""
                + "" file format!\nValid options are:\n  ""
                + str(list(result_file_format.keys())))

        return result_properties, result_file, result_file_format
        
    def set_system_properties(
        self,
        charge: int,
        multiplicity: int,
    ):
        """"""
        Check system properties

        """"""
        
        # Check charge
        if charge is not None and not utils.is_numeric(charge):
            raise SyntaxError(
                ""System charge input is not a numeric input!"")

        # Check multiplicity
        if multiplicity is not None and not utils.is_numeric(multiplicity):
            raise SyntaxError(
                ""System spin multiplicity input is not a numeric input!"")

        return charge, multiplicity

    def write_input(
        self,
        atoms: ase.Atoms,
        properties: Optional[List[str]] = None, 
        system_changes: Optional[List[str]] = None,
        charge: Optional[int] = None,
        multiplicity: Optional[int] = None,
    ):
        """"""
        Write input files by copying template files to the working directory
        while applying the replacement tasks.
        
        Parameters
        ----------
        atoms: ase.Atoms
            Reference atoms object
        properties: list(str), optional, default None
            List of to computing system properties
        system_changes: list(str), optional, default None
            Detailed list of changed system parameters with regard to 
            previously computed system.

        """"""
        
        # Execute parent class 'write_input' function that checks and
        # eventually create the working directory.
        FileIOCalculator.write_input(self, atoms, properties, system_changes)
        
        # Get parameter set
        parameters = Parameters(self.parameters.copy())
        
        # Set system charge and spin multiplicity
        if charge is not None:
            charge = self.set_system_properties(charge, multiplicity)[0]
        elif self.charge is None and 'charge' in atoms.info:
            charge = self.set_system_properties(atoms.info['charge'], None)[0]
        elif self.charge is None:
            charge = 0
        else:
            charge = self.charge
        if multiplicity is None:
            multiplicity = self.multiplicity
        elif self.multiplicity is None and 'multiplicity' in atoms.info:
            multiplicity = self.set_system_properties(
                None, atoms.info['multiplicity'])[1]
        elif self.multiplicity is None:
            multiplicity = 1
        else:
            multiplicity = self.multiplicity
        parameters['charge'] = charge
        parameters['multiplicity'] = multiplicity
        
        # Write calculator options to file
        parameters.write(self.label + '.ase')
        parameters['label'] = self.label
        
        # Prepare and write input files
        self.write_shell_input(
            atoms,
            parameters,
            self.files,
            self.files_replace)

        return

    def write_shell_input(
        self,
        atoms: ase.Atoms,
        parameters: Dict[str, Any],
        files: List[str], 
        files_replace: List[Dict[str, Any]],
        **kwargs
    ):
        """"""
        Prepare and write shell input files

        """"""
    
        # Iterate over template files
        for ifile, file_i in enumerate(files):

            # Get tag replacement instructions
            file_replace = files_replace[ifile]
            
            # Read template file
            with open(file_i, 'r') as f:
                flines = f.read()

            # Replace tags
            for tag, item in file_replace.items():
                # Check replacement
                if utils.is_string(item):
                    if item in self.tag_replacer.functions:
                        item_str = self.tag_replacer[item](
                            atoms, parameters=parameters)
                    else:
                        item_str = item                    
                elif utils.is_callable(item):
                    item_str = item(atoms, parameters=parameters)
                else:
                    item_str = str(item)

                # Replace label tag with item string
                flines = flines.replace(
                    tag, 
                    item_str)

            # Write working file
            wfile_i = os.path.split(file_i)[1]
            with open(os.path.join(self.directory, wfile_i), 'w') as f:
                f.write(flines)

        return

    def calculate(
        self,
        atoms: Optional[ase.Atoms] = None,
        properties: Optional[List[str]] = ['energy'],
        system_changes: Optional[List[str]] = None,
        **kwargs,
    ):
        """"""
        Execute calculation and read results

        """"""
        
        # Prepare calculation by execution parent class function
        Calculator.calculate(self, atoms, properties, system_changes)
        
        # Write input files
        self.write_input(
            self.atoms, 
            properties, 
            system_changes,
            **kwargs)
        
        # Check shell command 
        if self.command is None:
            command = 'bash'
        else:
            command = self.command

        # Check executable file
        if self.execute_file is None:
            execute_file = os.path.split(self.files[0])[1]
        else:
            execute_file = self.execute_file

        # Execute command with executable file
        proc = subprocess.Popen([command, execute_file], cwd=self.directory)
        errorcode = proc.wait()
        if errorcode:
            msg = (
                f""Calculator '{self.label:s}' failed with command ""
                + f""'{command:s} {execute_file:s}' failed in ""
                + f""'{self.directory:}' with error code '{errorcode}'!"")
            raise OSError(msg)
        
        self.read_results()

        return

    def read_results(
        self,
    ):
        """"""
        Read results from the defined result file

        """"""
        
        # Read results from file
        self._valid_result_file_format[self.result_file_format](
            os.path.join(self.directory, self.result_file))

        # Check for completeness
        self.converged = True
        for prop_i in self.result_properties:
            if prop_i not in self.results:
                self.converged = False

        return

    def load_results_npz(self):
        raise NotImplementedError

    def load_results_json(
        self,
        result_file: str,
    ):
        
        # Open result file
        if os.path.exists(result_file):
            with open(result_file, 'r') as f:
                results = json.load(f)
        else:
            results = {}

        # Convert lists to np.ndarrays
        self.results = {}
        for prop_i, result in results.items():
            self.results[prop_i] = np.array(result, dtype=float)

        return",./Asparagus/asparagus/interface/shell_ase.py
PyCharmm_Calculator,"class PyCharmm_Calculator:
    """"""
    Calculator for the interface between PyCHARMM and Asparagus.

    Parameters
    ----------
    model_calculator: torch.nn.Module
        Asparagus model calculator object with already loaded parameter set
    ml_atom_indices: list(int)
        List of atom indices referring to the ML treated atoms in the total 
        system loaded in CHARMM
    ml_atomic_numbers: list(int)
        Respective atomic numbers of the ML atom selection
    ml_charge: float
        Total charge of the partial ML atom selection
    ml_fluctuating_charges: bool
        If True, electrostatic interaction contribution between the MM atom
        charges and the model predicted ML atom charges. Else, the ML atom
        charges are considered fixed as defined by the CHARMM psf file.
    mlmm_atomic_charges: list(float)
        List of all atomic charges of the system loaded to CHARMM.
        If 'ml_fluctuating_charges' is True, the atomic charges of the ML
        atoms are ignored (usually set to zero anyways) and their atomic
        charge prediction is used.
    mlmm_cutoff: float
        Interaction cutoff distance for ML/MM electrostatic interactions
    mlmm_cuton: float
        Lower atom pair distance to start interaction switch-off for ML/MM
        electrostatic interactions
    mlmm_lambda: float, optional, default None
        ML/MM electrostatic interactions scaling factor. If None, no scaling
        is applied.
    **kwargs
        Additional keyword arguments.

    """"""

    def __init__(
        self,
        model_calculator: Union[torch.nn.Module, List[torch.nn.Module]],
        ml_atom_indices: Optional[List[int]] = None,
        ml_atomic_numbers: Optional[List[int]] = None,
        ml_charge: Optional[float] = None,
        ml_fluctuating_charges: Optional[bool] = None,
        mlmm_atomic_charges: Optional[List[float]] = None,
        mlmm_cutoff: Optional[float] = None,
        mlmm_cuton: Optional[float] = None,
        mlmm_lambda: Optional[float] = None,
        **kwargs
    ):

        # Assign dtype
        self.dtype = model_calculator.dtype

        ################################
        # # # Set PyCHARMM Options # # #
        ################################
        
        # Number of machine learning (ML) atoms
        self.ml_num_atoms = torch.tensor(
            [len(ml_atom_indices)], dtype=torch.int64)

        # ML atom indices
        self.ml_atom_indices = torch.tensor(ml_atom_indices, dtype=torch.int64)
        self.tt = np.array(ml_atom_indices, dtype=int)
        # ML atomic numbers
        self.ml_atomic_numbers = torch.tensor(
            ml_atomic_numbers, dtype=torch.int64)
        
        # ML atom total charge
        self.ml_charge = torch.tensor(ml_charge, dtype=self.dtype)

        # ML fluctuating charges
        self.ml_fluctuating_charges = ml_fluctuating_charges

        # ML and MM atom charges
        self.mlmm_atomic_charges = torch.tensor(
            mlmm_atomic_charges, dtype=self.dtype)

        # ML and MM number of atoms
        self.mlmm_num_atoms = len(mlmm_atomic_charges)
        
        # ML and MM atom indices
        self.mlmm_atom_indices = torch.zeros(
            self.mlmm_num_atoms, dtype=torch.int64)
        self.mlmm_atom_indices[self.ml_atom_indices] = self.ml_atomic_numbers
        
        # ML atoms - atom indices pointing from MLMM position to ML position
        # 0, 1, 2 ..., ml_num_atoms: ML atom 1, 2, 3 ... ml_num_atoms + 1
        # ml_num_atoms + 1: MM atoms
        ml_idxp = np.full(self.mlmm_num_atoms, -1)
        for ia, ai in enumerate(ml_atom_indices):
            ml_idxp[ai] = ia
        self.ml_idxp = torch.tensor(ml_idxp, dtype=torch.int64)
        
        # Running number list
        self.mlmm_idxa = torch.arange(self.mlmm_num_atoms, dtype=torch.int64)

        # Non-bonding interaction range
        self.mlmm_cutoff = torch.tensor(mlmm_cutoff, dtype=self.dtype)
        self.mlmm_cuton = torch.tensor(mlmm_cuton, dtype=self.dtype)
        
        # Non-bonding electrostatic scaling factor
        if mlmm_lambda is None:
            self.mlmm_lambda = torch.tensor(1.0, dtype=self.dtype)
        else:
            self.mlmm_lambda = torch.tensor(mlmm_lambda, dtype=self.dtype)

        ################################
        # # # Set Model Calculator # # #
        ################################

        # In case of model calculator is a list of models
        if utils.is_array_like(model_calculator):
            self.model_calculator = None
            self.model_calculator_list = model_calculator
            self.model_calculator_num = len(model_calculator)
            self.model_ensemble = True
        else:
            self.model_calculator = model_calculator
            self.model_calculator_list = None
            self.model_calculator_num = 1
            self.model_ensemble = False

        # Get implemented model properties
        if self.model_ensemble:
            self.implemented_properties = (
                self.model_calculator_list[0].model_properties)
            # Check model properties and set evaluation mode
            for ic, calc in enumerate(self.model_calculator_list):
                for prop in self.implemented_properties:
                    if prop not in calc.model_properties:
                        raise SyntaxError(
                            f""Model calculator {ic:d} does not predict ""
                            + f""property {prop:s}!\n""
                            + ""Specify 'implemented_properties' with ""
                            + ""properties all model calculator support."")
        else:
            self.implemented_properties = (
                self.model_calculator.model_properties)

        # Check if model calculator has loaded a checkpoint file or stored
        # if model parameters are stored in a checkpoint file
        if self.model_calculator is None:
            for ic, calc in enumerate(self.model_calculator_list):
                if not calc.checkpoint_loaded:
                    raise SyntaxError(
                        f""Model calculator {ic:d} does not seem to have a ""
                        + ""proper parameter set loaded from a checkpoint file.""
                        + ""\nMake sure parameters are loaded otherwise ""
                        + ""model predictions are random."")
        else:
            if not self.model_calculator.checkpoint_loaded:
                raise SyntaxError(
                    ""The model calculator does not seem to have a ""
                    + ""proper parameter set loaded from a checkpoint file.""
                    + ""\nMake sure parameters are loaded otherwise ""
                    + ""model predictions are random."")

        #############################
        # # # Set ML/MM Options # # #
        #############################

        # Set model to evaluation mode
        if self.model_ensemble:
            for calc in self.model_calculator_list:
                calc.eval()
        else:
            self.model_calculator.eval()

        # Get property unit conversions from model units to CHARMM units
        self.model_unit_properties = (
            self.model_calculator.model_unit_properties)
        self.model2charmm_unit_conversion = {}

        # Positions unit conversion
        conversion, _ = utils.check_units(
            CHARMM_calculator_units['positions'],
            self.model_unit_properties.get('positions'))
        self.model2charmm_unit_conversion['positions'] = conversion

        # Implemented property units conversion
        for prop in self.implemented_properties:
            conversion, _ = utils.check_units(
                CHARMM_calculator_units[prop],
                self.model_unit_properties.get(prop))
            self.model2charmm_unit_conversion[prop] = conversion

        # Initialize the non-bonded interaction calculator
        if self.ml_fluctuating_charges:

            # Convert 1/(2*4*pi*epsilon) from e**2/eV/Ang to CHARMM units
            kehalf_ase = 7.199822675975274
            conversion, _ = utils.check_units(
                self.model_unit_properties.get('energy'))
            self.kehalf = torch.tensor(
                [kehalf_ase*1.**2/conversion/1.],
                dtype=self.dtype)

            self.electrostatics_calc = Electrostatic_shift(
                self.mlmm_cutoff,
                self.mlmm_cuton,
                self.ml_idxp,
                self.mlmm_atomic_charges,
                kehalf=self.kehalf,
                )

        else:

            self.electrostatics_calc = None

    def calculate_charmm(
        self,
        Natom: int,
        Ntrans: int,
        Natim: int,
        idxp: List[float],
        x: List[float],
        y: List[float],
        z: List[float],
        dx: List[float],
        dy: List[float],
        dz: List[float],
        Nmlp: int,
        Nmlmmp: int,
        idxi: List[int],
        idxj: List[int],
        idxjp: List[int],
        idxu: List[int],
        idxv: List[int],
        idxup: List[int],
        idxvp: List[int],
    ) -> float:
        """"""
        This function matches the signature of the corresponding MLPot class in
        PyCHARMM.

        Parameters
        ----------
        Natom: int
            Number of atoms in primary cell
        Ntrans: int
            Number of unit cells (primary + images)
        Natim: int
            Number of atoms in primary and image unit cells
        idxp: list(int)
            List of primary and primary to image atom index pointer
        x: list(float)
            List of x coordinates 
        y: list(float)
            List of y coordinates
        z: list(float)
            List of z coordinates
        dx: list(float)
            List of x derivatives
        dy: list(float)
            List of y derivatives
        dz: list(float)
            List of z derivatives
        Nmlp: int
            Number of ML atom pairs in the system
        Nmlmmp: int
            Number of ML/MM atom pairs in the system
        idxi: list(int)
            List of ML atom indices for ML potential
        idxj: list(int)
            List of ML atom indices for ML potential
        idxjp: list(int)
            List of image to primary ML atom index pointer
        idxu: list(int)
            List of ML atom indices for ML-MM embedding potential
        idxv: list(int)
            List of MM atom indices for ML-MM embedding potential
        idxup: list(int)
            List of image to primary ML atom index pointer
        idxvp: list(int)
            List of image to primary MM atom index pointer

        Return
        ------
        float
            ML potential plus ML-MM embedding potential
        """"""

        # Assign all positions
        if Ntrans:
            mlmm_R = torch.transpose(
                torch.tensor(
                    [x[:Natim], y[:Natim], z[:Natim]], dtype=self.dtype
                ),
                0, 1)
            mlmm_idxp = idxp[:Natim]
        else:
            mlmm_R = torch.transpose(
                torch.tensor(
                    [x[:Natom], y[:Natom], z[:Natom]], dtype=self.dtype
                ),
                0, 1)
            mlmm_idxp = idxp[:Natom]
        mlmm_R.requires_grad_(True)

        # Assign indices
        # ML-ML pair indices
        ml_idxi = torch.tensor(idxi[:Nmlp], dtype=torch.int64)
        ml_idxj = torch.tensor(idxj[:Nmlp], dtype=torch.int64)
        ml_idxjp = torch.tensor(idxjp[:Nmlp], dtype=torch.int64)
        ml_sysi = torch.zeros(self.ml_num_atoms, dtype=torch.int64)
        # ML-MM pair indices and pointer
        mlmm_idxu = torch.tensor(
            idxu[:Nmlmmp], dtype=torch.int64)
        mlmm_idxv = torch.tensor(
            idxv[:Nmlmmp], dtype=torch.int64)
        mlmm_idxup = torch.tensor(
            idxup[:Nmlmmp], dtype=torch.int64)
        mlmm_idxvp = torch.tensor(
            idxvp[:Nmlmmp], dtype=torch.int64)

        # Create batch for evaluating the model
        atoms_batch = {}
        atoms_batch['atoms_number'] = self.ml_num_atoms
        atoms_batch['atomic_numbers'] = self.ml_atomic_numbers
        atoms_batch['positions'] = mlmm_R
        atoms_batch['charge'] = self.ml_charge
        atoms_batch['idx_i'] = ml_idxi
        atoms_batch['idx_j'] = ml_idxj
        atoms_batch['sys_i'] = ml_sysi
        
        # PBC options
        atoms_batch['pbc_offset_ij'] = None
        atoms_batch['pbc_offset_uv'] = None
        atoms_batch['pbc_atoms'] = self.ml_atom_indices
        atoms_batch['pbc_idx'] = self.ml_idxp
        atoms_batch['pbc_idx_j'] = ml_idxjp

        # Compute model properties
        results = {}
        if self.model_ensemble:

            # TODO Test
            for ic, calc in enumerate(self.model_calculator_list):
                results[ic] = calc(atoms_batch)
            for prop in self.implemented_properties:
                prop_std = f""std_{prop:s}""
                results[prop_std], self.results[prop] = torch.std_mean(
                    torch.cat(
                        [
                            results[ic][prop]
                            for ic in range(self.model_calculator_num)
                        ],
                        dim=0),
                    dim=0)

        else:

            results = self.model_calculator(atoms_batch)

        # Unit conversion
        self.results = {}
        for prop in self.implemented_properties:
            self.results[prop] = (
                results[prop]*self.model2charmm_unit_conversion[prop])

        # Apply dtype conversion
        ml_Epot = self.results['energy'].detach().numpy()
        ml_F = self.results['forces'].detach().numpy().ctypes.data_as(
            ctypes.POINTER(ctypes.c_double))

        # Add forces to CHARMM derivative arrays
        for ai in self.ml_atom_indices:
            ii = 3*ai
            dx[ai] -= ml_F[ii]
            dy[ai] -= ml_F[ii+1]
            dz[ai] -= ml_F[ii+2]

        # Calculate electrostatic energy and force contribution
        if self.electrostatics_calc is not None:
            
            mlmm_Eele, mlmm_gradient = self.electrostatics_calc.run(
                mlmm_R,
                self.results['atomic_charges'],
                mlmm_idxu,
                mlmm_idxv,
                mlmm_idxup,
                mlmm_idxvp)

            # Add electrostatic interaction potential to ML energy
            self.mlmm_Eele = (
                mlmm_Eele*self.mlmm_lambda
                * self.model2charmm_unit_conversion['energy']
                ).detach().numpy()
            ml_Epot += self.mlmm_Eele

            # Apply dtype conversion
            mlmm_F = (
                -mlmm_gradient*self.mlmm_lambda
                * self.model2charmm_unit_conversion['forces']
                ).detach().numpy().ctypes.data_as(
                    ctypes.POINTER(ctypes.c_double)
                    )

            # Add electrostatic forces to CHARMM derivative arrays
            for ia, ai in enumerate(mlmm_idxp):
                ii = 3*ia
                dx[ai] -= mlmm_F[ii]
                dy[ai] -= mlmm_F[ii+1]
                dz[ai] -= mlmm_F[ii+2]

        return ml_Epot

    def mlmm_elec(self):
        return self.mlmm_Eele",./Asparagus/asparagus/interface/model_pycharmm.py
Electrostatic_shift,"class Electrostatic_shift:
    """"""
    Coulomb potential calculator between fluctuating ML atomic charges and
    static MM atomic charges.

    Parameters
    ----------
    mlmm_cutoff: torch.Tensor
        Interaction cutoff distance for ML/MM electrostatic interactions
    mlmm_cuton: torch.Tensor
        Lower atom pair distance to start interaction switch-off for ML/MM
        electrostatic interactions
    ml_idxp: torch.Tensor
        ML atoms ti atom indices pointing from MLMM position to ML position.
    mlmm_atomic_charges: torch.Tensor
        List of all atomic charges of the system loaded to CHARMM.
    kehalf: torch.Tensor
        Coulomb factor with respective unit
    switch_fn: str, optional, default 'Poly6_range'
        Type of switch off function
    
    """"""

    def __init__(
        self,
        mlmm_cutoff: torch.Tensor,
        mlmm_cuton: torch.Tensor,
        ml_idxp: torch.Tensor,
        mlmm_atomic_charges: torch.Tensor,
        kehalf: torch.Tensor,
        switch_fn='Poly6_range',
    ):

        self.mlmm_cutoff = mlmm_cutoff
        self.mlmm_cutoff2 = mlmm_cutoff**2
        self.mlmm_cuton = mlmm_cuton
        self.ml_idxp = ml_idxp
        self.mlmm_atomic_charges = mlmm_atomic_charges

        # Initialize the class for cutoff
        self.switch_fn = layer.get_cutoff_fn(switch_fn)(
            self.mlmm_cutoff, self.mlmm_cuton)
        self.kehalf = kehalf

    def calculate_mlmm_interatomic_distances(
        self,
        R: torch.Tensor,
        idxu: torch.Tensor,
        idxv: torch.Tensor,
        idxup: torch.Tensor,
        idxvp: torch.Tensor,
    ) -> (torch.Tensor, torch.Tensor, torch.Tensor):
        """"""
        Calculate ML-MM atom distances.

        Parameters
        ----------
        R: torch.tensor
            ML and MM atom positions
        idxu: torch.tensor
            List of ML atom indices for ML-MM embedding potential
        idxv: torch.tensor
            List of MM atom indices for ML-MM embedding potential
        idxup: torch.tensor
            List of image to primary ML atom index pointer
        idxvp: torch.tensor
            List of image to primary MM atom index pointer

        Return
        ------
        torch.tensor
            ML-MM atom pair distances
        torch.tensor
            ML atom pair indices
        torch.tensor
            MM atom pair indices

        """"""

        # Gather positions
        Ru = R[idxu]
        Rv = R[idxv]
        
        # Interacting atom pair distances within cutoff
        sum_distances = torch.sum((Ru - Rv)**2, dim=1)
        selection = sum_distances < self.mlmm_cutoff2
        Duv = torch.sqrt(sum_distances[selection])
        
        # Reduce the indexes to consider only interacting pairs (selection)
        # and point image atom indices to primary atom indices (idx_p)
        idxur = idxup[selection]
        idxvr = idxvp[selection]
        
        return Duv, idxur, idxvr

    def electrostatic_energy_per_atom_to_point_charge(
        self,
        Duv: torch.Tensor,
        Qau: torch.Tensor,
        Qav: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Calculate electrostatic interaction between ML atom charge and MM point
        charge based on shifted Coulomb potential scheme
        
        Parameters
        ----------
        Duv: torch.tensor
            ML-MM atom pair distances
        Qau: torch.tensor
            List of ML atomic charges
        Qav: torch.tensor
            List of MM atomic charges

        Return
        ------
        torch.tensor
            ML-MM atom pair electrostatic Coulomb interaction
        
        """"""

        # Cutoff weighted reciprocal distance
        switchoff = self.switch_fn(Duv)

        # Shifted Coulomb energy
        qq = 2.0*self.kehalf*Qau*Qav
        Eele = qq/Duv - qq/self.mlmm_cutoff*(2.0 - Duv/self.mlmm_cutoff)

        return torch.sum(switchoff*Eele)

    def run(
        self,
        mlmm_R: torch.Tensor,
        ml_Qa: torch.Tensor,
        mlmm_idxu: torch.Tensor,
        mlmm_idxv: torch.Tensor,
        mlmm_idxup: torch.Tensor,
        mlmm_idxvp: torch.Tensor,
    ) -> (torch.Tensor, torch.Tensor):
        """"""
        Calculates the electrostatic interaction between ML atoms in the 
        primary cell with all MM atoms in the primary or imaginary non-bonded
        lists.
        
        Parameters
        ----------
        mlmm_R: torch.tensor
            ML and MM atom positions
        ml_Qa: torch.tensor
            ML atomic charges
        mlmm_idxu: torch.tensor
            List of ML atom indices for ML-MM embedding potential
        mlmm_idxv: torch.tensor
            List of MM atom indices for ML-MM embedding potential
        mlmm_idxvp: torch.tensor
            List of image to primary ML atom index pointer
        mlmm_idxvp: torch.tensor
            List of image to primary MM atom index pointer

        Return
        ------
        torch.tensor
            Total ML-MM electrostatic Coulomb interaction
        torch.tensor
            ML and MM atom electrostatic Coulomb forces
        
        """"""
        
        # Calculate ML-MM atom distances
        mlmm_Duv, mlmm_idxur, mlmm_idxvr = (
            self.calculate_mlmm_interatomic_distances(
                mlmm_R, mlmm_idxu, mlmm_idxv, mlmm_idxup, mlmm_idxvp)
            )

        # Point from PyCHARMM ML atom indices to model calculator atom indices
        ml_idxur = self.ml_idxp[mlmm_idxur]
        
        # Get ML and MM charges
        ml_Qau = ml_Qa[ml_idxur]
        ml_Qav = self.mlmm_atomic_charges[mlmm_idxvr]

        # Calculate electrostatic energy and gradients
        Eele = self.electrostatic_energy_per_atom_to_point_charge(
            mlmm_Duv, ml_Qau, ml_Qav)
        Eele_gradient = torch.autograd.grad(
                torch.sum(Eele),
                mlmm_R,
                retain_graph=True)[0]

        return Eele, Eele_gradient",./Asparagus/asparagus/interface/model_pycharmm.py
ORCA,"class ORCA(FileIOCalculator):

    implemented_properties = ['energy', 'forces', 'dipole']

    if 'ORCA_COMMAND' in os.environ:
        command = os.environ['ORCA_COMMAND'] + ' PREFIX.inp > PREFIX.out'
    else:
        command = 'orca PREFIX.inp > PREFIX.out'

    default_parameters = dict(
        charge=0, mult=1,
        task='gradient',
        orcasimpleinput='tightscf PBE def2-SVP',
        orcablocks='%scf maxiter 200 end')

    def __init__(
        self,
        restart=None,
        label='orca',
        atoms=None,
        **kwargs
    ):
        """"""
        ASE interface to ORCA 5
        by Ragnar Bjornsson, Based on NWchem interface but simplified.
        Only supports energies and gradients (no dipole moments,
        orbital energies etc.) for now.

        For more ORCA-keyword flexibility, method/xc/basis etc.
        keywords are not used. Instead, two keywords:

            orcasimpleinput: str
                What you'd put after the ""!"" in an orca input file.

            orcablock: str
                What you'd put in the ""% ... end""-blocks.

        are used to define the ORCA simple-inputline and the ORCA-block input.
        This allows for more flexible use of any ORCA method or keyword
        available in ORCA instead of hardcoding stuff.

        Point Charge IO functionality added by A. Dohn.
        """"""
        FileIOCalculator.__init__(
            self,
            atoms=atoms,
            label=label,
            restart=restart,
            **kwargs)

        self.pcpot = None
        self.converged = False

    def set(self, **kwargs):
        changed_parameters = FileIOCalculator.set(self, **kwargs)
        if changed_parameters:
            self.reset()

    def write_input(self, atoms, properties=None, system_changes=None):
        FileIOCalculator.write_input(self, atoms, properties, system_changes)
        p = self.parameters
        p.write(self.label + '.ase')
        p['label'] = self.label
        if self.pcpot:  # also write point charge file and add things to input
            p['pcpot'] = self.pcpot

        self.write_orca(atoms, **p)

    def read(self, label):
        FileIOCalculator.read(self, label)
        if not os.path.isfile(self.label + '.out'):
            raise ReadError

        with open(self.label + '.inp') as fd:
            for line in fd:
                if line.startswith('geometry'):
                    break
            symbols = []
            positions = []
            for line in fd:
                if line.startswith('end'):
                    break
                words = line.split()
                symbols.append(words[0])
                positions.append([float(word) for word in words[1:]])

        self.converged = True
        self.parameters = Parameters.read(self.label + '.ase')
        self.read_results()

    def read_results(self):
        self.read_energy()
        if self.parameters.task.find('gradient') > -1:
            self.read_forces()
        self.read_dipole()

    def read_energy(self):
        """"""Read Energy from ORCA output file.""""""
        with open(self.label + '.out', mode='r', encoding='utf-8') as fd:
            text = fd.read()
        # Energy:
        re_energy = re.compile(r""FINAL SINGLE POINT ENERGY.*\n"")
        re_not_converged = re.compile(r""Wavefunction not fully converged"")
        found_line = re_energy.search(text)
        if found_line and not re_not_converged.search(found_line.group()):
            self.results['energy'] = float(found_line.group().split()[-1])*Hartree
        else:
            self.converged = False
        return

    def read_forces(self):
        """"""Read Forces from ORCA output file.""""""
        with open(f'{self.label}.engrad', 'r') as fd:
            lines = fd.readlines()
        getgrad = False
        gradients = []
        tempgrad = []
        for i, line in enumerate(lines):
            if line.find('# The current gradient') >= 0:
                getgrad = True
                gradients = []
                tempgrad = []
                continue
            if getgrad and ""#"" not in line:
                grad = line.split()[-1]
                tempgrad.append(float(grad))
                if len(tempgrad) == 3:
                    gradients.append(tempgrad)
                    tempgrad = []
            if '# The at' in line:
                getgrad = False
        self.results['forces'] = -np.array(gradients) * Hartree / Bohr

    def read_dipole(self):
        """"""Read Dipole from ORCA output file.""""""
        with open(self.label + '.out', mode='r', encoding='utf-8') as fd:
            text = fd.read()
        re_dipole = re.compile(r""Total Dipole Moment.*\n"")
        re_not_converged = re.compile(r""Wavefunction not fully converged"")
        found_line = re_dipole.search(text)
        if found_line and not re_not_converged.search(found_line.group()):
            self.results['dipole'] = np.array(
                found_line.group().split()[-3:], dtype=float) * Bohr
        else:
            self.converged = False
        return

    def embed(self, mmcharges=None, **parameters):
        """"""Embed atoms in point-charges (mmcharges)
        """"""
        self.pcpot = PointChargePotential(mmcharges, label=self.label)
        return self.pcpot

    def write_orca(self, atoms, **params):
        ''' Function to write ORCA input file '''
        charge = params['charge']
        mult = params['mult']
        label = params['label']

        if 'pcpot' in params.keys():
            pcpot = params['pcpot']
            pcstring = '% pointcharges \""' +\
                    label + '.pc\""\n\n'
            params['orcablocks'] += pcstring
            pcpot.write_mmcharges(label)

        with open(label + '.inp', 'w') as fd:
            fd.write(""! engrad %s \n"" % params['orcasimpleinput'])
            fd.write(""%s \n"" % params['orcablocks'])

            fd.write('*xyz')
            fd.write("" %d"" % charge)
            fd.write("" %d \n"" % mult)
            for atom in atoms:
                if atom.tag == 71:  # 71 is ascii G (Ghost)
                    symbol = atom.symbol + ' : '
                else:
                    symbol = atom.symbol + '   '
                fd.write(symbol +
                        str(atom.position[0]) + ' ' +
                        str(atom.position[1]) + ' ' +
                        str(atom.position[2]) + '\n')
            fd.write('*\n')

        return",./Asparagus/asparagus/interface/orca_ase.py
PointChargePotential,"class PointChargePotential:
    def __init__(self, mmcharges, label=None, positions=None, directory=None):
        """""" Point Charge Potential Interface to ORCA """"""
        if positions is not None:
            self.set_positions(positions)
        if directory is None:
            directory = os.getcwd()

        self.directory = directory + os.sep
        self.mmcharges = mmcharges
        self.label = label

    def set_positions(self, positions):
        self.positions = positions

    def set_charges(self, mmcharges):
        self.q_p = mmcharges

    def write_mmcharges(self, filename):
        pc_file = open(os.path.join(self.directory,
                                    filename + '.pc'), 'w')

        pc_file.write('{0:d}\n'.format(len(self.mmcharges)))
        for [pos, pc] in zip(self.positions, self.mmcharges):
            [x, y, z] = pos
            pc_file.write('{0:12.6f} {1:12.6f} {2:12.6f} {3:12.6f}\n'
                          .format(pc, x, y, z))

        pc_file.close()

    def get_forces(self, calc):
        ''' reads forces on point charges from .pcgrad file '''
        with open(os.path.join(self.directory, self.label + '.pcgrad'),
                  'r', encoding='utf-8') as fd:
            lines = fd.readlines()
        numpc = int(lines[0])
        forces = np.zeros((numpc, 3))
        for i in range(numpc):
            [fx, fy, fz] = [float(f) for f in lines[i + 1].split()]
            forces[i, :] = fx, fy, fz

        return -forces * Hartree / Bohr

    @property
    def calculator_tag(self):
        return self.label",./Asparagus/asparagus/interface/orca_ase.py
SlurmCalculator,"class SlurmCalculator(ShellCalculator):
    """"""
    ASE Calculator class modifying and executing a template slurm submission
    file which computes atoms properties and provide the results as compatible
    ASE format.

    Parameters
    ----------
    files: (str, list(str))
        Template input files to copy into working directory and regarding for
        tag replacement.
    files_replace: dict(str, any) or list(dict(str, any))
        Template file tag replacement commands in the form of a dictionary or
        a list of dictionaries. The keys of the dictionary is the tag in the
        template files which will be replaced by the respective item or
        its output if the item is a callable function. 
        
        If one dictionary is defined, the instructions are applied to all
        template files and if a list of dictionaries is given, each dictionary
        is applied on the template file of the same list index.
        
        The item of the dictionaries can be either a self defined callable
        function in form of 'func(ase.Atoms, **kwargs)' that returns a single
        string, a fix string itself or one of the following strings that will
        order the execution of one the pre-defined functions with the
        respective outputs:
            item        output
            '$xyz'      Lines of element symbols and Cartesian coordinates
            '$charge'   Integer of the ase.Atoms system charge
            '$dir'      Path of the working directory
            ...
    execute_file: str, optional, default files[0]
        Template slurm submission file, which will be executed by the shell 
        command. If not defined, the (first) template file in 'files' will be
        assumed as executable.
    result_properties: (str, list(str)), optional, default ['energy']
        List of system properties of the respective atoms object which are
        expected to be stored in the result file.
    result_file: str, optional, default 'result.json'
        Result file path where the calculation results are stored.
    result_file_path: str, optional, default 'json'
        Result file format to define the way of reading the results.
    atoms: ase.Atoms, optional, default None
        Optional Atoms object to which the calculator will be
        attached.  When restarting, atoms will get its positions and
        unit-cell updated from file.
    charge: int, optional, default 0
        Default atoms charge
    multiplicity: int, optional, default 1
        Default system spin multiplicity (2*S + 1)
    command: str, optional, default 'sbatch'
        Command to start the calculation.
    remote_client: str, optional, default None
        Remote client id (e.g. 'username@server.ch') to which a connection
        is established and the calculation are performed in the directory
        'username.hostname'. The connection is established by the 'ssh -tt',
        the working directory created by 'mkdir -p' and
        the calculation files are transferred via 'scp -r'.
    scan_interval: int, optional, default 5
        Scan interval checking for completeness of the submitted slurm job
    scan_command: str, optional, default f'squeue -u {os.environ['USER']}'
        Command to obtain the current slurm job list.
    label: str, optional, default 'shell'
        Name used for all files.  Not supported by all calculators.
        May contain a directory, but please use the directory parameter
        for that instead.
        Asparagus: May be used as 'calculator_tag'.
    directory: str or PurePath
        Working directory in which to read and write files and
        perform calculations.

    """"""

    # Default parameters dictionary for initialization
    default_parameters = dict(
        files=[],
        files_replace={},
        execute_file=None,
        result_properties=['energy'],
        result_file='results.json',
        result_file_format='json',
        command='sbatch',
        scan_interval=5)

    # Discard any results if parameters were changed
    discard_results_on_any_change = True

    def __init__(
        self,
        files: Union[str, List[str]],
        files_replace: Union[List[Dict[str, Any]], Dict[str, Any]],
        execute_file: Optional[str] = None,
        result_properties: Optional[Union[str, List[str]]] = ['energy'],
        result_file: Optional[str] = 'results.json',
        result_file_format: Optional[str] = 'json',
        atoms: Optional[ase.Atoms] = None,
        charge: Optional[int] = 0,
        multiplicity: Optional[int] = 1,
        command: Optional[str] = 'sbatch',
        remote_client: Optional[str] = None,
        scan_interval: Optional[int] = 1,
        scan_command: Optional[str] = None,
        scan_catch_id: Optional[callable] = None,
        scan_check_id: Optional[callable] = None,
        restart: Optional[bool] = None,
        label: Optional[str] = 'slurm',
        directory: Optional[str] = 'calc',
        **kwargs
    ):
        """"""
        Initialize Shell Calculator class.

        """"""
        
        # Valid result file formats
        self._valid_result_file_format = {
            'npz': self.load_results_npz,
            'json': self.load_results_json,
            }

        # Initialize parent class
        ShellCalculator.__init__(
            self,
            files=files,
            files_replace=files_replace,
            execute_file=execute_file,
            result_properties=result_properties,
            result_file=result_file,
            result_file_format=result_file_format,
            atoms=atoms,
            charge=charge,
            multiplicity=multiplicity,
            command=command,
            restart=restart,
            label=label,
            directory=directory,
            **kwargs)

        # Assign remote client address
        if remote_client is None or utils.is_string(remote_client):
            self.remote_client = remote_client
        else:
            raise SyntaxError(
                ""Remote client input 'remote_client' is not reconginzed as""
                + ""a client id!"")
        
        # Assign job scanning time interval in seconds
        if utils.is_numeric(scan_interval):
            self.scan_interval = scan_interval
        else:
            raise SyntaxError(
                ""Submitted job scan interval 'scan_interval' is not a ""
                ""numeric value!"")

        # Assign command to obtain an output of active jobs and their ids
        # which are compared with the own job id number by the function
        # 'scan_check_id'.
        if scan_command is None:
            self.scan_command = f""squeue -u {os.environ['USER']:s}""
        elif utils.is_string(scan_command):
            self.scan_command = scan_command
        elif utils.is_string_array(scan_command):
            self.scan_command = "" "".join(scan_command)
        else:
            raise SyntaxError(
                ""Scan command input 'scan_command' is not reconginzed as""
                + ""a command string or list of strings!"")

        # Assign a user defined function to obtain job id of the submitted job
        # from the output of the submission command 'command'
        if scan_catch_id is None:
            self.scan_catch_id = None
        elif utils.is_callable(scan_catch_id):
            self.scan_catch_id = scan_catch_id
        else:
            raise SyntaxError(
                ""Submitted job id catch function 'scan_catch_id' is not a ""
                ""callable function!"")

        # Assign a user defined function to obtain job id list from the output
        # of 'scan_command'
        if scan_check_id is None:
            self.scan_check_id = None
        elif utils.is_callable(scan_check_id):
            self.scan_check_id = scan_check_id
        else:
            raise SyntaxError(
                ""Submitted job id check function 'scan_check_id' is not a ""
                ""callable function!"")

        return

    def __str__(self):
        return f""SlurmCalculator {self.execute_file:s}""

    def calculate(
        self,
        atoms: Optional[ase.Atoms] = None,
        properties: Optional[List[str]] = ['energy'],
        system_changes: Optional[List[str]] = None,
        **kwargs,
    ):
        """"""
        Execute calculation and read results
        """"""
        
        # Prepare calculation by execution parent class function
        Calculator.calculate(self, atoms, properties, system_changes)
        
        # Write input files
        self.write_input(
            self.atoms, 
            properties, 
            system_changes,
            **kwargs)
        
        # Prepare shell command 
        if self.command is None:
            command = ['sbatch']
        else:
            command = self.command.split()

        # Prepare scan command
        if self.scan_command is None:
            scan_command = ['squeue']
        else:
            scan_command = self.scan_command.split()

        # Check executable file
        if self.execute_file is None:
            execute_file = [os.path.split(self.files[0])[1]]
        else:
            execute_file = [self.execute_file]

        # Run the calculation
        if self.remote_client is None:

            # Execute command with executable file
            proc = subprocess.run(
                command + execute_file,
                cwd=self.directory,
                capture_output=True)
            
            # Catch submission output
            stdout = proc.stdout.decode()

        else:

            # Execute command with executable file on remote client,
            # catching the submission output and the calculation directory
            # on the remote client
            stdout, calculation_directory = self.run_remote(
                command,
                execute_file)

        # Get slurm id
        if self.scan_catch_id is None:
            slurm_id = int(stdout.split()[-1])
        else:
            slurm_id = self.scan_catch_id(stdout)

        # Check for job completeness
        done = False
        while not done:
            
            # Get and check task id with active task ids
            if self.scan_check_id is None:

                # Run check command
                if self.remote_client is None:

                    # Catch submission output
                    proc = subprocess.run(
                        scan_command,
                        capture_output=True)
                    stdout = proc.stdout.decode()

                else:
                    
                    stdout = self.scan_remote(scan_command)

                # Check if job id is still job id list
                active_id = [
                    int(tasks.split()[0])
                    for tasks in stdout.split('\n')[1:-1]]
                done = not slurm_id in active_id

            else:

                done = self.scan_check_id(slurm_id)

            # Wait for next scan step
            time.sleep(self.scan_interval)

        # If calculation is done on a remote client, copy result file to
        # local machine
        if self.remote_client is not None:
            self.copy_remote(calculation_directory)

        # Read results from result file
        self.read_results()

        return

    def read_results(
        self,
    ):
        """"""
        Read results from the defined result file
        """"""
        
        # Read results from file
        self._valid_result_file_format[self.result_file_format](
            os.path.join(self.directory, self.result_file))

        # Check for completeness
        self.converged = True
        for prop_i in self.result_properties:
            if prop_i not in self.results:
                self.converged = False

        return

    def load_results_npz(self):
        raise NotImplementedError

    def load_results_json(
        self,
        result_file: str,
    ):

        # Open result file
        if os.path.exists(result_file):
            with open(result_file, 'r') as f:
                results = json.load(f)
        else:
            results = {}

        # Convert lists to np.ndarrays
        self.results = {}
        for prop_i, result in results.items():
            self.results[prop_i] = np.array(result, dtype=float)

        return

    def run_remote(
        self,
        command: List[str],
        execute_file: List[str],
    ) -> str:
        """"""
        Run calculation on a remote client
        
        Parameters
        ----------
        command: str
            Submission command as string list
        execute_file: str
            Execution file as list

        Returns
        -------
        str
            Submission command standard output

        """"""
    
        # Working directory on remote client
        wdir = f""{os.environ['USER']:s}.{socket.gethostname():s}""

        # Target directory for working files on remote client
        tdir = f""{self.remote_client:s}:{wdir:}""

        # Calculation directory on remote client
        cdir = os.path.join(wdir, self.directory.split(""/"")[-1])

        # Execution command
        exe_command = (
            "" "".join(command + execute_file)
            + "" > job.id\n"")

        # Echo job.id command
        cat_command = ""cat job.id\n""

        # Logout command
        logout_command = f""logout\n""

        # First establish connection to remote client and create working
        # directory
        proc = subprocess.Popen(
            ['ssh', '-tt', self.remote_client],
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

        proc.stdin.write(f""mkdir -p {wdir:s}\n"".encode('utf-8'))
        proc.stdin.write(logout_command.encode('utf-8'))
        proc.stdin.close()

        # Second, copy working files to remote client working directory
        tdir = f""{self.remote_client:s}:{wdir:}""
        proc = subprocess.Popen(
            ['scp', '-r', self.directory, tdir],
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

        # Third establish connection to remote client and run the execute
        # command with executable file
        proc = subprocess.Popen(
            ['ssh', '-tt', self.remote_client],
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

        proc.stdin.write(f""cd {cdir:s}\n"".encode('utf-8'))
        proc.stdin.write(exe_command.encode('utf-8'))
        proc.stdin.write(cat_command.encode('utf-8'))
        proc.stdin.write(logout_command.encode('utf-8'))
        proc.stdin.close()
        proc.wait()

        # Catch submission output
        stdout_list = proc.stdout.readlines()
        stdout = """"
        submission_flag = False
        for line_encoded in stdout_list:
            line_decoded = line_encoded.decode('utf-8')
            if logout_command[:-1] in line_decoded:
                submission_flag = False
            if submission_flag:
                if line_decoded[-1] == '\n':
                    stdout += line_decoded[:-1]
                else:
                    stdout += line_decoded
            if cat_command[:-1] in line_decoded:
                submission_flag = True
        
        return stdout, cdir
    
    def scan_remote(
        self,
        scan_command: List[str],
    ) -> str:
        """"""
        Run scan command on a remote client
        
        Parameters
        ----------
        scan_command: str
            Scan command as string list

        Returns
        -------
        str
            Scan command standard output

        """"""

        # Execution command
        scan_line_command = "" "".join(scan_command) + ""\n""

        # Logout command
        logout_command = f""logout\n""

        # First establish connection to remote client and request
        # job queue list
        proc = subprocess.Popen(
            ['ssh', '-tt', self.remote_client],
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)

        proc.stdin.write(scan_line_command.encode('utf-8'))
        proc.stdin.write(logout_command.encode('utf-8'))
        proc.stdin.close()

        # Catch scan output
        stdout_list = proc.stdout.readlines()
        stdout = """"
        scan_flag = False
        for line_encoded in stdout_list:
            line_decoded = line_encoded.decode('utf-8')
            if logout_command[:-1] in line_decoded:
                scan_flag = False
            if scan_flag:
                stdout += line_decoded
            if scan_line_command[:-1] in line_decoded:
                scan_flag = True

        return stdout

    def copy_remote(
        self,
        calculation_directory: List[str],
    ):
        """"""
        Run scan command on a remote client
        
        Parameters
        ----------
        calculation_directory: str
            Calculation directory on remote client

        """"""

        # Copy files from remote clients calculation directory to local 
        # machines working directory
        source_file = os.path.join(calculation_directory, self.result_file)
        source_command = f""{self.remote_client:s}:{source_file:}""
        target_command = os.path.join(self.directory, self.result_file)
        proc = subprocess.Popen(
            ['scp', source_command, target_command],
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)
        proc.wait()

        return",./Asparagus/asparagus/interface/slurm_ase.py
Model_PhysNet,"class Model_PhysNet(model.BaseModel): 
    """"""
    PhysNet model calculator

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to config json file (str)
    model_properties: list(str), optional, default '['energy', 'forces']'
        Properties to predict by calculator model
    model_unit_properties: dict, optional, default {}
        Unit labels of the predicted model properties. If not defined,
        prediction results are assumed as ASE units but for during training the
        units from the reference data container are adopted.
    model_cutoff: float, optional, default 12.0
        Upper atom interaction cutoff
    model_cuton: float, optional, default None
        Lower atom pair distance to start interaction switch-off
    model_switch_range: float, optional, default 2.0
        Atom interaction cutoff switch range to switch of interaction to zero.
        If 'model_cuton' is defined, this input will be ignored.
    model_repulsion: bool, optional, default False
        Use close-range nuclear repulsion model.
    model_repulsion_cutoff: float, optional, default 1.0
        Nuclear repulsion model cutoff range.
    model_repulsion_cuton: float, optional, default 0.0
        Nuclear repulsion model inner cutoff (cuton) radii to start
        switch-off function.
    model_repulsion_trainable: bool, optional, default True
        If True, repulsion model parameter are trainable. Else, default
        parameter values are fix.
    model_electrostatic: bool, optional, default True
        Use electrostatic potential between atomic charges for energy
        prediction.
    model_dispersion: bool, optional, default True
        Use Grimme's D3 dispersion model for energy prediction.
    model_dispersion_trainable: bool, optional, default False
        If True, empirical parameter in the D3 dispersion model are
        trainable. If False, empirical parameter are fixed to default
    model_num_threads: int, optional, default None
        Sets the number of threads used for intraop parallelism on CPU.
        if None, no thread number is set.
    device: str, optional, default global setting
        Device type for model variable allocation
    dtype: dtype object, optional, default global setting
        Model variables data type

    """"""
    
    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for PhysNet model
    _default_args = {
        'model_properties':             None,
        'model_unit_properties':        None,
        'model_cutoff':                 12.0,
        'model_cuton':                  None,
        'model_switch_range':           2.0,
        'model_repulsion':              False,
        'model_repulsion_cutoff':       1.0,
        'model_repulsion_cuton':        0.0,
        'model_repulsion_trainable':    True,
        'model_electrostatic':          None,
        'model_dispersion':             True,
        'model_dispersion_trainable':   False,
        'model_num_threads':            None,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'model_properties':             [utils.is_string_array, utils.is_None],
        'model_unit_properties':        [utils.is_dictionary, utils.is_None],
        'model_cutoff':                 [utils.is_numeric],
        'model_cuton':                  [utils.is_numeric, utils.is_None],
        'model_switch_range':           [utils.is_numeric],
        'model_repulsion':              [utils.is_bool],
        'model_repulsion_cutoff':       [utils.is_numeric],
        'model_repulsion_cuton':        [utils.is_numeric, utils.is_None],
        'model_repulsion_trainable':    [utils.is_bool],
        'model_electrostatic':          [utils.is_bool, utils.is_None],
        'model_dispersion':             [utils.is_bool],
        'model_dispersion_trainable':   [utils.is_bool],
        'model_num_threads':            [utils.is_integer, utils.is_None],
        }

    # Model type label
    _model_type = 'PhysNet'

    # Default module types of the model calculator
    _default_modules = {
        'input_type':                   'PhysNet',
        'graph_type':                   'PhysNet',
        'output_type':                  'PhysNet',
        }

    _default_model_properties = ['energy', 'forces', 'dipole']

    _supported_model_properties = [
        'energy',
        'atomic_energies',
        'forces',
        'atomic_charges',
        'dipole']

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None,
        model_properties: Optional[List[str]] = None,
        model_unit_properties: Optional[Dict[str, str]] = None,
        model_cutoff: Optional[float] = None,
        model_cuton: Optional[float] = None,
        model_switch_range: Optional[float] = None,
        model_repulsion: Optional[bool] = None,
        model_repulsion_cutoff: Optional[float] = None,
        model_repulsion_cuton: Optional[float] = None,
        model_repulsion_trainable: Optional[bool] = None,
        model_electrostatic: Optional[bool] = None,
        model_dispersion: Optional[bool] = None,
        model_dispersion_trainable: Optional[bool] = None,
        model_num_threads: Optional[int] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PhysNet Calculator model.

        """"""

        super(Model_PhysNet, self).__init__()
        self.model_type = 'PhysNet'

        #############################
        # # # Check Class Input # # #
        #############################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, model),
            check_dtype=utils.get_dtype_args(self, model))

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Set model calculator number of threads
        if self.model_num_threads is not None:
            torch.set_num_threads(self.model_num_threads)

        #####################################
        # # # Check PhysNet Model Input # # #
        #####################################

        # Check model properties
        self.model_properties = self.check_model_properties(
            config,
            self.model_properties)
        
        # Check model properties - Energy and energy gradient properties
        self.model_properties = self.set_model_energy_properties(
            self.model_properties,
            model_energy_properties=['atomic_energies', 'energy'])

        # Check model properties - Electrostatics properties
        self.model_properties = self.set_model_electrostatic_properties(
            self.model_properties,
            model_electrostatics_properties=['atomic_charges', 'dipole'])

        # Check model property units
        self.model_unit_properties = self.check_model_property_units(
            self.model_properties,
            self.model_unit_properties,
            model_default_properties=['positions', 'charge'])

        # Check lower cutoff switch-off range
        self.model_cuton, self.model_switch_range = self.check_cutoff_ranges(
            self.model_cutoff,
            self.model_cuton,
            self.model_switch_range)

        # Update global configuration dictionary
        config_update = {
            'model_properties': self.model_properties,
            'model_unit_properties': self.model_unit_properties,
            'model_cutoff': self.model_cutoff,
            'model_cuton': self.model_cuton,
            'model_switch_range': self.model_switch_range}
        config.update(
            config_update,
            verbose=verbose)

        #################################
        # # # PhysNet Modules Setup # # #
        #################################

        # Assign model calculator base modules
        self.input_module, self.graph_module, self.output_module = (
            self.base_modules_setup(
                config,
                verbose=verbose,
                **kwargs)
            )

        # If electrostatic energy contribution is undefined, activate 
        # contribution if atomic charges are predicted.
        if self.model_electrostatic is None:
            if self.model_energy and self.model_atomic_charges:
                self.model_electrostatic = True
            else:
                self.model_electrostatic = False

        # Check repulsion, electrostatic and dispersion module requirement
        if self.model_repulsion and not self.model_energy:
            self.logger.error(
                ""Repulsion energy contribution is requested without ""
                + ""having 'energy' assigned as model property!\n""
                + ""Repulsion potential module will not be used!"")
            self.model_repulsion = False
        if self.model_electrostatic and not self.model_energy:
            self.logger.error(
                ""Electrostatic energy contribution is requested without ""
                + ""having 'energy' assigned as model property!\n""
                + ""Electrostatic potential module will not be used!"")
            self.model_electrostatic = False
        if self.model_electrostatic and not self.model_atomic_charges:
            self.logger.error(
                ""Electrostatic energy contribution is requested without ""
                + ""having 'atomic_charges' or 'dipole' assigned as model ""
                + ""property!\n""
                + ""Electrostatic potential module will not be used!"")
            self.model_electrostatic = False
        if self.model_dispersion and not self.model_energy:
            self.logger.error(
                ""Dispersion energy contribution is requested without ""
                + ""having 'energy' assigned as model property!\n""
                + ""Dispersion potential module will not be used!"")
            self.model_dispersion = False

        # Assign atom repulsion module
        if self.model_repulsion:
            # Check nuclear repulsion cutoff
            input_radial_cutoff = config.get('input_radial_cutoff')
            if (
                input_radial_cutoff is not None
                and self.model_repulsion_cutoff > input_radial_cutoff
            ):
                raise SyntaxError(
                    ""Nuclear repulsion cutoff radii is larger than the ""
                    + ""input module radial cutoff!"")
            # Get Ziegler-Biersack-Littmark style nuclear repulsion potential
            self.repulsion_module = module.ZBL_repulsion(
                self.model_repulsion_cutoff,
                self.model_repulsion_cuton,
                self.model_repulsion_trainable,
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                **kwargs)

        # Assign electrostatic interaction module
        if self.model_electrostatic:
            # Get electrostatic point charge model calculator
            self.electrostatic_module = module.PC_damped_electrostatics(
                self.model_cutoff,
                config.get('input_radial_cutoff'),
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                truncation='force',
                **kwargs)

        # Assign dispersion interaction module
        if self.model_dispersion:

            # Grep dispersion correction parameters
            d3_s6 = config.get(""model_dispersion_d3_s6"")
            d3_s8 = config.get(""model_dispersion_d3_s8"")
            d3_a1 = config.get(""model_dispersion_d3_a1"")
            d3_a2 = config.get(""model_dispersion_d3_a2"")

            # Get Grimme's D3 dispersion model calculator
            self.dispersion_module = module.D3_dispersion(
                self.model_cutoff,
                self.model_cuton,
                self.model_dispersion_trainable,
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                truncation='force',
                d3_s6=d3_s6,
                d3_s8=d3_s8,
                d3_a1=d3_a1,
                d3_a2=d3_a2,
            )

        #######################################
        # # # PhysNet Miscellaneous Setup # # #
        #######################################
        
        # Assign atomic masses list for center of mass calculation
        if self.model_dipole:
            # Convert atomic masses list to requested data type
            self.atomic_masses = torch.tensor(
                utils.atomic_masses,
                device=self.device,
                dtype=self.dtype)

        return

    def __str__(self):
        return self.model_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return model and module information
        """"""

        # Initialize info dictionary
        info = {}

        # Collect module info
        if hasattr(self.input_module, ""get_info""):
            info = {**info, **self.input_module.get_info()}
        if hasattr(self.graph_module, ""get_info""):
            info = {**info, **self.graph_module.get_info()}
        if hasattr(self.output_module, ""get_info""):
            info = {**info, **self.output_module.get_info()}
        if (
            self.model_repulsion
            and hasattr(self.repulsion_module, ""get_info"")
        ):
            info = {**info, **self.repulsion_module.get_info()}
        if (
            self.model_electrostatic
            and hasattr(self.electrostatic_module, ""get_info"")
        ):
            info = {**info, **self.electrostatic_module.get_info()}
        if (
            self.model_dispersion
            and hasattr(self.dispersion_module, ""get_info"")
        ):
            info = {**info, **self.dispersion_module.get_info()}

        return {
            **info,
            'model_type': self._model_type,
            'model_properties': self.model_properties,
            'model_unit_properties': self.model_unit_properties,
            'model_cutoff': self.model_cutoff,
            'model_cuton': self.model_cuton,
            'model_switch_range': self.model_switch_range,
            'model_repulsion': self.model_repulsion,
            'model_repulsion_trainable': self.model_repulsion_trainable,
            'model_electrostatic': self.model_electrostatic,
            'model_dispersion': self.model_dispersion,
            'model_dispersion_trainable': self.model_dispersion_trainable,
        }

    def set_model_electrostatic_properties(
        self,
        model_properties: List[str],
        model_electrostatics_properties: 
            Optional[List[str]] = ['atomic_charges', 'dipole'],
    ) -> List[str]:
        """"""
        Set model energy property parameters.
        
        Parameters
        ----------
        model_properties: list(str)
            Properties to predict by calculator model
        model_electrostatic_properties: list(str)
            Model electrostatics related properties

        Returns
        ----------
        list(str)
            Checked property labels

        """"""

        # Check model properties - Electrostatics properties
        if 'dipole' in model_properties:
            self.model_atomic_charges = True
            self.model_dipole = True
            for prop in model_electrostatics_properties:
                if prop not in model_properties:
                    model_properties.append(prop)
        elif 'atomic_charges' in model_properties:
            self.model_atomic_charges = True
            self.model_dipole = False
        else:
            self.model_atomic_charges = False
            self.model_dipole = False

        return model_properties

    def set_model_unit_properties(
        self,
        model_unit_properties: Dict[str, str],
    ):
        """"""
        Set or change unit property parameter in respective model layers

        Parameter
        ---------
        model_unit_properties: dict
            Unit labels of the predicted model properties

        """"""

        # Change unit properties for electrostatic and dispersion layers
        if self.model_electrostatic:
            # Synchronize total and atomic charge units
            if model_unit_properties.get('charge') is not None:
                model_unit_properties['atomic_charges'] = (
                    model_unit_properties.get('charge'))
            elif model_unit_properties.get('atomic_charges') is not None:
                model_unit_properties['charge'] = (
                    model_unit_properties.get('atomic_charges'))
            else:
                raise SyntaxError(
                    ""For electrostatic potential contribution either the""
                    + ""model unit for the 'charge' or 'atomic_charges' must ""
                    + ""be defined!"")
            self.electrostatic_module.set_unit_properties(
                model_unit_properties)
        if self.model_dispersion:
            self.dispersion_module.set_unit_properties(model_unit_properties)

        return

    def get_trainable_parameters(
        self,
        no_weight_decay: Optional[bool] = True,
    ) -> Dict[str, List]:
        """"""
        Return a  dictionary of lists for different optimizer options.

        Parameters
        ----------
        no_weight_decay: bool, optional, default True
            Separate parameters on which weight decay should not be applied

        Returns
        -------
        dict(str, List)
            Dictionary of trainable model parameters. Contains 'default' entry
            for all parameters not affected by special treatment. Further
            entries are, if true, the parameter names of the input
        """"""

        # Trainable parameter dictionary
        trainable_parameters = {}
        trainable_parameters['default'] = []
        if no_weight_decay:
            trainable_parameters['no_weight_decay'] = []

        # Iterate over all trainable model parameters
        for name, parameter in self.named_parameters():
            # Catch all parameters to not apply weight decay on
            if no_weight_decay and 'output_scaling' in name:
                trainable_parameters['no_weight_decay'].append(parameter)
            elif no_weight_decay and 'dispersion_module' in name:
                trainable_parameters['no_weight_decay'].append(parameter)
            else:
                trainable_parameters['default'].append(parameter)

        return trainable_parameters

    # @torch.compile # Not supporting backwards propagation with torch.float64
    # @torch.jit.export  # No effect, as 'forward' already is
    def forward(
        self,
        batch: Dict[str, torch.Tensor],
        no_derivation: Optional[bool] = False,
        verbose_results: Optional[bool] = False,
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of PhysNet Calculator model.

        Parameters
        ----------
        batch : dict(str, torch.Tensor)
            Dictionary of input data tensors for forward pass.
            Basic keys are:
                'atoms_number': torch.Tensor(n_systems)
                    Number of atoms per molecule in batch
                'atomic_numbers': torch.Tensor(n_atoms)
                    Atomic numbers of the batch of molecules
                'positions': torch.Tensor(n_atoms, 3)
                    Atomic positions of the batch of molecules
                'charge': torch.Tensor(n_systems)
                    Total charge of molecules in batch
                'idx_i': torch.Tensor(n_pairs)
                    Atom i pair index
                'idx_j': torch.Tensor(n_pairs)
                    Atom j pair index
                'sys_i': torch.Tensor(n_atoms)
                    System indices of atoms in batch
            Extra keys are:
                'pbc_offset': torch.Tensor(n_pairs)
                    Periodic boundary atom pair vector offset
                'pbc_atoms': torch.Tensor(n_atoms)
                    Primary atom indices for the supercluster approach
                'pbc_idx': torch.Tensor(n_pairs)
                    Image atom to primary atom index pointer for the atom
                    pair indices in a supercluster
                'pbc_idx_j': torch.Tensor(n_pairs)
                    Atom j pair index pointer from image atom to respective
                    primary atom index in a supercluster
        no_derivation: bool, optional, default False
            If True, only predict non-derived properties.
            Else, predict all properties even if backwards derivation is
            required (e.g. forces).
        verbose_results: bool, optional, default False
            If True, store extended model property contributions in the result
            dictionary.

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""

        # Assign input
        atoms_number = batch['atoms_number']
        atomic_numbers = batch['atomic_numbers']
        positions = batch['positions']
        charge = batch['charge']
        idx_i = batch['idx_i']
        idx_j = batch['idx_j']
        idx_u = batch.get('idx_u')
        idx_v = batch.get('idx_v')
        sys_i = batch['sys_i']

        # PBC: Cartesian offset method
        pbc_offset_ij = batch.get('pbc_offset_ij')
        pbc_offset_uv = batch.get('pbc_offset_uv')

        # PBC: Supercluster method
        pbc_atoms = batch.get('pbc_atoms')
        pbc_idx_pointer = batch.get('pbc_idx')
        pbc_idx_j = batch.get('pbc_idx_j')

        # Activate back propagation if derivatives with regard to atom positions
        # is requested.
        if self.model_forces:
            positions.requires_grad_(True)

        # Run input model
        features, distances, cutoffs, rbfs, distances_uv = self.input_module(
            atomic_numbers, positions,
            idx_i, idx_j, pbc_offset_ij=pbc_offset_ij,
            idx_u=idx_u, idx_v=idx_v, pbc_offset_uv=pbc_offset_uv)

        # PBC: Supercluster approach - Point from image atoms to primary atoms
        if pbc_idx_pointer is not None:
            idx_i = pbc_idx_pointer[idx_i]
            idx_j = pbc_idx_pointer[pbc_idx_j]

        # Check long-range atom pair indices
        if idx_u is None:
            # Assign atom pair indices
            idx_u = idx_i
            idx_v = idx_j
        elif pbc_idx_pointer is not None:
            idx_u = pbc_idx_pointer[idx_u]
            idx_v = pbc_idx_pointer[idx_v]

        # Run graph model
        features_list = self.graph_module(
            features, distances, cutoffs, rbfs, idx_i, idx_j)

        # Run output model
        results = self.output_module(
            features_list,
            atomic_numbers=atomic_numbers)
        if verbose_results:
            for prop in self.output_module.output_properties:
                verbose_prop = f""output_{prop:s}""
                results[verbose_prop] = results[prop].detach()

        # Add repulsion model contribution
        if self.model_repulsion:
            repulsion_atomic_energies = self.repulsion_module(
                atomic_numbers, distances, idx_i, idx_j)
            results['atomic_energies'] = (
                results['atomic_energies'] + repulsion_atomic_energies)
            if verbose_results:
                results['repulsion_atomic_energies'] = (
                    repulsion_atomic_energies.detach())

        # Add dispersion model contributions
        if self.model_dispersion:
            dispersion_atomic_energies = self.dispersion_module(
                atomic_numbers, distances_uv, idx_u, idx_v)
            results['atomic_energies'] = (
                results['atomic_energies'] + dispersion_atomic_energies)
            if verbose_results:
                results['dispersion_atomic_energies'] = (
                    dispersion_atomic_energies.detach())

        # Scale atomic charges to ensure correct total charge
        if self.model_atomic_charges:
            charge_deviation = (
                charge - utils.scatter_sum(
                    results['atomic_charges'], sys_i, dim=0,
                    shape=charge.shape))/atoms_number
            results['atomic_charges'] = (
                results['atomic_charges'] + charge_deviation[sys_i])

        # Add electrostatic model contribution
        if self.model_electrostatic:
            electrostatic_atomic_energies = self.electrostatic_module(
                results, distances_uv, idx_u, idx_v)
            results['atomic_energies'] = (
                results['atomic_energies'] + electrostatic_atomic_energies)
            if verbose_results:
                results['electrostatic_atomic_energies'] = (
                    electrostatic_atomic_energies.detach())

        # Compute property - Energy
        if self.model_energy:
            results['energy'] = torch.squeeze(
                utils.scatter_sum(
                    results['atomic_energies'], sys_i, dim=0,
                    shape=atoms_number.shape)
            )
            if verbose_results:
                atomic_energies_properies = [
                    prop for prop in results
                    if 'atomic_energies' in prop[-len('atomic_energies'):]]
                for prop in atomic_energies_properies:
                    verbose_prop = (
                        f""{prop[:-len('atomic_energies')]:s}energy"")
                    results[verbose_prop] = torch.squeeze(
                        utils.scatter_sum(
                            results[prop], sys_i, dim=0,
                            shape=atoms_number.shape)
                    )

        # Compute gradients and Hessian if demanded
        if self.model_forces and not no_derivation:

            gradient = torch.autograd.grad(
                torch.sum(results['energy']),
                positions,
                create_graph=True)[0]

            # Avoid crashing if forces are none
            if gradient is not None:
                results['forces'] = -gradient
            else:
                self.logger(
                    ""WARNING:\nError in force calculation ""
                    + ""(backpropagation)!"")
                results['forces'] = torch.zeros_like(positions)

            if self.model_hessian:
                hessian = results['energy'].new_zeros(
                    (3*gradient.size(0), 3*gradient.size(0)))
                #for ig in range(3*gradient.size(0)):
                for ig, grad_i in enumerate(gradient.view(-1)):
                    hessian_ig = torch.autograd.grad(
                        [grad_i],
                        positions,
                        retain_graph=(ig < 3*gradient.size(0)))[0]
                    if hessian_ig is not None:
                        hessian[ig] = hessian_ig.view(-1)
                results['hessian'] = hessian

        # Compute molecular dipole if demanded
        if self.model_dipole:

            # For supercluster method, just use primary cell atom positions
            if pbc_atoms is None:
                positions_dipole = positions
            else:
                positions_dipole = positions[pbc_atoms]

            # In case of non-zero system charges, shift origin to center of
            # mass
            atomic_masses = self.atomic_masses[atomic_numbers]
            system_mass = utils.scatter_sum(
                atomic_masses, sys_i, dim=0,
                shape=atoms_number.shape)
            system_com = (
                utils.scatter_sum(
                    atomic_masses[..., None]*positions_dipole,
                    sys_i, dim=0, shape=(*atoms_number.shape, 3)
                    ).reshape(-1, 3)
                )/system_mass[..., None]
            positions_com = positions_dipole - system_com[sys_i]

            # Compute molecular dipole moment from atomic charges
            results['dipole'] = utils.scatter_sum(
                results['atomic_charges'][..., None]*positions_com,
                sys_i, dim=0, shape=(*atoms_number.shape, 3)
                ).reshape(-1, 3)

        return results",./Asparagus/asparagus/model/physnet.py
Model_PaiNN,"class Model_PaiNN(model.BaseModel):
    """"""
    PaiNN model calculator

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to config json file (str)
    model_properties: list(str), optional, default '['energy', 'forces']'
        Properties to predict by calculator model
    model_unit_properties: dict, optional, default {}
        Unit labels of the predicted model properties. If not defined,
        prediction results are assumed as ASE units but for during training the
        units from the reference data container are adopted.
    model_cutoff: float, optional, default 12.0
        Upper atom interaction cutoff
    model_cuton: float, optional, default None
        Lower atom pair distance to start interaction switch-off
    model_switch_range: float, optional, default 2.0
        Atom interaction cutoff switch range to switch of interaction to zero.
        If 'model_cuton' is defined, this input will be ignored.
    model_repulsion: bool, optional, default False
        Use close-range atom repulsion model.
    model_repulsion_cutoff: float, optional, default 1.0
        Nuclear repulsion model cutoff range.
    model_repulsion_cuton: float, optional, default 0.0
        Nuclear repulsion model inner cutoff (cuton) radii to start
        switch-off function.
    model_repulsion_trainable: bool, optional, default True
        If True, repulsion model parameter are trainable. Else, default
        parameter values are fix.
    model_electrostatic: bool, optional, default None
        Use electrostatic potential between atomic charges for energy
        prediction. If None, electrostatic potential model is applied if
        atomic charges are available.
    model_electrostatic_dipole: bool, optional, default True
        Include atomic dipole moments to compute the electrostatic potential
        between atom pairs for energy prediction if available.
    model_dispersion: bool, optional, default True
        Use Grimme's D3 dispersion model for energy prediction.
    model_dispersion_trainable: bool, optional, default False
        If True, empirical parameter in the D3 dispersion model are
        trainable. If False, empirical parameter are fixed to default
    model_num_threads: int, optional, default 4
        Sets the number of threads used for intraop parallelism on CPU.

    """"""
    
    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for PaiNN model
    _default_args = {
        'model_properties':             None,
        'model_unit_properties':        None,
        'model_cutoff':                 12.0,
        'model_cuton':                  None,
        'model_switch_range':           2.0,
        'model_repulsion':              False,
        'model_repulsion_cutoff':       1.0,
        'model_repulsion_cuton':        0.0,
        'model_repulsion_trainable':    True,
        'model_electrostatic':          None,
        'model_electrostatic_dipole':   True,
        'model_dispersion':             True,
        'model_dispersion_trainable':   False,
        'model_num_threads':            4,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'model_properties':             [utils.is_string_array, utils.is_None],
        'model_unit_properties':        [utils.is_dictionary, utils.is_None],
        'model_cutoff':                 [utils.is_numeric],
        'model_cuton':                  [utils.is_numeric, utils.is_None],
        'model_switch_range':           [utils.is_numeric],
        'model_repulsion':              [utils.is_bool],
        'model_repulsion_cutoff':       [utils.is_numeric],
        'model_repulsion_cuton':        [utils.is_numeric, utils.is_None],
        'model_repulsion_trainable':    [utils.is_bool],
        'model_electrostatic':          [utils.is_bool, utils.is_None],
        'model_electrostatic_dipole':   [utils.is_bool],
        'model_dispersion':             [utils.is_bool],
        'model_dispersion_trainable':   [utils.is_bool],
        'model_num_threads':            [utils.is_integer],
        }

    # Model type label
    _model_type = 'PaiNN'

    # Default module types of the model calculator
    _default_modules = {
        'input_type':                   'PaiNN',
        'graph_type':                   'PaiNN',
        'output_type':                  'PaiNN',
        }

    _default_model_properties = ['energy', 'forces', 'dipole']

    _supported_model_properties = [
        'energy',
        'atomic_energies',
        'forces',
        'atomic_charges',
        'dipole',
        'atomic_dipoles']

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        model_properties: Optional[List[str]] = None,
        model_unit_properties: Optional[Dict[str, str]] = None,
        model_cutoff: Optional[float] = None,
        model_cuton: Optional[float] = None,
        model_switch_range: Optional[float] = None,
        model_repulsion: Optional[bool] = None,
        model_repulsion_cutoff: Optional[float] = None,
        model_repulsion_cuton: Optional[float] = None,
        model_repulsion_trainable: Optional[bool] = None,
        model_electrostatic: Optional[bool] = None,
        model_electrostatic_dipole: Optional[bool] = None,
        model_dispersion: Optional[bool] = None,
        model_dispersion_trainable: Optional[bool] = None,
        model_num_threads: Optional[int] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PaiNN Calculator model.

        """"""

        super(Model_PaiNN, self).__init__()
        model_type = 'PaiNN'

        #############################
        # # # Check Class Input # # #
        #############################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, model),
            check_dtype=utils.get_dtype_args(self, model))

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Set model calculator number of threads
        if self.model_num_threads is not None:
            torch.set_num_threads(self.model_num_threads)

        ###################################
        # # # Check PaiNN Model Input # # #
        ###################################

        # Check model properties
        self.model_properties = self.check_model_properties(
            config,
            self.model_properties)

        # Check model properties - Energy and energy gradient properties
        self.model_properties = self.set_model_energy_properties(
            self.model_properties,
            model_energy_properties=['atomic_energies', 'energy'])

        # Check model properties - Electrostatics properties
        self.model_properties = self.set_model_electrostatic_properties(
            self.model_properties,
            model_electrostatics_properties=[
                'atomic_charges', 'atomic_dipoles', 'dipole'])

        # Check model property units
        self.model_unit_properties = self.check_model_property_units(
            self.model_properties,
            self.model_unit_properties,
            model_default_properties=['positions', 'charge'])

        # Check lower cutoff switch-off range
        self.model_cuton, self.model_switch_range = self.check_cutoff_ranges(
            self.model_cutoff,
            self.model_cuton,
            self.model_switch_range)

        # Update global configuration dictionary
        config_update = {
            'model_properties': self.model_properties,
            'model_unit_properties': self.model_unit_properties,
            'model_cutoff': self.model_cutoff,
            'model_cuton': self.model_cuton,
            'model_switch_range': self.model_switch_range}
        config.update(
            config_update,
            verbose=verbose)

        ###############################
        # # # PaiNN Modules Setup # # #
        ###############################

        # Assign model calculator base modules
        self.input_module, self.graph_module, self.output_module = (
            self.base_modules_setup(
                config,
                verbose=verbose,
                **kwargs)
            )

        # If electrostatic energy contribution is undefined, activate 
        # contribution if atomic charges are predicted.
        if self.model_electrostatic is None:
            if self.model_atomic_charges:
                self.model_electrostatic = True
            else:
                self.model_electrostatic = False

        # Check repulsion, electrostatic and dispersion module requirement
        if self.model_repulsion and not self.model_energy:
            raise SyntaxError(
                ""Nuclear rRepulsion energy contribution is requested without ""
                + ""having 'energy' assigned as model property!"")
        if self.model_electrostatic and not self.model_energy:
            raise SyntaxError(
                ""Electrostatic energy contribution is requested without ""
                + ""having 'energy' assigned as model property!"")
        if self.model_electrostatic and not self.model_atomic_charges:
            raise SyntaxError(
                ""Electrostatic energy contribution is requested without ""
                + ""having 'atomic_charges' or 'dipole' assigned as model ""
                + ""property!"")
        if self.model_dispersion and not self.model_energy:
            raise SyntaxError(
                ""Dispersion energy contribution is requested without ""
                + ""having 'energy' assigned as model property!"")

        # Assign atom repulsion module
        if self.model_repulsion:
            # Check nuclear repulsion cutoff
            input_radial_cutoff = config.get('input_radial_cutoff')
            if (
                input_radial_cutoff is not None
                and self.model_repulsion_cutoff > input_radial_cutoff
            ):
                raise SyntaxError(
                    ""Nuclear repulsion cutoff radii is larger than the ""
                    + ""input module radial cutoff!"")
            # Get Ziegler-Biersack-Littmark style nuclear repulsion potential
            self.repulsion_module = module.ZBL_repulsion(
                self.model_repulsion_cutoff,
                self.model_repulsion_cuton,
                self.model_repulsion_trainable,
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                **kwargs)

        # Assign electrostatic interaction module
        if self.model_electrostatic and self.model_electrostatic_dipole:
            # Get electrostatic atomic charge and dipole model calculator
            self.electrostatic_module = module.PC_Dipole_damped_electrostatics(
                self.model_cutoff,
                config.get('input_radial_cutoff'),
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                truncation='force',
                **kwargs)
        elif self.model_electrostatic:
            # Get electrostatic atomic charge model calculator
            self.electrostatic_module = module.PC_damped_electrostatics(
                self.model_cutoff,
                config.get('input_radial_cutoff'),
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                truncation='force',
                **kwargs)

        # Assign dispersion interaction module
        if self.model_dispersion:

            # Grep dispersion correction parameters
            d3_s6 = config.get(""model_dispersion_d3_s6"")
            d3_s8 = config.get(""model_dispersion_d3_s8"")
            d3_a1 = config.get(""model_dispersion_d3_a1"")
            d3_a2 = config.get(""model_dispersion_d3_a2"")

            # Get Grimme's D3 dispersion model calculator
            self.dispersion_module = module.D3_dispersion(
                self.model_cutoff,
                self.model_cuton,
                self.model_dispersion_trainable,
                self.device,
                self.dtype,
                unit_properties=self.model_unit_properties,
                truncation='force',
                d3_s6=d3_s6,
                d3_s8=d3_s8,
                d3_a1=d3_a1,
                d3_a2=d3_a2,
            )

        #####################################
        # # # PaiNN Miscellaneous Setup # # #
        #####################################
        
        # Assign atomic masses list for center of mass calculation
        if self.model_dipole:
            # Convert atomic masses list to requested data type
            self.atomic_masses = torch.tensor(
                utils.atomic_masses,
                device=self.device,
                dtype=self.dtype)

        return

    def __str__(self):
        return self.model_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return model and module information
        """"""

        # Initialize info dictionary
        info = {}

        # Collect module info
        if hasattr(self.input_module, ""get_info""):
            info = {**info, **self.input_module.get_info()}
        if hasattr(self.graph_module, ""get_info""):
            info = {**info, **self.graph_module.get_info()}
        if hasattr(self.output_module, ""get_info""):
            info = {**info, **self.output_module.get_info()}
        if (
            self.model_repulsion
            and hasattr(self.repulsion_module, ""get_info"")
        ):
            info = {**info, **self.repulsion_module.get_info()}
        if (
            self.model_electrostatic
            and hasattr(self.electrostatic_module, ""get_info"")
        ):
            info = {**info, **self.electrostatic_module.get_info()}
        if (
            self.model_dispersion
            and hasattr(self.dispersion_module, ""get_info"")
        ):
            info = {**info, **self.dispersion_module.get_info()}

        return {
            **info,
            'model_type': self._model_type,
            'model_properties': self.model_properties,
            'model_unit_properties': self.model_unit_properties,
            'model_cutoff': self.model_cutoff,
            'model_cuton': self.model_cuton,
            'model_switch_range': self.model_switch_range,
            'model_repulsion': self.model_repulsion,
            'model_repulsion_trainable': self.model_repulsion_trainable,
            'model_electrostatic': self.model_electrostatic,
            'model_dispersion': self.model_dispersion,
            'model_dispersion_trainable': self.model_dispersion_trainable,
        }

    def set_model_electrostatic_properties(
        self,
        model_properties: List[str],
        model_electrostatics_properties: Optional[List[str]] = [
            'atomic_charges', 'atomic_dipoles', 'dipole'],
    ) -> List[str]:
        """"""
        Set model energy property parameters.
        
        Parameters
        ----------
        model_properties: list(str)
            Properties to predict by calculator model
        model_electrostatic_properties: list(str)
            Model electrostatics related properties

        Returns
        ----------
        list(str)
            Checked property labels

        """"""

        # Check model properties - Electrostatics properties
        if 'dipole' in model_properties:
            self.model_atomic_charges = True
            self.model_atomic_dipoles = True
            self.model_dipole = True
            for prop in model_electrostatics_properties:
                if prop not in model_properties:
                    model_properties.append(prop)
        elif 'atomic_dipoles' in self.model_properties:
            self.model_atomic_charges = True
            self.model_atomic_dipoles = True
            self.model_dipole = False
            for prop in model_electrostatics_properties:
                if (
                    prop not in model_properties
                    and prop not in ['dipole']
                ):
                    model_properties.append(prop)
        elif 'atomic_charges' in self.model_properties:
            self.model_atomic_charges = True
            self.model_atomic_dipoles = False
            self.model_dipole = False
            for prop in model_electrostatics_properties:
                if (
                    prop not in model_properties
                    and prop not in ['dipole', 'atomic_dipoles']
                ):
                    model_properties.append(prop)
        else:
            self.model_atomic_dipoles = False
            self.model_atomic_charges = False
            self.model_dipole = False

        return model_properties

    def set_model_unit_properties(
        self,
        model_unit_properties: Dict[str, str],
    ):
        """"""
        Set or change unit property parameter in respective model layers

        Parameter
        ---------
        model_unit_properties: dict
            Unit labels of the predicted model properties

        """"""

        # Change unit properties for electrostatic and dispersion layers
        if self.model_electrostatic:
            # Synchronize total and atomic charge units
            if model_unit_properties.get('charge') is not None:
                model_unit_properties['atomic_charges'] = (
                    model_unit_properties.get('charge'))
            elif model_unit_properties.get('atomic_charges') is not None:
                model_unit_properties['charge'] = (
                    model_unit_properties.get('atomic_charges'))
            else:
                raise SyntaxError(
                    ""For electrostatic potential contribution either the""
                    + ""model unit for the 'charge' or 'atomic_charges' must ""
                    + ""be defined!"")
            self.electrostatic_module.set_unit_properties(
                model_unit_properties)
        if self.model_dispersion:
            self.dispersion_module.set_unit_properties(model_unit_properties)

        return

    def get_trainable_parameters(
        self,
        no_weight_decay: Optional[bool] = True,
    ) -> Dict[str, List]:
        """"""
        Return a  dictionary of lists for different optimizer options.

        Parameters
        ----------
        no_weight_decay: bool, optional, default True
            Separate parameters on which weight decay should not be applied

        Returns
        -------
        dict(str, List)
            Dictionary of trainable model parameters. Contains 'default' entry
            for all parameters not affected by special treatment. Further
            entries are, if true, the parameter names of the input
        """"""

        # Trainable parameter dictionary
        trainable_parameters = {}
        trainable_parameters['default'] = []
        if no_weight_decay:
            trainable_parameters['no_weight_decay'] = []

        # Iterate over all trainable model parameters
        for name, parameter in self.named_parameters():
            # Catch all parameters to not apply weight decay on
            if no_weight_decay and 'output_scaling' in name:
                trainable_parameters['no_weight_decay'].append(parameter)
            elif no_weight_decay and 'dispersion_module' in name:
                trainable_parameters['no_weight_decay'].append(parameter)
            else:
                trainable_parameters['default'].append(parameter)

        return trainable_parameters

    # @torch.jit.export  # No effect, as 'forward' already is
    # @torch.compile # Not supporting double backwards autograd (forces, loss)
    def forward(
        self,
        batch: Dict[str, torch.Tensor],
        no_derivation: Optional[bool] = False,
        verbose_results: Optional[bool] = False,
    ) -> Dict[str, torch.Tensor]:
        """"""
        Forward pass of PaiNN Calculator model.

        Parameters
        ----------
        batch : dict(str, torch.Tensor)
            Dictionary of input data tensors for forward pass.
            Basic keys are:
                'atoms_number': torch.Tensor(n_systems)
                    Number of atoms per molecule in batch
                'atomic_numbers': torch.Tensor(n_atoms)
                    Atomic numbers of the batch of molecules
                'positions': torch.Tensor(n_atoms, 3)
                    Atomic positions of the batch of molecules
                'charge': torch.Tensor(n_systems)
                    Total charge of molecules in batch
                'idx_i': torch.Tensor(n_pairs)
                    Atom i pair index
                'idx_j': torch.Tensor(n_pairs)
                    Atom j pair index
                'sys_i': torch.Tensor(n_atoms)
                    System indices of atoms in batch
            Extra keys are:
                'pbc_offset': torch.Tensor(n_pairs)
                    Periodic boundary atom pair vector offset
                'pbc_atoms': torch.Tensor(n_atoms)
                    Primary atom indices for the supercluster approach
                'pbc_idx': torch.Tensor(n_pairs)
                    Image atom to primary atom index pointer for the atom
                    pair indices in a supercluster
                'pbc_idx_j': torch.Tensor(n_pairs)
                    Atom j pair index pointer from image atom to repsective
                    primary atom index in a supercluster
        no_derivation: bool, optional, default False
            If True, only predict non-derived properties.
            Else, predict all properties even if backwards derivation is
            required (e.g. forces).
        verbose_results: bool, optional, default False
            If True, store extended model property contributions in the result
            dictionary.

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""

        # Assign input
        atoms_number = batch['atoms_number']
        atomic_numbers = batch['atomic_numbers']
        positions = batch['positions']
        charge = batch['charge']
        idx_i = batch['idx_i']
        idx_j = batch['idx_j']
        idx_u = batch.get('idx_u')
        idx_v = batch.get('idx_v')
        sys_i = batch['sys_i']

        # PBC: Cartesian offset method
        pbc_offset_ij = batch.get('pbc_offset_ij')
        pbc_offset_uv = batch.get('pbc_offset_uv')

        # PBC: Supercluster method
        pbc_atoms = batch.get('pbc_atoms')
        pbc_idx_pointer = batch.get('pbc_idx')
        pbc_idx_j = batch.get('pbc_idx_j')

        # Activate back propagation if derivatives with regard to
        # atom positions is requested.
        if self.model_forces:
            positions.requires_grad_(True)

        # Run input model
        (
            features, distances, vectors, cutoffs, rbfs, 
            distances_uv, vectors_uv
        ) = (
            self.input_module(
                atomic_numbers, positions,
                idx_i, idx_j, pbc_offset_ij=pbc_offset_ij,
                idx_u=idx_u, idx_v=idx_v, pbc_offset_uv=pbc_offset_uv)
        )

        # PBC: Supercluster approach - Point from image atoms to primary atoms
        if pbc_idx_pointer is not None:
            idx_i = pbc_idx_pointer[idx_i]
            idx_j = pbc_idx_pointer[pbc_idx_j]

        # Check long-range atom pair indices
        if idx_u is None:
            # Assign atom pair indices
            idx_u = idx_i
            idx_v = idx_j
        elif pbc_idx_pointer is not None:
            idx_u = pbc_idx_pointer[idx_u]
            idx_v = pbc_idx_pointer[idx_v]

        # Run graph model
        sfeatures, efeatures = self.graph_module(
            features, distances, vectors, cutoffs, rbfs, idx_i, idx_j)

        # Run output model
        results = self.output_module(
            sfeatures,
            efeatures,
            atomic_numbers=atomic_numbers)
        if verbose_results:
            for prop in self.output_module.output_properties:
                verbose_prop = f""output_{prop:s}""
                results[verbose_prop] = results[prop].detach()

        # Add repulsion model contribution
        if self.model_repulsion:
            repulsion_atomic_energies = self.repulsion_module(
                atomic_numbers, distances, idx_i, idx_j)
            results['atomic_energies'] = (
                results['atomic_energies'] + repulsion_atomic_energies)
            if verbose_results:
                results['repulsion_atomic_energies'] = (
                    repulsion_atomic_energies.detach())

        # Add dispersion model contributions
        if self.model_dispersion:
            dispersion_atomic_energies = self.dispersion_module(
                atomic_numbers, distances_uv, idx_u, idx_v)
            results['atomic_energies'] = (
                results['atomic_energies'] + dispersion_atomic_energies)
            if verbose_results:
                results['dispersion_atomic_energies'] = (
                    dispersion_atomic_energies.detach())

        # Scale atomic charges to ensure correct total charge
        if self.model_atomic_charges:
            charge_deviation = (
                charge - utils.scatter_sum(
                    results['atomic_charges'], sys_i, dim=0,
                    shape=charge.shape))/atoms_number
            results['atomic_charges'] = (
                results['atomic_charges'] + charge_deviation[sys_i])

        # Add electrostatic model contribution
        if self.model_electrostatic:
            electrostatic_atomic_energies = self.electrostatic_module(
                results, distances_uv, idx_u, idx_v, vectors=vectors_uv)
            results['atomic_energies'] = (
                results['atomic_energies'] + electrostatic_atomic_energies)
            if verbose_results:
                results['electrostatic_atomic_energies'] = (
                    electrostatic_atomic_energies.detach())

        # Compute property - Energy
        if self.model_energy:
            results['energy'] = torch.squeeze(
                utils.scatter_sum(
                    results['atomic_energies'], sys_i, dim=0,
                    shape=atoms_number.shape))
            if verbose_results:
                atomic_energies_properies = [
                    prop for prop in results
                    if 'atomic_energies' in prop[-len('atomic_energies'):]]
                for prop in atomic_energies_properies:
                    verbose_prop = (
                        f""{prop[:-len('atomic_energies')]:s}energy"")
                    results[verbose_prop] = torch.squeeze(
                        utils.scatter_sum(
                            results[prop], sys_i, dim=0,
                            shape=atoms_number.shape)
                    )

        # Compute gradients and Hessian if demanded
        if self.model_forces and not no_derivation:

            gradient = torch.autograd.grad(
                torch.sum(results['energy']),
                positions,
                create_graph=True)[0]

            # Avoid crashing if forces are none
            if gradient is not None:
                results['forces'] = -gradient
            else:
                self.logger.warning(
                    ""Error in force calculation ""
                    + ""(backpropagation)!"")
                results['forces'] = torch.zeros_like(positions)

            if self.model_hessian:
                hessian = results['energy'].new_zeros(
                    (3*gradient.size(0), 3*gradient.size(0)))
                #for ig in range(3*gradient.size(0)):
                for ig, grad_i in enumerate(gradient.view(-1)):
                    hessian_ig = torch.autograd.grad(
                        [grad_i],
                        positions,
                        retain_graph=(ig < 3*gradient.size(0)))[0]
                    if hessian_ig is not None:
                        hessian[ig] = hessian_ig.view(-1)
                results['hessian'] = hessian

        # Compute molecular dipole
        if self.model_dipole:

            # For supercluster method, just use primary cell atom positions
            if pbc_atoms is None:
                positions_dipole = positions
            else:
                positions_dipole = positions[pbc_atoms]

            # In case of non-zero system charges, shift origin to center of
            # mass
            atomic_masses = self.atomic_masses[atomic_numbers]
            system_mass = utils.scatter_sum(
                atomic_masses, sys_i, dim=0,
                shape=atoms_number.shape)
            system_com = (
                utils.scatter_sum(
                    atomic_masses[..., None]*positions_dipole,
                    sys_i, dim=0, shape=(*atoms_number.shape, 3)
                    ).reshape(-1, 3)
                )/system_mass[..., None]
            positions_com = positions_dipole - system_com[sys_i]

            # Compute molecular dipole moment from atomic charges
            results['dipole'] = utils.scatter_sum(
                results['atomic_charges'][..., None]*positions_com,
                sys_i, dim=0, shape=(*atoms_number.shape, 3)
                ).reshape(-1, 3)

            # Refine molecular dipole moment with atomic dipole moments
            if self.model_atomic_dipoles:
                results['dipole'] = (
                    results['dipole'] + utils.scatter_sum(
                        results['atomic_dipoles'],
                        sys_i, dim=0, shape=(*atoms_number.shape, 3)
                        ).reshape(-1, 3)
                )

        return results",./Asparagus/asparagus/model/painn.py
BaseModel,"class BaseModel(torch.nn.Module): 
    """"""
    Asparagus calculator base model

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    _default_model_properties = ['energy', 'forces']

    _supported_model_properties = [
        'energy',
        'atomic_energies',
        'forces']

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        device: Optional[str] = None,
        dtype: Optional[object] = None,
        **kwargs
    ):
        """"""
        Initialize BaseModel Calculator
        """"""

        super(BaseModel, self).__init__()
        self.model_type = 'BaseModel'
        
        # Initialize loaded checkpoint flag
        self.checkpoint_loaded = False
        self.checkpoint_file = None

        return

    def __str__(self):
        return self.model_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return model and module information
        """"""
        return {}

    def load(
        self,
        checkpoint: Dict[str, Any],
        checkpoint_file: Optional[str] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Load model parameters from checkpoint file.

        Parameters
        ----------
        checkpoint: dict(str, Any)
            Torch module checkpoint file for the model calculator
        checkpoint_file: str, optional, default None
            Torch checkpoint file path for logger info

        """"""

        # Load model checkpoint file
        if checkpoint is None and verbose:
            
            if checkpoint_file is None:
                checkpoint_state = "".""
            else:
                checkpoint_state = f"" from file '{checkpoint_file:s}'.""
            self.logger.info(
                f""No checkpoint file is loaded{checkpoint_state:s}"")

        else:

            self.load_state_dict(
                checkpoint['model_state_dict'])
            self.checkpoint_loaded = True
            self.checkpoint_file = checkpoint_file

            if verbose:
                if checkpoint_file is None:
                    checkpoint_state = "".""
                else:
                    checkpoint_state = f"" from file '{checkpoint_file:s}'.""
                self.logger.info(
                    f""Checkpoint file is loaded{checkpoint_state:s}"")

        return

    def check_model_properties(
        self,
        config: settings.Configuration,
        model_properties: List[str],
    ) -> List[str]:
        """"""
        Check model property input.
        
        Parameters
        ----------
        config: settings.Configuration
            Asparagus settings.Configuration class object of global parameter
        model_properties: list(str)
            Properties to predict by calculator model

        Returns
        ----------
        list(str)
            Checked property labels

        """"""
        
        # Check model properties - Selection
        if model_properties is None:
            # If no model properties are defined but data properties are,
            # adopt all supported data properties as model properties,
            # else adopt default model properties
            if config.get('data_properties') is None:
                model_properties = self._default_model_properties
            else:
                model_properties = []
                for prop in config.get('data_properties'):
                    if prop in self._supported_model_properties:
                        model_properties.append(prop)

        # Check model properties - Labels
        for prop in model_properties:
            if not utils.check_property_label(prop, return_modified=False):
                raise SyntaxError(
                    f""Model property label '{prop:s}' is not a valid ""
                    + ""property label! Valid property labels are:\n""
                    + str(list(settings._valid_properties)))
        
        return list(model_properties)

    def set_model_energy_properties(
        self,
        model_properties: List[str],
        model_energy_properties: 
            Optional[List[str]] = ['atomic_energies', 'energy'],
    ) -> List[str]:
        """"""
        Set model energy property parameters.
        
        Parameters
        ----------
        model_properties: list(str)
            Properties to predict by calculator model
        model_energy_properties: list(str)
            Model energy related properties

        Returns
        ----------
        list(str)
            Checked property labels

        """"""

        # Check model properties - Energy and energy gradient properties
        if any([
            prop in model_properties
            for prop in model_energy_properties]
        ):
            self.model_energy = True
            for prop in model_energy_properties:
                if prop not in model_properties:
                    model_properties.append(prop)
        else:
            self.model_energy = False
        if 'hessian' in model_properties:
            self.model_forces = True
            self.model_hessian = True
        elif 'forces' in model_properties:
            self.model_forces = True
            self.model_hessian = False
        else:
            self.model_forces = False
            self.model_hessian = False
        if self.model_forces and not self.model_energy:
            raise SyntaxError(
                f""{self.model_type:s} Model cannot predict energy gradient ""
                + ""properties such as forces or hessians without predicting ""
                + ""energies!"")

        return model_properties

    def check_model_property_units(
        self,
        model_properties: List[str],
        model_unit_properties: Dict[str, str],
        model_default_properties: 
            Optional[List[str]] = ['positions', 'charge'],
    ) -> Dict[str, str]:
        """"""
        Check model property units input.
        
        Parameters
        ----------
        model_properties: list(str)
            Properties to predict by calculator model
        model_unit_properties: dict
            Unit labels of the predicted model properties.
        model_default_properties: list(str), optional, 
                default ['positions', 'charge']
            Default properties where default unit settings are adopted even
            if not defined as model property.

        Returns
        ----------
        dict(str, str)
            Checked unit labels of the predicted model properties.

        """"""

        # Check property units input
        if model_unit_properties is None:
            model_unit_properties = {}

        # Initialize checked property units dictionary
        checked_unit_properties = {}

        # Check if default properties units are defined
        for prop in model_default_properties:
            if prop not in model_unit_properties:
                checked_unit_properties[prop] = (
                    settings._default_units[prop])
                self.logger.info(
                    f""Unit for property '{prop}' is set to the default unit ""
                    + f""'{settings._default_units[prop]}'!"")
            else:
                checked_unit_properties[prop] = (
                    model_unit_properties[prop])

        # Check if all model property units are defined
        for prop in model_properties:
            if prop not in model_unit_properties:
                # Check if a related property unit is defined
                for rel_props in settings._related_unit_properties:
                    if prop in rel_props:
                        for rprop in rel_props:
                            if rprop in model_unit_properties:
                                checked_unit_properties[prop] = (
                                    model_unit_properties[rprop])
                                break
                # Else, take default
                self.logger.warning(
                    f""No unit defined for property '{prop}'!\n""
                    + f""Default unit of '{settings._default_units[prop]}' ""
                    + ""will be used."")
                checked_unit_properties[prop] = (
                    settings._default_units[prop])
            else:
                checked_unit_properties[prop] = (
                    model_unit_properties[prop])

        return checked_unit_properties

    def check_cutoff_ranges(
        self,
        model_cutoff: float,
        model_cuton: float,
        model_switch_range: float,
    ) -> (float, float, float):
        """"""
        Check model cutoff range option.
        
        Parameters
        ----------
        model_cutoff: float
            Upper atom interaction cutoff
        model_cuton: float
            Lower atom pair distance to start interaction switch-off
        model_switch_range: float
            Atom interaction cutoff switch range to switch of interaction to 
            zero. If 'model_cuton' is defined, this input will be ignored.

        Returns
        ----------
        float
            Model start interaction switch-off range
        float
            Model interaction cutoff switch range

        """"""
        
        # Check lower cutoff switch-off range
        if model_cuton is None:
            if model_switch_range > model_cutoff:
                raise SyntaxError(
                    ""Model cutoff switch-off range ""
                    + f""({model_switch_range:.2f}) is larger than the ""
                    + f""upper cutoff range ({model_cutoff:.2f})!"")
            model_cuton = model_cutoff - model_switch_range
        elif model_cuton > model_cutoff:
            message = (
                    ""Lower atom pair cutoff distance 'model_cuton' ""
                    + f""({model_cuton:.2f}) is larger than the upper cutoff ""
                    + f""distance ({model_cutoff:.2f})!"")
            if model_switch_range is None:
                raise SyntaxError(message)
            else:
                model_cuton = model_cutoff - model_switch_range
                self.logger.warning(
                    f""{message:s}\n""
                    + ""Lower atom pair cutoff distance is changed switched ""
                    + f""to '{model_cuton:.2f}'."")
        else:
            model_switch_range = model_cutoff - model_cuton
        if model_cuton < 0.0:
            raise SyntaxError(
                ""Lower atom pair cutoff distance 'model_cuton' is negative ""
                + f""({model_cuton:.2f})!"")
        
        return model_cuton, model_switch_range

    def get_cutoff_ranges(self) -> List[float]:
        """"""
        Get model cutoff or, eventually, short range descriptor and long
        range cutoff list.

        Return
        ------
        list(float)
            List of the long range model and, eventually, short range
            descriptor cutoff (if defined and not short range equal long range
            cutoff).
        """"""

        long_range_cutoff = self.model_cutoff
        if hasattr(self.input_module, 'input_radial_cutoff'):
            short_range_cutoff = (
                self.input_module.input_radial_cutoff)
            if short_range_cutoff != long_range_cutoff:
                cutoffs = [short_range_cutoff, long_range_cutoff]
        else:
            cutoffs = [long_range_cutoff]

        return cutoffs

    def base_modules_setup(
        self,
        config: settings.Configuration,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Setup model calculator base modules input, graph and output
        
        Parameters
        ----------
        config: settings.Configuration
            Asparagus settings.Configuration class object of global parameter
        **kwargs
            Base module options

        """"""

        # Check for input module object in configuration input,
        # otherwise initialize input module
        if config.get('input_module') is not None:

            input_module = config.get('input_module')

        else:

            if config.get('input_type') is None:
                input_type = self._default_modules.get('input_type')
            else:
                input_type = config.get('input_type')
            input_module = module.get_input_module(
                input_type,
                config=config,
                device=self.device,
                dtype=self.dtype,
                verbose=verbose,
                **kwargs)

        # Check for graph module object in configuration input,
        # otherwise initialize graph module
        if config.get('graph_module') is not None:

            graph_module = config.get('graph_module')

        else:

            if config.get('graph_type') is None:
                graph_type = self._default_modules.get('graph_type')
            else:
                graph_type = config.get('graph_type')
            graph_module = module.get_graph_module(
                graph_type,
                config=config,
                device=self.device,
                dtype=self.dtype,
                verbose=verbose,
                **kwargs)

        # Check for output module object in input,
        # otherwise initialize output module
        if config.get('output_module') is not None:

            output_module = config.get('output_module')

        else:

            if config.get('output_type') is None:
                output_type = self._default_modules.get('output_type')
            else:
                output_type = config.get('output_type')
            output_module = module.get_output_module(
                output_type,
                config=config,
                device=self.device,
                dtype=self.dtype,
                verbose=verbose,
                **kwargs)
        
        return input_module, graph_module, output_module

    def get_scaleable_properties(
        self
    ) -> List[str]:
        """"""
        Return list of properties which are scaled by a scaling factor and
        shift term, which initial guess are derived from reference data.

        Returns
        -------
        list(str)
            Scalable model properties

        """"""
        return self.output_module.output_properties

    def set_property_scaling(
        self,
        property_scaling: Optional[
            Dict[str, Union[List[float], Dict[int, List[float]]]]] = None,
        set_shift_term: Optional[bool] = True,
        set_scaling_factor: Optional[bool] = True,
    ):
        """"""
        Set property scaling factor and shift terms and set atomic type
        energies shift.

        Parameters
        ----------
        property_scaling: dict(str, (list(float), dict(int, float)), optional,
                default None
            Model property scaling parameter to shift and scale output module
            results
        set_shift_term: bool, optional, default True
            If True, set or update the shift term. Else, keep previous
            value.
        set_scaling_factor: bool, optional, default True
            If True, set or update the scaling factor. Else, keep previous
            value.

        """"""

        # Set property scaling factors and shift terms
        if (
            property_scaling is not None
            and hasattr(self.output_module, ""set_property_scaling"")
        ):

            self.output_module.set_property_scaling(
                property_scaling,
                set_shift_term=set_shift_term,
                set_scaling_factor=set_scaling_factor)

        return

    def get_property_scaling(
        self,
    ) -> Dict[str, Union[List[float], Dict[int, List[float]]]]:
        """"""
        Get current property scaling factor and shift term dictionary.

        Returns
        -------
        dict(str, (list(float), dict(int, float))
            Current model property scaling parameter to shift and scale output
            module results

        """"""

        # Get property scaling factors and shift terms
        if hasattr(self.output_module, ""get_property_scaling""):
            property_scaling = self.output_module.get_property_scaling()
        else:
            property_scaling = {}

        return property_scaling

    def set_model_unit_properties(
        self,
        model_unit_properties: Dict[str, str],
    ):
        """"""
        Set or change unit property parameter in model layers or modules

        Parameters
        ----------
        model_unit_properties: dict
            Dictionary with the units of the model properties to initialize 
            correct conversion factors.

        """"""
        raise NotImplementedError

    def get_trainable_parameters(
        self,
        no_weight_decay: Optional[bool] = True,
    ) -> Dict[str, List]:
        """"""
        Return a  dictionary of lists for different optimizer options.

        Parameters
        ----------
        no_weight_decay: bool, optional, default True
            Separate parameters on which weight decay should not be applied

        Returns
        -------
        dict(str, List)
            Dictionary of trainable model parameters. Contains 'default' entry
            for all parameters not affected by special treatment. Further
            entries are, if true, the parameter names of the input

        """"""

        # Trainable parameter dictionary
        trainable_parameters = {}
        trainable_parameters['default'] = []
        if no_weight_decay:
            trainable_parameters['no_weight_decay'] = []

        # Iterate over all trainable model parameters
        for name, parameter in self.named_parameters():
            # Catch all parameters to not apply weight decay on
            if no_weight_decay and 'scaling' in name.split('.')[0].split('_'):
                trainable_parameters['no_weight_decay'].append(parameter)
            elif no_weight_decay and 'shift' in name.split('.')[0].split('_'):
                trainable_parameters['no_weight_decay'].append(parameter)
            else:
                trainable_parameters['default'].append(parameter)

        return trainable_parameters

    # @torch.compile # Not supporting backwards propagation with torch.float64
    # @torch.jit.export  # No effect, as 'forward' already is
    def forward(
        self,
        batch: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """"""
        Forward pass of PhysNet Calculator model.

        Parameters
        ----------
        batch : dict(str, torch.Tensor)
            Dictionary of input data tensors for forward pass.
            Basic keys are:
                'atoms_number': torch.Tensor(n_systems)
                    Number of atoms per molecule in batch
                'atomic_numbers': torch.Tensor(n_atoms)
                    Atomic numbers of the batch of molecules
                'positions': torch.Tensor(n_atoms, 3)
                    Atomic positions of the batch of molecules
                'charge': torch.Tensor(n_systems)
                    Total charge of molecules in batch
                'idx_i': torch.Tensor(n_pairs)
                    Atom i pair index
                'idx_j': torch.Tensor(n_pairs)
                    Atom j pair index
                'sys_i': torch.Tensor(n_atoms)
                    System indices of atoms in batch
            Extra keys are:
                'pbc_offset': torch.Tensor(n_pairs)
                    Periodic boundary atom pair vector offset
                'pbc_atoms': torch.Tensor(n_atoms)
                    Primary atom indices for the supercluster approach
                'pbc_idx': torch.Tensor(n_pairs)
                    Image atom to primary atom index pointer for the atom
                    pair indices in a supercluster
                'pbc_idx_j': torch.Tensor(n_pairs)
                    Atom j pair index pointer from image atom to respective
                    primary atom index in a supercluster

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""
        raise NotImplementedError

    def calculate(
        self,
        atoms: ase.Atoms,
        charge: Optional[float] = None,
        **kwargs,
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of the calculator model with an ASE Atoms object.

        Parameters
        ----------
        atoms: ase.Atoms
            ASE Atoms object to calculate properties
        charge: float, optional, default None
            Total system charge. If None, charge is estimated from atomic
            charges, if available, Else, charge is set as zero.

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""

        # Create atoms batch dictionary
        atoms_batch = self.create_batch(
            atoms,
            charge=charge)

        return self.forward(atoms_batch, **kwargs)

    def create_batch(
        self,
        atoms: Union[ase.Atoms, List[ase.Atoms]],
        charge: Optional[Union[float, List[float]]] = None,
        conversion: Optional[Dict[str, float]] = {},
    ) -> Dict[str, torch.Tensor]:
        """"""
        Create a systems batch dictionary as input for the model calculator.
        
        Parameters
        ----------
        atoms: (ase.Atoms, list(ase.Atoms))
            ASE Atoms object or list of ASE Atoms object to prepare batch
            dictionary.
        charge: (float list(float)), optional, default 0.0
            Total system charge or list of total system charge.
            If None, charge is estimated from the ASE Atoms objects, mostly
            set as zero.
        conversion: dict(str, float), optional, default {}
            ASE Atoms conversion dictionary from ASE units to model units.

        Returns
        -------
        dict(str, torch.Tensor)
            System(s) batch dictionary used as model calculator input

        """"""

        # Check atoms input
        if utils.is_ase_atoms(atoms):
            atoms = [atoms]
        elif not utils.is_ase_atoms_array(atoms):
            raise ValueError(
                ""Input 'atoms' is not an ASE Atoms object or list of ASE ""
                + ""atoms objects!"")

        # Initialize atoms batch
        batch = {}

        # Number of atoms
        batch['atoms_number'] = torch.tensor(
            [len(atms) for atms in atoms],
            device=self.device, dtype=torch.int64)

        # System segment index of atom i
        batch['sys_i'] = torch.repeat_interleave(
            torch.arange(len(atoms), device=self.device, dtype=torch.int64),
            repeats=batch['atoms_number'], dim=0).to(
                device=self.device, dtype=torch.int64)

        # Atomic numbers properties
        batch['atomic_numbers'] = torch.cat(
            [
                torch.tensor(atms.get_atomic_numbers(), dtype=torch.int64)
                for atms in atoms
            ], 0).to(
                device=self.device, dtype=torch.int64)

        # Atom positions
        if conversion.get('positions') is None:
            fconv = 1.0
        else:
            fconv = conversion['positions']
        batch['positions'] = torch.cat(
            [
                torch.tensor(atms.get_positions()*fconv, dtype=self.dtype)
                for atms in atoms
            ], 0).to(
                device=self.device, dtype=self.dtype)

        # Atom periodic boundary conditions
        batch['pbc'] = torch.tensor(
            np.array([atms.get_pbc() for atms in atoms]),
            dtype=torch.bool, device=self.device)

        # Atom cell information
        if conversion.get('positions') is None:
            fconv = 1.0
        else:
            fconv = conversion['positions']
        batch['cell'] = torch.tensor(
            np.array([atms.get_cell()[:]*fconv for atms in atoms]),
            dtype=self.dtype, device=self.device)

        # Total atomic system charge
        if conversion.get('charge') is None:
            fconv = 1.0
        else:
            fconv = conversion['charge']
        if charge is None:
            try:
                charge = [np.sum(atms.get_charges())*fconv for atms in atoms]
            except RuntimeError:
                charge = [
                    np.sum(atms.get_initial_charges())*fconv
                    for atms in atoms]
        elif utils.is_numeric(charge):
            charge = [charge*fconv]*len(atoms)
        elif utils.is_numeric_array(charge):
            charge = np.array(charge)*fconv
        else:
            charge = [0.0]*len(atoms)
        batch['charge'] = torch.tensor(
            charge, dtype=self.dtype, device=self.device)

        # Compute atom pair indices
        if not hasattr(self, 'neighbor_list'):
            self.neighbor_list = module.TorchNeighborListRangeSeparated(
                self.get_cutoff_ranges(),
                self.device,
                self.dtype)
        batch = self.neighbor_list(batch)

        return batch

    def create_batch_copies(
        self,
        atoms: ase.Atoms,
        ncopies: Optional[int] = None,
        positions: Optional[List[float]] = None,
        cell: Optional[List[float]] = None,
        charge: Optional[float] = None,
        conversion: Optional[Dict[str, float]] = {},
    ) -> Dict[str, torch.Tensor]:
        """"""
        Create a systems batch dictionary as input for the model calculator.

        Parameters
        ----------
        atoms: (ase.Atoms)
            ASE Atoms object to prepare multiple copies in a batch dictionary.
        ncopies: int, optional, default None
            Number of copies of the ASE atoms system in the batch.
            If None, number of copies are taken from 'positions' or 'cell'
            input, otherwise is 1.
        positions: list(float), optional, default None
            Array of shape ('Ncopies', 'Natoms', 3) where 'Ncopies' is the
            number of copies of the ASE Atoms system in the batch and 'Natoms'
            is the number of atoms.
            If None, the positions of the ASE Atoms object is taken.
        cell: list(float), optional, default None
            Array of ASE Atoms cell parameter of shape ('Ncopies', 3).
            If None, the cell parameters from the ASE Atoms object is taken.
        charge: float, optional, default 0.0
            Total system charge of the ASE atoms object.
            If None, charge is estimated from the ASE Atoms objects, mostly
            set as zero.
        conversion: dict(str, float), optional, default {}
            ASE Atoms conversion dictionary from ASE units to model units.

        Returns
        -------
        dict(str, torch.Tensor)
            System(s) batch dictionary used as model calculator input

        """"""

        # Get number of copies
        if ncopies is None and positions is None and cell is None:
            ncopies = 1
        elif ncopies is None and positions is not None:
            ncopies = len(positions)
        elif ncopies is None and cell is not None:
            ncopies = len(cell)

        # Check positions and cell input
        if positions is not None and ncopies != len(positions):
            raise SyntaxError(
                f""Number of copies ({ncopies:d}) and positions ""
                + f""({len(positions):d}) does not match!"")
        if cell is not None and ncopies != len(cell):
            raise SyntaxError(
                f""Number of copies ({ncopies:d}) and cells ""
                + f""({len(cell):d}) does not match!"")

        # Initialize atoms batch
        batch = {}

        # Number of atoms
        batch['atoms_number'] = torch.tensor(
            [len(atoms)]*ncopies, device=self.device, dtype=torch.int64)

        # System segment index of atom i
        batch['sys_i'] = torch.repeat_interleave(
            torch.arange(ncopies, device=self.device, dtype=torch.int64),
            repeats=len(atoms), dim=0).to(
                device=self.device, dtype=torch.int64)

        # Atomic numbers properties
        batch['atomic_numbers'] = torch.cat(
            [
                torch.tensor(atoms.get_atomic_numbers(), dtype=torch.int64)
                for _ in range(ncopies)
            ], 0).to(
                device=self.device, dtype=torch.int64)

        # Atom positions
        if conversion.get('positions') is None:
            fconv = 1.0
        else:
            fconv = conversion['positions']
        if positions is None:
            batch['positions'] = torch.cat(
                [
                    torch.tensor(atoms.get_positions()*fconv, dtype=self.dtype)
                    for _ in range(ncopies)
                ], 0).to(
                    device=self.device, dtype=self.dtype)
        else:
            batch['positions'] = torch.cat(
                [
                    torch.tensor(positions_i, dtype=self.dtype)*fconv
                    for positions_i in positions
                ], 0).to(
                    device=self.device, dtype=self.dtype)

        # Atom periodic boundary conditions
        batch['pbc'] = torch.tensor(
            atoms.get_pbc().repeat(ncopies).reshape(ncopies, 3),
            dtype=torch.bool, device=self.device)

        # Atom cell information
        if conversion.get('positions') is None:
            fconv = 1.0
        else:
            fconv = conversion['positions']
        if cell is None:
            batch['cell'] = torch.tensor(
                (atoms.get_cell()[:]*fconv).repeat(ncopies).reshape(
                    ncopies, 3, 3),
                dtype=self.dtype, device=self.device)
        else:
            batch['cell'] = torch.tensor(
                [cell_i for cell_i in cell],
                dtype=self.dtype, device=self.device)*fconv

        # Total atomic system charge
        if conversion.get('charge') is None:
            fconv = 1.0
        else:
            fconv = conversion['charge']
        if charge is None:
            try:
                charge = [
                    np.sum(atoms.get_charges())*fconv for _ in range(ncopies)]
            except RuntimeError:
                charge = [
                    np.sum(atoms.get_initial_charges())*fconv
                    for _ in range(ncopies)]
        elif utils.is_numeric(charge):
            charge = [charge*fconv]*ncopies
        elif utils.is_numeric_array(charge):
            charge = np.array(charge)*fconv
        else:
            charge = [0.0]*ncopies
        batch['charge'] = torch.tensor(
            charge, dtype=self.dtype, device=self.device)

        # Compute atom pair indices
        if not hasattr(self, 'neighbor_list'):
            self.neighbor_list = module.TorchNeighborListRangeSeparated(
                self.get_cutoff_ranges(),
                self.device,
                self.dtype)
        batch = self.neighbor_list(batch)

        return batch

    def calculate_data(
        self,
        dataset: Union[data.DataContainer, data.DataSet, data.DataSubSet],
        batch_size: Optional[int] = 32,
        num_workers: Optional[int] = 1,
        **kwargs
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of the calculator model with an Asparagus data set.

        Parameters
        ----------
        dataset: (data.DataContainer, data.DataSet, data.DataSubSet)
            Asparagus DataContainer or DataSet object
        batch_size: int, optional, default 32
            Data loader batch size
        num_workers: int, optional, default 1
            Number of data loader workers

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""

        # Prepare data loader for the data set
        dataloader = data.DataLoader(
            dataset,
            batch_size,
            False,
            num_workers,
            self.device,
            self.dtype)

        # Iterate over data batches
        for ib, batch in enumerate(dataloader):
            
            # Predict model properties from data batch
            prediction = self.forward(batch, **kwargs)

            # Append prediction to result dictionary
            for prop, item in prediction.items():
                if results.get(prop) is None:
                    results[prop] = [item.cpu().detach()]
                else:
                    results[prop].append(item.cpu().detach())
    
        # Concatenate results
        for prop, item in results.items():
            if ib and item[0].shape:
                results[prop] = torch.cat(item)
            elif ib:
                results[prop] = torch.cat(
                    [item_i.reshape(1) for item_i in item], dim=0)
            else:
                results[prop] = item[0]

        return prediction",./Asparagus/asparagus/model/basemodel.py
EnsembleModel,"class EnsembleModel(torch.nn.Module):
    """"""
    Asparagus ensemble model calculator

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to config json file (str)
    model_calculator_class: callable, optional, default None
        Number of model calculator in ensemble. If None and
        'model_ensemble' is True, take all available models found.
    model_ensemble_num: int, optional, default None
        Number of model calculator in ensemble. If None and
        'model_ensemble' is True, take all available models found.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for graph module
    _default_args = {
        'model_calculator_class':       None,
        'model_ensemble_num':           3,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'model_calculator_class':       [utils.is_None, utils.is_callable],
        'model_ensemble_num':           [utils.is_integer],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        model_calculator_class: Optional[Callable] = None,
        model_ensemble_num: Optional[int] = None,
        device: Optional[str] = None,
        dtype: Optional[object] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize EnsembleModel Calculator


        """"""

        super(EnsembleModel, self).__init__()

        #############################
        # # # Check Class Input # # #
        #############################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, model),
            check_dtype=utils.get_dtype_args(self, model))

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        #############################
        # # # Check Model Input # # #
        #############################

        # Check model calculator class
        if self.model_calculator_class is None:
            if (
                kwargs.get('model_type') is None
                and config.get('model_type') is None
            ):
                model_type = settings._default_calculator_model
            elif config.get('model_type') is None:
                model_type = kwargs.get('model_type')
            else:
                model_type = config.get('model_type')

            # Get requested calculator model
            self.model_calculator_class = (
                model.calculator._get_model_calculator(model_type))

        #######################################
        # # # Initialize Model Calculator # # #
        #######################################

        # Initialize model calculator list
        model_calculator_list = []

        # Iterate over number of model calculator
        for _ in range(self.model_ensemble_num):

            # Initialize model calculator
            model_calculator_list.append(self.model_calculator_class(
                config=config,
                verbose=verbose,
                device=self.device,
                dtype=self.dtype,
                **kwargs)
            )

        # Convert model calculator list to torch module list
        self.model_calculator_list = torch.nn.ModuleList(
            model_calculator_list)

        ##################################
        # # # Adopt Model Parameters # # #
        ##################################

        # Assign model ensemble flag
        self.model_ensemble = True

        # Model type label
        self.model_type = self.model_calculator_list[0].model_type
        self._model_type = self.model_calculator_list[0]._model_type

        # Model properties and units
        self.model_properties = (
            self.model_calculator_list[0].model_properties.copy())
        self.model_unit_properties = (
            self.model_calculator_list[0].model_unit_properties.copy())
        for prop in self.model_calculator_list[0].model_properties:
            prop_std = f""std_{prop:s}""
            self.model_properties.append(prop_std)
            self.model_unit_properties[prop_std] = (
                self.model_calculator_list[0].model_unit_properties[prop])

        # Model cutoff ranges
        if hasattr(self.model_calculator_list[0], 'model_cutoff'):
            self.model_cutoff = (
                self.model_calculator_list[0].model_cutoff)
        if hasattr(self.model_calculator_list[0], 'model_cuton'):
            self.model_cuton = (
                self.model_calculator_list[0].model_cuton)
        if hasattr(self.model_calculator_list[0], 'model_switch_range'):
            self.model_switch_range = (
                self.model_calculator_list[0].model_switch_range)

        return

    def __str__(self):
        return f""{self._model_type:s} Ensemble""

    def __len__(
        self
    ) -> int:
        return len(self.model_calculator_list)

    def __getitem__(
        self,
        idx: int
    ):
        return self.model_calculator_list[idx]

    def __iter__(
        self,
    ):
        # Start model counter and model ensemble number
        self.counter = 0
        self.Nmodels = len(self)
        return self

    def __next__(
        self
    ):
        # Check counter within number of data range
        if self.counter < self.Nmodels:
            model = self.model_calculator_list[self.counter]
            self.counter += 1
            return model
        else:
            raise StopIteration

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return model ensemble, model and module information
        """"""

        # Initialize info dictionary
        info = {}

        # Collect model and module info
        if hasattr(self.model_calculator_list[0], ""get_info""):
            info = {**info, **self.model_calculator_list[0].get_info()}

        return {
            **info,
            'model_type': self._model_type,
            'model_ensemble': self.model_ensemble,
            'model_ensemble_num': self.model_ensemble_num,
        }

    @property
    def checkpoint_loaded(self):
        return all([
            model_calculator.checkpoint_loaded
            for model_calculator in self.model_calculator_list])

    @property
    def checkpoint_file(self):
        return [
            model_calculator.checkpoint_file
            for model_calculator in self.model_calculator_list]

    def load(
        self,
        checkpoint: List[Dict[str, Any]],
        checkpoint_file: Optional[List[str]] = None,
        verbose: Optional[bool] = True,
    ):
        """"""
        Load model parameters from checkpoint file.

        Parameters
        ----------
        checkpoint: list(dict(str, Any))
            Torch module checkpoint files for each model calculator in the
            ensemble
        checkpoint_file: list(str), optional, default None
            List of torch checkpoint file pathways for logger info

        """"""

        # Load model checkpoint file
        if (
            (checkpoint is None or utils.is_None_array(checkpoint))
            and verbose
        ):

            # Prepare and print loading info
            if (
                checkpoint_file is None
                or utils.is_None_array(checkpoint_file)
            ):
                checkpoint_state = "".""
            else:
                checkpoint_state = "" from files:\n""
                for ickpt, ckpt_file in enumerate(checkpoint_file):
                    checkpoint_state += (
                        f"" Model {ickpt:d} - '{ckpt_file:s}'\n"")
                checkpoint_state = checkpoint_state[:-1]
            self.logger.info(
                f""No checkpoint files are loaded{checkpoint_state:s}"")

        else:

            # Prepare loading info
            if checkpoint_file is None:
                checkpoint_state = "".""
            else:
                checkpoint_state = "" from files:\n""

            # Iterate over number of model calculator
            for ickpt, (model_calculator, ckpt_i) in enumerate(zip(
                self.model_calculator_list,
                checkpoint
            )):

                if ckpt_i is None:
                    
                    checkpoint_state += (f"" Model {ickpt:d} - not loaded!\n"")
                
                else:

                    model_calculator.load_state_dict(
                        ckpt_i['model_state_dict'])
                    model_calculator.checkpoint_loaded = True
                    if (
                        checkpoint_file is None 
                        or checkpoint_file[ickpt] is None
                    ):
                        model_calculator.checkpoint_file = None
                        checkpoint_state += (
                            f"" Model {ickpt:d} - loaded!\n"")
                    else:
                        model_calculator.checkpoint_file = (
                            checkpoint_file[ickpt])
                        checkpoint_state += (
                            f"" Model {ickpt:d} - ""
                            + f""'{checkpoint_file[ickpt]:s}' loaded!\n"")

            # Print loading info
            if verbose:
                checkpoint_state = checkpoint_state[:-1]
                self.logger.info(
                    f""Checkpoint files are loaded{checkpoint_state:s}"")

        return

    def get_cutoff_ranges(self) -> List[float]:
        """"""
        Get model cutoff or, eventually, short range descriptor and long
        range cutoff list.

        Return
        ------
        list(float)
            List of the long range model and, eventually, short range
            descriptor cutoff (if defined and not short range equal long range
            cutoff).

        """"""
        return self.model_calculator_list[0].get_cutoff_ranges()

    # @torch.compile # Not supporting backwards propagation with torch.float64
    # @torch.jit.export  # No effect, as 'forward' already is
    def forward(
        self,
        batch: Dict[str, torch.Tensor],
        no_derivation: Optional[bool] = False,
        verbose_results: Optional[bool] = False,
    ) -> Dict[str, torch.Tensor]:
        """"""
        Forward pass of the model ensemble calculator.

        Parameters
        ----------
        batch: dict(str, torch.Tensor)
            Dictionary of input data tensors for forward pass.
            Basic keys are:
                'atoms_number': torch.Tensor(n_systems)
                    Number of atoms per molecule in batch
                'atomic_numbers': torch.Tensor(n_atoms)
                    Atomic numbers of the batch of molecules
                'positions': torch.Tensor(n_atoms, 3)
                    Atomic positions of the batch of molecules
                'charge': torch.Tensor(n_systems)
                    Total charge of molecules in batch
                'idx_i': torch.Tensor(n_pairs)
                    Atom i pair index
                'idx_j': torch.Tensor(n_pairs)
                    Atom j pair index
                'sys_i': torch.Tensor(n_atoms)
                    System indices of atoms in batch
            Extra keys are:
                'pbc_offset': torch.Tensor(n_pairs)
                    Periodic boundary atom pair vector offset
                'pbc_atoms': torch.Tensor(n_atoms)
                    Primary atom indices for the supercluster approach
                'pbc_idx': torch.Tensor(n_pairs)
                    Image atom to primary atom index pointer for the atom
                    pair indices in a supercluster
                'pbc_idx_j': torch.Tensor(n_pairs)
                    Atom j pair index pointer from image atom to respective
                    primary atom index in a supercluster
        no_derivation: bool, optional, default False
            If True, only predict non-derived properties.
            Else, predict all properties even if backwards derivation is
            required (e.g. forces).
        verbose_results: bool, optional, default False
            If True, store single model property predictions and extended model
            property contributions.

        Returns
        -------
        dict(str, torch.Tensor)
            Model ensemble average property predictions, uncertainties and,
            if requested, single model property predictions.

        """"""

        # Initialize model and model ensemble result dictionaries
        model_results = {}
        ensemble_results = {}

        # Iterate over ensemble models
        for ic, model_calculator in enumerate(self.model_calculator_list):
            model_results[ic] = model_calculator(
                batch,
                no_derivation=no_derivation,
                verbose_results=verbose_results)

        # Accumulate model results
        for prop in model_results[0]:
            prop_std = f""std_{prop:s}""
            ensemble_results[prop_std], ensemble_results[prop] = (
                torch.std_mean(
                    torch.stack(
                        [results[prop] for results in model_results.values()]),
                    dim=0)
                )

        # Update model ensemble results with single results, if requested
        if verbose_results:
            ensemble_results.update(model_results)

        return ensemble_results

    def calculate(
        self,
        atoms: ase.Atoms,
        charge: Optional[float] = 0.0,
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of the ensemble model calculator with an ASE Atoms
        object.

        Parameters
        ----------
        atoms: ase.Atoms
            ASE Atoms object to calculate properties
        charge: float, optional, default 0.0
            Total system charge

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""

        # Create atoms batch dictionary
        atoms_batch = self.create_batch(
            atoms,
            charge=charge)

        return self.forward(atoms_batch)

    def create_batch(
        self,
        atoms: Union[ase.Atoms, List[ase.Atoms]],
        charge: Optional[Union[float, List[float]]] = None,
        conversion: Optional[Dict[str, float]] = {},
    ) -> Dict[str, torch.Tensor]:
        """"""
        Create a systems batch dictionary as input for the model calculator.

        Parameters
        ----------
        atoms: (ase.Atoms, list(ase.Atoms))
            ASE Atoms object or list of ASE Atoms object to prepare batch
            dictionary.
        charge: (float list(float)), optional, default 0.0
            Total system charge or list of total system charge.
            If None, charge is estimated from the ASE Atoms objects, mostly
            set as zero.
        conversion: dict(str, float), optional, default {}
            ASE Atoms conversion dictionary from ASE units to model units.

        Returns
        -------
        dict(str, torch.Tensor)
            System(s) batch dictionary used as model calculator input

        """"""

        return self.model_calculator_list[0].create_batch(
            atoms,
            charge=charge,
            conversion=conversion)

    def create_batch_copies(
        self,
        atoms: ase.Atoms,
        ncopies: Optional[int] = None,
        positions: Optional[List[float]] = None,
        cell: Optional[List[float]] = None,
        charge: Optional[float] = None,
        conversion: Optional[Dict[str, float]] = {},
    ) -> Dict[str, torch.Tensor]:
        """"""
        Create a systems batch dictionary as input for the model calculator.

        Parameters
        ----------
        atoms: (ase.Atoms)
            ASE Atoms object to prepare multiple copies in a batch dictionary.
        ncopies: int, optional, default None
            Number of copies of the ASE atoms system in the batch.
            If None, number of copies are taken from 'positions' or 'cell'
            input, otherwise is 1.
        positions: list(float), optional, default None
            Array of shape ('Ncopies', 'Natoms', 3) where 'Ncopies' is the
            number of copies of the ASE Atoms system in the batch and 'Natoms'
            is the number of atoms.
            If None, the positions of the ASE Atoms object is taken.
        cell: list(float), optional, default None
            Array of ASE Atoms cell parameter of shape ('Ncopies', 3).
            If None, the cell parameters from the ASE Atoms object is taken.
        charge: float, optional, default 0.0
            Total system charge of the ASE atoms object.
            If None, charge is estimated from the ASE Atoms objects, mostly
            set as zero.
        conversion: dict(str, float), optional, default {}
            ASE Atoms conversion dictionary from ASE units to model units.

        Returns
        -------
        dict(str, torch.Tensor)
            System(s) batch dictionary used as model calculator input

        """"""

        return self.model_calculator_list[0].create_batch_copies(
            atoms,
            ncopies=ncopies,
            positions=positions,
            cell=cell,
            charge=charge,
            conversion=conversion)

    def calculate_data(
        self,
        dataset: Union[data.DataContainer, data.DataSet, data.DataSubSet],
        batch_size: Optional[int] = 32,
        num_workers: Optional[int] = 1,
        verbose_results: Optional[bool] = False,
        **kwargs
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of the calculator model with an Asparagus data set.

        Parameters
        ----------
        dataset: (data.DataContainer, data.DataSet, data.DataSubSet)
            Asparagus DataContainer or DataSet object
        batch_size: int, optional, default 32
            Data loader batch size
        num_workers: int, optional, default 1
            Number of data loader workers
        verbose_results: bool, optional, default False
            If True, store single model property predictions and extended model
            property contributions.

        Returns
        -------
        dict(str, torch.Tensor)
            Model property predictions

        """"""

        # Prepare data loader for the data set
        dataloader = data.DataLoader(
            dataset,
            batch_size,
            False,
            num_workers,
            self.device,
            self.dtype)

        # Initialize model ensemble result dictionary
        results = {}

        # Iterate over data batches
        for ib, batch in enumerate(dataloader):
            
            # Predict model properties from data batch
            prediction = self.forward(
                batch, 
                verbose_results=verbose_results,
                **kwargs)

            # Append prediction to result dictionary
            for prop, item in prediction.items():
                if verbose_results and utils.is_dictionary(item):
                    if results.get(prop) is None:
                        results[prop] = {}
                    for sub_prop, sub_item in item.items():
                        if results[prop].get(sub_prop) is None:
                            results[prop][sub_prop] = [sub_item.cpu().detach()]
                        else:
                            results[prop][sub_prop].append(
                                sub_item.cpu().detach())
                elif results.get(prop) is None:
                    results[prop] = [item.cpu().detach()]
                else:
                    results[prop].append(item.cpu().detach())

        # Concatenate results
        for prop, item in results.items():
            if verbose_results and utils.is_dictionary(item):
                for sub_prop, sub_item in item.items():
                    if ib and sub_item[0].shape:
                        results[prop][sub_prop] = torch.cat(sub_item)
                    elif ib:
                        results[prop][sub_prop] = torch.cat(
                            [item_i.reshape(1) for item_i in sub_item], dim=0)
                    else:
                        results[prop][sub_prop] = sub_item[0]
            elif ib and item[0].shape:
                results[prop] = torch.cat(item)
            elif ib:
                results[prop] = torch.cat(
                    [item_i.reshape(1) for item_i in item], dim=0)
            else:
                results[prop] = item[0]

        return results",./Asparagus/asparagus/model/ensemble.py
FileManager,"class FileManager():
    """"""
    File manager for loading and storing model parameter and training files.
    Manage checkpoint creation and loading writer to tensorboardX

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to config json file (str)
    model_calculator: torch.nn.Module, optional, default None
        Model calculator to to manage.
    model_directory: str, optional, default None
        Model directory that contains checkpoint and log files.
    model_max_checkpoints: int, optional, default 1
        Maximum number of checkpoint files.
    **kwargs: dict
        Additional keyword arguments for tensorboards 'SummaryWriter'

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for graph module
    _default_args = {
        'model_directory':              None,
        'model_max_checkpoints':        1,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'model_directory':              [utils.is_string, utils.is_None],
        'model_max_checkpoints':        [utils.is_integer],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        model_calculator: Optional[torch.nn.Module] = None,
        model_directory: Optional[str] = None,
        model_max_checkpoints: Optional[int] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize file manager class.

        """"""

        ####################################
        # # # Check File Manager Input # # #
        ####################################

        # Get configuration object
        config = settings.get_config(config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, None),
            check_dtype=utils.get_dtype_args(self, None))
        
        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)
        
        #######################################
        # # # Prepare Directory Parameter # # #
        #######################################

        # Get model parameter
        if self.model_calculator is None:

            self.model_type = config.get('model_type')
            self.model_ensemble = config.get('model_ensemble')
            self.model_ensemble_num = config.get('model_ensemble_num')

        else:

            if hasattr(self.model_calculator, 'model_type'):
                self.model_type = self.model_calculator.model_type
            else:
                self.model_type = config.get('model_type')
            if hasattr(self.model_calculator, 'model_ensemble'):
                self.model_ensemble = self.model_calculator.model_ensemble
            else:
                self.model_ensemble = False
            if hasattr(self.model_calculator, 'model_ensemble_num'):
                self.model_ensemble_num = (
                    self.model_calculator.model_ensemble_num)
            elif self.model_ensemble:
                self.model_ensemble_num = config.get('model_ensemble_num')
            else:
                self.model_ensemble_num = None

        # Take either defined model directory path or a generate a generic one
        if self.model_directory is None:
            if self.model_type is None:
                model_label = """"
            else:
                model_label = self.model_type + ""_""
            self.model_directory = (
                model_label
                + datetime.datetime.now().strftime(""%Y%m%d%H%M%S""))
            config.update(
                {'model_directory': self.model_directory},
                verbose=verbose)

        # Prepare model subdirectory paths
        if self.model_ensemble:
            self.best_dir = [
                os.path.join(self.model_directory, f'{imodel:d}', 'best')
                for imodel in range(self.model_ensemble_num)]
            self.ckpt_dir = [
                os.path.join(
                    self.model_directory, f'{imodel:d}', 'checkpoints')
                for imodel in range(self.model_ensemble_num)]
            self.logs_dir = [
                os.path.join(self.model_directory, f'{imodel:d}', 'logs')
                for imodel in range(self.model_ensemble_num)]
        else:

            self.best_dir = os.path.join(self.model_directory, 'best')
            self.ckpt_dir = os.path.join(self.model_directory, 'checkpoints')
            self.logs_dir = os.path.join(self.model_directory, 'logs')

        return

    def __str__(self):
        return f""FileManager '{self.model_directory:s}'""

    def create_model_directory(self):
        """"""
        Create folders for checkpoints and tensorboardX
        """"""

        # Create model directory
        if not os.path.exists(self.model_directory):
            os.makedirs(self.model_directory)
        # Create directory for best model checkpoints
        if not os.path.exists(self.best_dir):
            if self.model_ensemble:
                for imodel in range(self.model_ensemble_num):
                    os.makedirs(self.best_dir[imodel])
            else:
                os.makedirs(self.best_dir)
        # Create directory for model parameter checkpoints
        if not os.path.exists(self.ckpt_dir):
            if self.model_ensemble:
                for imodel in range(self.model_ensemble_num):
                    os.makedirs(self.ckpt_dir[imodel])
            else:
                os.makedirs(self.ckpt_dir)
        # Create directory for tensorboardX/logs
        if not os.path.exists(self.logs_dir):
            if self.model_ensemble:
                for imodel in range(self.model_ensemble_num):
                    os.makedirs(self.logs_dir[imodel])
            else:
                os.makedirs(self.logs_dir)

        return

    def save_checkpoint(
        self,
        model_calculator: ""model.BaseModel"",
        optimizer: Optional[""torch.Optimizer""] = None,
        scheduler: Optional[""torch.Scheduler""] = None,
        epoch: Optional[int] = 0,
        best: Optional[bool] = False,
        best_loss: Optional[float] = None,
        num_checkpoint: Optional[int] = None,
        max_checkpoints: Optional[int] = None,
        imodel: Optional[int] = None,
    ):
        """"""
        Save model parameters and training state to checkpoint file.

        Parameters
        ----------
        model_calculator: model.BaseModel
            Torch calculator model
        optimizer: torch.Optimizer, optional, default None
            Torch optimizer
        scheduler: torch.Scheduler, optional, default None
            Torch scheduler
        epoch: int, optional, default 0
            Training epoch of calculator model 
        best: bool, optional, default False
            If True, save as best model checkpoint file.
        best_loss: float, optional, default None
            Best loss value of the training run.
        num_checkpoint: int, optional, default None
            Alternative checkpoint index other than epoch.
        max_checkpoints: int, optional, default 1
            Maximum number of checkpoint files. If the threshold is reached and
            a checkpoint of the best model (best=True) or specific number
            (num_checkpoint is not None), respectively many checkpoint files
            with the lowest indices will be deleted.
        imodel: int, optional, default None
            Model index in case of a model ensemble

        """"""

        # Check 'imodel' input in case of a model ensemble
        if self.model_ensemble and imodel is None:
            raise SyntaxError(
                ""Checkpoint state of a model in a model ensemble cannot be ""
                + ""stored without definition of the model index 'imodel'."")

        # Check existence of the directories
        self.create_model_directory()

        # Prepare state dictionary to store
        state = {
            'model_state_dict': model_calculator.state_dict(),
            'best_loss': best_loss,
            'epoch': epoch,
        }

        # For best model, just store model parameter, epoch and loss value
        if best:
            pass
        # Else the complete current model training state if available
        else:
            if optimizer is not None:
                state.update(
                    {'optimizer_state_dict': optimizer.state_dict()}
                )
            if scheduler is not None:
                state.update(
                    {'scheduler_state_dict': scheduler.state_dict()}
                )

        # Checkpoint file name
        if best:
            if self.model_ensemble:
                best_dir = self.best_dir[imodel]
            else:
                best_dir = self.best_dir
            ckpt_name = os.path.join(best_dir, 'best_model.pt')
        elif num_checkpoint is None:
            if self.model_ensemble:
                ckpt_dir = self.ckpt_dir[imodel]
            else:
                ckpt_dir = self.ckpt_dir
            ckpt_name = os.path.join(ckpt_dir, f'model_{epoch:d}.pt')
        else:
            if utils.is_integer(num_checkpoint):
                if self.model_ensemble:
                    ckpt_dir = self.ckpt_dir[imodel]
                else:
                    ckpt_dir = self.ckpt_dir
                ckpt_name = os.path.join(
                    ckpt_dir, f'model_{num_checkpoint:d}.pt')
            else:
                raise ValueError(
                    ""Checkpoint file index number 'num_checkpoint' is not ""
                    + ""an integer!"")

        # Write checkpoint file
        torch.save(state, ckpt_name)

        # Store latest checkpoint file in model calculator
        model_calculator.checkpoint_loaded = True
        model_calculator.checkpoint_file = ckpt_name

        # Check number of epoch checkpoints
        if not best and num_checkpoint is None:
            self.check_max_checkpoints(
                max_checkpoints=max_checkpoints,
                imodel=imodel)

        return

    def load_checkpoint(
        self,
        checkpoint_label: Union[str, int],
        return_name: Optional[bool] = False,
        imodel: Optional[int] = None,
        verbose: Optional[bool] = True,
    ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
        """"""
        Load model parameters and training state from checkpoint file.

        Parameters
        ----------
        checkpoint_label: (str, int)
            If None, load checkpoint file with best loss function value.
            If string and a valid file path, load the respective checkpoint 
            file.
            If string 'best' or 'last', load respectively the best checkpoint 
            file (as with None) or the with the highest epoch number.
            If integer, load the checkpoint file of the respective epoch 
            number.
        return_name: bool, optional, default False
            If True, return checkpoint state and checkpoint file batch.
            Else, only return checkpoint state
        imodel: int, optional, default None
            Model index in case of a model ensemble

        Returns
        -------
        Any
            Torch module checkpoint file

        """"""

        # Check checkpoint label input
        if checkpoint_label is None:
            checkpoint_label = 'best'

        # Load best model checkpoint
        if (
            utils.is_string(checkpoint_label)
            and checkpoint_label.lower() == 'best'
        ):

            ckpt_name = self.get_best_checkpoint(
                imodel=imodel,
                verbose=verbose)

        # Load last model checkpoint
        elif (
            utils.is_string(checkpoint_label)
            and checkpoint_label.lower() == 'last'
        ):

            ckpt_name = self.get_last_checkpoint(
                imodel=imodel,
                verbose=verbose)

        else:

            ckpt_name = self.get_specific_checkpoint(
                checkpoint_label=checkpoint_label,
                imodel=imodel,
                verbose=verbose)

        # Load checkpoint(s)
        if utils.is_array_like(ckpt_name):
            checkpoint = []
            for ckpt_name_i in ckpt_name:
                if ckpt_name_i is None:
                    checkpoint.append(None)
                else:
                    checkpoint.append(
                        torch.load(ckpt_name_i, weights_only=False))
        elif ckpt_name is None:
            checkpoint = None
        else:
            checkpoint = torch.load(ckpt_name, weights_only=False)

        if return_name:
            return checkpoint, ckpt_name
        else:
            return checkpoint

    def check_max_checkpoints(
        self,
        max_checkpoints: Optional[int] = None,
        imodel: Optional[int] = None,
    ):
        """"""
        Check number of checkpoint files and in case of exceeding the
        maximum checkpoint threshold, delete the ones with lowest indices.

        Parameters
        ----------
        max_checkpoints: int, optional, default None
             Maximum number of checkpoint files. If None, the threshold is
             taken from the class attribute 'self.model_max_checkpoints'.
        imodel: int, optional, default None
            Model index in case of a model ensemble

        """"""

        # Check 'imodel' input in case of a model ensemble
        if self.model_ensemble and imodel is None:
            raise SyntaxError(
                ""Missing model index 'imodel' definition of a model in a ""
                + ""model ensemble."")
        
        # Skip in checkpoint threshold is None
        if max_checkpoints is None and self.model_max_checkpoints is None:
            return
        elif max_checkpoints is None:
            max_checkpoints = self.model_max_checkpoints

        # Get checkpoint directory
        if self.model_ensemble:
            ckpt_dir = self.ckpt_dir[imodel]
        else:
            ckpt_dir = self.ckpt_dir

        # Gather checkpoint files
        num_checkpoints = []
        for ckpt_file in os.listdir(ckpt_dir):
            ckpt_num = re.findall(""model_(\d+).pt"", ckpt_file)
            if ckpt_num:
                num_checkpoints.append(int(ckpt_num[0]))
        num_checkpoints = sorted(num_checkpoints)

        # Delete in case the lowest checkpoint files
        if len(num_checkpoints) >= max_checkpoints:
            # Get checkpoints to delete
            if max_checkpoints > 0:
                remove_num_checkpoints = num_checkpoints[:-max_checkpoints]
            # If max_checkpoints is zero (or less), delete everyone
            else:
                remove_num_checkpoints = num_checkpoints
            # Delete checkpoint files
            for ckpt_num in remove_num_checkpoints:
                ckpt_name = os.path.join(ckpt_dir, f'model_{ckpt_num:d}.pt')
                os.remove(ckpt_name)

        return

    def save_config(
        self,
        config: object,
        max_backup: Optional[int] = 1,
        imodel: Optional[int] = None,
    ):
        """"""
        Save config object in current model directory with the default file
        name. If such file already exist, backup the old one and overwrite.

        Parameters
        ----------
        config: object
            Config class object
        max_backup: optional, int, default 100
            Maximum number of backup config files
        imodel: int, optional, default None
            Model index in case of a model ensemble

        """"""

        # Check 'imodel' input in case of a model ensemble
        if self.model_ensemble and imodel is None:
            raise SyntaxError(
                ""Missing model index 'imodel' definition of a model in a ""
                + ""model ensemble."")

        # Model directory and config file path
        if self.model_ensemble:
            model_directory = os.path.join(self.model_directory, f'{imodel:d}')
        else:
            model_directory = self.model_directory
        config_file = os.path.join(
            model_directory, settings._default_args['config_file'])

        # Check for old config files
        if os.path.exists(config_file):

            default_file = settings._default_args['config_file']

            # Check for backup config files
            list_backups = []
            for f in os.listdir(self.model_directory):
                num_backup = re.findall(
                    ""(\d+)_"" + default_file, f)
                num_backup = (int(num_backup[0]) if num_backup else -1)
                if num_backup >= 0:
                    list_backups.append(num_backup)
            list_backups = sorted(list_backups)

            # Rename old config file
            if len(list_backups):
                backup_file = os.path.join(
                    model_directory,
                    f""{list_backups[-1] + 1:d}_"" + default_file)
            else:
                backup_file = os.path.join(
                    model_directory, f""{1:d}_"" + default_file)
            os.rename(config_file, backup_file)

            # If maximum number of back file reached, delete the oldest
            while len(list_backups) >= max_backup:
                backup_file = os.path.join(
                    model_directory,
                    f""{list_backups[0]:d}_"" + default_file)
                os.remove(backup_file)
                del list_backups[0]

        # Dump config in file path
        config.dump(config_file=config_file)

        return

    def get_best_checkpoint(
        self,
        imodel: Optional[int] = None,
        verbose: Optional[bool] = True,
    ) -> Union[str, List[str]]:
        """"""
        Get best model checkpoint file path

        Parameters
        ----------
        imodel: int, optional, default None
            Model index in case of a model ensemble

        Returns
        -------
        (str, list(str))
            Torch module checkpoint file path(s)

        """"""

        # For model ensembles without certain model definition get all
        # best model checkpoint files
        if self.model_ensemble and imodel is None:

            # Get model checkpoint file paths
            ckpt_name = [
                os.path.join(best_dir, 'best_model.pt')
                for best_dir in self.best_dir]

            # Check if best checkpoint file exists or return None
            message = ""Ensemble model best checkpoint files:\n""
            for imdl, ckpt_name_i in enumerate(ckpt_name):
                if os.path.exists(ckpt_name_i):
                    message += (
                        f"" Found '{ckpt_name_i:s}' ""
                        + f""for model {imdl:d}!\n"")
                else:
                    ckpt_name[imdl] = None
                    message += (
                        f"" No file found in {self.best_dir[imdl]:s} ""
                        + f""for model {imdl:d}!\n"")
            message = message[:-1]

            if verbose:
                self.logger.info(message)

        # Get best model checkpoint file
        else:

            # Get model checkpoint file path
            if self.model_ensemble:
                best_dir = self.best_dir[imodel]
            else:
                best_dir = self.best_dir
            ckpt_name = os.path.join(self.best_dir, 'best_model.pt')

            # Check if best checkpoint file exists or return None
            if not os.path.exists(ckpt_name):
                ckpt_name = None
                if verbose:
                    self.logger.info(
                        f""No best checkpoint file found in {best_dir:s}!"")
            elif self.model_ensemble and verbose:
                self.logger.info(
                    f""Checkpoint file '{ckpt_name:s}' found ""
                    + f""for model {imdl:d}!"")
            elif verbose:
                self.logger.info(
                    f""Checkpoint file '{ckpt_name:s}' found."")

        return ckpt_name

    def get_last_checkpoint(
        self,
        imodel: Optional[int] = None,
        verbose: Optional[bool] = True,
    ) -> Union[str, List[str]]:
        """"""
        Get last or specific model checkpoint file path with the highest
        epoch number.

        Parameters
        ----------
        imodel: int, optional, default None
            Model index in case of a model ensemble

        Returns
        -------
        (str, list(str))
            Torch module checkpoint file path(s)

        """"""

        # For model ensembles without certain model definition get all
        # respective last model checkpoint files
        if self.model_ensemble and imodel is None:

            # Get model checkpoint file paths
            ckpt_name = []

            # Get highest index checkpoint file each
            message = ""Latest model checkpoint files:\n""
            for imdl, ckpt_dir in enumerate(self.ckpt_dir):

                # Get highest index checkpoint file
                ckpt_max = -1
                if os.path.exists(ckpt_dir):
                    for ckpt_file in os.listdir(ckpt_dir):
                        ckpt_num = re.findall(""model_(\d+).pt"", ckpt_file)
                        ckpt_num = (int(ckpt_num[0]) if ckpt_num else -1)
                        if ckpt_max < ckpt_num:
                            ckpt_max = ckpt_num

                # If no checkpoint files available return None
                if ckpt_max < 0:
                    ckpt_name.append(None)
                    message += (
                        f"" No file found in {ckpt_dir:s} for model ""
                        + f""{imdl:d}!\n"")
                else:
                    ckpt_name_i = os.path.join(
                        ckpt_dir, f'model_{ckpt_max:d}.pt')
                    ckpt_name.append(ckpt_name_i)
                    message += (
                        f"" Found '{ckpt_name_i:s}' for model {imdl:d}!\n"")
            message = message[:-1]

            if verbose:
                self.logger.info(message)

        # Get last model checkpoint file
        else:

            # Get model checkpoint file path
            if self.model_ensemble:
                ckpt_dir = self.ckpt_dir[imodel]
            else:
                ckpt_dir = self.ckpt_dir

            # Get highest index checkpoint file
            ckpt_max = -1
            if os.path.exists(ckpt_dir):
                for ckpt_file in os.listdir(ckpt_dir):
                    ckpt_num = re.findall(""model_(\d+).pt"", ckpt_file)
                    ckpt_num = (int(ckpt_num[0]) if ckpt_num else -1)
                    if ckpt_max < ckpt_num:
                        ckpt_max = ckpt_num

            # If no checkpoint files available return None
            if ckpt_max < 0:
                ckpt_name = None
                if verbose and self.model_ensemble:
                    self.logger.info(
                        f""No latest checkpoint file found in {ckpt_dir:s} ""
                        + f""for model {imdl:d}!"")
                elif verbose:
                    self.logger.info(
                        f""No latest checkpoint file found in {ckpt_dir:s}!"")
            else:
                ckpt_name = os.path.join(ckpt_dir, f'model_{ckpt_max:d}.pt')
                if verbose and self.model_ensemble:
                    self.logger.info(
                        f""Latest checkpoint file '{ckpt_name}' found ""
                        + f""for model {imdl:d}!"")
                elif verbose:
                    self.logger.info(
                        f""Latest checkpoint file '{ckpt_name}' found"")

        return ckpt_name

    def get_specific_checkpoint(
        self,
        checkpoint_label: Union[int, str],
        imodel: Optional[int] = None,
        verbose: Optional[bool] = True,
    ) -> Union[str, List[str]]:
        """"""
        Get last or specific model checkpoint file path with the highest
        epoch number.

        Parameters
        ----------
        checkpoint_label: (str, int)
            If string and a valid file path, load the respective checkpoint
            file.
            If integer, load the checkpoint file of the respective epoch
            number.
        imodel: int, optional, default None
            Model index in case of a model ensemble

        Returns
        -------
        (str, list(str))
            Torch module checkpoint file path(s)

        """"""

        # Load specific model checkpoint number
        if utils.is_integer(checkpoint_label):

            # For model ensembles without certain model definition check
            # in all model subdirectories
            if self.model_ensemble and imodel is None:

                # Get model checkpoint file paths
                ckpt_name = [
                    os.path.join(ckpt_dir, f'model_{checkpoint_label:d}.pt')
                    for ckpt_dir in self.ckpt_dir]

                # Check existence of each file
                message = """"
                for imdl, ckpt_name_i in enumerate(ckpt_name):
                    if not os.path.exists(ckpt_name_i):
                        ckpt_name[imdl] = None
                        message += (
                            f"" Checkpoint file '{ckpt_name_i:s}' for model ""
                            + f""{imdl:d} not found!\n"")
                    else:
                        message += (
                            f"" Found checkpoint file '{ckpt_name_i:s}' for ""
                            + f""model {imdl:d}!\n"")
                message = message[:-1]

                if np.any(
                    [ckpt_name_i is None for ckpt_name_i in ckpt_name]
                ):
                    message = ""Checkpoint files not found:\n"" + message
                    raise FileNotFoundError(message)
                elif verbose:
                    message = ""Checkpoint files found:\n"" + message
                    self.logger.info(message)

            else:

                # Get model checkpoint file path
                if self.model_ensemble:
                    ckpt_dir = self.ckpt_dir[imodel]
                else:
                    ckpt_dir = self.ckpt_dir
                ckpt_name = os.path.join(
                    ckpt_dir, f'model_{checkpoint_label:d}.pt')

                # Check existence
                if not os.path.exists(ckpt_name):
                    raise FileNotFoundError(
                        f""Checkpoint file '{ckpt_name}' not found!"")
                elif verbose and self.model_ensemble:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found ""
                        + f""for model {imdl:d}!"")
                elif verbose:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found!"")

        # Load specific model checkpoint number for each model
        elif utils.is_integer_array(checkpoint_label):

            # For model ensembles without certain model definition check
            # in all model subdirectories
            if self.model_ensemble and imodel is None:

                # Check checkpoint label number
                if len(checkpoint_label) != self.model_ensemble_num:
                    raise SyntaxError(
                        ""Number of checkpoints in input 'checkpoint_label' ""
                        + f""({len(checkpoint_label):d}) does not match the ""
                        + ""number of models in the model ensemble ""
                        + f""({self.model_ensemble_num:d})."")

                # Get model checkpoint file paths
                ckpt_name = [
                    os.path.join(ckpt_dir, f'model_{ckpt_num:d}.pt')
                    for ckpt_num, ckpt_dir in zip(
                        checkpoint_label, self.ckpt_dir)
                    ]

                # Check existence of each file
                message = """"
                for imdl, ckpt_name_i in enumerate(ckpt_name):
                    if not os.path.exists(ckpt_name_i):
                        ckpt_name[imdl] = None
                        message += (
                            f"" Checkpoint file '{ckpt_name_i:s}' for model ""
                            + f""{imdl:d} not found!\n"")
                    else:
                        message += (
                            f"" Found checkpoint file '{ckpt_name_i:s}' for ""
                            + f""model {imdl:d}!\n"")
                message = message[:-1]

                if np.any(
                    [ckpt_name_i is None for ckpt_name_i in ckpt_name]
                ):
                    message = ""Checkpoint files not found:\n"" + message
                    raise FileNotFoundError(message)
                elif verbose:
                    message = ""Checkpoint files found:\n"" + message
                    self.logger.info(message)

            else:

                # Get model checkpoint file path
                if self.model_ensemble:
                    ckpt_dir = self.ckpt_dir[imodel]
                    ckpt_num = checkpoint_label[imodel]
                else:
                    ckpt_dir = self.ckpt_dir
                    ckpt_num = 0
                ckpt_name = os.path.join(ckpt_dir, f'model_{ckpt_num:d}.pt')

                # Check existence
                if not os.path.exists(ckpt_name):
                    raise FileNotFoundError(
                        f""Checkpoint file '{ckpt_name}' not found!"")
                elif verbose and self.model_ensemble:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found ""
                        + f""for model {imodel:d}!"")
                elif verbose:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found!"")

        # Load specific model checkpoint file
        elif utils.is_string(checkpoint_label):

            # For model ensembles without certain model definition, the single
            # checkpoint label is applied to all models
            if self.model_ensemble and imodel is None:

                # Get model checkpoint file paths
                ckpt_name = [
                    checkpoint_label for _ in range(self.model_ensemble_num)]

                # Check existence of each file
                message = """"
                for imdl, ckpt_name_i in enumerate(ckpt_name):
                    if not os.path.exists(ckpt_name_i):
                        ckpt_name[imdl] = None
                        message += (
                            f"" Checkpoint file '{ckpt_name_i:s}' for model ""
                            + f""{imdl:d} not found!\n"")
                    else:
                        message += (
                            f"" Found checkpoint file '{ckpt_name_i:s}' for ""
                            + f""model {imdl:d}!\n"")
                message = message[:-1]

                if np.any(
                    [ckpt_name_i is None for ckpt_name_i in ckpt_name]
                ):
                    message = ""Checkpoint files not found:\n"" + message
                    raise FileNotFoundError(message)
                elif verbose:
                    message = ""Checkpoint files found:\n"" + message
                    self.logger.info(message)

            # Check for checkpoint file
            else:

                # Get model checkpoint file path
                ckpt_name = checkpoint_label

                # Check existence
                if not os.path.exists(ckpt_name):
                    raise FileNotFoundError(
                        f""Checkpoint file '{ckpt_name}' not found!"")
                elif verbose and self.model_ensemble:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found ""
                        + f""for model {imodel:d}!"")
                elif verbose:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found!"")

        # Load specific model checkpoint files
        elif utils.is_string_array(checkpoint_label):

            # For model ensembles without certain model definition check
            # in all model subdirectories
            if self.model_ensemble and imodel is None:

                # Check checkpoint label number
                if len(checkpoint_label) != self.model_ensemble_num:
                    raise SyntaxError(
                        ""Number of checkpoints in input 'checkpoint_label' ""
                        + f""({len(checkpoint_label):d}) does not match the ""
                        + ""number of models in the model ensemble ""
                        + f""({self.model_ensemble_num:d})."")

                # Get model checkpoint file paths
                ckpt_name = checkpoint_label

                # Check existence of each file
                message = """"
                for imdl, ckpt_name_i in enumerate(ckpt_name):
                    if not os.path.exists(ckpt_name_i):
                        ckpt_name[imdl] = None
                        message += (
                            f"" Checkpoint file '{ckpt_name_i:s}' for model ""
                            + f""{imdl:d} not found!\n"")
                    else:
                        message += (
                            f"" Found checkpoint file '{ckpt_name_i:s}' for ""
                            + f""model {imdl:d}!\n"")
                message = message[:-1]

                if np.any(
                    [ckpt_name_i is None for ckpt_name_i in ckpt_name]
                ):
                    message = ""Checkpoint files not found:\n"" + message
                    raise FileNotFoundError(message)
                elif verbose:
                    message = ""Checkpoint files found:\n"" + message
                    self.logger.info(message)

            else:

                # Get model checkpoint file path
                ckpt_name = checkpoint_label[0]

                # Check existence
                if not os.path.exists(ckpt_name):
                    raise FileNotFoundError(
                        f""Checkpoint file '{ckpt_name}' not found!"")
                elif verbose and self.model_ensemble:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found ""
                        + f""for model {imodel:d}!"")
                elif verbose:
                    self.logger.info(
                        f""Checkpoint file '{ckpt_name}' found!"")

        else:

            raise SyntaxError(
                ""Input for the model checkpoint label 'checkpoint_label' ""
                + ""is not a valid data type!"")

        return ckpt_name",./Asparagus/asparagus/model/filemanager.py
Configuration,"class Configuration():
    """"""
    Global configuration object that contains all parameter about the
    model and training procedure.

    Parameters
    ----------
    config: (str, dict), optional, default None
        Either the path to json file (str) or dictionary (dict) containing
        global model parameters
    config_file: str, optional, default see settings.default['config_file']
        Store global parameter configuration in json file of given path.
    config_from: (str, Configuration), optional, default None
        Location, defined as class instance or string, from where the new
        configuration parameter dictionary comes from.
    kwargs: dict, optional, default {}
        Keyword arguments for configuration parameter which are added to
        'config' or overwrite 'config' content.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    def __init__(
        self,
        config: Optional[Union[str, dict]] = None,
        config_file: Optional[str] = None,
        config_from: Optional[Union[str, 'Configuration']] = None,
        verbose: Optional[bool] = True,
        **kwargs,
    ):
        """"""
        Initialize config object.
        """"""

        # Initialize class config dictionary
        self.config_dict = {}
        self.config_indent = 2

        # Check configuration source input
        # Case 1: Neither defined - Get config dict at default file path
        if config is None and config_file is None:
            self.config_file = settings._default_args.get('config_file')
            self.config_dict = self.read(self.config_file)
        # Case 2: Only config file is defined - Read config dict from file path
        elif config is None:
            if utils.is_string(config_file):
                self.config_file = config_file
                self.config_dict = self.read(self.config_file)
            else:
                raise SyntaxError(
                    ""Config input 'config_file' is not a string of ""
                    + ""a valid file path!"")
        # Case 3: Only config is defined - Check type
        elif config_file is None:
            # If config is actually a file path - use as file path like case 2
            if utils.is_string(config):
                self.config_file = config
                self.config_dict = self.read(self.config_file)
            # If config is a dictionary - assign and read file path
            elif utils.is_dictionary(config):
                self.config_dict = config
                if self.config_dict.get('config_file') is None:
                    config_file = settings._default_args.get('config_file')
                else:
                    config_file = self.config_dict.get('config_file')
                self.config_file = config_file
            else:
                raise SyntaxError(
                    ""Config input 'config' is neither a dictionary nor a ""
                    + ""string of a  valid file path!"")
        # Case 4: Both are defined - Update config dict from config file with
        # config input
        else:
            # Read config dict from file path and assign as self.config_dict
            if utils.is_string(config_file):
                self.config_file = config_file
                self.config_dict = self.read(self.config_file)
            else:
                raise SyntaxError(
                    ""Config input 'config_file' is not a string of ""
                    + ""a valid file path!"")
            # If config is actually a file path - read config dict and update
            # self.config_dict
            if utils.is_string(config):
                self.update(
                    self.read(config),
                    verbose=verbose)
            # If config is a dictionary - update self.config_dict
            elif utils.is_dictionary(config):
                self.update(
                    config,
                    verbose=verbose)
            else:
                raise SyntaxError(
                    ""Config input 'config' is neither a dictionary nor a ""
                    + ""string of a  valid file path!"")
        # In all cases, config_dict and config_file are class variables and
        # defined as dictionary and string.

        # Set config file path to dictionary
        self.set_config_file(
            self.config_file,
            verbose=verbose)

        # Update configuration dictionary with keyword arguments
        if len(kwargs):
            self.update(
                kwargs,
                config_from=config_from,
                verbose=verbose,
                )

        # Save current configuration dictionary to file
        self.dump()

        # Adopt default settings arguments and their valid dtypes
        self.default_args = settings._default_args
        self.dtypes_args = settings._dtypes_args

    def __str__(self) -> str:
        msg = f""Config file in '{self.config_file:s}':\n""
        for arg, item in self.config_dict.items():
            msg += f""  '{arg:s}': {str(item):s}\n""
        return msg

    def __getitem__(self, args: str) -> Any:
        return self.config_dict.get(args)

    def __setitem__(self, arg: str, item: Any):
        self.config_dict[arg] = item
        self.dump()
        return

    def __contains__(self, arg: str) -> bool:
        return arg in self.config_dict.keys()

    def __call__(self, args: str) -> Any:
        return self.config_dict.get(args)

    def items(self) -> (str, Any):
        for key, item in self.config_dict.items():
            yield key, item

    def get(self, args: Union[str, List[str]]) -> Union[Any, List[Any]]:
        if utils.is_array_like(args):
            return [self.config_dict.get(arg) for arg in args]
        else:
            return self.config_dict.get(args)

    def keys(self) -> List[str]:
        return self.config_dict.keys()

    def set_config_file(
        self,
        config_file: str,
        verbose: Optional[bool] = True,
    ):

        # Check input
        if utils.is_string(config_file):
            self.config_file = config_file
        else:
            raise SyntaxError(
                ""Config input 'config_file' is not a string of ""
                + ""a valid file path!"")

        # Set config file path to dictionary
        if self.config_dict.get('config_file') is None:
            if verbose:
                self.logger.info(
                    ""Configuration file path set to ""
                    + f""'{self.config_file:s}'!"")
            self.config_dict['config_file'] = self.config_file
        else:
            if self.config_dict.get('config_file') != self.config_file:
                if verbose:
                    self.logger.info(
                        ""Configuration file path will be changed from ""
                        + f""'{self.config_dict.get('config_file'):s}' to ""
                        + f""'{self.config_file:s}'!"")
                self.config_dict['config_file'] = self.config_file

        # Generate, eventually, the directory for the config file
        config_dir = os.path.dirname(self.config_file)
        if not os.path.isdir(config_dir) and len(config_dir):
            os.makedirs(os.path.dirname(self.config_file))

        return

    def read(
        self,
        config_file: str,
    ) -> Dict[str, Any]:

        # Read json file
        if os.path.exists(config_file):
            with open(config_file, 'r') as f:
                config_dict = json.load(f)
        else:
            config_dict = {}

        # Check for convertible parameter keys and convert
        for key, item in config_dict.items():
            if self.is_convertible(key):
                config_dict[key] = self.convert(key, item, 'read')

        return config_dict

    def update(
        self,
        config_new: Union[str, dict, object],
        config_from: Optional[Union[object, str]] = None,
        overwrite: Optional[bool] = True,
        verbose: Optional[bool] = True,
    ):
        """"""
        Update configuration dictionary.

        Parameters
        ----------

        config_new: (str, dict, object)
            Either the path to json file (str), dictionary (dict) or
            configuration object of the same class (object) containing
            new model parameters.
        config_from: (object, str), optional, default None
            Location, defined as class instance or string, from where the new
            configuration parameter dictionary comes from.
        overwrite: bool, optional, default True
            If True, 'config_new' input will be added and eventually overwrite
            existing entries in the configuration dictionary.
            If False, each input in 'config_new' will be only added if it is
            not already defined in the configuration dictionary.
        verbose: bool, optional, default True
            For conflicting entries between 'config_new' and current
            configuration dictionary, return further information.

        """"""

        # Check config_new input
        if utils.is_string(config_new):
            config_new = self.read(config_new)
        elif utils.is_dictionary(config_new):
            pass
        elif utils.is_callable(config_new):
            config_new = config_new.get_dictionary()
        else:
            raise ValueError(
                ""Input 'config_new' is not of valid data type!\n"" +
                ""Data type 'dict', 'str' or a config class object "" +
                f""is expected but '{type(config_new)}' is given."")

        # Return if update dictionary is empty
        if not len(config_new):
            self.logger.debug(""Empty update configuration dictionary!"")
            return

        # Show update information
        msg = (
            f""Parameter update in '{self.config_file}'\n"")
        if config_from is not None:
            msg += f""  (called from '{config_from}')\n""
        if overwrite:
            msg += ""  (overwrite conflicts)\n""
        else:
            msg += ""  (ignore conflicts)\n""

        # Iterate over new configuration dictionary
        n_all, n_add, n_equal, n_overwrite = 0, 0, 0, 0
        for key, item in config_new.items():

            # Skip if parameter value is None
            if item is None:
                continue
            else:
                n_all += 1

            # Check for conflicting keyword
            conflict = key in self.config_dict.keys()

            # For conflicts, check for changed parameter
            if conflict:
                equal = str(item) == str(self.config_dict[key])
                if equal:
                    n_equal += 1

            # Add or update parameter
            if conflict and overwrite and not equal:

                if self.is_convertible(key):
                    self.config_dict[key] = self.convert(
                        key, config_new.get(key), 'read')
                else:
                    self.config_dict[key] = config_new.get(key)
                n_overwrite += 1
                if verbose:
                    msg += f""Overwrite parameter '{key}'.\n""

            elif conflict and not equal:

                if verbose:
                    msg += f""Ignore parameter '{key}'.\n""

            elif not conflict:

                if self.is_convertible(key):
                    self.config_dict[key] = self.convert(
                        key, config_new.get(key), 'read')
                else:
                    self.config_dict[key] = config_new.get(key)
                n_add += 1
                if verbose:
                    msg += f""Adding parameter '{key}'.\n""

        # Add numbers
        msg += (
            f""{n_all:d} new parameter: {n_add:d} added, ""
            + f""{n_equal:d} equal, {n_overwrite:d} overwritten"")
        # Show additional information output
        if verbose:
            self.logger.debug(msg)

        # Store changes in file
        self.dump()

        return

    def dump(
        self,
        config_file: Optional[str] = None,
    ):
        """"""
        Save configuration dictionary to json file

        Parameters
        ----------
        config_file: str, optional, default None
            Dump current config dictionary in this file path.

        """"""

        # Convert config dictionary to json compatible dictionary
        config_dump = self.make_dumpable(self.config_dict)

        # Check config file
        if config_file is None:
            config_file = self.config_file

        # Generate, eventually, the directory for the config file
        config_dir = os.path.dirname(config_file)
        if not os.path.isdir(config_dir) and len(config_dir):
            os.makedirs(os.path.dirname(config_file))

        # Dumb converted config dictionary
        with open(config_file, 'w') as f:
            json.dump(
                config_dump, f,
                indent=self.config_indent,
                default=str)

        return

    def make_dumpable(
        self,
        config_source: Dict[str, Any],
    ) -> Dict[str, Any]:
        """"""
        Convert config items to json compatible dictionary
        """"""

        # Initialize dictionary with JSON compatible parameter types
        config_dump = {}

        # Iterate over configuration parameters
        for key, item in config_source.items():

            # Skip callable class objects
            if utils.is_callable(item):
                continue

            # Convert numeric values to integer or float
            if utils.is_integer(item):
                config_dump[key] = int(item)
            elif utils.is_numeric(item):
                config_dump[key] = float(item)
            # Also store dictionaries,
            elif utils.is_dictionary(item):
                config_dump[key] = self.make_dumpable(item)
            # strings or bools
            elif utils.is_string(item) or utils.is_bool(item):
                config_dump[key] = item
            # and converted arrays as python lists,
            # but nothing else which might be to fancy
            elif utils.is_array_like(item):
                config_dump[key] = list(item)
            elif self.is_convertible(key):
                config_dump[key] = self.convert(key, item, 'dump')
            else:
                continue

        return config_dump

    def check(
        self,
        check_default: Optional[Dict] = None,
        check_dtype: Optional[Dict] = None,
    ):
        """"""
        Check configuration parameter for correct data type and, eventually,
        set default values for parameters with entry None.

        Parameters
        ----------
        check_default: dict, optional, default None
            Default argument parameter dictionary.
        check_dtype: dict, optional, default None
            Default argument data type dictionary.

        """"""

        for arg, item in self.config_dict.items():

            # Check if input parameter is None, if so take default value
            if check_default is not None and item is None:
                if arg in check_default:
                    item = check_default[arg]
                    self[arg] = item

            # Check datatype of defined arguments
            if check_dtype is not None and arg in check_dtype:
                _ = utils.check_input_dtype(
                    arg, item, check_dtype, raise_error=True)

        # Save successfully checked configuration
        self.dump()

        return

    def set(
        self,
        instance: Optional[object] = None,
        argitems: Optional[Iterator] = None,
        argsskip: Optional[List[str]] = None,
        check_default: Optional[Dict] = None,
        check_dtype: Optional[Dict] = None,
    ) -> Dict[str, Any]:
        """"""
        Iterate over arg, item pair, eventually check for default and dtype,
        and set as class variable of instance

        Parameters
        ----------
        instance: object, optional, default None
            Class instance to set arg, item pair as class variable. If None,
            skip.
        argitems: iterator, optional, default None
            Iterator for arg, item pairs. If None, skip.
        argskipt: list(str), optional, default None
            List of arguments to skip.
        check_default: dict, optional, default None
            Default argument parameter dictionary.
        check_dtype: dict, optional, default None
            Default argument data type dictionary.

        Returns
        -------
        dict[str, any]
            Updated config dictionary

        """"""

        # Return empty dictionary if no arg, item pair iterator is defined
        if argitems is None:
            return {}
        else:
            config_dict_update = {}

        # Check arguments to skip
        default_argsskip = [
            'self', 'config', 'config_file', 'logger', 'verbose', 'kwargs',
            '__class__']
        if argsskip is None:
            argsskip = default_argsskip
        else:
            argsskip = default_argsskip + list(argsskip)
        argsskip.append('default_args')

        # Iterate over arg, item pairs
        for arg, item in argitems.items():

            # Skip exceptions
            if arg in argsskip:
                continue

            # If item is None, take from class config
            if item is None:
                item = self.get(arg)

            # Check if input parameter is None, if so take default value
            if check_default is not None and item is None:
                if arg in check_default:
                    item = check_default[arg]

            # Check datatype of defined arguments
            if check_dtype is not None and arg in check_dtype:
                _ = utils.check_input_dtype(
                    arg, item, check_dtype, raise_error=True)

            # Append arg, item pair to update dictionary
            config_dict_update[arg] = item

            # Set item as class parameter arg to instance
            if instance is not None:
                setattr(instance, arg, item)

        return config_dict_update

    def get_file_path(self):
        return self.config_file

    def get_dictionary(self):
        return self.config_dict

    def conversion_dict(self):
        """"""
        Generate conversion dictionary.
        """"""

        self.convertible_dict = {
            'dtype': self.convert_dtype
            }

        return

    def is_convertible(self, key: str) -> bool:
        """"""
        Check if parameter 'key' is in the convertible dictionary.

        Parameters
        ----------
        key: str
            Parameter name

        Returns
        -------
        bool
            Flag if item is convertible and included in the dictionary of
            converted items.

        """"""

        # Check if convertible dictionary is already initialized
        if not hasattr(self, 'convertible_dict'):
            self.conversion_dict()

        # Look for parameter in conversion dictionary
        return key in self.convertible_dict

    def convert(
        self,
        key: str,
        arg: Any,
        operation: str,
    ) -> Any:
        """"""
        Convert argument 'arg' of parameter 'key' between json compatible
        format and internal type.

        Parameters
        ----------
        key: str
            Parameter name
        arg: Any
            Parameter value
        operation: str
            Convert direction such as 'dump' (internal -> json) or 'read'
            (json -> internal).

        Returns
        -------
        Any
            Converted item into a json dumpable format.

        """"""

        # Check if convertible dictionary is already initialized
        if hasattr(self, 'convertible_dict'):
            self.conversion_dict()

        # Provide conversion result
        return self.convertible_dict[key](arg, operation)

    def convert_dtype(
        self,
        arg: Any,
        operation: str
    ):
        """"""
        Convert data type to data label

        Parameters
        ----------
        arg: Any
            Parameter value of dtype
        operation: str
            Convert direction such as 'dump' (internal -> json) or 'read'
            (json -> internal).

        Returns
        -------
        Any
            Either converted dtype string into dtype object ('read') or vice
            versa ('dump').

        """"""
        if operation == 'dump':
            for dlabel, dtype in settings._dtype_library.items():
                if arg is dtype:
                    return dlabel
        elif operation == 'read':
            for dlabel, dtype in settings._dtype_library.items():
                if arg == dlabel:
                    return dtype
        return None",./Asparagus/asparagus/settings/config.py
MDSampler,"class MDSampler(sampling.Sampler):
    """"""
    Molecular Dynamics Sampler class

    Parameters
    ----------
    config: (str, dict, settings.Configuration)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    md_temperature: (float, dict(str, float)), optional, default 300
        Target temperature in Kelvin of the MD simulation controlled by a
        Langevin thermostat.
        Can be provided as dictionary for varying temperatures during the run.
        Expected dictionary keys and items are (only first four characters
        of the keys are considered and case insensitive):
            'Tstart': float     - Initial temperature in Kelvin
            'Tend': float       - Final temperature in Kelvin
            'Tincrement': float - Temperature step in K
            'Tinterval': float  - Time interval for temperature increase in fs
    md_time_step: float, optional, default 1.0 (1 fs)
        MD Simulation time step in fs
    md_simulation_time: float, optional, default 1E5 (100 ps)
        Total MD Simulation time in fs
    md_save_interval: int, optional, default 10
        MD Simulation step interval to store system properties of
        the current frame to dataset.
    md_langevin_friction: float, optional, default 0.01
        Langevin thermostat friction coefficient in Kelvin. Generally
        within the magnitude of 1E-2 (fast heating/cooling) to 1E-4 (slow)
    md_equilibration_time: float, optional, default 0 (no equilibration)
        Total MD Simulation time in fs for a equilibration run prior to
        the production run.
    md_initial_temperature: (float, bool), optional, default False
        Temperature for initial atom velocities according to a Maxwell-
        Boltzmann distribution.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for sample module
    sampling.Sampler._default_args.update({
        'md_temperature':               300.,
        'md_time_step':                 1.,
        'md_simulation_time':           1.E5,
        'md_save_interval':             10,
        'md_langevin_friction':         1.E-2,
        'md_equilibration_time':        None,
        'md_initial_temperature':       False,
        })
    
    # Expected data types of input variables
    sampling.Sampler._dtypes_args.update({
        'md_temperature':               [
            utils.is_numeric, utils.is_dictionary],
        'md_time_step':                 [utils.is_numeric],
        'md_simulation_time':           [utils.is_numeric],
        'md_save_interval':             [utils.is_integer],
        'md_langevin_friction':         [utils.is_numeric],
        'md_equilibration_time':        [utils.is_numeric],
        'md_initial_temperature':       [utils.is_numeric, utils.is_bool],
        })

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None, 
        md_temperature: Optional[Union[float, Dict[str, float]]] = None,
        md_time_step: Optional[float] = None,
        md_simulation_time: Optional[float] = None,
        md_save_interval: Optional[float] = None,
        md_langevin_friction: Optional[float] = None,
        md_equilibration_time: Optional[float] = None,
        md_initial_temperature: Optional[Union[float, bool]] = None,
        **kwargs,
    ):
        """"""
        Initialize MD sampling class

        """"""
        
        # Sampler class label
        self.sample_tag = 'md'

        # Initialize parent class
        super().__init__(
            sample_tag=self.sample_tag,
            config=config,
            config_file=config_file,
            **kwargs
            )

        ################################
        # # # Check MD Class Input # # #
        ################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)
        
        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, sampling),
            check_dtype=utils.get_dtype_args(self, sampling)
        )

        # Check sample properties for energy and forces properties which are 
        # required for MD sampling
        if 'energy' not in self.sample_properties:
            self.sample_properties.append('energy')
        if 'forces' not in self.sample_properties:
            self.sample_properties.append('forces')

        return
    
    def get_info(self):
        """"""
        Returns a dictionary with the information of the MD sampler.

        Returns
        -------
        dict
            Dictionary with the information of the MD sampler.
        """"""
        
        info = super().get_info()
        info.update({
            'md_temperature': self.md_temperature,        
            'md_time_step': self.md_time_step,
            'md_simulation_time': self.md_simulation_time,
            'md_save_interval': self.md_save_interval,
            'md_langevin_friction': self.md_langevin_friction,
            'md_equilibration_time': self.md_equilibration_time,
            'md_initial_temperature': self.md_initial_temperature,
            })
        
        return info

    def run_systems(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        **kwargs,
    ):
        """"""
        Perform Molecular Dynamics simulations on the sample system.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue, optional, default None
            Queue object including sample systems or to which 'sample_systems' 
            input will be added. If not defined, an empty queue will be 
            assigned.
        """"""

        # Check sample system queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue()

        # Initialize auxiliary array
        self.first_increment = [True]*sample_systems_queue.qsize()

        # Initialize thread continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )
        
        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_systems_queue.put('stop')

        # Initialize sample number list
        self.Nsamples = [0 for ithread in range(self.sample_num_threads)]

        if self.sample_num_threads == 1:
            
            self.run_system(sample_systems_queue)
        
        else:

            # Create threads
            threads = [
                threading.Thread(
                    target=self.run_system, 
                    args=(sample_systems_queue, ),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()
  
        return

    def run_system(
        self, 
        sample_systems_queue: queue.Queue,
        ithread: Optional[int] = None,
    ):
        """"""
        Perform MD Simulation with the sample system.

        Parameters
        ----------
        sample_systems_queue: queue.Queue
            Queue of sample system information providing tuples of ASE atoms
            objects, index number and respective sample source and the total
            sample index.
        ithread: int, optional, default None
            Thread number

        """"""

        while self.keep_going(ithread):
            
            # Get sample parameters or wait
            sample = sample_systems_queue.get()
            
            # Check for stop flag
            if sample == 'stop':
                if ithread is None:
                    self.thread_keep_going[0] = False
                else:
                    self.thread_keep_going[ithread] = False
                continue
            
            # Extract sample system to optimize
            (system, isample, source, index) = sample

            # If requested, perform structure optimization
            if self.sample_systems_optimize:

                # Perform structure optimization
                system = self.run_optimization(
                    sample_system=system,
                    sample_index=isample,
                    ithread=ithread)

            # Initialize log file
            sample_log_file = self.sample_log_file.format(isample)
            
            # Initialize trajectory file
            if self.sample_save_trajectory:
                trajectory_file = self.sample_trajectory_file.format(isample)
            else:
                trajectory_file = None

            # Assign calculator
            system = self.assign_calculator(
                system,
                ithread=ithread)

            # Perform MD simulation
            self.run_langevin_md(
                system,
                log_file=sample_log_file,
                trajectory_file=trajectory_file,
                ithread=ithread)
            
            # Print sampling info
            if ithread is None:
                isample = 0
            else:
                isample = ithread
            message = (
                f""Sampling method '{self.sample_tag:s}' complete for system ""
                + f""of index {index:d} from '{source}!'\n"")
            if self.Nsamples[isample] == 0:
                message += f""No samples written to ""
            if self.Nsamples[isample] == 1:
                message += f""{self.Nsamples[isample]:d} sample written to ""
            else:
                message += f""{self.Nsamples[isample]:d} samples written to ""
            message += f""'{self.sample_data_file[0]:s}'.""
            self.logger.info(message)

        return

    def run_langevin_md(
        self,
        system: ase.Atoms,
        temperature: Optional[Union[float, Dict[str, float]]] = None,
        time_step: Optional[float] = None,
        simulation_time: Optional[float] = None,
        langevin_friction: Optional[float] = None,
        equilibration_time: Optional[float] = None,
        initial_velocities: Optional[bool] = None,
        initial_temperature: Optional[Union[float, bool]] = None,
        log_file: Optional[str] = None,
        trajectory_file: Optional[str] = None,
        ithread: Optional[int] = None,
    ):
        """"""
        This does a Molecular Dynamics simulation using Langevin thermostat
        and verlocity Verlet algorithm for an NVT ensemble.

        In the future we could add more sophisticated sampling methods
        (e.g. MALA or HMC)

        Parameters
        ----------
        system: ase.Atoms
            System to be sampled.
        temperature: (float, dict(str, float)), optional, default None
            MD Simulation temperature in Kelvin
            Can be provided as dictionary for varying temperatures during the
            run. Expected dictionary keys and items are (only first four
            characters of the keys are considered and case insensitive):
                'Tstart': float     - Initial temperature in Kelvin
                'Tend': float       - Final temperature in Kelvin
                'Tincrement': float - Temperature step in K
                'Tinterval': float  - Time interval for temperature increase in
                                      fs
        time_step: float, optional, default None
            MD Simulation time step in fs
        simulation_time: float, optional, default None
            Total MD Simulation time in fs
        langevin_friction: float, optional, default None
            Langevin thermostat friction coefficient in Kelvin.
        equilibration_time: float, optional, default None
            Total MD Simulation time in fs for a equilibration run prior to
            the production run.
        initial_temperature: (float, bool), optional, default None
            Temperature for initial atom velocities according to a Maxwell-
            Boltzmann distribution.
        log_file: str, optional, default None
            Log file for sampling information
        trajectory_file: str, optional, default None
            ASE Trajectory file path to append sampled system if requested
        ithread: int, optional, default None
            Thread number

        """"""

        # Check input parameters
        if temperature is None:
            temperature = self.md_temperature
        if time_step is None:
            time_step = self.md_time_step
        if simulation_time is None:
            simulation_time = self.md_simulation_time
        if langevin_friction is None:
            langevin_friction = self.md_langevin_friction
        if equilibration_time is None:
            equilibration_time = self.md_equilibration_time
        if initial_temperature is None:
            initial_temperature = self.md_initial_temperature

        # Check temperature input
        temperature, temperature_program = self.check_temperature(
            temperature,
            simulation_time)

        # Set initial atom velocities if requested
        if utils.is_bool(initial_temperature):
            if initial_temperature:
                MaxwellBoltzmannDistribution(
                    system,
                    temperature_K=temperature)
        elif initial_temperature > 0.:
            MaxwellBoltzmannDistribution(
                system, 
                temperature_K=initial_temperature)
        
        # Initialize MD simulation propagator
        md_dyn = Langevin(
            system, 
            timestep=time_step*units.fs,
            temperature_K=temperature,
            friction=langevin_friction,
            logfile=log_file,
            loginterval=self.md_save_interval)

        # Perform MD equilibration simulation if requested
        if equilibration_time is not None and equilibration_time > 0.:
            
            # Run equilibration simulation
            equilibration_step = round(equilibration_time/time_step)
            md_dyn.run(equilibration_step)
            
        # Attach system properties saving function
        md_dyn.attach(
            self.save_properties,
            interval=self.md_save_interval,
            system=system,
            ithread=ithread)

        # Attach trajectory writer
        if self.sample_save_trajectory:
            md_dyn.attach(
                self.write_trajectory, 
                interval=self.md_save_interval,
                system=system,
                trajectory_file=trajectory_file)

        # Attach sample number filter
        if self.sample_nsamples_threshold is not None:
            md_dyn.attach(
                self.check_sample_number,
                interval=self.md_save_interval,
                dyn=md_dyn)

        # If defined, attach temperature program function to change reference
        # temperature by an temperature increment every specified time interval
        if temperature_program:
            temperature_interval = int(temperature_program['tint']/time_step)
            if ithread is None:
                ithread = 0
            md_dyn.attach(
                self.update_temperature,
                interval=temperature_interval,
                dyn=md_dyn,
                increment=temperature_program['tinc'],
                ithread=ithread)

        # Run MD simulation
        simulation_steps = round(simulation_time/time_step)
        md_dyn.run(simulation_steps)

        return

    def check_temperature(
        self,
        temperature: Union[float, Dict[str, float]],
        simulation_time: float,
    ) -> (float, Dict[str, float]):
        """"""
        Check temperature parameter input and, eventually, prepare temperature
        program parameters

        Parameters
        ----------
        temperature: (float, dict(str, float))
            MD Simulation temperature in Kelvin
        simulation_time: float
            Total MD Simulation time in fs

        Returns
        -------
        float
            MD Simulation temperature in Kelvin
        dict(str, float)
            Temperature program parameter to change the reference temperature
            during the MD simulation. If empty, reference temperature stays
            constant

        """"""

        # If temperature is numeric - constant reference temperature
        if utils.is_numeric(temperature):

            return temperature, {}

        elif utils.is_dictionary(temperature):

            # Reduce dictionary keys to 4 letter codes
            temperature_short = {}
            for key, item in temperature.items():
                key_short = f""{key.lower():<4s}""[:4]
                temperature_short[key_short] = float(item)

            # Check temperature program parameters
            # Essential starting temperature
            if 'tsta' in temperature_short:
                tstart = temperature_short['tsta']
            else:
                raise SyntaxError(
                    ""Initial MD simulation temperature 'tstart 'is not ""
                    + ""defined!"")
            # Optional final temperature if increment and interval is given
            if 'tend' in temperature_short:
                tend = temperature_short['tend']
            elif not 'tint' in temperature_short:
                raise SyntaxError(
                    ""Final MD simulation temperature 'tend 'is not ""
                    + ""defined!"")
            else:
                tend = None
            # Optional temperature increment if final temperature is given
            if 'tinc' in temperature_short:
                tincrement = temperature_short['tinc']
            elif (
                'tend' in temperature_short
                and 'tint' in temperature_short
            ):
                tsteps = int(simulation_time/temperature_short['tint']) - 1
                tincrement = (tstart - tend)/tsteps
            else:
                tincrement = 10.0
            # Optional temperature increment interval
            if 'tint' in temperature_short:
                tinterval = temperature_short['tint']
            elif 'tend' in temperature_short:
                tsteps = int((tstart - tend)/tincrement) + 1
                tinterval = simulation_time/tsteps
            else:
                raise SyntaxError(
                    ""Interval for MD simulation temperature increase 'tint' ""
                    + ""is not defined!"")

            # Complete temperature program dictionary
            if tend is None:
                tsteps = int(simulation_time/tinterval) - 1
                tend = tstart + tsteps*tincrement
            temperature_program = {
                'tsta': tstart,
                'tend': tend,
                'tinc': tincrement,
                'tint': tinterval,
                }

            return tstart, temperature_program

        else:

            raise SyntaxError(
                ""MD simulation temperature input is invalid!"")

        return

    def update_temperature(
        self,
        dyn: Langevin,
        increment: float,
        ithread: int,
    ):
        """"""
        Apply Langevin reference temperature step

        Parameters
        ----------
        dyn: ase.md.langevin.Langevin
            ASE Langevin dynamics instance
        increment: float
            Temperature increment
        ithread: int
            Thread number

        """"""

        if self.first_increment[ithread]:
            self.first_increment[ithread] = False
            return

        dyn.temp = dyn.temp + units.kB*increment
        dyn.updatevars()

        return

    def check_sample_number(
        self,
        dyn: Langevin,
    ):
        """"""
        If number of samples written to the database reach the threshold,
        manipulate ASE dynamics instance to terminate the run.

        Parameters
        ----------
        dyn: ase.md.langevin.Langevin
            ASE Langevin dynamics instance

        """"""
        # Check if number of sample threshold is reached
        if (
            self.sample_nsamples_threshold is not None
            and np.sum(self.Nsamples) >= self.sample_nsamples_threshold
        ):
            dyn.max_steps = dyn.nsteps

        return",./Asparagus/asparagus/sampling/md.py
Sampler,"class Sampler:
    """"""
    Conformation Sampler main class for generation of reference structures.

    Parameters
    ----------
    config: (str, dict, settings.Configuration)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    sample_data_file: str, optional, default None
        Database file name to store a selected set of systems with
        computed reference data. If None, data file name is the respective
        sample method tag.
    sample_directory: str, optional, default None
        Working directory where to store eventually temporary ASE
        calculator files, ASE trajectory files and/or model calculator
        files. If None, files will be stored in parent directory.
    sample_system_queue: queue.Queue, optional, default None
        Queue object including sample systems or where 'sample_systems' input
        will be added. If not defined, an empty queue will be assigned.
    sample_systems: (str, list, object), optional, default ''
        System coordinate file or a list of system coordinate files or
        ASE atoms objects that are considered as initial conformations for
        reference structure sampling.
    sample_systems_format: (str, list), optional, default ''
        System coordinate file format string (e.g. 'xyz') for the
        definition in 'sample_systems' in case of file paths.
    sample_systems_indices: (int, list), optional, default None
        List of sample number indices for specific selection of systems
        in the sample system files.
    sample_system_fragments: (list, dict), optional, default None
        System specific fragment definition to assign system atoms to 
        difference fragments which are, e.g., compute at different level of
        theory or get treated differently by the model potential.
        It can be either a list containing lists of atom indices assigned to
        fragment 0, 1, ..., or a dictionary of fragment name (key) and atom
        indices (item).
        The atom indices can be given as an integer atom index or a string of
        a single index (e.g. '0') or an index range (e.g. '0-10').
        Not defined atoms are always added to a new fragment of the last index
        plus 1.
        If multiple sample systems are defined, the system fragment definition
        is applied on each system.
    sample_calculator: (str, callable object), optional, default 'XTB'
        Definition of the ASE calculator type for reference data
        computation. The input can be either directly a ASE calculator
        class object or a string with available ASE calculator classes.
    sample_calculator_args: dict, optional, default {}
        In case of string type input for 'sample_calculator', this
        dictionary is passed as keyword arguments at the initialization
        of the ASE calculator.
    sample_save_trajectory: bool, optional, default True
        If True, add sampled systems added to the database file also to an
        ASE trajectory file.
    sample_num_threads: int, optional, default 1
        Number of parallel threads of property calculation using the sample
        calculator. Default is 1 (serial computation). Parallel computation
        is not possible for all sampling methods but for:
            Sampler, NMSampler, NMScanner
    sample_properties: List[str], optional, default None
        List of system properties which are computed by the ASE
        calculator class. Requested properties will be checked with the
        calculator available property list and return an error when one
        requested property is unavailable. By default all available
        properties will be stored.
        If None, compute all properties implemented in the ASE calculator.
    sample_systems_optimize: bool, optional, default False
        Instruction flag if the system coordinates shall be
        optimized using the ASE calculator defined by 'sample_calculator'.
    sample_systems_optimize_fmax: float, optional, default 0.01
        Instruction flag, if the system coordinates shall be
        optimized using the ASE calculator defined by 'sample_calculator'.
    sample_data_overwrite: bool, optional, default False
        If False, add new sampling data to an eventually existing data
        file. If True, overwrite an existing one.
    sample_property_threshold: dict, optional, default None
        Dictionary of model property (key) minimum threshold values (item) to
        decide if samples or frames are added to the sample data file.
        For model ensembles, this could be the energy standard deviation
        ('str_energy').
    sample_nsamples_threshold: int, optional, default None
        If not None, the sampling will be terminated if the sample number
        threshold is reached.
    sample_tag: str, optional, default 'sample'
        Sampling method tag of the specific sampling methods for
        log and ASE trajectory files or the data file name if not defined.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for sample module
    _default_args = {
        'sample_directory':             None,
        'sample_data_file':             None,
        'sample_systems_queue':         None,
        'sample_systems':               None,
        'sample_systems_format':        None,
        'sample_systems_indices':       None,
        'sample_system_fragments':      None,
        'sample_calculator':            'XTB',
        'sample_calculator_args':       {},
        'sample_save_trajectory':       True,
        'sample_num_threads':           1,
        'sample_properties':            None,
        'sample_systems_optimize':      False,
        'sample_systems_optimize_fmax': 0.001,
        'sample_data_overwrite':        False,
        'sample_property_threshold':    None,
        'sample_nsamples_threshold':    None,
        'sample_tag':                   'sample',
        }

    # Expected data types of input variables
    _dtypes_args = {
        'sample_directory':             [utils.is_string, utils.is_None],
        'sample_data_file':             [
            utils.is_string, utils.is_string_array_inhomogeneous,
            utils.is_None],
        'sample_systems':               [
            utils.is_None, utils.is_string, utils.is_string_array,
            utils.is_ase_atoms, utils.is_ase_atoms_array],
        'sample_systems_format':        [
            utils.is_None, utils.is_string, utils.is_string_array],
        'sample_systems_indices':       [
            utils.is_integer, utils.is_integer_array],
        'sample_system_fragments':      [
            utils.is_array_like, utils.is_dictionary],
        'sample_calculator':            [utils.is_string, utils.is_object],
        'sample_calculator_args':       [utils.is_dictionary],
        'sample_save_trajectory':       [utils.is_bool],
        'sample_num_threads':           [utils.is_integer],
        'sample_properties':            [
            utils.is_None, utils.is_string, utils.is_string_array],
        'sample_systems_optimize':      [
            utils.is_bool, utils.is_boolean_array],
        'sample_systems_optimize_fmax': [utils.is_numeric],
        'sample_data_overwrite':        [utils.is_bool],
        'sample_property_threshold':    [utils.is_None, utils.is_dictionary],
        'sample_nsamples_threshold':    [utils.is_None, utils.is_integer],
        'sample_tag':                   [utils.is_string],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None, 
        sample_data_file: Optional[str] = None,
        sample_directory: Optional[str] = None,
        sample_systems_queue: Optional[queue.Queue] = None,
        sample_systems: Optional[Union[str, List[str], object]] = None,
        sample_systems_format: Optional[Union[str, List[str]]] = None,
        sample_systems_indices: Optional[Union[int, List[int]]] = None,
        sample_system_fragments: 
            Optional[Union[List[int], Dict[int, Any]]] = None,
        sample_calculator: Optional[Union[str, object]] = None,
        sample_calculator_args: Optional[Dict[str, Any]] = None,
        sample_save_trajectory: Optional[bool] = None,
        sample_num_threads: Optional[int] = None,
        sample_properties: Optional[List[str]] = None,
        sample_systems_optimize: Optional[bool] = None,
        sample_systems_optimize_fmax: Optional[float] = None,
        sample_data_overwrite: Optional[bool] = None,
        sample_property_threshold: Optional[Dict[str, float]] = None,
        sample_nsamples_threshold: Optional[int] = None,
        sample_tag: Optional[str] = None,
        **kwargs,
    ):
        """"""
        Initialize Sampler class.
        """"""

        #####################################
        # # # Check Sampler Class Input # # #
        #####################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, sampling),
            check_dtype=utils.get_dtype_args(self, sampling)
        )

        # Set global configuration as class parameter
        self.config = config

        # Check system input
        if self.sample_systems is None and self.sample_systems_queue is None:
            self.logger.warning(
                ""No input in 'sample_systems' is given!\n""
                + ""Please provide either a chemical structure file or ""
                + ""an ASE Atoms object as initial sample structure."")
            self.sample_systems = []

        #####################################
        # # # Prepare Sample Calculator # # #
        #####################################

        # Get ASE calculator
        ase_calculator, ase_calculator_tag = (
            interface.get_ase_calculator(
                self.sample_calculator,
                self.sample_calculator_args))

        # Assign calculator tag for info dictionary
        self.sample_calculator_tag = ase_calculator_tag

        # Check requested system properties
        self.sample_properties, self.sample_unit_properties = (
            self.check_properties(
                self.sample_properties,
                ase_calculator)
            )

        # Check number of calculation threads
        if self.sample_num_threads <= 0:
            raise ValueError(
                ""Number of sample threads 'sample_num_threads' must be ""
                + ""larger or equal 1, but ""
                + f""'{self.sample_num_threads:d}' is given!"")

        # Check if sample calculator is thread safe
        if self.sample_num_threads > 1:
            message = (
                f""Sample calculator {self.sample_calculator_tag:s} is ""
                + ""selected for multi-threading sampling with ""
                + f""{self.sample_num_threads:d} threads."")
            if interface.is_ase_calculator_threadsafe(
                self.sample_calculator_tag
            ):
                self.logger.info(message)
            else:
                message += (
                    ""\nBut the selected calculator is NOT thread safe!""
                    ""\nNumber of parallel threads is set back to just 1."")
                self.logger.warning(message)
                self.sample_num_threads = 1

        ############################
        # # # Prepare Sampling # # #
        ############################

        # Initialize sampling counter
        if config.get('sample_counter') is None:
            self.sample_counter = 1
        else:
            self.sample_counter = config.get('sample_counter') + 1

        # Generate working directory
        if self.sample_directory is None or not len(self.sample_directory):
            self.sample_directory = '.'
        elif not os.path.exists(self.sample_directory):
            os.makedirs(self.sample_directory)

        # Check sample data file
        if self.sample_data_file is None:
            self.sample_data_file = f'{self.sample_tag:s}.db'
        if utils.is_string(self.sample_data_file):
            self.sample_data_file = (
                self.sample_data_file,
                data.check_data_format(
                    self.sample_data_file, is_source_format=False)
                )
        elif utils.is_string_array(self.sample_data_file):
            self.sample_data_file = tuple(self.sample_data_file)
        else:
            raise ValueError(
                ""Sample data file 'sample_data_file' must be a string ""
                + ""of a valid file path but is of type ""
                + f""'{type(self.sample_data_file)}'!"")

        # Define sample log file path and trajectory file
        self.sample_log_file = os.path.join(
            self.sample_directory,
            f'{self.sample_counter:d}_{self.sample_tag:s}_{{:d}}.log')
        self.sample_trajectory_file = os.path.join(
            self.sample_directory, 
            f'{self.sample_counter:d}_{self.sample_tag:s}_{{:d}}.traj')

        # Initialize the multithreading lock
        self.lock = threading.Lock()

        # Check property threshold parameter
        if self.sample_property_threshold is not None:
            self.check_property_threshold(
                self.sample_property_threshold,
                sample_calculator=ase_calculator)

        # Check sample number threshold parameter
        if self.sample_nsamples_threshold is not None:
            self.check_nsamples_threshold()

        #############################
        # # # Prepare Optimizer # # #
        #############################

        if self.sample_systems_optimize:

            # Assign ASE optimizer
            self.optimizer_tag = 'bfgs'
            self.ase_optimizer = optimize.BFGS

        #####################################
        # # # Initialize Sample DataSet # # #
        #####################################

        self.sample_dataset = data.DataSet(
            self.sample_data_file,
            data_properties=self.sample_properties,
            data_unit_properties=self.sample_unit_properties,
            data_overwrite=self.sample_data_overwrite)

        return

    def __str__(self):
        """"""
        Return class descriptor
        """"""
        return ""Sampler class""

    def read_systems(
        self,
        sample_systems_queue,
        sample_systems,
        sample_systems_format,
        sample_systems_indices,
        sample_system_fragments,
    ):
        """"""
        Iterator to read next sample system and return as ASE atoms object
        """"""
        
        # Check system and format input
        if sample_systems is None or not len(sample_systems):
            return sample_systems_queue, ['Queue']
        
        if not utils.is_array_like(sample_systems):
            sample_systems = [sample_systems]
        
        if sample_systems_format is None:
            sample_systems_format = []
            for system in sample_systems:
                if utils.is_string(system):
                    sample_systems_format.append(system.split('.')[-1])
                else:
                    sample_systems_format.append(None)
        elif utils.is_string(sample_systems_format):
            sample_systems_format = (
                [sample_systems_format]*len(sample_systems))
        elif len(sample_systems) != len(sample_systems_format):
            raise ValueError(
                ""Sample system input 'sample_systems' and ""
                + ""'sample_systems_format' have different input size of ""
                + f""{len(sample_systems):d} and ""
                + f""{len(sample_systems_format):d}, respectively."")
        
        # Check system index selection
        if utils.is_integer(sample_systems_indices):
            sample_systems_indices = [sample_systems_indices]
        if (
            sample_systems_indices is not None 
            and len(sample_systems_indices) == 0
        ):
            sample_systems_indices = None

        # If number of samples sources is larger 1, prepare system index 
        # selection for sample sources
        Nsystems = len(sample_systems)
        if Nsystems > 1 and sample_systems_indices is not None:
            indices = sample_systems_indices.copy()
            for ii, idx in enumerate(sample_systems_indices):
                if idx >= Nsystems or idx < (-1*Nsystems):
                    raise SyntaxError(
                        ""System index selection 'sample_systems_indices' ""
                        + f""contains index ({idx:d}) which is outside of ""
                        + f""the sample number range of ({Nsystems:d})!"")
                if idx < 0:
                    indices[ii] = Nsystems + idx

        # Iterate over system input and eventually read file to store as
        # (ASE Atoms object, index, sample source)
        for isample, (source, source_format) in enumerate(
            zip(sample_systems, sample_systems_format)
        ):
            
            # If sample index not in indices list in case of multiple sources
            if Nsystems > 1 and sample_systems_indices is not None:
                if isample not in indices:
                    continue

            # Check for ASE Atoms object or read system file
            if utils.is_ase_atoms(source):
                
                # Store ASE Atoms object as xyf file
                source_file = os.path.join(
                    self.sample_directory,
                    f""{self.sample_counter:d}_sample_system_{isample:d}.xyz"")
                ase.io.write(source_file, source, format='xyz')

                # Assign system fragment definition to atoms object
                if sample_system_fragments is not None:
                    fragments = self.get_system_fragments(
                        sample_system_fragments,
                        source)
                    source.info['fragments'] = fragments

                # Add sample system to queue
                sample_systems_queue.put((source, isample, source_file, 1))
            
            # Check for an Asparagus dataset
            # elif source_format.lower() == 'db':
            elif (
                data.check_data_format(source_format, ignore_error=True)
                is not None
            ):
                
                # Open dataset
                dataset = data.DataSet(source)
                
                # Prepare system index selection in case of just one sample 
                # source
                if Nsystems == 1 and sample_systems_indices is not None:
                    Ndata = len(dataset)
                    indices = sample_systems_indices.copy()
                    for ii, idx in enumerate(sample_systems_indices):
                        if idx >= Ndata or idx < (-1*Ndata):
                            raise SyntaxError(
                                ""System index selection ""
                                + ""'sample_systems_indices' contains index ""
                                + f""({idx:d}) which is outside of the data ""
                                + f""number range of ({Ndata:d}) in sample ""
                                + f""file '{source:s}'!"")
                        if idx < 0:
                            indices[ii] = Ndata + idx

                # Iterate over dataset
                for isys, data_i in enumerate(dataset):

                    # Skip if system index not in indices list 
                    if Nsystems == 1 and sample_systems_indices is not None:
                        if isys not in indices:
                            continue

                    # Check cell parameter
                    cell = data_i['cell'].numpy().reshape(-1)
                    if cell.shape == (9,):
                        cell = cell.reshape(3, 3)

                    # Create and append atoms object to sample queue
                    system = ase.Atoms(
                        data_i['atomic_numbers'],
                        positions=data_i['positions'],
                        pbc=data_i['pbc'].numpy().reshape(-1),
                        cell=cell)
                    if 'charge' in data_i:
                        system.info['charge'] = int(
                            data_i['charge'].numpy()[0])
                    
                    # Assign system fragment definition to atoms object
                    if sample_system_fragments is not None:
                        fragments = self.get_system_fragments(
                            sample_system_fragments,
                            system)
                        system.info['fragments'] = fragments
                    elif (
                        'fragments' in data_i
                        and data_i['fragments'] is not None
                    ):
                        system.info['fragments'] = data_i['fragments'].numpy()
                    
                    sample_systems_queue.put((system, isample, source, isys))
            
            # Else, use ase.read function with respective format
            else:
                
                counter=0
                complete = False
                while not complete:
                    try:
                        if (
                            Nsystems == 1
                            and sample_systems_indices is not None
                        ):
                            isys = sample_systems_indices[counter]
                        else:
                            isys = counter

                        # Read sample system from source file
                        system = ase.io.read(
                            source, index=isys, format=source_format)

                        # Assign system fragment definition to atoms object
                        if sample_system_fragments is not None:
                            fragments = self.get_system_fragments(
                                sample_system_fragments,
                                system)
                            system.info['fragments'] = fragments

                        sample_systems_queue.put(
                            (system, isample, source, isys))
                    
                    except (StopIteration, AssertionError):
                        complete = True
                    
                    else:
                        counter += 1
                        if (
                            Nsystems == 1
                            and sample_systems_indices is not None
                            and counter >= len(sample_systems_indices)
                        ):
                            complete = True

        return sample_systems_queue, sample_systems

    def assign_calculator(
        self,
        sample_system: ase.Atoms,
        sample_calculator: Optional[Union[str, object]] = None,
        sample_calculator_args: Optional[Dict[str, Any]] = None,
        ithread: Optional[int] = None,
    ):
        """"""
        Assign calculator to a list of sample ASE Atoms objects

        Parameters
        ----------
        sample_system : ase.Atoms
            ASE Atoms object to assign the calculator
        sample_calculator : (str, object), optional, default None
            ASE calculator object or string of an ASE calculator class
            name to assign to the sample systems
        sample_calculator_args : dict, optional, default None
            Dictionary of keyword arguments to initialize the ASE
            calculator
        ithread: int, optional, default None
            Thread number to avoid conflict between files written by the
            calculator.

        """"""

        # Check calculator input
        if sample_calculator is None:
            sample_calculator = self.sample_calculator
        if sample_calculator_args is None:
            sample_calculator_args = self.sample_calculator_args

        # Get ASE calculator
        # Special case: Asparagus model calculator is stored to avoid repeating
        # initialization for multiple sample systems
        if (
            utils.is_string(sample_calculator)
            and sample_calculator.lower() in ['asparagus', 'model']
        ):

            if hasattr(self, 'model_calculator'):

                # Get stored model calculator
                ase_calculator = self.model_calculator
                ase_calculator_tag = self.model_calculator_tag

            else:

                # Initialize and store model calculator
                ase_calculator, ase_calculator_tag = (
                    interface.get_ase_calculator(
                        sample_calculator,
                        sample_calculator_args,
                        ithread=ithread)
                    )
                self.model_calculator = ase_calculator
                self.model_calculator_tag = ase_calculator_tag

        else:

            ase_calculator, ase_calculator_tag = (
                interface.get_ase_calculator(
                    sample_calculator,
                    sample_calculator_args,
                    ithread=ithread)
                )

        # Check requested system properties
        self.sample_properties, self.sample_unit_properties = (
            self.check_properties(
                self.sample_properties,
                ase_calculator)
            )

        # Assign ASE calculator
        sample_system.calc = ase_calculator

        return sample_system

    def check_properties(
        self,
        sample_properties: List[str],
        sample_calculator: 'ase.Calculator'
    ):
        """"""
        Check requested sample properties and units with implemented properties
        of the calculator
        
        Parameters
        ----------
        sample_properties: list
            List of system properties to check for availability by the
            calculator.
        sample_calculator: ase.Calculator
            ASE Calculator to compare requested and available properties
            from the calculator.

        Returns
        -------
        list:
            Checked sample properties
        list:
            Sample properties (+ positions, charge) units

        """"""

        # If sample properties are not defined (None), take all available
        # properties from ASE calculator
        if sample_properties is None:
            sample_properties = []
            for prop in sample_calculator.implemented_properties:
                if prop in settings._valid_properties:
                    sample_properties.append(prop)
                elif 'std_' in prop and prop[4:] in settings._valid_properties:
                    sample_properties.append(prop)

        # Check requested system properties
        for prop in sample_properties:
            # TODO Special calculator properties list for special properties
            # not supported by ASE such as, e.g., charge, hessian, etc.
            if prop not in sample_calculator.implemented_properties:
                raise ValueError(
                    f""Requested property '{prop:s}' is not implemented ""
                    + f""in the ASE calculator '{sample_calculator}'! ""
                    + ""Available ASE calculator properties are:\n""
                    + f""{sample_calculator.implemented_properties}"")

        # Define positions and property units
        sample_unit_properties = {}
        for prop in sample_properties:
            if 'std_' in prop:
                sample_unit_properties[prop] = (
                    interface.ase_calculator_units.get(prop[4:]))
            else:
                sample_unit_properties[prop] = (
                    interface.ase_calculator_units.get(prop))
        if 'positions' not in sample_unit_properties:
            sample_unit_properties['positions'] = (
                interface.ase_calculator_units.get('positions'))
        if 'charge' not in sample_unit_properties:
            sample_unit_properties['charge'] = (
                interface.ase_calculator_units.get('charge'))

        return sample_properties, sample_unit_properties

    def get_info(self):
        """"""
        Dummy function for sampling parameter dictionary
        """"""
        return {            
            'sample_data_file': self.sample_data_file,
            'sample_directory': self.sample_directory,
            'sample_systems': self.sample_systems,
            'sample_systems_format': self.sample_systems_format,
            'sample_system_fragments': self.sample_system_fragments,
            'sample_calculator': self.sample_calculator_tag,
            'sample_calculator_args': self.sample_calculator_args,
            'sample_properties': self.sample_properties,
            'sample_systems_optimize': self.sample_systems_optimize,
            'sample_systems_optimize_fmax': self.sample_systems_optimize_fmax,
            'sample_data_overwrite': self.sample_data_overwrite,
        }

    def run(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        sample_systems: Optional[Union[str, List[str], object]] = None,
        sample_systems_format: Optional[Union[str, List[str]]] = None,
        sample_systems_indices: Optional[Union[int, List[int]]] = None,
        sample_system_fragments: 
            Optional[Union[List[int], Dict[int, Any]]] = None,
        **kwargs
    ):
        """"""
        Perform sampling of all sample systems or a selection of them.
        """"""

        ################################
        # # # Check Sampling Input # # #
        ################################
        
        # Check input
        if sample_systems_queue is None:
            sample_systems_queue = self.sample_systems_queue
        if sample_systems is None:
            sample_systems = self.sample_systems
        if sample_systems_format is None:
            sample_systems_format = self.sample_systems_format
        if sample_systems_indices is None:
            sample_systems_indices = self.sample_systems_indices
        if sample_system_fragments is None:
            sample_system_fragments = self.sample_system_fragments

        # Collect sampling parameters
        config_sample_tag = f'{self.sample_counter}_{self.sample_tag}'
        config_sample = {
            config_sample_tag: self.get_info()
            }
        
        # Read sample systems into queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue()
        sample_systems_queue, sample_systems = self.read_systems(
            sample_systems_queue,
            sample_systems,
            sample_systems_format,
            sample_systems_indices,
            sample_system_fragments)

        # Check if sample systems queue is empty
        if sample_systems_queue.empty():
            raise SyntaxError(
                ""No sample systems defined (empty queue)! ""
                + ""Check 'sample_systems' input."")

        # Update configuration file with sampling parameters
        if 'sampler_schedule' in self.config:
            config_sample = {
                **self.config['sampler_schedule'],
                **config_sample,
                }
        self.config.update({
            'sampler_schedule': config_sample,
            'sample_counter': self.sample_counter
            })

        # Print sampling overview
        message = (
            f""Perform sampling method '{self.sample_tag:s}' on systems:\n"")
        for isys, system in enumerate(sample_systems):
            if utils.is_ase_atoms(system):
                system = system.get_chemical_formula()
            message += f"" {isys + 1:3d}. '{system:s}'\n""
        self.logger.info(message)

        ##########################
        # # # Start Sampling # # #
        ##########################
        
        self.run_systems(
            sample_systems_queue=sample_systems_queue,
            **kwargs
            )

        # Increment sample counter
        self.sample_counter += 1

        return

    def run_systems(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        **kwargs,
    ):
        """"""
        Apply sample calculation on sample systems.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue, optional, default None
            Queue object including sample systems or to which 'sample_systems' 
            input will be added. If not defined, an empty queue will be 
            assigned.
        """"""

        # Check sample system queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue()

        # Initialize thread continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )
        
        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_systems_queue.put('stop')

        # Initialize sample number list
        self.Nsamples = [0 for ithread in range(self.sample_num_threads)]

        # Run sampling over sample systems
        if self.sample_num_threads == 1:
            
            self.run_system(sample_systems_queue)
        
        else:

            # Create threads
            threads = [
                threading.Thread(
                    target=self.run_system, 
                    args=(sample_systems_queue, ),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()
        
        return
        
    def run_system(
        self, 
        sample_systems_queue: queue.Queue,
        ithread: Optional[int] = None,
    ):
        """"""
        Apply sample calculator on system input and write properties to 
        database.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue
            Queue of sample system information providing tuples of ase Atoms
            objects, index number and respective sample source and the total
            sample index.
        ithread: int, optional, default None
            Thread number

        """"""

        while self.keep_going(ithread):
            
            # Get sample parameters or wait
            sample = sample_systems_queue.get()

            # Check for stop flag
            if sample == 'stop':
                self.thread_keep_going[ithread] = False
                continue
            
            # Extract sample system to optimize
            (system, isample, source, index) = sample

            # Assign calculator
            system = self.assign_calculator(
                system, 
                ithread=ithread)

            # If requested, perform structure optimization
            if self.sample_systems_optimize:

                # Perform structure optimization
                system = self.run_optimization(
                    sample_system=system,
                    sample_index=isample)

            # Compute system properties
            system, converged = self.run_calculation(
                system, isample, source, index)

            # Store results
            if converged:
                self.save_properties(system, ithread)

            if converged and self.sample_save_trajectory:
                self.write_trajectory(
                    system, self.sample_trajectory_file.format(isample))

        # Print sampling info
        if ithread is None:
            isample = 0
        else:
            isample = ithread
        if self.Nsamples[isample] == 0:
            message += f""No samples written to ""
        else:
            message = (
                f""Sampling method '{self.sample_tag:s}' complete for system ""
                + f""from '{source}!'\n"")
            message += f""{self.Nsamples[isample]:d} sample written to ""
            if self.Nsamples[isample] == 1:
                message += ""sample ""
            else:
                message += f""samples ""
            message += ""written to ""
        message += f""'{self.sample_data_file[0]:s}'.""
        self.logger.info(message)

        return

    def run_calculation(
        self,
        system: ase.Atoms,
        isample: Optional[int] = None,
        source: Optional[str] = None,
        index: Optional[int] = None,
        try_again: Optional[bool] = True,
    ) -> (ase.Atoms, bool):
        """"""
        Apply sample calculator on system input and write properties to 
        database.
        
        Parameters
        ----------
        system: ase.Atoms
            ASE atoms object with linked calculator to run the reference
            calculation with.
        isample: int, optional, default None
            For log file, sample number from 'source'.
        source: str, optional, default None
            For log file, sample system 'source'.
        index: int, optional, default None
            For log file, sample system 'source' index.
        try_again: bool, optional, default True
            If the calculation failed (converged == False), try the reference
            calculation for a second time with 'try_again=False' then.

        Returns
        -------
        ase.Atoms
            ASE atoms object with linked calculator after reference calculation
            was run.
        bool
            Flag for converged (True) or failed (False) reference calculation

        """"""

        # Run calculation
        try:

            system.calc.calculate(
                system,
                properties=self.sample_properties,
                system_changes=system.calc.implemented_properties)

            if hasattr(system.calc, 'converged'):
                converged = system.calc.converged
            elif (
                self.sample_properties[0] in system.calc.results
                and (
                    system.calc.results[self.sample_properties[0]] is None
                    or np.any(np.isnan(
                        system.calc.results[self.sample_properties[0]]))
                )
            ):
                converged = False
            else:
                converged = True

        except (
            ase.calculators.calculator.CalculationFailed
            or subprocess.CalledProcessError
        ):

            converged = False

        if not converged and try_again:
            system, converged = self.run_calculation(system, try_again=False)

        # Store calculation state to log file
        if isample is None:
            isample = -1
        if source is None:
            source = 'None'
        if index is None:
            index = -1
        message = f""{time.asctime():s} ""
        if converged:
            message += ""CONVERGED ""
        else:
            message += ""FAILED ""
        message += (
            f""calculation for sample {isample:d} of '{source:s}' ({index:d})"")
        with self.lock:
            with open(self.sample_log_file, 'a') as flog:
                flog.write(message)

        return system, converged

    def run_optimization(
        self,
        sample_system: Optional[ase.Atoms] = None,
        sample_index: Optional[int] = None,
        sample_systems_queue: Optional[queue.Queue] = None,
        sample_optimzed_queue: Optional[queue.Queue] = None,
        ithread: Optional[int] = None,
    ):
        """"""
        Perform structure optimization on sample system
        
        Parameters
        ----------
        sample_system: ase.Atoms, optional, default None
            ASE Atoms object which will be optimized using the optimizer 
            defined in self.ase_optimizer.
        sample_index: int, optional, default None
            Sample index number of the ASE atoms object to optimize.
        sample_systems_queue: queue.Queue, optional, default None
            Sample system queue cotaining ASE Atoms object which will be 
            optimized using the optimizer defined in self.ase_optimizer.
            If sample_system is not None, this queue will be ignored.
        sample_optimzed_queue: queue.Queue, optional, default None
            If defined, the optimized sample system will be put into the
            queue.
        ithread: int, optional, default None
            Thread number
        
        Returns
        -------
        ase.Atoms
            Optimized ASE atoms object

        """"""
        
        # Check sample system input
        if sample_system is None and sample_systems_queue is None:
            return None
        
        # Optimize sample system
        if sample_system is not None:
            
            # Prepare optimization log and trajectory file name
            if sample_index is None:
                ase_optimizer_log_file = os.path.join(
                    self.sample_directory,
                    f'{self.sample_counter:d}_{self.optimizer_tag:s}.log')
                ase_optimizer_trajectory_file = os.path.join(
                    self.sample_directory,
                    f'{self.sample_counter:d}_{self.optimizer_tag:s}.traj')
            else:
                ase_optimizer_log_file = os.path.join(
                    self.sample_directory,
                    f'{self.sample_counter:d}_{self.optimizer_tag:s}'
                    + f'_{sample_index:d}.log')
                ase_optimizer_trajectory_file = os.path.join(
                    self.sample_directory,
                    f'{self.sample_counter:d}_{self.optimizer_tag:s}'
                    + f'_{sample_index:d}.traj')

            # Assign calculator
            system = self.assign_calculator(
                sample_system,
                ithread=ithread)

            # Perform structure optimization
            self.ase_optimizer(
                system,
                logfile=ase_optimizer_log_file,
                trajectory=ase_optimizer_trajectory_file,
                ).run(
                    fmax=self.sample_systems_optimize_fmax)

            # Add optimized ASE atoms object to the queue if defined
            if sample_optimzed_queue is not None:
                sample_optimzed_queue.put((system, isample, str(system), 1))

            return system
        
        else:
            
            while self.keep_going(ithread):
                
                # Get sample parameters or wait
                sample = sample_systems_queue.get()
                
                # Check for stop flag
                if sample == 'stop':
                    self.thread_keep_going[ithread] = False
                    continue
                
                # Extract sample system to optimize
                (system, isample, source, index) = sample

                # Prepare optimization log and trajectory file name
                ase_optimizer_log_file = os.path.join(
                    self.sample_directory,
                    f'{self.sample_counter:d}_{self.optimizer_tag:s}'
                    + f'_{isample:d}.log')
                ase_optimizer_trajectory_file = os.path.join(
                    self.sample_directory,
                    f'{self.sample_counter:d}_{self.optimizer_tag:s}'
                    + f'_{isample:d}.traj')

                # Assign calculator
                system = self.assign_calculator(
                    system,
                    ithread=ithread)

                # Perform structure optimization
                try:

                    ase_optimizer = self.ase_optimizer(
                        system,
                        logfile=ase_optimizer_log_file,
                        trajectory=ase_optimizer_trajectory_file,
                        )
                    ase_optimizer.run(fmax=self.sample_systems_optimize_fmax)
                
                except ase.calculators.calculator.CalculationFailed:

                    message = (
                        ""Single point calculation of the system ""
                        + f""from '{source}' of index {index:d} ""
                        + ""is not converged ""
                        + ""during structure optimization for sampling method ""
                        + f""'{self.sample_tag:s}' ""
                        + f""(see log file '{ase_optimizer_log_file:s}')!"")
                    if sample_optimzed_queue is not None:
                        message += (
                            ""\nSystem will be skipped for further sampling."")
                    self.logger.error(message)
                    
                else:

                    message = (
                        ""Optimization of system ""
                        + f""from '{source}' of index {index:d} is converged ""
                        + f""(see log file '{ase_optimizer_log_file:s}')."")
                    self.logger.info(message)

                # Add optimized ASE atoms object to the queue if defined
                if sample_optimzed_queue is not None:
                    sample_optimzed_queue.put((system, isample, source, index))

        return

    def keep_going(
        self,
        ithread
    ) -> bool:
        """"""
        Return thread continuation flag
        
        Parameters
        ----------
        ithread: int
            Thread number

        """"""
        
        # Check if number of sample threshold is reached
        if (
            self.sample_nsamples_threshold is not None
            and np.sum(self.Nsamples) >= self.sample_nsamples_threshold
        ):
            if ithread is None:
                self.thread_keep_going[0] = False
            else:
                self.thread_keep_going[ithread] = False
            return False

        # Return thread continuation flag
        if ithread is None:
            return self.thread_keep_going[0]
        else:
            return self.thread_keep_going[ithread]

        return False

    def get_properties(self, system) -> List[str]:
        """"""
        Collect system properties and calculator results

        """"""
        return interface.get_ase_properties(system, self.sample_properties)

    def save_properties(self, system, ithread) -> int:
        """"""
        Save system properties
        """"""

        # Collect system properties
        system_properties = self.get_properties(system)

        # Sample list index
        if ithread is None:
            isample = 0
        else:
            isample = ithread

        # Check for property thresholds
        if self.sample_property_threshold is not None:
            for prop, value in self.sample_property_threshold.items():
                if system.calc.results[prop] < value:
                    return

        # Check for number of sample threshold
        if (
            self.sample_nsamples_threshold is not None
            and np.sum(self.Nsamples) >= self.sample_nsamples_threshold
        ):
            return

        # Add to sample data file
        with self.lock:
            self.sample_dataset.add_atoms(system, system_properties)
            self.Nsamples[isample] += 1

        return

    def write_trajectory(self, system, trajectory_file):
        """"""
        Write current image to trajectory file but without constraints
        """"""
        
        # Check trajectory file
        if trajectory_file is None:
            return

        # Check for property thresholds
        # If accepted, write to another trajectory file
        threshold_trajectory_file = None
        if self.sample_property_threshold is not None:
            accepted = True
            for prop, value in self.sample_property_threshold.items():
                if system.calc.results[prop] < value:
                    accepted = False
            if accepted:
                threshold_trajectory_file = (
                    f'{trajectory_file:s}.threshold.traj')
                threshold_trajectory_properties = (
                    self.sample_properties
                    + list(self.sample_property_threshold.keys()))

        # Save system without constraint to the trajectory file
        with self.lock:
            trajectory = ase.io.Trajectory(
                trajectory_file, atoms=system,
                mode='a', properties=self.sample_properties)
            system_noconstraint = system.copy()
            system_noconstraint.calc = system.calc
            system_noconstraint.set_constraint()
            trajectory.write(system_noconstraint)
            trajectory.close()
            if threshold_trajectory_file is not None:
                trajectory = ase.io.Trajectory(
                    threshold_trajectory_file, atoms=system_noconstraint,
                    mode='a', properties=threshold_trajectory_properties)
                trajectory.write(system_noconstraint)
                trajectory.close()

        return

    def check_property_threshold(
        self,
        sample_property_threshold: Dict[str, float],
        sample_calculator: Optional['ase.Calculator'] = None,
    ):
        """"""
        Check property threshold input

        Parameters
        ----------
        sample_property_threshold: dict
            Dictionary of model property (key) minimum threshold values (item)
            to decide if samples or frames are added to the sample data file.
        sample_calculator: ASE.Calculator, optional, default None
            ASE Calculator used for sampling. If defined, check if the
            calculators implemented properties contain the requested property,
            else check with sample properties.

        """"""

        # Get available property list
        if sample_calculator is None:
            available_properties = self.sample_properties
        else:
            available_properties = sample_calculator.implemented_properties

        # Get available property unit dictionary
        available_unit_properties = {}
        for prop in available_properties:
            if prop in self.sample_unit_properties:
                available_unit_properties[prop] = (
                    self.sample_unit_properties[prop])
            elif 'std_' in prop:
                available_unit_properties[prop] = (
                    interface.ase_calculator_units.get(prop[4:]))
            else:
                available_unit_properties[prop] = (
                    interface.ase_calculator_units.get(prop))

        if (
            sample_property_threshold is not None
            and utils.is_dictionary(sample_property_threshold)
        ):
            message = (
                ""Property threshold filter is applied to store samples in ""
                + f""'{self.sample_data_file[0]:s}'!\n""
                + f"" {'Property':<12s}   Threshold\n""
                + ""-""*30 + ""\n"")
            for prop, value in sample_property_threshold.items():
                if prop not in available_properties:
                    raise ValueError(
                        ""Property threshold dictionary ""
                        + ""'sample_property_threshold' contains invalid ""
                        + f""property '{prop:s}' which is not a calculator ""
                        + f""property:\n{available_properties}"")
                if utils.is_numeric(value):
                    sample_property_threshold[prop] = float(value)
                    message += (
                        f"" {prop:>12} > ""
                        + f""{sample_property_threshold[prop]:.2E}""
                        + f"" {available_unit_properties[prop]:s}\n"")
                else:
                    raise ValueError(
                        ""Property threshold dictionary ""
                        + ""'sample_property_threshold' contains invalid ""
                        + f""threshold value for property '{prop:s}' of type ""
                        + f""'{value}'!"")
            self.logger.info(message)
        elif sample_property_threshold is not None:
            raise ValueError(
                ""Property threshold dictionary 'sample_property_threshold' ""
                + ""must be a dictionary of a valid property (key) and ""
                + ""threshold value (value), but is of type ""
                + f""'{type(sample_property_threshold)}'!"")

        return

    def check_nsamples_threshold(
        self
    ):
        """"""
        Check sample number threshold input

        """"""

        if (
            self.sample_nsamples_threshold is not None
            and utils.is_integer(self.sample_nsamples_threshold)
            and self.sample_nsamples_threshold > 0
        ):
            message = (
                ""Sample number threshold filter for a maximum of ""
                + f""{self.sample_nsamples_threshold:d} is applied that are ""
                + f""stored in '{self.sample_data_file[0]:s}'!\n"")
            self.logger.info(message)
        elif self.sample_nsamples_threshold is not None:
            raise ValueError(
                ""Sample number threshold input 'sample_nsamples_threshold' ""
                + ""must be a positive integer but is of type ""
                + f""'{type(self.sample_nsamples_threshold)}'!"")

        return",./Asparagus/asparagus/sampling/sampler.py
NormalModeScanner,"class NormalModeScanner(sampling.Sampler):
    """"""
    Normal Mode Scanning class


    Parameters
    ----------
    config: (str, dict, settings.Configuration)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    nms_harmonic_energy_step: float, optional, default 0.05
        Within the harmonic approximation the initial normal mode
        displacement from the equilibrium positions is scaled to match
        the potential difference with the given energy value.
    nms_energy_limits: (float, list(float)), optional, default 1.0
        Potential energy limit in eV from the initial system conformation
        to which additional normal mode displacements steps are added.
        If one numeric value is give, the energy limit is used as upper
        potential energy limit and the lower limit in case the initial
        system conformation might not be the global minimum.
        If a list with two numeric values are given, the first two are the
        lower and upper potential energy limit, respectively.
    nms_number_of_coupling: int, optional, default 2
        Maximum number of coupled normal mode displacements to sample
        the system conformations.
    nms_limit_of_steps: int, optional, default 10
        Maximum limit of coupled normal mode displacements in one direction
        to sample the system conformations.
    nms_limit_com_shift: float, optional, default 0.1 Angstrom
        Center of mass shift threshold to identify translational normal
        modes from vibrational (and rotational). Normalized Normal modes
        with a center of mass shift larger than the threshold are not
        considered in the normal mode scan.
    nms_limit_mode_shift: float, optional, default 1.e-8 Angstrom
        Normal mode shift threshold to identify translational and rotational
        normal modes from vibrational.
    nms_save_displacements: bool, optional, default False
        If True, add results of atom displacement calculations from the normal
        mode analysis to the dataset.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for sample module
    sampling.Sampler._default_args.update({
        'nms_harmonic_energy_step':     0.05,
        'nms_energy_limits':            1.0,
        'nms_number_of_coupling':       1,
        'nms_limit_of_steps':           10,
        'nms_limit_com_shift':          0.1,
        'nms_limit_mode_shift':         1.e-8,
        'nms_save_displacements':       False
        })
    
    # Expected data types of input variables
    sampling.Sampler._dtypes_args.update({
        'nms_harmonic_energy_step':     [utils.is_numeric],
        'nms_energy_limits':            [
            utils.is_numeric, utils.is_numeric_array],
        'nms_number_of_coupling':       [utils.is_numeric],
        'nms_limit_of_steps':           [utils.is_numeric],
        'nms_limit_com_shift':          [utils.is_numeric],
        'nms_limit_mode_shift':         [utils.is_numeric],
        'nms_save_displacements':       [utils.is_bool],
        })

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None, 
        nms_harmonic_energy_step: Optional[float] = None,
        nms_energy_limits: Optional[Union[float, List[float]]] = None,
        nms_number_of_coupling: Optional[int] = None,
        nms_limit_of_steps: Optional[int] = None,
        nms_limit_com_shift: Optional[float] = None,
        nms_limit_mode_shift: Optional[float] = None,
        nms_save_displacements: Optional[bool] = None,
        **kwargs,
    ):

        # Sampler class label
        self.sample_tag = 'nmscan'

        # Initialize parent class
        super().__init__(
            sample_tag=self.sample_tag,
            config=config,
            config_file=config_file,
            **kwargs
            )

        #################################
        # # # Check NMS Class Input # # #
        #################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)
        
        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, sampling),
            check_dtype=utils.get_dtype_args(self, sampling)
        )

        # Check sample properties for energy property which is required for
        # normal mode scanning
        if 'energy' not in self.sample_properties:
            self.sample_properties.append('energy')
        if 'forces' not in self.sample_properties:
            self.sample_properties.append('forces')

        # Check potential energy limits
        if utils.is_numeric(self.nms_energy_limits):
            self.nms_energy_limits = [
                -abs(self.nms_energy_limits), abs(self.nms_energy_limits)]

        return

    def get_info(self):
        """"""
        Obtain information about the Normal Mode Scanning class object.

        Returns
        -------
        dict
            Dictionary with information about the Normal Mode Scanning class
        """"""

        info = super().get_info()
        info.update({
            'nms_harmonic_energy_step': self.nms_harmonic_energy_step,
            'nms_energy_limits': self.nms_energy_limits,
            'nms_number_of_coupling': self.nms_number_of_coupling,
            'nms_limit_com_shift': self.nms_limit_com_shift,
            'nms_limit_mode_shift': self.nms_limit_mode_shift,
            'nms_limit_of_steps': self.nms_limit_of_steps,
            'nms_save_displacements': self.nms_save_displacements
            })
        return info

    def run_systems(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        nms_indices: Optional[List[int]] = None,
        nms_exclude_modes: Optional[List[int]] = None,
        nms_frequency_range: Optional[List[Tuple[str, float]]] = None,
        nms_clean: Optional[bool] = True,
        **kwargs,
    ):
        """"""
        Perform Normal Mode Scanning on the sample system.
        Iterate over systems using 'sample_num_threads' threads.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue, optional, default None
            Queue object including sample systems or where 'sample_systems' 
            input will be added. If not defined, an empty queue will be 
            assigned.
        nms_indices: list[int], optional, default None
            List of atom indices to include in normal mode analysis.
            If none, indices if a full list of atom indices with length to the
            atom number of the system.
            Atom indices from atoms constraint by FixAtoms are removed from
            index list and the normal mode analysis.
        nms_exclude_modes: list[int], optional, default None
            List of vibrational modes, sorted by wave number, to exclude
            from the sampling procedure.
        nms_frequency_range: list[tuple(str, float)], optional, default None
            Frequency range conditions for normal modes to be included in the
            scan.
        nms_clean: bool, optional, default True
            If True, checkpoint files for atom displacement calculations
            in {sample_directory}/vib_{isample} will be deleted.
            Else, results from available  checkpoint files will be used.
        """"""
        
        # Check sample system queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue()
        
        # Optimize sample systems or take as normal mode analysis input
        if self.sample_systems_optimize:
            
            # Add stop flag
            for _ in range(self.sample_num_threads):
                sample_systems_queue.put('stop')
            
            # Initialize continuation flag
            self.thread_keep_going = np.array(
                [True for ithread in range(self.sample_num_threads)],
                dtype=bool
                )

            # Initialize optimized sample system into queue
            sample_input_queue = queue.Queue()
            
            if self.sample_num_threads == 1:

                # Run sample system optimization
                self.run_optimization(
                    sample_systems_queue=sample_systems_queue,
                    sample_optimzed_queue=sample_input_queue)
            
            else:

                # Create threads for sample system optimization
                threads = [
                    threading.Thread(
                        target=self.run_optimization,
                        kwargs={
                            'sample_systems_queue': sample_systems_queue,
                            'sample_optimzed_queue': sample_input_queue,
                            'ithread': ithread}
                        )
                    for ithread in range(self.sample_num_threads)]

                # Start threads
                for thread in threads:
                    thread.start()

                # Wait for threads to finish
                for thread in threads:
                    thread.join()

        else:
            
            # Set sample system queue as optimized sample system queue
            sample_input_queue = sample_systems_queue

        # Run normal mode scanning
        while not sample_input_queue.empty():
            self.run_system(
                sample_input_queue,
                nms_indices,
                nms_exclude_modes,
                nms_frequency_range,
                nms_clean,
                **kwargs)
        
        return
    
    def run_system(
        self,
        sample_systems_queue: queue.Queue,
        nms_indices: List[int],
        nms_exclude_modes: List[int],
        nms_frequency_range: List[Tuple[str, float]],
        nms_clean: bool,
        **kwargs
    ):
        """"""
        Perform Normal Mode Scanning on the sample system.

        Parameters
        ----------
        sample_systems_queue: queue.Queue
            Queue object including sample systems.
        """"""

        # Initialize normal mode analysis queue
        sample_calculate_queue = queue.Queue()
        
        # Get sample system for normal mode analysis
        (system, isample, source, index) = sample_systems_queue.get()

        # Print sampler info
        message = (
            ""Start normal mode scanning of the system ""
            + f""from '{source}' of index {index:d}."")
        self.logger.info(message)
        
        # Get non-fixed atoms indices
        if nms_indices is None:
            atom_indices = np.arange(
                system.get_global_number_of_atoms(), dtype=int)
        else:
            atom_indices = np.array(nms_indices, dtype=int)
        for constraint in system.constraints:
            if isinstance(constraint, FixAtoms):
                atom_indices = [
                    idx for idx in atom_indices
                    if idx not in constraint.index]
        atom_indices = np.array(atom_indices)
        
        # Prepare system parameter
        Natoms = len(atom_indices)
        Nmodes = 3*Natoms

        # Initialize Vibration class
        system_vibrations = Vibrations_Asparagus(
            self,
            system,
            self.sample_calculator,
            self.sample_calculator_args,
            self.nms_save_displacements,
            indices=atom_indices,
            name=os.path.join(self.sample_directory, f""vib_{isample:d}""),
            **kwargs
            )
        if nms_clean:
            system_vibrations.clean()
        
        # Add jobs to calculation queue
        system_vibrations.add_calculations(
            sample_calculate_queue)

        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_calculate_queue.put('stop')
        
        # Initialize continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )
        
        if self.sample_num_threads == 1:
            
            # Run job calculations
            system_vibrations.run(
                sample_calculate_queue,
                sample_calculator=self.sample_calculator,
                sample_calculator_args=self.sample_calculator_args)
        
        else:

            # Create threads for job calculations
            threads = [
                threading.Thread(
                    target=system_vibrations.run,
                    args=(sample_calculate_queue, ),
                    kwargs={
                        'sample_calculator': self.sample_calculator,
                        'sample_calculator_args': self.sample_calculator_args,
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()

        # Print sampler info
        message = (
            ""Normal mode analysis calculation completed for ""
            + f""the system from '{source}' of index {index:d}."")
        self.logger.info(message)
        
        # Finish normal mode analysis and diagonalize Hessian matrix
        system_vibrations.summary(**kwargs)
        
        # Get initial (equilibrium) energy of the sampling system
        system_initial_results = system_vibrations.get_initial_results()
        
        # (Trans. + Rot. + ) Vibrational frequencies in cm**-1
        system_frequencies = system_vibrations.get_frequencies()

        # (Trans. + Rot. + ) Vibrational modes normalized to 1
        system_modes = np.array([
            system_vibrations.get_mode(imode)[atom_indices].reshape(Natoms, 3)
            / np.sqrt(np.sum(
                system_vibrations.get_mode(imode)[atom_indices].reshape(
                    Natoms, 3)**2))
            for imode in range(Nmodes)])

        # Reduced mass per mode (in amu)
        system_redmass = np.array([
            1./np.sum(
                system_modes[imode]**2
                / system.get_masses()[atom_indices].reshape(Natoms, 1))
            for imode in range(Nmodes)])

        # Force constant per mode (in eV/Angstrom**2)
        system_forceconst = (
            4.0*np.pi**2*(np.abs(system_frequencies)*1.e2*units._c)**2
            * system_redmass*units._amu*units.J*1.e-20)

        # Compute and store equilibrium positions and center of mass,
        # moments of inertia and principle axis of inertia
        system_init_positions = system.get_positions()
        system_init_com = system[atom_indices].get_center_of_mass()

        # Compute and compare same quantities for displaced system
        system_com_shift = np.zeros(Nmodes, dtype=float)
        system_mode_shift = np.zeros(Nmodes, dtype=float)

        for imode, mode in enumerate(system_modes):

            # Normal mode displacements
            check_scale = 0.1
            system_mode_positions = system_init_positions.copy()
            system_mode_positions[atom_indices] += mode*check_scale
            
            # COM shift
            system.set_positions(system_mode_positions, apply_constraint=False)
            system_displ_com = system[atom_indices].get_center_of_mass()
            system_com_shift[imode] = np.sqrt(np.sum(
                (system_init_com - system_displ_com)**2))

            # Mode shift
            system_mode_shift[imode] = (
                np.sum(
                    np.sum(
                        system.get_masses().reshape(Natoms, 1)
                        * mode*check_scale
                        / np.sum(system.get_masses()),
                        axis=0
                    )**2
                )
            )

        # Reset system positions
        system.set_positions(system_init_positions)

        # Vibrational modes are assumed with center of mass shifts and mode
        # shifts smaller than the threshold
        system_vib_modes = np.logical_and(
            system_com_shift < self.nms_limit_com_shift,
            system_mode_shift < self.nms_limit_mode_shift)

        # Apply exclusion list if defined
        if nms_exclude_modes is not None:

            for imode in nms_exclude_modes:
                if imode < len(system_vib_modes):
                    system_vib_modes[imode] = False
                else:
                    self.logger.warning(
                        f""Vibrational mode {imode:d} in the ""
                        + ""exclusion list is larger than the number of ""
                        + ""vibrational modes!"")

        if nms_frequency_range is not None:
            
            # Initially include all modes
            include_modes = np.ones_like(system_vib_modes)
            
            # Iterate over all exclusion conditions
            for (condition, frequency) in nms_frequency_range:
                
                # Compare absolute if requested
                if '||' in condition:
                    comp_frequencies = np.abs(system_frequencies)
                else:
                    comp_frequencies = system_frequencies
                
                # Modes are still include if conditions are matched
                if '<=' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies <= frequency)
                elif '>=' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies >= frequency)
                elif '<' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies < frequency)
                elif '>' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies >= frequency)
                else:
                    raise SyntaxError(
                        f""Normal mode selection condition '{condition}' in ""
                        + ""'nms_frequency_range' selection input is not ""
                        + ""recognized! Choose between ('<', '<=', '>=', '>') ""
                        + ""or for comparing absolute frequencies ""
                        + ""('<||', '<=||', '>=||', '>||')"")

            # Combine normal mode exclusion list
            system_vib_modes = np.logical_and(system_vib_modes, include_modes)

        # Displacement factor for energy step (in eV)
        system_displfact = np.sqrt(
            2.*self.nms_harmonic_energy_step/system_forceconst)
        
        # Add normal mode analysis results to log file
        message = ""\nStart Normal Mode Sampling at system: ""
        if self.sample_data_file is None:
            message += f""{system.get_chemical_formula():s}\n""
        else:
            message += f""{self.sample_data_file[0]:s}\n""
        message = (
            f"" {'Index':5s} |""
            + f"" {'Frequency (cm**-1)':18s} |""
            + f"" {'Vib. Mode':9s} |""
            + f"" {'CoM displacemnt':<16s} |""
            + f"" {'Mode displacemnt':<16s}\n""
            + f"" {'':5s} |""
            + f"" {'':18s} |""
            + f"" {'Selected':9s} |""
            + f"" {'(< ' + f'{self.nms_limit_com_shift:5.2E}' + ')':<16s} |""
            + f"" {'(< ' + f'{self.nms_limit_mode_shift:5.2E}' + ')':<16s}\n""
            + "" ""
            + ""-""*(7 + 21 + 11 + 2*19)
            + ""\n"")
        for ivib, freq in enumerate(system_frequencies):
            message += f"" {ivib + 1:5d} | {freq:18.2f} |""
            if system_vib_modes[ivib]:
                message +=  f"" {'   x   ':9s} |""
            else:
                message +=  f"" {'':9s} |""
            message +=  f"" {system_com_shift[ivib]:16.2E} |""
            message +=  f"" {system_mode_shift[ivib]:16.2E}\n""
        with open(self.sample_log_file.format(isample), 'a') as flog:
            flog.write(message)
        self.logger.info(message)
        
        # Iterate over number of normal mode combinations
        vib_modes = np.where(system_vib_modes)[0]
        
        # Initialize normal mode combination queue
        sample_calculate_queue = queue.Queue()
        
        # Prepare normal mode combination jobs
        irun = 0
        for icomp in range(1, self.nms_number_of_coupling + 1):

            # Prepare sign combinations
            all_signs = np.array(list(
                itertools.product((-1, 1), repeat=icomp)))

            # Prepare step size list
            steps = np.arange(1, self.nms_limit_of_steps + 1, 1)
            all_steps = np.array(list(
                itertools.product(steps, repeat=icomp)))
            Nsteps = all_steps.shape[0]

            # Iterate over vib. normal mode indices and their combinations
            for imodes in itertools.combinations(vib_modes, icomp):

                # Iterate over sign combinations
                for isign, signs in enumerate(all_signs):
                    
                    # Add mode combination job parameters
                    sample_calculate_queue.put(
                        (isample, irun, icomp, imodes, isign, signs))
                    irun += 1

        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_calculate_queue.put('stop')

        # Initialize sample number list
        self.Nsamples = [0 for ithread in range(self.sample_num_threads)]

        # Initialize continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )

        # Run 
        if self.sample_num_threads == 1:
                
            # Run job calculations
            self.run_scan(
                system,
                sample_calculate_queue,
                system_initial_results['energy'],
                system_displfact,
                system_modes,
                atom_indices)
        
        else:

            # Create threads for job calculations
            threads = [
                threading.Thread(
                    target=self.run_scan,
                    args=(
                        system,
                        sample_calculate_queue, 
                        system_initial_results['energy'],
                        system_displfact,
                        system_modes,
                        atom_indices),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()
                
        # Print sampling info
        for ithread in range(self.sample_num_threads):
            if self.Nsamples[ithread] == 0:
                message += f""No samples written to ""
            else:
                message = (
                    f""Sampling method '{self.sample_tag:s}' complete for ""
                    + f""system from '{source}!'\n"")
                message += f""{self.Nsamples[ithread]:d} sample written to ""
                if self.Nsamples[ithread] == 1:
                    message += ""sample ""
                else:
                    message += f""samples ""
                message += ""written to ""
            message += f""'{self.sample_data_file[0]:s}'.""
            self.logger.info(message)

    def run_scan(
        self,
        sample_system: ase.Atoms,
        sample_calculate_queue: queue.Queue,
        system_initial_potential: float,
        system_displfact: List[float],
        system_modes: List[float],
        atom_indices: List[float],
        ithread: Optional[int] = None
    ):
        """"""
        Run normal mode scanning for normal mode combinations.
        
        Parameters
        ----------
        sample_system: ase.Atoms
            Initial/equilibrium sample system to apply on the normal mode
            combinations.
        sample_systems_queue: queue.Queue
            Queue containing normal mode combination parameters
        ithread: int, optional, default None
            Thread number
        """"""
        
        # Get ASE calculator
        ase_calculator, ase_calculator_tag = (
            interface.get_ase_calculator(
                self.sample_calculator,
                self.sample_calculator_args,
                ithread=ithread)
            )
        
        # Assign calculator
        system = sample_system.copy()
        system.calc = ase_calculator

        # Store equilibrium positions
        system_initial_positions = system.get_positions()
        
        while self.keep_going(ithread):
            
            # Get sample parameters or wait
            sample = sample_calculate_queue.get()

            # Check for stop flag
            if sample == 'stop':
                self.thread_keep_going[ithread] = False
                continue
            
            # Extract normal mode combination parameters
            (isample, irun, icomp, imodes, isign, signs) = sample
            
            # Prepare sign combinations
            all_signs = np.array(list(
                itertools.product((-1, 1), repeat=icomp)))

            # Prepare step size list
            steps = np.arange(1, self.nms_limit_of_steps + 1, 1)
            all_steps = np.array(list(
                itertools.product(steps, repeat=icomp)))
            Nsteps = all_steps.shape[0]
            
            # Iterate through steps
            istep = 0
            done = False
            while not done:

                # Get current step size
                step_size = np.array(all_steps[istep])

                # Get normal mode elongation step
                current_step = np.array(signs)*step_size

                # Set elongation step on initial system positions
                current_step_positions = system_initial_positions.copy()
                for imode, modei in enumerate(imodes):
                    current_step_positions[atom_indices] += (
                        system_displfact[modei]*current_step[imode]
                        * system_modes[modei])
                system.set_positions(
                    current_step_positions,
                    apply_constraint=False)

                # Compute observables
                system, converged = self.run_calculation(system)

                # Check potential threshold
                current_properties = system.calc.results
                current_potential = current_properties['energy']
                if not converged or np.isnan(current_potential):
                    converged = False
                    threshold_reached = True
                else:
                    if current_potential < system_initial_potential:
                        threshold_reached = (
                            (
                                current_potential 
                                - system_initial_potential
                            ) < self.nms_energy_limits[0])
                    else:
                        threshold_reached = (
                            (
                                current_potential 
                                - system_initial_potential
                            ) > self.nms_energy_limits[1])

                # Add to dataset
                if converged:
                    self.save_properties(system, ithread)

                # Attach to trajectory
                if converged and self.sample_save_trajectory:
                    self.write_trajectory(
                        system, self.sample_trajectory_file.format(isample))

                # Check energy threshold
                if threshold_reached:

                    # Check for next suitable step size index and avoid
                    # combination of step sizes which were already
                    # above the energy threshold before.
                    istep += 1
                    for jstep in range(istep, Nsteps):
                        if np.any(
                            np.array(all_steps[jstep]) < step_size
                        ):
                            istep = jstep
                            break
                    else:
                        done = True

                else:

                    # Increment step size index
                    istep += 1
            
                # Check if number of sample threshold is reached
                if (
                    self.sample_nsamples_threshold is not None
                    and np.sum(self.Nsamples) >= self.sample_nsamples_threshold
                ):
                    done = True

                # Check step size progress
                if done or istep >= Nsteps:

                    # Update log file
                    msg = ""Vib. modes: (""
                    for imode, isign in zip(imodes, signs):
                        if isign > 0:
                            msg += f""+{imode + 1:d}, ""
                        else:
                            msg += f""-{imode + 1:d}, ""
                    msg += f"") - {istep:4d} steps added\n""
                    log_file = self.sample_log_file.format(isample)
                    with open(log_file, 'a') as flog:
                        flog.write(msg)

                    # Set flag in case maximum Nsteps is reached
                    done = True

        return",./Asparagus/asparagus/sampling/nms.py
NormalModeSampler,"class NormalModeSampler(sampling.Sampler):
    """"""
    Normal Mode Sampling class.

    This is the simple version of the normal mode class as implemented in:
    Chem. Sci., 2017, 8, 3192-3203

    Parameters
    ----------
    config: (str, dict, settings.Configuration)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    nms_temperature: float, optional, default 300
        Temperature in Kelvin to sample the normal modes.
    nms_nsamples: int, optional, default 100
        Number of samples to generate.
    nms_limit_com_shift: float, optional, default 0.01 Angstrom
        Center of mass shift threshold to identify translational normal
        modes from vibrational (and rotational). Normalized Normal modes
        with a center of mass shift larger than the threshold are not
        considered in the normal mode scan.
    nms_limit_mode_shift: float, optional, default 1.e-8 Angstrom
        Normal mode shift threshold to identify translational and rotational
        normal modes from vibrational.
    nms_save_displacements: bool, optional, default False
        If True, add results of atom displacement calculations from the normal
        mode analysis to the dataset.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for sample module
    sampling.Sampler._default_args.update({
        'nms_temperature':              300.0,
        'nms_nsamples':                 100,
        'nms_limit_com_shift':          1.e-2,
        'nms_limit_mode_shift':         1.e-8,
        'nms_save_displacements':       False,
        })
    
    # Expected data types of input variables
    sampling.Sampler._dtypes_args.update({
        'nms_temperature':              [utils.is_numeric],
        'nms_nsamples':                 [utils.is_integer],
        'nms_limit_com_shift':          [utils.is_numeric],
        'nms_limit_mode_shift':         [utils.is_numeric],
        'nms_save_displacements':       [utils.is_bool],
        })

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None, 
        nms_temperature: Optional[float] = None,
        nms_nsamples: Optional[int] = None,
        nms_limit_com_shift: Optional[float] = None,
        nms_limit_mode_shift: Optional[float] = None,
        nms_save_displacements: Optional[bool] = None,
        **kwargs
    ):

        # Sampler class label
        self.sample_tag = 'nmsmpl'

        # Initialize parent class
        super().__init__(
            sample_tag=self.sample_tag,
            config=config,
            config_file=config_file,
            **kwargs
            )

        #################################
        # # # Check NMS Class Input # # #
        #################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)
        
        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, sampling),
            check_dtype=utils.get_dtype_args(self, sampling)
        )

        # Check sample properties for energy and forces property which is 
        # required for normal mode scanning
        if 'energy' not in self.sample_properties:
            self.sample_properties.append('energy')
        if 'forces' not in self.sample_properties:
            self.sample_properties.append('forces')

        return

    def get_info(self):
        """"""
        Obtain information about the Normal Mode Sampling class object.

        Returns
        -------
        dict
            Dictionary with information about the Normal Mode Sampling class
        """"""
        
        info = super().get_info()
        info.update({
            'nms_temperature': self.nms_temperature,
            'nms_nsamples': self.nms_nsamples,
            'nms_limit_com_shift': self.nms_limit_com_shift,
            'nms_limit_mode_shift': self.nms_limit_mode_shift,
            'nms_save_displacements': self.nms_save_displacements,
            })
        
        return info

    def run_systems(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        nms_indices: Optional[List[int]] = None,
        nms_exclude_modes: Optional[List[int]] = None,
        nms_frequency_range: Optional[List[Tuple[str, float]]] = None,
        nms_clean: Optional[bool] = True,
        **kwargs,
    ):
        """"""
        Perform Normal Mode Scanning on the sample system.
        Iterate over systems using 'sample_num_threads' threads.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue, optional, default None
            Queue object including sample systems or where 'sample_systems' 
            input will be added. If not defined, an empty queue will be 
            assigned.
        nms_indices: list[int], optional, default None
            List of atom indices to include in normal mode analysis.
            If none, indices if a full list of atom indices with length ot the
            atom number of the system.
            Atom indices from atoms constraint by FixAtoms are removed from
            index list and the normal mode analysis.
        nms_exclude_modes: list[int], optional, default None
            List of vibrational modes, sorted by wave number, to exclude
            from the sampling procedure.
        nms_frequency_range: list[tuple(str, float)], optional, default None
            Frequency range conditions for normal modes to be included in the
            sampling.
        nms_clean: bool, optional, default True
            If True, checkpoint files for atom displacement calculations
            in {sample_directory}/vib_{isample} will be deleted.
            Else, results from available  checkpoint files will be used.
        """"""

        # Check sample system queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue()

        # Optimize sample systems or take as normal mode analysis input
        if self.sample_systems_optimize:
            
            # Add stop flag
            for _ in range(self.sample_num_threads):
                sample_systems_queue.put('stop')
            
            # Initialize continuation flag
            self.thread_keep_going = np.array(
                [True for ithread in range(self.sample_num_threads)],
                dtype=bool
                )
            
            # Initialize optimized sample system into queue
            sample_input_queue = queue.Queue()
            
            if self.sample_num_threads == 1:

                # Run sample system optimization
                self.run_optimization(
                    sample_systems_queue=sample_systems_queue,
                    sample_optimzed_queue=sample_input_queue)
            
            else:

                # Create threads for sample system optimization
                threads = [
                    threading.Thread(
                        target=self.run_optimization,
                        kwargs={
                            'sample_systems_queue': sample_systems_queue,
                            'sample_optimzed_queue': sample_input_queue,
                            'ithread': ithread}
                        )
                    for ithread in range(self.sample_num_threads)]

                # Start threads
                for thread in threads:
                    thread.start()

                # Wait for threads to finish
                for thread in threads:
                    thread.join()

        else:
            
            # Set sample system queue as optimized sample system queue
            sample_input_queue = sample_systems_queue

        # Run normal mode sampling
        while not sample_input_queue.empty():
            self.run_system(
                sample_input_queue,
                nms_indices,
                nms_exclude_modes,
                nms_frequency_range,
                nms_clean,
                **kwargs)
        
        return

    def run_system(
        self,
        sample_systems_queue: queue.Queue,
        nms_indices: List[int],
        nms_exclude_modes: List[int],
        nms_frequency_range: List[Tuple[str, float]],
        nms_clean: bool,
        **kwargs
    ):
        """"""
        Perform Normal Mode Sampling on the sample system.

        Parameters
        ----------
        sample_systems_queue: queue.Queue
            Queue object including sample systems.

        """"""

        # Initialize normal mode analysis queue
        sample_calculate_queue = queue.Queue()

        # Get sample system for normal mode analysis
        (system, isample, source, index) = sample_systems_queue.get()

        # Print sampler info
        message = (
            ""Start normal mode sampling of the system ""
            + f""from '{source}' of index {index:d}."")
        self.logger.info(message)
        
        # Get non-fixed atoms indices
        if nms_indices is None:
            atom_indices = np.arange(
                system.get_global_number_of_atoms(), dtype=int)
        else:
            atom_indices = np.array(nms_indices, dtype=int)
        for constraint in system.constraints:
            if isinstance(constraint, FixAtoms):
                atom_indices = [
                    idx for idx in atom_indices
                    if idx not in constraint.index]
        atom_indices = np.array(atom_indices)
        
        # Prepare system parameter
        Natoms = len(atom_indices)
        Nmodes = 3*Natoms

        # Initialize Vibration class
        system_vibrations = Vibrations_Asparagus(
            self,
            system,
            self.sample_calculator,
            self.sample_calculator_args,
            self.nms_save_displacements,
            indices=atom_indices,
            name=os.path.join(self.sample_directory, f""vib_{isample:d}""),
            **kwargs
            )
        if nms_clean:
            system_vibrations.clean()
        
        # Add jobs to calculation queue
        system_vibrations.add_calculations(
            sample_calculate_queue)

        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_calculate_queue.put('stop')
        
        # Initialize continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )

        if self.sample_num_threads == 1:
            
            # Run job calculations
            system_vibrations.run(
                sample_calculate_queue,
                sample_calculator=self.sample_calculator,
                sample_calculator_args=self.sample_calculator_args)
        
        else:

            # Create threads for job calculations
            threads = [
                threading.Thread(
                    target=system_vibrations.run,
                    args=(sample_calculate_queue, ),
                    kwargs={
                        'sample_calculator': self.sample_calculator,
                        'sample_calculator_args': self.sample_calculator_args,
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()

        # Print sampler info
        message = (
            ""Normal mode analysis calculation completed for ""
            + f""the system from '{source}' of index {index:d}."")
        self.logger.info(message)
        
        # Finish normal mode analysis and diagonalize Hessian matrix
        system_vibrations.summary(**kwargs)
        
        # Get initial (equilibrium) energy of the sampling system
        system_initial_results = system_vibrations.get_initial_results()
        
        # Store equilibrium positions
        system_initial_positions = system.get_positions()

        # (Trans. + Rot. + ) Vibrational frequencies in cm**-1
        system_frequencies = system_vibrations.get_frequencies()

        # (Trans. + Rot. + ) Vibrational modes normalized to 1
        system_modes = np.array([
            system_vibrations.get_mode(imode)[atom_indices].reshape(Natoms, 3)
            / np.sqrt(np.sum(
                system_vibrations.get_mode(imode)[atom_indices].reshape(
                    Natoms, 3)**2))
            for imode in range(Nmodes)])

        # Reduced mass per mode (in amu)
        system_redmass = np.array([
            1./np.sum(
                system_modes[imode]**2
                / system.get_masses()[atom_indices].reshape(Natoms, 1))
            for imode in range(Nmodes)])

        # Force constant per mode (in eV/Angstrom**2)
        system_forceconst = (
            4.0*np.pi**2*(np.abs(system_frequencies)*1.e2*units._c)**2
            * system_redmass*units._amu*units.J*1.e-20)

        # Compute and store equilibrium positions and center of mass,
        # moments of inertia and principle axis of inertia
        system_init_positions = system.get_positions()
        system_init_com = system[atom_indices].get_center_of_mass()

        # Compute and compare same quantities for displaced system
        system_com_shift = np.zeros(Nmodes, dtype=float)
        system_mode_shift = np.zeros(Nmodes, dtype=float)

        for imode, mode in enumerate(system_modes):

            # Normal mode displacements
            check_scale = 0.1
            system_mode_positions = system_init_positions.copy()
            system_mode_positions[atom_indices] += mode*check_scale
            
            # COM shift
            system.set_positions(system_mode_positions, apply_constraint=False)
            system_displ_com = system[atom_indices].get_center_of_mass()
            system_com_shift[imode] = np.sqrt(np.sum(
                (system_init_com - system_displ_com)**2))

            # Mode shift
            system_mode_shift[imode] = (
                np.sum(
                    np.sum(
                        system.get_masses().reshape(Natoms, 1)
                        * mode*check_scale
                        / np.sum(system.get_masses()),
                        axis=0
                    )**2
                )
            )

        # Reset system positions
        system.set_positions(system_init_positions)

        # Vibrational modes are assumed with center of mass shifts and mode
        # shifts smaller than the threshold
        system_vib_modes = np.logical_and(
            system_com_shift < self.nms_limit_com_shift,
            system_mode_shift < self.nms_limit_mode_shift)

        # Apply exclusion list if defined
        if nms_exclude_modes is not None:

            for imode in nms_exclude_modes:
                if imode < len(system_vib_modes):
                    system_vib_modes[imode] = False
                else:
                    self.logger.warning(
                        f""Vibrational mode {imode:d} in the ""
                        + ""exclusion list is larger than the number of ""
                        + ""vibrational modes!"")

        if nms_frequency_range is not None:
            
            # Initially include all modes
            include_modes = np.ones_like(system_vib_modes)
            
            # Iterate over all exclusion conditions
            for (condition, frequency) in nms_frequency_range:
                
                # Compare absolute if requested
                if '||' in condition:
                    comp_frequencies = np.abs(system_frequencies)
                else:
                    comp_frequencies = system_frequencies
                
                # Modes are still include if conditions are matched
                if '<=' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies <= frequency)
                elif '>=' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies >= frequency)
                elif '<' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies < frequency)
                elif '>' in condition:
                    include_modes = np.logical_and(
                        include_modes, 
                        comp_frequencies >= frequency)
                else:
                    raise SyntaxError(
                        f""Normal mode selection condition '{condition}' in ""
                        + ""'nms_frequency_range' selection input is not ""
                        + ""recognized! Choose between ('<', '<=', '>=', '>') ""
                        + ""or for comparing absolute frequencies ""
                        + ""('<||', '<=||', '>=||', '>||')"")

            # Combine normal mode exclusion list
            system_vib_modes = np.logical_and(system_vib_modes, include_modes)

        # Add normal mode analysis results to log file
        message = ""\nStart Normal Mode Sampling at system: ""
        if self.sample_data_file is None:
            message += f""{system.get_chemical_formula():s}\n""
        else:
            message += f""{self.sample_data_file[0]:s}\n""
        message = (
            f"" {'Index':5s} |""
            + f"" {'Frequency (cm**-1)':18s} |""
            + f"" {'Vib. Mode':9s} |""
            + f"" {'CoM displacemnt':<16s} |""
            + f"" {'Mode displacemnt':<16s}\n""
            + f"" {'':5s} |""
            + f"" {'':18s} |""
            + f"" {'Selected':9s} |""
            + f"" {'(< ' + f'{self.nms_limit_com_shift:5.2E}' + ')':<16s} |""
            + f"" {'(< ' + f'{self.nms_limit_mode_shift:5.2E}' + ')':<16s}\n""
            + "" ""
            + ""-""*(7 + 21 + 11 + 2*19)
            + ""\n"")
        for ivib, freq in enumerate(system_frequencies):
            message += f"" {ivib + 1:5d} | {freq:18.2f} |""
            if system_vib_modes[ivib]:
                message +=  f"" {'   x   ':9s} |""
            else:
                message +=  f"" {'':9s} |""
            message +=  f"" {system_com_shift[ivib]:16.2E} |""
            message +=  f"" {system_mode_shift[ivib]:16.2E}\n""
        with open(self.sample_log_file.format(isample), 'a') as flog:
            flog.write(message)
        self.logger.info(message)

        # Initialize normal mode sampling queue
        sample_calculate_queue = queue.Queue()

        # Initialize sample number list
        self.Nsamples = [0 for ithread in range(self.sample_num_threads)]

        # Initialize continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )

        # Start calculation run
        if self.sample_num_threads == 1:
            
            # Add calculation jobs
            for irun in range(self.nms_nsamples):
                sample_position = self.get_sample_positions(
                    system_initial_positions,
                    Nmodes, 
                    system_modes, 
                    system_redmass, 
                    system_forceconst,
                    system_vib_modes,
                    self.nms_temperature)
                sample_calculate_queue.put((isample, irun, sample_position))
            
            # Add stop flag
            for _ in range(self.sample_num_threads):
                sample_calculate_queue.put('stop')
                
            # Run job calculations
            self.run_sampling(
                system,
                sample_calculate_queue)
        
        else:

            # Create threads for job calculations
            threads = [
                threading.Thread(
                    target=self.run_sampling,
                    args=(
                        system,
                        sample_calculate_queue),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Add calculation jobs
            for irun in range(self.nms_nsamples):
                sample_position = self.get_sample_positions(
                    system_initial_positions,
                    Nmodes, 
                    system_modes, 
                    system_redmass, 
                    system_forceconst,
                    system_vib_modes,
                    self.nms_temperature)
                sample_calculate_queue.put((isample, irun, sample_position))
            
            # Add stop flag
            for _ in range(self.sample_num_threads):
                sample_calculate_queue.put('stop')

            # Wait for threads to finish
            for thread in threads:
                thread.join()

        # Print sampling info
        for ithread in range(self.sample_num_threads):
            if self.Nsamples[ithread] == 0:
                message += f""No samples written to ""
            else:
                message = (
                    f""Sampling method '{self.sample_tag:s}' complete for ""
                    + f""system from '{source}!'\n"")
                message += f""{self.Nsamples[ithread]:d} sample written to ""
                if self.Nsamples[ithread] == 1:
                    message += ""sample ""
                else:
                    message += f""samples ""
                message += ""written to ""
            message += f""'{self.sample_data_file[0]:s}'.""
            self.logger.info(message)

        return

    def run_sampling(
        self,
        sample_system: ase.Atoms,
        sample_calculate_queue: queue.Queue,
        ithread: Optional[int] = None
    ):
        """"""
        Run normal mode scanning for normal mode combinations.
        
        Parameters
        ----------
        sample_system: ase.Atoms
            Initial/equilibrium sample system to apply on the normal mode
            combinations.
        sample_systems_queue: queue.Queue
            Queue containing normal mode combination parameters
        ithread: int, optional, default None
            Thread number

        """"""

        # Get ASE calculator
        ase_calculator, ase_calculator_tag = (
            interface.get_ase_calculator(
                self.sample_calculator,
                self.sample_calculator_args,
                ithread=ithread)
            )
        
        # Assign calculator
        system = sample_system.copy()
        system.set_calculator(ase_calculator)

        while self.keep_going(ithread):
            
            # Get sample parameters or wait
            sample = sample_calculate_queue.get()

            # Check for stop flag
            if sample == 'stop':
                self.thread_keep_going[ithread] = False
                continue

            # Extract normal mode combination parameters
            (isample, irun, sample_positions) = sample

            # Set sample positions to calculate
            system.set_positions(sample_positions)

            # Compute observables and check potential threshold
            system, converged = self.run_calculation(system)

            # Add to dataset
            if converged:
                self.save_properties(system, ithread)

            # Attach to trajectory
            if converged and self.sample_save_trajectory:
                self.write_trajectory(
                    system, self.sample_trajectory_file.format(isample))

        return
        
    def get_sample_positions(
        self, 
        initial_positions, 
        Nmodes: int,
        vib_modes: List[float],
        vib_masses: List[float], 
        vib_fcnts: List[float],
        vib_include: List[bool],
        vib_temp: float
    ):
        """"""
        Create the new coordinates for the system.

        Parameters
        ----------
        initial_positions: np.ndarray(float)
            Sample system position to add normal mode displacement
        Nmodes: int
            Normal modes of the system
        vib_modes: np.ndarray(float)
            Normalized vibrational modes
        vib_masses: np.ndarray(float)
            Reduced mass of the normal modes
        vib_fcnts: np.ndarray(float)
            Force constant per normal mode (in eV/Angstrom**2)
        vib_include: np.ndarray(bool)
            Selection of normal mode to include in sampling
        vib_temp: float
            Temperature in Kelvin to sample the normal modes.

        Returns
        -------
        np.ndarray
            Sample coordinates for the system

        """"""

        Natoms = initial_positions.shape[0]
        Rx = self.R(Natoms, Nmodes, vib_fcnts, vib_temp)
        sample_positions = initial_positions.copy()

        for imode, mode_i in enumerate(vib_modes):
            if vib_include[imode]:
                disp_i = Rx[imode]*mode_i/np.linalg.norm(mode_i)
                sample_positions += disp_i

        return sample_positions

    def R(self, Natoms, Nmodes, fcnts, temp):
        """"""
        Made a random displacement for each of the modes in the system.
        The displacements follow a Bernoulli distribution with $P=0.5$

        Parameters
        ----------
        Natoms: int
            Number of atoms
        Nmodes: int
            Number of modes in the system
        fcnts: np.ndarray(float)
            Force constant per mode (in eV/Angstrom**2)
        temp: float
            Temperature in Kelvin to sample the normal modes.

        Returns
        -------
        np.ndarray
            Array of displacements for each mode in the system

        """"""

        # Uniform random number in the range [0, 1]
        random_num = np.random.uniform(0, 1, size=Nmodes)
        random_num *= np.random.uniform(0, 1)/np.sum(random_num)
        
        # Sign selection
        sign = np.random.choice([-1.0, 1.0], size=Nmodes)

        # Constraint force constants to a minimum to avoid large displacement
        # factors (R ~ 1/fcnts), mostly for low frequencies such as 
        # translation or rotation if not removed from sampling otherwise.
        fix_fcnts = [0.10 if i < 0.10 else i for i in fcnts]

        # Get random displacement factors
        R = []
        for i, fcnt in enumerate(fix_fcnts):
            R_i = sign[i] * np.sqrt(3*random_num[i]*Natoms*units.kB*temp/fcnt)
            R.append(R_i)

        return np.array(R)",./Asparagus/asparagus/sampling/nms.py
Vibrations_Asparagus,"class Vibrations_Asparagus(Vibrations_ASE):
    """"""
    Calculate vibrational modes using finite difference or calculated Hessian 
    directly.
    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    def __init__(
        self,
        sampler,
        sample_atoms,
        sample_calculator,
        sample_calculator_args,
        nms_save_displacements,
        indices=None, 
        name='vib',
        delta=0.01, 
        nfree=2,
        **kwargs
    ):
        
        # Assign calculator to atoms object
        sample_atoms = sampler.assign_calculator(
            sample_atoms,
            sample_calculator=sample_calculator,
            sample_calculator_args=sample_calculator_args)
        
        # Initialize ASE Vibrations parent class
        super().__init__(
            sample_atoms, 
            indices=indices, 
            name=name, 
            delta=delta, 
            nfree=nfree
            )
        
        # Check for Hessian in calculator implemented properties
        if 'hessian' in self.calc.implemented_properties:
            self.hessian_avail = True
        else:
            self.hessian_avail = False
        
        # Assign class parameter
        self.sampler = sampler
        self.nms_save_displacements = nms_save_displacements

        # Initialize equilibrium/initial sample system result dictionary
        self.eq_results = {}

        return

    def add_calculations(
        self,
        calculate_queue: queue.Queue,
    ):
        """"""
        Add vibration calculations to job queue.
        
        Parameters
        ----------
        calculate_queue : queue.Queue
            Sample system queue
        """"""
        
        # Check for writing rights and old result files
        if not self.cache.writable:
            raise RuntimeError(
                ""Cannot run calculation. ""
                + ""Cache must be removed or split in order ""
                + ""to have only one sort of data structure at a time."")
        self._check_old_pickles()
        
        # Add jobs to compute Hessian or forces at atom displacements to the
        # job queue
        if self.hessian_avail:
            
            displacements = self.displacements()
            eq_disp = next(displacements)
            assert eq_disp.name == 'eq'
            calculate_queue.put(
                (self.atoms, eq_disp.name))
            
        else:
            
            for disp, atoms in self.iterdisplace(inplace=False):
                calculate_queue.put(
                    (atoms, disp.name))
                
        return calculate_queue
    
    def run(
        self,
        calculate_queue: queue.Queue,
        sample_calculator: Optional[Union[str, object]] = None,
        sample_calculator_args: Optional[Dict[str, Any]] = None,
        ithread: Optional[int] = None,
    ):
        """"""
        Run the vibration calculations.
        
        Parameters
        ----------
        calculate_queue : queue.Queue
            Sample system queue
        sample_calculator : (str, object), optional, default None
            ASE calculator object or string of an ASE calculator class
            name to assign to the sample systems
        sample_calculator_args : dict, optional, default None
            Dictionary of keyword arguments to initialize the ASE
            calculator
        ithread: int, optional, default None
            Thread number
        """"""

        # Get ASE calculator
        ase_calculator, ase_calculator_tag = (
            interface.get_ase_calculator(
                sample_calculator,
                sample_calculator_args,
                ithread=ithread)
            )

        while self.sampler.keep_going(ithread):
        
            # Get job parameters or wait
            sample = calculate_queue.get()

            # Check for stop flag
            if sample == 'stop':
                self.sampler.thread_keep_going[ithread] = False
                continue

            # Get job to run
            (atoms, name) = sample
            
            # Assign calculator
            atoms.calc = ase_calculator

            # Run job
            with self.cache.lock(name) as handle:

                # Read results if result file exist
                if handle is None:

                    results = self.cache[name]
                    atoms.calc.results = results

                # Or run calculation
                else:

                    # Compute essential results
                    atoms, results = self.run_calculation(atoms)

                    # Add additional results
                    for prop, result in ase_calculator.results.items():
                        results[prop] = result

                    if world.rank == 0:
                        handle.save(results)

            # Store results for equilibrium/initial sample system
            if name == 'eq':
                self.eq_results = results.copy()
                atoms_properties = self.sampler.get_properties(atoms)
                self.sampler.sample_dataset.add_atoms(
                    atoms, atoms_properties)
            else:
                # Store results of for atom displacement calculations if
                # requested
                if self.nms_save_displacements:
                    atoms_properties = self.sampler.get_properties(atoms)
                    self.sampler.sample_dataset.add_atoms(
                        atoms, atoms_properties)

        return

    def run_calculation(
        self,
        atoms: ase.Atoms,
        try_again: Optional[bool] = True,
    ) -> (ase.Atoms, Dict[str, np.ndarray]):
        """"""
        Apply calculator on atoms input
        
        Parameters
        ----------
        system: ase.Atoms
            ASE atoms object with linked calculator to run the reference
            calculation with.
        try_again: bool, optional, default True
            If the calculation failed (converged == False), try the reference
            calculation for a second time with 'try_again=False' then.

        Returns
        -------
        ase.Atoms
            ASE atoms object with linked calculator after reference calculation
            was run.
        dict(str, np.ndarray)
            Essential result array containing hessian or forces.

        """"""

        # Initialize result dictionary
        results = {}
        
        try:

            # Compute essential results
            if self.hessian_avail:
                results['hessian'] = atoms.get_hessian()
            else:
                results['forces'] = atoms.get_forces()

            if hasattr(atoms.calc, 'converged'):
                converged = atoms.calc.converged
            elif self.hessian_avail and results['hessian'] is None:
                converged = False
            elif results['forces'] is None:
                converged = False
            else:
                converged = True                

        except (
            ase.calculators.calculator.CalculationFailed
            or subprocess.CalledProcessError
        ):

            converged = False

        if not converged and try_again:
            atoms, results = self.run_calculation(atoms, try_again=False)

        return atoms, results

    def get_initial_results(
        self,
    ):
        """"""
        Return equilibrium/initial sample system results
        """"""
        return self.eq_results

    def summary(
        self,
        method='standard',
        direction='central',
        **kwargs,
    ):
        """"""
        Run modified summary function with respect to the ASE version.
        """"""
        
        energies = self.get_energies(method=method, direction=direction)
        summary_lines = VibrationsData._tabulate_from_energies(energies)
        log_text = '\n'.join(summary_lines) + '\n'
        self.logger.info(log_text)
        
        return
        
    def read(
        self,
        method='standard', 
        direction='central'
    ):
        """"""
        Read calculation results, get Hessian and solve eigenvalue problem.
        """"""
        
        self.method = method
        self.direction = direction
        assert self.method in ['standard', 'frederiksen']
        assert self.direction in ['central', 'forward', 'backward']

        # Initialize Hesse matrix
        n = 3 * len(self.indices)
        H = np.empty((n, n))
        
        eq_disp = self._eq_disp()
        
        if self.hessian_avail:
            
            Hfull = eq_disp.vib._cached['hessian']
            selection = np.zeros(n, dtype=int)
            for ii, index in enumerate(self.indices):
                selection[3*ii + 0] = 3*index + 0
                selection[3*ii + 1] = 3*index + 1
                selection[3*ii + 2] = 3*index + 2
            H = Hfull[selection, selection]

        else:
            
            r = 0
            
            if self.direction != 'central':
                feq = eq_disp.forces()

            for a, i in self._iter_ai():
                disp_minus = self._disp(a, i, -1)
                disp_plus = self._disp(a, i, 1)
                fminus = disp_minus.forces()
                fplus = disp_plus.forces()
                if self.method == 'frederiksen':
                    fminus[a] -= fminus.sum(0)
                    fplus[a] -= fplus.sum(0)
                if self.nfree == 4:
                    fminusminus = self._disp(a, i, -2).forces()
                    fplusplus = self._disp(a, i, 2).forces()
                    if self.method == 'frederiksen':
                        fminusminus[a] -= fminusminus.sum(0)
                        fplusplus[a] -= fplusplus.sum(0)
                if self.direction == 'central':
                    if self.nfree == 2:
                        H[r] = .5 * (fminus - fplus)[self.indices].ravel()
                    else:
                        assert self.nfree == 4
                        H[r] = H[r] = (-fminusminus +
                                    8 * fminus -
                                    8 * fplus +
                                    fplusplus)[self.indices].ravel() / 12.0
                elif self.direction == 'forward':
                    H[r] = (feq - fplus)[self.indices].ravel()
                else:
                    assert self.direction == 'backward'
                    H[r] = (fminus - feq)[self.indices].ravel()
                H[r] /= 2 * self.delta
                r += 1

            
        H += H.copy().T
        self.H = H
            
        masses = self.atoms.get_masses()
        if any(masses[self.indices] == 0):
            raise RuntimeError('Zero mass encountered in one or more of '
                               'the vibrated atoms. Use Atoms.set_masses()'
                               ' to set all masses to non-zero values.')

        self.im = np.repeat(masses[self.indices]**-0.5, 3)
        self._vibrations = self.get_vibrations(read_cache=False)

        omega2, modes = np.linalg.eigh(self.im[:, None] * H * self.im)
        self.modes = modes.T.copy()

        # Conversion factor:
        s = units._hbar * 1e10 / np.sqrt(units._e * units._amu)
        self.hnu = s * omega2.astype(complex)**0.5
        
        return",./Asparagus/asparagus/sampling/nms.py
MCSampler,"class MCSampler(sampling.Sampler):
    """"""
    A very simple Monte Carlo (MC) sampler class.
    Uses the Metropolis algorithm to generate samples for a molecule.

    Parameters
    ----------
    config: (str, dict, settings.Configuration)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    mc_temperature: float, optional, default 300
        Temperature of the MC simulation in Kelvin.
    mc_steps: integer, optional, default 1000
        Number of MC simulations steps
    mc_save_interval: int, optional, default 1
        Step interval to store system properties of the current frame.
    mc_max_displacement: float
        Maximum displacement of the MC simulation in Angstrom.
    
    Returns
    -------
    object
        Monta-Carlo Sampler class object
    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for sample module
    sampling.Sampler._default_args.update({
        'mc_temperature':               300.,
        'mc_steps':                     1000,
        'mc_max_displacement':          0.1,
        'mc_save_interval':             1,
        })
    
    # Expected data types of input variables
    sampling.Sampler._dtypes_args.update({
        'mc_temperature':               [utils.is_numeric],
        'mc_steps':                     [utils.is_numeric],
        'mc_max_displacement':          [utils.is_numeric],
        'mc_save_interval':             [utils.is_integer],
        })

    def __init__(
        self,
        config: Optional[Union[str, dict, settings.Configuration]] = None,
        config_file: Optional[str] = None, 
        mc_temperature: Optional[float] = None,
        mc_steps: Optional[int] = None,
        mc_max_displacement: Optional[float] = None,
        mc_save_interval: Optional[int] = None,
        **kwargs,
    ):

        # Sampler class label
        self.sample_tag = 'mc'

        # Initialize parent class
        super().__init__(
            sample_tag=self.sample_tag,
            config=config,
            config_file=config_file,
            **kwargs
            )
        
        ################################
        # # # Check MC Class Input # # #
        ################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)
        
        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, sampling),
            check_dtype=utils.get_dtype_args(self, sampling)
        )
        
        # Check sample properties for energy properties which are required for 
        # MC sampling
        if 'energy' not in self.sample_properties:
            self.sample_properties.append('energy')

        return

    def get_info(self):
        """"""
        Returns a dictionary with the information of the MC sampler.
        
        Returns
        -------
        dict
            Dictionary with the information of the MC sampler.
        """"""
        
        info = super().get_info()
        info.update({
            'mc_temperature': self.mc_temperature,
            'mc_steps': self.mc_steps,
            'mc_max_displacement': self.mc_max_displacement,
            'mc_save_interval': self.mc_save_interval,
            })

        return info

    def run_systems(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        **kwargs,
    ):
        """"""
        Perform Normal Mode Scanning on the sample system.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue, optional, default None
            Queue object including sample systems or to which 'sample_systems' 
            input will be added. If not defined, an empty queue will be 
            assigned.
        """"""

        # Check sample system queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue()

        # Initialize thread continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )
        
        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_systems_queue.put('stop')

        # Initialize sample number list
        self.Nsamples = [0 for ithread in range(self.sample_num_threads)]

        if self.sample_num_threads == 1:
            
            self.run_system(sample_systems_queue)
        
        else:

            # Create threads
            threads = [
                threading.Thread(
                    target=self.run_system, 
                    args=(sample_systems_queue, ),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()

        return

    def run_system(
        self, 
        sample_systems_queue: queue.Queue,
        ithread: Optional[int] = None,
    ):
        """"""
        Perform a very simple MC Simulation using the Metropolis algorithm with
        the sample system.

        Parameters
        ----------
        sample_systems_queue: queue.Queue
            Queue of sample system information providing tuples of ASE atoms
            objects, index number and respective sample source and the total
            sample index.
        ithread: int, optional, default None
            Thread number

        """"""

        while self.keep_going(ithread):
            
            # Get sample parameters or wait
            sample = sample_systems_queue.get()
            
            # Check for stop flag
            if sample == 'stop':
                if ithread is None:
                    self.thread_keep_going[0] = False
                else:
                    self.thread_keep_going[ithread] = False
                continue

            # Extract sample system to optimize
            (system, isample, source, index) = sample

            # If requested, perform structure optimization
            if self.sample_systems_optimize:

                # Perform structure optimization
                system = self.run_optimization(
                    sample_system=system,
                    sample_index=isample,
                    ithread=ithread)

            # Initialize trajectory file
            if self.sample_save_trajectory:
                trajectory_file = self.sample_trajectory_file.format(isample)
            else:
                trajectory_file = None

            # Assign calculator
            system = self.assign_calculator(
                system,
                ithread=ithread)

            # Perform Monte-Carlo simulation
            self.monte_carlo_steps(
                system, 
                trajectory_file=trajectory_file,
                ithread=ithread)
            
            # Print sampling info
            if ithread is None:
                isample = 0
            else:
                isample = ithread
            if self.Nsamples[isample] == 0:
                message += f""No samples written to ""
            else:
                message = (
                    f""Sampling method '{self.sample_tag:s}' complete for ""
                    + f""system from '{source}!'\n"")
                message += f""{self.Nsamples[isample]:d} sample written to ""
                if self.Nsamples[isample] == 1:
                    message += ""sample ""
                else:
                    message += f""samples ""
                message += ""written to ""
            message += f""'{self.sample_data_file[0]:s}'.""
            self.logger.info(message)
        
        return

    def monte_carlo_steps(
        self,
        system: ase.Atoms,
        Nsteps: Optional[int] = None,
        temperature: Optional[float] = None,
        max_displacement: Optional[float] = None,
        trajectory_file: Optional[str] = None,
        ithread: Optional[int] = None,
    ):
        """"""
        This does a simple Monte-Carlo simulation using the Metropolis
        algorithm.

        In the future we could add more sophisticated sampling methods
        (e.g. MALA or HMC)

        Parameters
        ----------
        system: ase.Atoms
            System to be sampled.
        Nsteps: int, optional, default None
            Number of Monte-Carlo steps to perform.
        temperature: float, optional, default None
            Sample temperature 
        max_displacement: float, optional, default None
            Maximum displacement of the MC simulation in Angstrom.
        trajectory_file: str, optional, default None
            ASE Trajectory file path to append sampled system if requested
        ithread: int, optional, default None
            Thread number

        """"""
    
        # Check sample steps
        if Nsteps is None:
            Nsteps = self.mc_steps
        
        # Check Temperature parameter
        if temperature is None:
            temperature = self.mc_temperature
        beta = 1.0/(units.kB*self.mc_temperature)
        
        # Check maximum atom displacement
        if max_displacement is None:
            max_displacement = self.mc_max_displacement
        
        # Monte-Carlo acceptance and stored sample system counter
        Naccept = 0
        Nsample = 0
        
        # Compute current energy
        system, converged = self.run_calculation(system)
        if converged:
            system_properties = system.calc.results
            current_energy = system_properties['energy']
        else:
            self.logger.error(""Initial MC reference calculation failed!"")
            return

        # Store initial system properties
        self.save_properties(system, ithread)
        if self.sample_save_trajectory:
            self.write_trajectory(system, trajectory_file)
        
        # Get selectable system atoms
        atom_indices = np.arange(
            system.get_global_number_of_atoms(), dtype=int)
        for constraint in system.constraints:
            if isinstance(constraint, FixAtoms):
                atom_indices = [
                    idx for idx in atom_indices
                    if idx not in constraint.index]
        atom_indices = np.array(atom_indices)
        Natoms = len(atom_indices)
        
        # Iterate over MC steps
        for istep in range(Nsteps):

            # First, randomly select an atom
            selected_atom = atom_indices[np.random.randint(Natoms)]

            # Store current selected atom position
            old_position = system.positions[selected_atom].copy()

            # Propose a new random atom position
            displacement = np.random.uniform(
                -max_displacement, max_displacement, 3)
            system.positions[selected_atom] += displacement

            # Get the potential energy of the new system
            system, converged = self.run_calculation(system)
            if converged:
                system_properties = system.calc.results
                new_energy = system_properties['energy']
            else:
                new_energy = np.inf

            # Metropolis acceptance criterion
            threshold = np.exp(-beta*(new_energy - current_energy))

            # If accepted 
            if converged and np.random.rand() < threshold:

                # Set the new position of the atom
                current_energy = new_energy
                Naccept += 1
                
                # Store system properties
                if not Naccept%self.mc_save_interval:
                    self.save_properties(system, ithread)
                    if self.sample_save_trajectory:
                        self.write_trajectory(
                            system, trajectory_file)
            
                # Check if number of sample threshold is reached
                if (
                    self.sample_nsamples_threshold is not None
                    and np.sum(self.Nsamples) >= self.sample_nsamples_threshold
                ):
                    break

            # If not accepted
            else:
                
                # Reset the position of the atom
                system.positions[selected_atom] = old_position

        return",./Asparagus/asparagus/sampling/mc.py
MetaSampler,"class MetaSampler(sampling.Sampler):
    """"""
    Meta(-Dynamic) Sampler class

    Parameters
    ----------
    config: (str, dict, settings.Configuration)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    meta_cv: list(list(int)), optional, default []
        List of sublists defining collective variables (CVs) / reaction
        coordinates to add Gaussian potentials. The number of atom indices
        in the sublist defines either bonds (2), angles (3) or
        dihedrals (4). Example [[1, 2], [1, 2, 3], [4, 1, 2, 3]]
    meta_gaussian_height: float, optional, default 0.05
        Potential energy height in eV of the Gaussian potential.
    meta_gaussian_widths: (float, list(floats)), optional, default 0.1
        Gaussian width for all CVs or a list of widths per CV that define
        the FWHM of Gaussian potential.
    meta_gaussian_interval: int, optional, default 10
        Step interval to add Gaussian potential at current set of
        collective variable.
    meta_hookean: list(list(int,float)), optional, default []
        It is always recommended for bond type collective variables to
        define a hookean constraint that limit the distance between two
        atoms. Otherwise gaussian potential could be only added to an ever
        increasing atoms bond distance. Hookean are defined by a list of
        sublists containing first two atom indices followed by one upper
        distance limit and, optionally, a Hookean force constant k (default
        is defined by meta_hookean_force_constant).
        For example: [1, 2, 4.0] or [1, 2, 4.0, 5.0]
    meta_hookean_force_constant: float, optional, default 5.0
        Default Hookean force constant if not specifically defined in
        Hookean constraint list meta_hookean.
    meta_temperature: float, optional, default 300
        Target temperature in Kelvin of the MD simulation controlled by a
        Langevin thermostat
    meta_time_step: float, optional, default 1.0 (1 fs)
        MD Simulation time step in fs
    meta_simulation_time: float, optional, default 1E5 (100 ps)
        Total MD Simulation time in fs
    meta_save_interval: int, optional, default 10
        MD Simulation step interval to store system properties of
        the current frame to dataset.
    meta_langevin_friction: float, optional, default 0.1
        Langevin thermostat friction coefficient in Kelvin. The coefficient
        should be much higher than in classical MD simulations  (1E-2 to
        1E-4) due to the fast heating of the systems due to the Gaussian
        potentials.
    meta_initial_velocities: bool, optional, default False
        Instruction flag if initial atom velocities are assigned with
        respect to a Maxwell-Boltzmann distribution at temperature
        'md_initial_temperature'.
    meta_initial_temperature: float, optional, default 300
        Temperature for initial atom velocities according to a Maxwell-
        Boltzmann distribution.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for sample module
    sampling.Sampler._default_args.update({
        'meta_cv':                      [],
        'meta_gaussian_height':         0.05,
        'meta_gaussian_widths':         0.1,
        'meta_gaussian_interval':       10,
        'meta_hookean':                 [],
        'meta_hookean_force_constant':  5.0,
        'meta_temperature':             300.,
        'meta_time_step':               1.,
        'meta_simulation_time':         1.E5,
        'meta_save_interval':           100,
        'meta_langevin_friction':       1.E-0,
        'meta_initial_velocities':      False,
        'meta_initial_temperature':     300.,
        'meta_parallel':                False,
        })
    
    # Expected data types of input variables
    sampling.Sampler._dtypes_args.update({
        'meta_cv':                      [utils.is_array_like],
        'meta_gaussian_height':         [utils.is_numeric],
        'meta_gaussian_width':          [
            utils.is_numeric, utils.is_numeric_array],
        'meta_gaussian_interval':       [utils.is_integer],
        'meta_hookean':                 [utils.is_array_like],
        'meta_hookean_force_constant':  [utils.is_numeric],
        'meta_temperature':             [utils.is_numeric],
        'meta_time_step':               [utils.is_numeric],
        'meta_simulation_time':         [utils.is_numeric],
        'meta_save_interval':           [utils.is_integer],
        'meta_langevin_friction':       [utils.is_numeric],
        'meta_initial_velocities':      [utils.is_bool],
        'meta_initial_temperature':     [utils.is_numeric],
        'meta_parallel':                [utils.is_bool],
        })

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None, 
        meta_cv: Optional[List[int]] = None,
        meta_gaussian_height: Optional[float] = None,
        meta_gaussian_widths: Optional[Union[float, List[float]]] = None,
        meta_gaussian_interval: Optional[int] = None,
        meta_hookean: Optional[List[Union[int, float]]] = None,
        meta_hookean_force_constant: Optional[float] = None,
        meta_temperature: Optional[float] = None,
        meta_time_step: Optional[float] = None,
        meta_simulation_time: Optional[float] = None,
        meta_save_interval: Optional[float] = None,
        meta_langevin_friction: Optional[float] = None,
        meta_initial_velocities: Optional[bool] = None,
        meta_initial_temperature: Optional[float] = None,
        meta_parallel: Optional[bool] = None,
        **kwargs,
    ):
        """"""
        Initialize Normal Mode Scanning class

        """"""
        
        # Sampler class label
        self.sample_tag = 'meta'

        # Initialize parent class
        super().__init__(
            sample_tag=self.sample_tag,
            config=config,
            config_file=config_file,
            **kwargs
            )
        
        ##################################
        # # # Check Meta Class Input # # #
        ##################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)
        
        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, sampling),
            check_dtype=utils.get_dtype_args(self, sampling)
        )

        # Get number of collective variables
        Ncv = len(self.meta_cv)
        
        # Check collective variables
        self.cv_type = []
        self.cv_type_dict = {
            2: 'bond',
            3: 'angle',
            4: 'dihedral',
            5: 'reactive_bond'}
        for icv, cv in enumerate(self.meta_cv):
            
            # Check cv data type
            if not utils.is_array_like(cv):
                raise ValueError(
                    f""Collective variable number {icv:d} is not an integer ""
                    + f""list but of type '{type(cv)}'!"")
            
            # Get cv type
            if self.cv_type_dict.get(len(cv)) is None:
                raise ValueError(
                    f""Collective variable number {icv:} is not of valid ""
                    + f""length but of length '{len(cv):d}'!"")
            else:
                self.cv_type.append(self.cv_type_dict.get(len(cv)))
        
        # Check meta sampling input format
        if utils.is_numeric(self.meta_gaussian_widths):
            self.meta_gaussian_widths = [self.meta_gaussian_widths]*Ncv
        elif Ncv > len(self.meta_gaussian_widths):
            raise ValueError(
                f""Insufficient number of Gaussian width defined ""
                + f""({len(self.meta_gaussian_widths):d}) for the number ""
                + f""of collective variables with {Ncv:d}!"")
        
        # Check Hookean constraints
        for ihk, hk in enumerate(self.meta_hookean):
            
            # Check Hookean list data type
            if not utils.is_numeric_array(hk):
                raise ValueError(
                    f""Hookean constraint number {ihk:d} is not a numeric ""
                    + f""list but of type '{type(hk):s}'!"")
            
            # Check Hookean constraint definition validity
            if len(hk)==3:
                # Add default Hookean force constant
                hk.append(self.meta_hookean_force_constant)
            elif not len(hk)==4:
                raise ValueError(
                    f""Hookean constraint number {ihk:d} is expected to be ""
                    + f""length 3 or 4 but has a length of {len(hk):d}!"")
            
            # Check atom definition type
            for ii, idx in enumerate(hk[:2]):
                if not utils.is_integer(idx):
                    raise ValueError(
                        f""Atom index {ii:d} in Hookean constraint number ""
                        + f""{ihk:d} is not an integer but of type ""
                        + f""{type(idx):s}!"")

        # Define collective variable log file paths
        self.meta_gaussian_log_file = os.path.join(
            self.sample_directory, 
            f'{self.sample_counter:d}_{self.sample_tag:s}_{{:d}}_gaussian.log')
        
        # Check sample properties for energy and forces properties which are 
        # required for Meta sampling
        if 'energy' not in self.sample_properties:
            self.sample_properties.append('energy')
        if 'forces' not in self.sample_properties:
            self.sample_properties.append('forces')

        return

    def get_info(self):
        """"""
        Get information about the current sampler object

        Returns
        -------
        dict
            Dictionary with sampler information
        """"""
        
        info = super().get_info()
        info.update({
            'meta_cv': self.meta_cv,        
            'meta_gaussian_height': self.meta_gaussian_height,
            'meta_gaussian_widths': self.meta_gaussian_widths,
            'meta_gaussian_interval': self.meta_gaussian_interval,
            'meta_hookean': self.meta_hookean,
            'meta_hookean_force_constant': self.meta_hookean_force_constant,
            'meta_temperature': self.meta_temperature,
            'meta_time_step': self.meta_time_step,
            'meta_simulation_time': self.meta_simulation_time,
            'meta_save_interval': self.meta_save_interval,
            'meta_langevin_friction': self.meta_langevin_friction,
            'meta_initial_velocities': self.meta_initial_velocities,
            'meta_initial_temperature': self.meta_initial_temperature,
            'meta_parallel': self.meta_parallel,
            })
        
        return info

    def run_systems(
        self,
        sample_systems_queue: Optional[queue.Queue] = None,
        **kwargs,
    ):
        """"""
        Perform Meta-Dynamics simulations on the sample system.
        
        Parameters
        ----------
        sample_systems_queue: queue.Queue, optional, default None
            Queue object including sample systems or to which 'sample_systems' 
            input will be added. If not defined, an empty queue will be 
            assigned.
        """"""

        # Check sample system queue
        if sample_systems_queue is None:
            sample_systems_queue = queue.Queue
        
        # Initialize thread continuation flag
        self.thread_keep_going = np.array(
            [True for ithread in range(self.sample_num_threads)],
            dtype=bool
            )
        
        # Add stop flag
        for _ in range(self.sample_num_threads):
            sample_systems_queue.put('stop')

        # Initialize sample number list
        self.Nsamples = [0 for ithread in range(self.sample_num_threads)]

        if self.sample_num_threads == 1 or self.meta_parallel:
            
            self.run_system(sample_systems_queue)
        
        else:

            # Create threads
            threads = [
                threading.Thread(
                    target=self.run_system, 
                    args=(sample_systems_queue, ),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.sample_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()

        return

    def run_system(
        self, 
        sample_systems_queue: queue.Queue,
        ithread: Optional[int] = None,
    ):
        """"""
        Perform a coarse Meta-Dynamics simulation with the sample systems.

        Parameters
        ----------
        sample_systems_queue: queue.Queue
            Queue of sample system information providing tuples of ASE atoms
            objects, index number and respective sample source and the total
            sample index.
        ithread: int, optional, default None
            Thread number

        """"""
        
        while self.keep_going(ithread):
            
            # Get sample parameters or wait
            sample = sample_systems_queue.get()
            
            # Check for stop flag
            if sample == 'stop':
                if ithread is None:
                    self.thread_keep_going[0] = False
                else:
                    self.thread_keep_going[ithread] = False
                continue
            
            # Extract sample system to optimize
            (system, isample, source, index) = sample
            
            # If requested, perform structure optimization
            if self.sample_systems_optimize:

                # Perform structure optimization
                system = self.run_optimization(
                    sample_system=system,
                    sample_index=isample,
                    ithread=ithread)

            # Initialize log file
            sample_log_file = self.sample_log_file.format(isample)

            # Initialize trajectory file
            if self.sample_save_trajectory:
                trajectory_file = self.sample_trajectory_file.format(isample)
            else:
                trajectory_file = None

            # Initialize Gaussian log file
            gaussian_log_file = self.meta_gaussian_log_file.format(isample)
            
            # Perform parallel meta dynamics simulation
            # TODO I think this wouldn't work yet ...
            if self.meta_parallel and self.sample_num_threads > 1:
                
                # Prepare parallel meta dynamics simulations
                systems = [
                    system.copy() for _ in range(self.sample_num_threads)]

                # Create threads for job calculations
                threads = [
                    threading.Thread(
                        target=self.run_meta,
                        args=(systems[ithread2], ),
                        kwargs={
                            'gaussian_log_file': gaussian_log_file,
                            'log_file': sample_log_file,
                            'trajectory_file': trajectory_file,
                            'ithread': ithread2},
                        )
                    for ithread2 in range(self.sample_num_threads)]

                # Start threads
                for thread in threads:
                    thread.start()
                    
                # Wait for threads to finish
                for thread in threads:
                    thread.join()
                    
            # Perform single meta dynamics simulation
            else:
            
                self.run_meta(
                    system, 
                    gaussian_log_file=gaussian_log_file,
                    log_file=sample_log_file,
                    trajectory_file=trajectory_file,
                    ithread=ithread)
            
            # Print sampling info
            if ithread is None:
                isample = 0
            else:
                isample = ithread
            message = (
                f""Sampling method '{self.sample_tag:s}' complete for system ""
                + f""of index {index:d} from '{source}!'\n"")
            if self.Nsamples[isample] == 0:
                message += f""No samples written to ""
            if self.Nsamples[isample] == 1:
                message += f""{self.Nsamples[isample]:d} sample written to ""
            else:
                message += f""{self.Nsamples[isample]:d} samples written to ""
            message += f""'{self.sample_data_file[0]:s}'.""
            self.logger.info(message)
            
        return
            
    def run_meta(
        self,
        system: ase.Atoms,
        cv: Optional[List[int]] = None,
        gaussian_height: Optional[float] = None,
        gaussian_widths: Optional[float] = None,
        gaussian_interval: Optional[int] = None,
        gaussian_log_file: Optional[str] = None,
        hookean: Optional[List[Union[int, float]]] = None,
        hookean_force_constant: Optional[float] = None,
        temperature: Optional[float] = None,
        time_step: Optional[float] = None,
        simulation_time: Optional[float] = None,
        langevin_friction: Optional[float] = None,
        initial_velocities: Optional[bool] = None,
        initial_temperature: Optional[float] = None,
        log_file: Optional[str] = None,
        trajectory_file: Optional[str] = None,
        ithread: Optional[int] = None,
    ):
        """"""
        This does a Meta Dynamics simulation using a Meta constraint with a
        Langevin thermostat and Verlocity Verlet algorithm for an NVT ensemble.

        Parameters
        ----------
        system: ase.Atoms
            System to be sampled.
        cv: list(int), optional, default None
            List of sublists defining collective variables (CVs).
        gaussian_height: float, optional, default None
            Potential energy height in eV of the Gaussian potential.
        gaussian_widths: (float, list(floats)), optional, default None
            Gaussian width for all CVs or a list of widths per CV that define
            the FWHM of Gaussian potential.
        gaussian_interval: int, optional, default None
            Step interval to add Gaussian potential.
        gaussian_log_file: str, optional, default None
            Collective variable position log file path
        hookean: list(list(int,float)), optional, default None
            Hookean constraint informations that limit bond distances.
        hookean_force_constant: float, optional, default None
            Default Hookean force constant if not specifically defined in
            Hookean constraint list 'meta_hookean'.
        temperature: float, optional, default None
            Meta dynamics simulation temperature in Kelvin
        time_step: float, optional, default None
            Meta dynamics simulation time step in fs
        simulation_time: float, optional, default None
            Total meta dynamics simulation time in fs
        langevin_friction: float, optional, default None
            Langevin thermostat friction coefficient in Kelvin.
        initial_velocities: bool, optional, default None
            Instruction flag if initial atom velocities are assigned.
        initial_temperature: float, optional, default None
            Temperature for initial atom velocities according to a Maxwell-
            Boltzmann distribution.
        log_file: str, optional, default None
            Log file for sampling information
        trajectory_file: str, optional, default None
            ASE Trajectory file path to append sampled system if requested
        ithread: int, optional, default None
            Thread number

        """"""

        # Check input parameters
        if cv is None:
            cv = self.meta_cv
        if gaussian_height is None:
            gaussian_height = self.meta_gaussian_height
        if gaussian_widths is None:
            gaussian_widths = self.meta_gaussian_widths
        if gaussian_interval is None:
            gaussian_interval = self.meta_gaussian_interval
        if gaussian_log_file is None:
            gaussian_log_file = self.meta_gaussian_log_file.format(0)
        if hookean is None:
            hookean = self.meta_hookean
        if hookean_force_constant is None:
            hookean_force_constant = self.meta_hookean_force_constant
        if temperature is None:
            temperature = self.meta_temperature
        if time_step is None:
            time_step = self.meta_time_step
        if simulation_time is None:
            simulation_time = self.meta_simulation_time
        if langevin_friction is None:
            langevin_friction = self.meta_langevin_friction
        if initial_velocities is None:
            initial_velocities = self.meta_initial_velocities
        if initial_temperature is None:
            initial_temperature = self.meta_initial_temperature    
    
        # Assign calculator
        system = self.assign_calculator(
            system,
            ithread=ithread)

        # Current system constraints
        system_constraints = system.constraints

        # Initialize meta dynamic constraint
        meta_constraint = MetaConstraint(
            cv,
            gaussian_widths,
            gaussian_height,
            gaussian_log_file,
            gaussian_interval)
        
        # Initialize Hookean constraint
        hookean_constraint = []
        for hk in hookean:
            hookean_constraint.append(
                Hookean(hk[0], hk[1], hk[2], rt=hk[3]))
        
        # Set constraints to system
        system.set_constraint(
            [meta_constraint] + hookean_constraint + system_constraints)

        # Set initial atom velocities if requested
        if initial_velocities:
            MaxwellBoltzmannDistribution(
                system, 
                temperature_K=initial_temperature)
        
        # Initialize MD simulation propagator
        meta_dyn = Langevin(
            system, 
            timestep=time_step*units.fs,
            temperature_K=temperature,
            friction=langevin_friction,
            logfile=log_file,
            loginterval=self.meta_save_interval)
        
        # Attach system properties saving function
        meta_dyn.attach(
            self.save_properties,
            interval=self.meta_save_interval,
            system=system,
            ithread=ithread)
        
        # Attach trajectory writer
        if self.sample_save_trajectory:
            meta_dyn.attach(
                self.write_trajectory, 
                interval=self.meta_save_interval,
                system=system,
                trajectory_file=trajectory_file)

        # Attach sample number filter
        if self.sample_nsamples_threshold is not None:
            meta_dyn.attach(
                self.check_sample_number,
                interval=self.meta_save_interval,
                dyn=meta_dyn)

        # Attach collective variables writer
        meta_cv_logger = MetaDynamicLogger(
            meta_constraint, 
            system)
        meta_dyn.attach(
            meta_cv_logger.log, 
            interval=gaussian_interval)
        
        # Run MD simulation
        simulation_steps = round(
            simulation_time/time_step)
        meta_dyn.run(simulation_steps)

        return

    def check_sample_number(
        self,
        dyn: Langevin,
    ):
        """"""
        If number of samples written to the database reach the threshold,
        manipulate ASE dynamics instance to terminate the run.

        Parameters
        ----------
        dyn: ase.md.langevin.Langevin
            ASE Langevin dynamics instance

        """"""
        # Check if number of sample threshold is reached
        if (
            self.sample_nsamples_threshold is not None
            and np.sum(self.Nsamples) >= self.sample_nsamples_threshold
        ):
            dyn.max_steps = dyn.nsteps

        return",./Asparagus/asparagus/sampling/meta.py
MetaConstraint,"class MetaConstraint:
    """"""
    Constraint class to perform Meta Dynamics simulations by adding
    artificial Gaussian potentials.

    Forces atoms of cluster to stay close to the center.

    Parameters
    ----------
    cv : list
        List of collective variables - e.g.:
        [[1,2], [2,4], [1,2,3], [2,3,4,1]]
        [a,b] bond variable between atom a and b
        [a,b,c] angle variable between angle a->b->c
        [a,b,c,d] dihedral angle variable between angle a->b->c->d
    widths : array
        Width if Gaussian distribution for cv i
    heights : float
        Maximum height of the artificial Gaussian potential
    logfile: str
        Log file path
    logwrite: int
        Interval of writing cv coordinates to the log file

    """"""
    
    def __init__(
        self, 
        cv: List[float],
        widths: List[float],
        height: float,
        logfile: str,
        logwrite: int,
    ):
        """"""
        Initialize meta-dynamics constraint class
        """"""
        
        self.cv = cv
        self.Ncv = len(cv)
        self.widths = np.asarray(widths)
        self.height = height
        self.logfile = logfile
        self.logwrite = logwrite
        
        self.cv_list = np.array([], dtype=float)
        self.last_cv = None
        self.ilog = 0
        
        if self.Ncv != len(self.widths):
            raise IOError(
                'Please provide an array of width values of them' +
                'same lengths as the number of collective variables cv.')
        
        self.removed_dof = 0
                
    def get_removed_dof(
        self,
        atoms: ase.Atoms,
    ):
        return 0

    def todict(self):
        dct = {'name': 'MetaConstraints'}
        dct['kwargs'] = {
            'cv': self.cv,
            'widths': self.widths,
            'height': self.height}
        return dct
    
    def adjust_positions(self, atoms, newpositions):
        pass

    def adjust_momenta(self, atoms, momenta):
        pass
    
    def adjust_forces(
        self,
        atoms: ase.Atoms,
        forces: np.ndarray,
    ):
        """"""
        Adjust the forces related to artificial Gaussian potential

        Parameters
        ----------
        atoms : ase.Atoms
            Atoms object
        forces : array
            Forces array of atoms object

        """"""
        
        # Get collective variable i and partial derivative dcvdR
        cv = np.zeros(self.Ncv, dtype=float)
        dcvdR = np.zeros([self.Ncv, *forces.shape], dtype=float)
        for icv, cvi in enumerate(self.cv):
            # Bond length
            if len(cvi)==2:
                cv[icv] = np.linalg.norm(
                    atoms.positions[cvi[0]] - atoms.positions[cvi[1]])
                dcvdR[icv, cvi[0]] = (
                    (atoms.positions[cvi[1]] - atoms.positions[cvi[0]])/cv[icv])
                dcvdR[icv, cvi[1]] = (
                    (atoms.positions[cvi[0]] - atoms.positions[cvi[1]])/cv[icv])
            # Angle
            elif len(cvi)==3:
                # Connection vectors
                ab = atoms.positions[cvi[0]] - atoms.positions[cvi[1]]
                cb = atoms.positions[cvi[2]] - atoms.positions[cvi[1]]
                # Vector distances
                d2ab = np.sum(ab**2)
                dab = np.sqrt(d2ab)
                d2cb = np.sum(cb**2)
                dcb = np.sqrt(d2cb)
                # Vector angles
                cabbc = np.sum(ab*cb)/(dab*dcb)
                cabbc = np.clip(cabbc, -1.0, 1.0)
                sabbc = np.sqrt(1.0 - cabbc**2)
                sabbc = np.clip(sabbc, 1.e-8, None)
                tabbc = np.arccos(cabbc)
                # Add to current collective variable
                cv[icv] = tabbc
                # Compute gradients dcvdRi
                a11 = sabbc*cabbc/d2ab
                a12 = -sabbc/(dab*dcb)
                a22 = sabbc*cabbc/d2cb
                fab = a11*ab + a12*cb
                fcb = a22*cb + a12*ab
                dcvdR[icv, cvi[0]] = -fab
                dcvdR[icv, cvi[1]] = fab + fcb
                dcvdR[icv, cvi[2]] = -fcb
            # Dihedral angle
            elif len(cvi)==4:
                # Connection vectors
                ab = atoms.positions[cvi[0]] - atoms.positions[cvi[1]]
                cb = atoms.positions[cvi[2]] - atoms.positions[cvi[1]]
                dc = atoms.positions[cvi[3]] - atoms.positions[cvi[2]]
                bc = -cb
                ## Plane vectors
                aa = np.cross(ab, bc)
                bb = np.cross(dc, bc)
                ## Vector distances
                d2aa = np.sum(aa**2)
                d2bb = np.sum(bb**2)
                d2bc = np.sum(bc**2)
                dbc = np.sqrt(d2bc)
                dinvab = 1./np.sqrt(d2aa*d2bb)
                # Vector angles
                cabcd = dinvab*np.sum(aa*bb)
                cabcd = np.clip(cabcd, -1.0, 1.0)
                sabcd = dbc*dinvab*np.sum(aa*dc)
                ## Dihedral multiplicity scalars 
                ## (multiplicity: number of minima in dihedral 0 to 360)
                ## Here: multiplicity equal 1
                #p, dfab, ddfab = 1.0, 0.0, 0.0
                #for ii in range(1):
                    #ddfab = p*cabcd - dfab*sabcd
                    #dfab = p*sabcd + dfab*cabcd
                    #p = ddfab
                raise NotImplementedError
            # Reaction coordinate
            elif len(cvi)==5:
                if cvi[0]=='-':
                    a = np.linalg.norm(
                        atoms.positions[cvi[1]] - atoms.positions[cvi[2]])
                    b = np.linalg.norm(
                        atoms.positions[cvi[3]] - atoms.positions[cvi[4]])
                    cv[icv] = a - b
                    dcvdR[icv, cvi[1]] = (
                        (atoms.positions[cvi[2]] - atoms.positions[cvi[1]])/a)
                    dcvdR[icv, cvi[2]] = (
                        (atoms.positions[cvi[1]] - atoms.positions[cvi[2]])/a)
                    dcvdR[icv, cvi[3]] = (
                        (atoms.positions[cvi[3]] - atoms.positions[cvi[4]])/b)
                    dcvdR[icv, cvi[4]] = (
                        (atoms.positions[cvi[4]] - atoms.positions[cvi[3]])/b)
                else:
                    raise NotImplementedError
            else:
                raise IOError('Check cv list of Metadynamic constraint!')
        
        # Put to last cv
        self.last_cv = cv.copy()
        
        # If CV list is empty return zero forces
        # (Important to put it after last_cv update for the add_to_cv function)
        if self.cv_list.shape[0]==0:
            return np.zeros_like(forces)
        
        # Compute Gaussian exponents
        exponents = np.sum(
            (self.cv_list - np.expand_dims(cv, axis=0))**2
            /np.expand_dims(2.0*self.widths**2, axis=0),
            axis=1)
        #(Nlist)
        
        # Compute Gaussians
        gaussians = -1.0*self.height*np.exp(-exponents)
        #(Nlist)
        
        # Compute partial derivative d exponent d cv
        dexpdcv = (
            (self.cv_list -  np.expand_dims(cv, axis=0))
            /np.expand_dims(self.widths**2, axis=0))
        #(Nlist, Ncv)
        
        # Add up gradient with respective to cv
        dgausdcv = np.sum(np.expand_dims(gaussians, axis=1)*dexpdcv, axis=0)
        #(Ncv)
        
        # Compute gradient with respect to Cartesian
        gradient = np.sum(np.expand_dims(dgausdcv, axis=(1,2))*dcvdR, axis=0)
        #(Natoms, Ncart)
        
        forces -= gradient
        
        return
        
    def adjust_potential_energy(
        self,
        atoms: ase.Atoms,
    ):
        """"""
        Returns the artificial Gaussian potential
        """"""
        
        # Get collective variable i
        cv = np.zeros(self.Ncv, dtype=float)
        for icv, cvi in enumerate(self.cv):
            
            # Bond length
            if len(cvi)==2:
                cv[icv] = np.linalg.norm(
                    atoms.positions[cvi[0]] - atoms.positions[cvi[1]])
            # Angle
            elif len(cvi)==3:
                # Connection vectors
                ab = atoms.positions[cvi[0]] - atoms.positions[cvi[1]]
                cb = atoms.positions[cvi[2]] - atoms.positions[cvi[1]]
                # Vector distances
                d2ab = np.sum(ab**2)
                dab = np.sqrt(d2ab)
                d2cb = np.sum(cb**2)
                dcb = np.sqrt(d2cb)
                # Vector angles
                cabbc = np.sum(ab*cb)/(dab*dcb)
                cabbc = np.clip(cabbc, -1.0, 1.0)
                tabbc = np.arccos(cabbc)
                # Add to current collective variable
                cv[icv] = tabbc
            # Dihedral angle
            elif len(cvi)==4:
                raise NotImplementedError
            # Special
            elif len(cvi)==5:
                if cvi[0]=='-':
                    a = np.linalg.norm(
                        atoms.positions[cvi[1]] - atoms.positions[cvi[2]])
                    b = np.linalg.norm(
                        atoms.positions[cvi[3]] - atoms.positions[cvi[4]])
                    cv[icv] = a - b
                else:
                    raise NotImplementedError
            else:
                raise IOError('Check cv list of Metadynamic constraint!')
        
        # Update to last cv
        self.last_cv = cv.copy()
        
        # If CV list is empty return zero potential
        # (Important to put it after last_cv update for the add_to_cv function)
        if self.cv_list.shape[0]==0:
            return 0.0
        
        # Compute Gaussian exponents
        exponents = np.sum(
            (self.cv_list - np.expand_dims(cv, axis=0))**2
            /np.expand_dims(2.0*self.widths**2, axis=0),
            axis=1)
        #(Nlist)
        
        # Compute gaussians
        gaussians = self.height*np.exp(-exponents)
        #(Nlist)
        
        # Add up potential
        potential = np.sum(gaussians)
        
        return potential
    
    def add_to_cv(
        self,
        atoms: ase.Atoms
    ):
        
        # Get collective variable i
        if self.last_cv is None:
            return
        else:
            cv = self.last_cv
        
        # Add cv to list
        if self.cv_list.shape[0]==0:
            self.cv_list = np.array([cv], dtype=float)
        else:
            self.cv_list = np.append(self.cv_list, [cv], axis=0)
        
        # Write cv list but just every logwrite time to avoid repeated writing 
        # of large files
        if not self.ilog%self.logwrite:
            np.savetxt(self.logfile, self.cv_list)
        self.ilog += 1
        
    def get_indices(self):
        raise NotImplementedError
        
    def index_shuffle(self, atoms, ind):
        raise NotImplementedError

    def __repr__(self):
        return (
            'Metadynamics constraint')",./Asparagus/asparagus/sampling/meta.py
MetaDynamicLogger,"class MetaDynamicLogger:
    """""" 
    Store defined amount of images of a trajectory and save the images 
    within a defined time window around a point when an action happened.

    Parameters
    ----------
    metadynamic: object
        Metadynamic constraint class

    """"""
    
    def __init__(self, metadynamic, system):
        """"""
        Initialize meta-dynamics collective variable position logger class.
        """"""
        
        # Allocate parameter
        self.metadynamic = metadynamic
        self.system = system
        
    def __exit__(self, exc_type, exc_value, tb):
        self.close()
        
    def log(self, system=None, **kwargs):
        """"""
        Execute 
        """"""
        
        if system is None:
            system = self.system
        self.metadynamic.add_to_cv(system)
        
        return",./Asparagus/asparagus/sampling/meta.py
Input_PaiNN,"class Input_PaiNN(torch.nn.Module):
    """"""
    PaiNN input module class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    input_n_atombasis: int, optional, default 128
        Number of atomic features (length of the atomic feature vector)
    input_radial_fn: (str, callable), optional, default 'GaussianRBF'
        Type of the radial basis function.
    input_n_radialbasis: int, optional, default 64
        Number of input radial basis centers
    input_cutoff_fn: (str, callable), optional, default 'Poly6'
        Cutoff function type for radial basis function scaling
    input_radial_cutoff: float, optional, default 8.0
        Cutoff distance radial basis function
    input_rbf_center_start: float, optional, default 1.0
        Lowest radial basis center distance.
    input_rbf_center_end: float, optional, default None (input_radial_cutoff)
        Highest radial basis center distance. If None, radial basis cutoff
        distance is used.
    input_rbf_trainable: bool, optional, default True
        If True, radial basis function parameter are optimized during training.
        If False, radial basis function parameter are fixed.
    input_n_maxatom: int, optional, default 94 (Plutonium)
        Highest atom order number to initialize atom feature vector library.

    """"""

    # Default arguments for input module
    _default_args = {
        'input_n_atombasis':            128,
        'input_radial_fn':              'GaussianRBF',
        'input_n_radialbasis':          64,
        'input_cutoff_fn':              'Poly6',
        'input_radial_cutoff':          8.0,
        'input_rbf_center_start':       1.0,
        'input_rbf_center_end':         None,
        'input_rbf_trainable':          True,
        'input_n_maxatom':              94,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'input_n_atombasis':            [utils.is_integer],
        'input_radial_fn':              [utils.is_string, utils.is_callable],
        'input_n_radialbasis':          [utils.is_integer],
        'input_cutoff_fn':              [utils.is_string, utils.is_callable],
        'input_radial_cutoff':          [utils.is_numeric],
        'input_rbf_center_start':       [utils.is_numeric],
        'input_rbf_center_end':         [utils.is_None, utils.is_numeric],
        'input_rbf_trainable':          [utils.is_bool],
        'input_n_maxatom':              [utils.is_integer],
        }

    # Input type module
    _input_type = 'PaiNN'

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        input_n_atombasis: Optional[int] = None,
        input_radial_fn: Optional[Union[str, object]] = None,
        input_n_radialbasis: Optional[int] = None,
        input_cutoff_fn: Optional[Union[str, object]] = None,
        input_radial_cutoff: Optional[float] = None,
        input_rbf_center_start: Optional[float] = None,
        input_rbf_center_end: Optional[float] = None,
        input_rbf_trainable: Optional[bool] = None,
        input_n_maxatom: Optional[int] = None,
        input_atom_features_range: Optional[float] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PaiNN input model.

        """"""

        super(Input_PaiNN, self).__init__()
        input_type = 'PaiNN'

        ####################################
        # # # Check Module Class Input # # #
        ####################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, module),
            check_dtype=utils.get_dtype_args(self, module)
        )
            
        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)
        
        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)
        
        # Check general model cutoff with radial basis cutoff
        if config.get('model_cutoff') is None:
            raise ValueError(
                ""No general model interaction cutoff 'model_cutoff' is yet ""
                + ""defined for the model calculator!"")
        elif config['model_cutoff'] < self.input_radial_cutoff:
            raise ValueError(
                ""The model interaction cutoff distance 'model_cutoff' ""
                + f""({self.model_cutoff:.2f}) must be larger or equal ""
                + ""the descriptor range 'input_radial_cutoff' ""
                + f""({config.get('input_radial_cutoff'):.2f})!"")

        ####################################
        # # # Input Module Class Setup # # #
        ####################################
        
        # Initialize atomic feature vectors
        self.atom_features = torch.nn.Embedding(
            self.input_n_maxatom + 1,
            self.input_n_atombasis,
            padding_idx=0,
            max_norm=self.input_n_atombasis,
            device=self.device, 
            dtype=self.dtype)

        # Initialize radial cutoff function
        self.cutoff = layer.get_cutoff_fn(self.input_cutoff_fn)(
            self.input_radial_cutoff,
            device=self.device,
            dtype=self.dtype)
        
        # Get upper RBF center range
        if self.input_rbf_center_end is None:
            self.input_rbf_center_end = self.input_radial_cutoff
        
        # Initialize Radial basis function
        radial_fn = layer.get_radial_fn(self.input_radial_fn)
        self.radial_fn = radial_fn(
            self.input_n_radialbasis,
            self.input_rbf_center_start, self.input_rbf_center_end,
            self.input_rbf_trainable, 
            self.device,
            self.dtype)

        return

    def __str__(self):
        return self._input_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""
        
        return {
            'input_type': self._input_type,
            'input_n_atombasis': self.input_n_atombasis,
            'input_radial_fn': str(self.input_radial_fn),
            'input_n_radialbasis': self.input_n_radialbasis,
            'input_radial_cutoff': self.input_radial_cutoff,
            'input_cutoff_fn': str(self.input_cutoff_fn),
            'input_rbf_trainable': self.input_rbf_trainable,
            'input_n_maxatom': self.input_n_maxatom,
            }

    def forward(
        self, 
        atomic_numbers: torch.Tensor,
        positions: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
        pbc_offset_ij: Optional[torch.Tensor] = None,
        idx_u: Optional[torch.Tensor] = None,
        idx_v: Optional[torch.Tensor] = None,
        pbc_offset_uv: Optional[torch.Tensor] = None,
    ) -> List[torch.Tensor]:
        """"""
        Forward pass of the input module.

        Parameters
        ----------
        atomic_numbers : torch.Tensor(N_atoms)
            Atomic numbers of the system
        positions : torch.Tensor(N_atoms, 3)
            Atomic positions of the system
        idx_i : torch.Tensor(N_pairs)
            Atom i pair index
        idx_j : torch.Tensor(N_pairs)
            Atom j pair index
        pbc_offset_ij : torch.Tensor(N_pairs, 3), optional, default None
            Position offset from periodic boundary condition
        idx_u : torch.Tensor(N_pairs), optional, default None
            Long-range atom u pair index
        idx_v : torch.Tensor(N_pairs), optional, default None
            Long-range atom v pair index
        pbc_offset_uv : torch.Tensor(N_pairs, 3), optional, default None
            Long-range position offset from periodic boundary condition

        Returns
        -------
        features: torch.tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        distances: torch.tensor(N_pairs)
            Atom pair distances
        vectors: torch.tensor(N_pairs, 3)
            Atom pair vectors
        cutoffs: torch.tensor(N_pairs)
            Atom pair distance cutoffs
        rbfs: torch.tensor(N_pairs, n_radialbasis)
            Atom pair radial basis functions
        distances_uv: torch.tensor(N_pairs_uv)
            Long-range atom pair distances
        vectors_uv: torch.tensor(N_pairs_uv, 3)
            Long-range atom pair vectors

        """"""
        
        # Collect atom feature vectors
        features = self.atom_features(atomic_numbers)

        # Compute pair connection vector
        if pbc_offset_ij is None:
            vectors = positions[idx_j] - positions[idx_i]
        else:
            vectors = positions[idx_j] - positions[idx_i] + pbc_offset_ij

        # Compute pair distances
        distances = torch.norm(vectors, dim=-1)

        # Compute long-range cutoffs
        if pbc_offset_uv is None and idx_u is not None:
            vectors_uv = positions[idx_v] - positions[idx_u]
            distances_uv = torch.norm(vectors_uv, dim=-1)
        elif idx_u is not None:
            vectors_uv = positions[idx_v] - positions[idx_u] + pbc_offset_uv
            distances_uv = torch.norm(vectors_uv, dim=-1)
        else:
            vectors_uv = vectors
            distances_uv = distances

        # Compute distance cutoff values
        cutoffs = self.cutoff(distances)

        # Compute radial basis functions
        rbfs = self.radial_fn(distances)

        return (
            features, distances, vectors, cutoffs, rbfs,
            distances_uv, vectors_uv)",./Asparagus/asparagus/module/painn_modules.py
Graph_PaiNN,"class Graph_PaiNN(torch.nn.Module): 
    """"""
    PaiNN message passing module class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    graph_n_blocks: int, optional, default 5
        Number of information processing cycles
    graph_activation_fn: (str, object), optional, default 'shifted_softplus'
        Activation function
    graph_stability_constant: float, optional, default 1.e-8
        Numerical stability constant added to scalar products of Cartesian 
        information vectors (guaranteed to be non-zero).

    """"""
    
    # Default arguments for graph module
    _default_args = {
        'graph_n_blocks':               5,
        'graph_activation_fn':          'silu',
        }

    # Expected data types of input variables
    _dtypes_args = {
        'graph_n_blocks':               [utils.is_integer],
        'graph_activation_fn':          [utils.is_string, utils.is_callable],
        }

    # Graph type module
    _graph_type = 'PaiNN'

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        graph_n_blocks: Optional[int] = None,
        graph_activation_fn: Optional[Union[str, object]] = None,
        graph_stability_constant: Optional[float] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize NNP graph model.

        """"""
        
        super(Graph_PaiNN, self).__init__()
        graph_type = 'PaiNN'
        
        ####################################
        # # # Check Module Class Input # # #
        ####################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, module),
            check_dtype=utils.get_dtype_args(self, module)
        )

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Get input to graph module interface parameters 
        self.n_atombasis = config.get('input_n_atombasis')
        self.n_radialbasis = config.get('input_n_radialbasis')
        
        ####################################
        # # # Graph Module Class Setup # # #
        ####################################
        
        # Initialize activation function
        self.activation_fn = layer.get_activation_fn(
            self.graph_activation_fn)
        
        # Initialize feature-wise, continuous-filter convolution network
        self.descriptors_filter = layer.DenseLayer(
            self.n_radialbasis,
            self.graph_n_blocks*self.n_atombasis*3,
            None,
            True,
            self.device,
            self.dtype)
        
        # Initialize message passing blocks
        self.interaction_block = torch.nn.ModuleList([
            layers_painn.PaiNNInteraction(
                self.n_atombasis, 
                self.activation_fn,
                self.device,
                self.dtype)
            for _ in range(self.graph_n_blocks)
            ])
        self.mixing_block = torch.nn.ModuleList([
            layers_painn.PaiNNMixing(
                self.n_atombasis, 
                self.activation_fn,
                self.device,
                self.dtype,
                stability_constant=self.graph_stability_constant)
            for _ in range(self.graph_n_blocks)
            ])

        return

    def __str__(self):
        return self._graph_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""
        
        return {
            'graph_type': self._graph_type,
            'graph_n_blocks': self.graph_n_blocks,
            'graph_activation_fn': str(self.graph_activation_fn),
            'graph_stability_constant': self.graph_stability_constant,
            }

    def forward(
        self, 
        features: torch.Tensor,
        distances: torch.Tensor,
        vectors: torch.Tensor,
        cutoffs: torch.Tensor,
        rbfs: torch.Tensor,
        idx_i: torch.Tensor, 
        idx_j: torch.Tensor,
    ) -> List[torch.Tensor]:

        """"""
        Forward pass of the graph model.
        
        Parameter
        ---------
        features: torch.tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        distances: torch.tensor(N_pairs)
            Atom pair distances
        vectors : torch.Tensor
            Atom pair connection vectors
        cutoffs: torch.tensor(N_pairs)
            Atom pair distance cutoffs
        rbfs: torch.tensor(N_pairs, n_radialbasis)
            Atom pair radial basis functions
        idx_i : torch.Tensor(N_pairs)
            Atom i pair index
        idx_j : torch.Tensor(N_pairs)
            Atom j pair index

        Returns
        -------
        sfeatures: torch.tensor(N_atoms, n_atombasis)
            Modified scalar atomic feature vectors
        vfeatures: torch.tensor(N_atoms, 3, n_atombasis)
            Modified vector atomic feature vectors

        """"""
        
        # Apply feature-wise, continuous-filter convolution
        descriptors = (
            self.descriptors_filter(rbfs[:, None, :])*cutoffs[:, None, None])
        descriptors_list = torch.split(
            descriptors, 3*self.n_atombasis, dim=-1)
        
        # Normalize atom pair vectors
        vectors_normalized = vectors/distances[:, None]

        # Assign isolated atomic feature vectors as scalar feature 
        # vectors
        fsize = features.shape # (len(atomic_numbers), n_atombasis)
        sfeatures = features[:, None, :]

        # Initialize vector feature vectors
        vfeatures = torch.zeros((fsize[0], 3, fsize[1]), device=self.device)

        # Apply message passing model to modify from isolated atomic features
        # vectors to molecular atomic features as a function of the chemical
        # environment
        for ii, (interaction, mixing) in enumerate(
            zip(self.interaction_block, self.mixing_block)
        ):
            sfeatures, vfeatures = interaction(
                sfeatures, 
                vfeatures, 
                descriptors_list[ii], 
                vectors_normalized, 
                idx_i, 
                idx_j,
                fsize[0], 
                fsize[1])
            sfeatures, vfeatures = mixing(
                sfeatures, 
                vfeatures, 
                fsize[1])

        # Flatten scalar atomic feature vector
        sfeatures = sfeatures.squeeze(1)
        
        return sfeatures, vfeatures",./Asparagus/asparagus/module/painn_modules.py
Output_PaiNN,"class Output_PaiNN(torch.nn.Module): 
    """"""
    PaiNN output module class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    output_properties: list(str), optional '['energy', 'forces']'
        List of output properties to compute by the model
        e.g. ['energy', 'forces', 'atomic_charges']
    output_properties_options: dict(str, Any), optional, default {}
        Dictionary of output block options (item) for a property (key).
        Dictionary inputs are, e.g.:
        output_properties_options = {
            'atomic_energies': {    # Output of a scalar type output block
                'output_type':          'scalar',
                'n_property':           1,
                'n_layer':              2,
                'n_neurons':            None,
                'activation_fn':        'silu',
                'bias_layer':           True,
                'bias_last':            True,
                'weight_init_layer':    torch.nn.init.xavier_uniform_,
                'weight_init_last':     torch.nn.init.xavier_uniform_,
                'bias_init_layer':      torch.nn.init.zeros_,
                'bias_init_last':       torch.nn.init.zeros_,
                }
            'atomic_charges': { # Scalar output of tensor type output block
                'output_type':          'tensor',
                'properties':           ['atomic_charges', 'atomic_dipoles'],
                'n_property':           1,
                'n_layer':              2,
                'n_neurons':            None,
                'scalar_activation_fn': 'silu',
                'hidden_activation_fn': 'silu',
                'bias_layer':           True,
                'bias_last':            True,
                'weight_init_layer':    torch.nn.init.xavier_uniform_,
                'weight_init_last':     torch.nn.init.xavier_uniform_,
                'bias_init_layer':      torch.nn.init.zeros_,
                'bias_init_last':       torch.nn.init.zeros_,
                },
            'atomic_dipoles': { # Tensor output of tensor type output block
                'output_type':          'tensor',
                'properties':           ['atomic_charges', 'atomic_dipoles'],
                'n_property':           1,
                'n_layer':              2,
                'n_neurons':            None,
                'scalar_activation_fn': 'silu',
                'hidden_activation_fn': 'silu',
                'bias_layer':           True,
                'bias_last':            True,
                'weight_init_layer':    torch.nn.init.xavier_uniform_,
                'weight_init_last':     torch.nn.init.xavier_uniform_,
                'bias_init_layer':      torch.nn.init.zeros_,
                'bias_init_last':       torch.nn.init.zeros_,
                },
            }
    output_n_residual: int, optional, default 1
        Number of residual layers for transformation from atomic feature vector
        to output results.
    output_property_scaling: dictionary, optional, default None
        Property average and standard deviation for the use as scaling factor 
        (standard deviation) and shift term (average) parameter pairs
        for each property.
    **kwargs: dict, optional
        Additional arguments

    """"""

    # Default arguments for graph module
    _default_args = {
        'output_properties':            None,
        'output_properties_options':    {},
        'output_n_residual':            1,
        'output_property_scaling':      None,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'output_properties':            [utils.is_string_array, utils.is_None],
        'output_properties_options':    [utils.is_dictionary],
        'output_n_residual':            [utils.is_integer],
        'output_property_scaling':      [utils.is_dictionary, utils.is_None],
        }

    # Output type module
    _output_type = 'PaiNN'
    
    # Default output block options for atom-wise scalar properties such as, 
    # e.g., 'atomic_energies'.
    _default_output_scalar = {
        'output_type':          'scalar',
        'n_property':           1,
        'n_layer':              2,
        'n_neurons':            None,
        'activation_fn':        'silu',
        'bias_layer':           True,
        'bias_last':            True,
        'weight_init_layer':    torch.nn.init.xavier_uniform_,
        'weight_init_last':     torch.nn.init.xavier_uniform_,
        'bias_init_layer':      torch.nn.init.zeros_,
        'bias_init_last':       torch.nn.init.zeros_,
        }
    
    # Default output block options for atom-wise tensor properties such as, 
    # e.g., 'atomic_dipole'.
    _default_output_tensor = {
        'output_type':          'tensor',
        'properties':           [None, None],
        'n_property':           1,
        'n_layer':              2,
        'n_neurons':            None,
        'scalar_activation_fn': 'silu',
        'hidden_activation_fn': 'silu',
        'bias_layer':           True,
        'bias_last':            True,
        'weight_init_layer':    torch.nn.init.xavier_uniform_,
        'weight_init_last':     torch.nn.init.xavier_uniform_,
        'bias_init_layer':      torch.nn.init.zeros_,
        'bias_init_last':       torch.nn.init.zeros_,
        }
    
    
    # Default output block assignment to properties
    # key: output property
    # item: list -> [0]:    str, Output block type 'scalar' or 'tensor'
    #                       None, no output block (skip property)
    #               [1]:    dict, output block options if [0] str
    #                       str, dependency to other property if [0] None
    #               [2]:    optional, int: scalar or tensor result from a
    #                           'tensor' type output block
    #       dict -> key:    str, either default case or case if respective
    #                           property included in output properties list
    #               item:   list -> [0]: Output block type (see item -> list)
    #                               [1]: Output block options ...
    #                               [2]: 'Vector' output block result ...
    _default_property_assignment = {
        'energy': [None, ['atomic_energies']],
        'atomic_energies': [_default_output_scalar],
        'forces': [None, ['energy']],
        'dipole': [None, ['atomic_charges', 'atomic_dipoles']],
        'atomic_charges': {
            'default': [_default_output_scalar],
            'atomic_dipoles': [_default_output_tensor, 0],
            },
        'atomic_dipoles': [_default_output_tensor, 1],
        }
    
    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        output_properties: Optional[List[str]] = None,
        output_properties_options: Optional[Dict[str, Any]] = None,
        output_property_scaling: Optional[
            Dict[str, Union[List[float], Dict[int, List[float]]]]] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PaiNN output model.

        """"""

        super(Output_PaiNN, self).__init__()
        output_type = 'PaiNN'

        ####################################
        # # # Check Module Class Input # # #
        ####################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, module),
            check_dtype=utils.get_dtype_args(self, module)
        )

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Get input and graph to output module interface parameters 
        self.n_maxatom = config.get('input_n_maxatom')
        self.n_atombasis = config.get('input_n_atombasis')
        self.n_blocks = config.get('graph_n_blocks')

        ##########################################
        # # # Check Output Module Properties # # #
        ##########################################

        # Get model properties to check with output module properties
        model_properties = config.get('model_properties')
        properties_all = []
        if self.output_properties is not None:
            properties_all += list(self.output_properties) 
        if model_properties is not None:
            properties_all += list(model_properties)

        # Initialize output module properties
        properties_list = []
        properties_options_scalar = {}
        properties_options_tensor = {}
        
        # Check defined output properties
        for prop in properties_all:
            # Prevent property repetition 
            if prop in properties_list:
                continue
            # Check if property is available
            elif prop in self._default_property_assignment:
                
                # Get default property output block instruction
                instructions = self._default_property_assignment[prop]
                
                # Check and get instruction for certain conditions
                if utils.is_dictionary(instructions):
                    # Select property-dependent output instructions
                    for case_prop, case_instructions in instructions.items():
                        if case_prop == 'default':
                            instructions = instructions.get('default')
                        elif case_prop in properties_all:
                            instructions = case_instructions
                            break
                
                # Add custom output options if defined
                if prop in self.output_properties_options:
                    
                    # Add property and output options
                    properties_list.append(prop)
                    
                    # Check output options for completeness by adding 
                    # default options for undefined keyword arguments
                    option = self.output_properties_options[prop]
                    if instructions[0] is not None:
                        option = {**instructions[0], **option}

                    # Assign output block to scalar or tensor property list
                    out_type = option.get('output_type')
                    if out_type == 'scalar':
                        properties_options_scalar[prop] = option
                    elif out_type == 'tensor':                        
                        properties_options_tensor[prop] = option
                    else:
                        raise SyntaxError(
                            ""Costum output block options for property ""
                            + f""{prop:s} has unknown type '{out_type:s}'!"")

                # If skip label is found in instructions
                elif instructions[0] is None:

                    # Check dependencies and skip property
                    if any([
                        prop_required not in properties_all
                        for prop_required in instructions[1]]
                    ):
                        raise SyntaxError(
                            f""Model property prediction for '{prop:s}' ""
                            + f""requires property '{instructions[1]}', ""
                            + ""which is not defined!"")
                    else:
                        pass

                # Else  output instructions are given
                else:

                    # Add property
                    properties_list.append(prop)

                    # Prepare output block options
                    option = instructions[0]
                    if len(instructions) == 1:
                        properties_options_scalar[prop] = option
                    elif len(instructions) > 1 and 'properties' in option:
                        option['properties'][instructions[1]] = prop
                        properties_options_tensor[prop] = option
                    else:
                        raise SyntaxError()

            else:
                raise NotImplementedError(
                    f""Output module of type {self.output_type:s} does not ""
                    + f""support the property prediction of {prop:s}!"")

        # Update output property list and global configuration dictionary
        self.output_properties = properties_list
        self.output_properties_options = {
            **properties_options_scalar, **properties_options_tensor}
        config_update = {
            'output_properties': self.output_properties,
            'output_properties_options': self.output_properties_options}
        config.update(
            config_update,
            verbose=verbose)
        
        #####################################
        # # # Output Module Class Setup # # #
        #####################################
        
        # Initialize property to output blocks dictionary
        self.output_property_scalar_block = torch.nn.ModuleDict({})
        self.output_property_tensor_block = torch.nn.ModuleDict({})
        
        # Initialize number of property per output block dictionary and 
        # output block tag list
        self.output_n_property = {}
        self.output_tag_properties = {}
        
        # Add output blocks for scalar properties
        for prop, options in properties_options_scalar.items():

            # Get activation function
            activation_fn = layer.get_activation_fn(
                options.get('activation_fn'))

            # Check essential number of property output parameter
            if options.get('n_property') is None:
                raise SynaxError(
                    ""Number of output properties 'n_property' for property ""
                    + f""{prop:s} is not defined!"")
            self.output_n_property[prop] = options.get('n_property')

            # Initialize scalar output block
            self.output_property_scalar_block[prop] = (
                layers_painn.PaiNNOutput_scalar(
                    self.n_atombasis,
                    options.get('n_property'),
                    self.device,
                    self.dtype,
                    n_layer=options.get('n_layer'),
                    n_neurons=options.get('n_neurons'),
                    activation_fn=activation_fn,
                    bias_layer=options.get('bias_last'),
                    bias_last=options.get('bias_last'),
                    weight_init_layer=options.get('weight_init_layer'),
                    weight_init_last=options.get('weight_init_last'),
                    bias_init_layer=options.get('bias_init_layer'),
                    bias_init_last=options.get('bias_init_last'),
                    )
                )
        
        # Add output blocks for (scalar +) tensor properties
        for prop, options in properties_options_tensor.items():

            # Get activation function
            scalar_activation_fn = layer.get_activation_fn(
                options.get('scalar_activation_fn'))
            hidden_activation_fn = layer.get_activation_fn(
                options.get('hidden_activation_fn'))

            # Check essential number of property output parameter
            if options.get('n_property') is None:
                raise SynaxError(
                    ""Number of output properties 'n_property' for property ""
                    + f""{prop:s} is not defined!"")
            self.output_n_property[prop] = options.get('n_property')

            # Get combined scalar and tensor property tag, skip if already done
            prop_tuple = tuple(options.get('properties'))
            prop_tag = '_&_'.join([str(p) for p in prop_tuple])
            if prop_tag in self.output_tag_properties:
                continue
            self.output_tag_properties[prop_tag] = prop_tuple

            # Initialize scalar output block
            self.output_property_tensor_block[prop_tag] = (
                layers_painn.PaiNNOutput_tensor(
                    self.n_atombasis,
                    options.get('n_property'),
                    self.device,
                    self.dtype,
                    n_layer=options.get('n_layer'),
                    n_neurons=options.get('n_neurons'),
                    scalar_activation_fn=scalar_activation_fn,
                    hidden_activation_fn=hidden_activation_fn,
                    bias_layer=options.get('bias_last'),
                    bias_last=options.get('bias_last'),
                    weight_init_layer=options.get('weight_init_layer'),
                    weight_init_last=options.get('weight_init_last'),
                    bias_init_layer=options.get('bias_init_layer'),
                    bias_init_last=options.get('bias_init_last'),
                    )
                )

        # Initialize property and atomic properties scaling dictionary
        self.output_scaling = torch.nn.ParameterDict()

        # Assign property and atomic properties scaling parameters
        self.set_property_scaling(self.output_property_scaling)

        return

    def __str__(self):
        return self._output_type
    
    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {
            'output_type': self._output_type,
            'output_properties': self.output_properties,
            'output_properties_options': self.output_properties_options,
            }

    def set_property_scaling(
        self,
        scaling_parameters: Dict[str, List[float]],
        trainable: Optional[bool] = True,
        set_shift_term: Optional[bool] = True,
        set_scaling_factor: Optional[bool] = True,
    ):
        """"""
        Update output atomic property scaling factor and shift terms.

        Parameters
        ----------
        scaling_parameters: dict(str, (list(float), dict(int, float))
            Dictionary of shift term and scaling factor for a model property
            ({'property': [shift, scaling]}) or shift term and scaling factor
            of a model property for each atom type
            ({'atomic property': {'atomic number': [shift, scaling]}}).
        trainable: bool, optional, default True
            If True the scaling factor and shift term parameters are trainable.
        set_shift_term: bool, optional, default True
            If True, set or update the shift term. Else, keep previous
            value.
        set_scaling_factor: bool, optional, default True
            If True, set or update the scaling factor. Else, keep previous
            value.

        """"""

        # Set scaling factor and shifts for output properties
        for prop in self.output_properties:

            # Initialize or get output property scaling parameter list
            if self.output_scaling.get(prop) is None:
                if 'atomic' in prop:
                    prop_scaling = np.array(
                        [[0.0, 1.0] for _ in range(self.n_maxatom + 1)],
                        dtype=float)
                else:
                    prop_scaling = np.array([0.0, 1.0], dtype=float)
            else:
                prop_scaling = (
                    self.output_scaling[prop].detach().cpu().numpy())

            # Assign scaling factor and shift
            if (
                scaling_parameters is not None
                and scaling_parameters.get(prop) is not None
            ):

                if utils.is_dictionary(scaling_parameters.get(prop)):

                    # Re-initialize output property scaling at atom resolved
                    # list
                    prop_scaling = np.array(
                        [[0.0, 1.0] for _ in range(self.n_maxatom + 1)],
                        dtype=float)

                    # Iterate over atom type specific scaling factor and shifts
                    for ai, pars in scaling_parameters.get(prop).items():

                        if set_shift_term:
                            prop_scaling[ai, 0] = pars[0]
                        if set_scaling_factor:
                            prop_scaling[ai, 1] = pars[1]

                else:

                    pars = scaling_parameters.get(prop)
                    if set_shift_term:
                        prop_scaling[0] = pars[0]
                    if set_scaling_factor:
                        prop_scaling[1] = pars[1]

            # Add list to property scaling dictionary
            if trainable:
                self.output_scaling[prop] = (
                    torch.nn.Parameter(
                        torch.tensor(
                            prop_scaling,
                            device=self.device,
                            dtype=self.dtype)
                        )
                    )
            else:
                self.output_scaling[prop] = (
                    self.register_buffer(
                        f""output_scaling:{prop:s}"",
                        torch.tensor(
                            prop_scaling,
                            device=self.device,
                            dtype=self.dtype)
                        )
                    )

        return

    def get_property_scaling(
        self,
    ) -> Dict[str, Union[List[float], Dict[int, List[float]]]]:
        """"""
        Get atomic property scaling factor and shift terms.

        Returns
        -------
        dict(str, (list(float), dict(int, float))
            Dictionary of shift term and scaling factor for a model property
            ({'property': [shift, scaling]}) or shift term and scaling factor
            of a model property for each atom type
            ({'atomic property': {'atomic number': [shift, scaling]}}).

        """"""

        # Get scaling factor and shifts for output properties
        scaling_parameters = {}
        for prop in self.output_properties:

            # Get output property scaling
            prop_scaling = (
                self.output_scaling[prop].detach().cpu().numpy())

            # Check for atom or system resolved scaling
            if prop_scaling.ndim == 1:
                scaling_parameters[prop] = prop_scaling
            else:
                scaling_parameters[prop] = {}
                for ai, pars in enumerate(prop_scaling):
                    scaling_parameters[prop][ai] = pars

        return scaling_parameters

    def forward(
        self,
        sfeatures: torch.Tensor,
        vfeatures: torch.Tensor,
        atomic_numbers: Optional[torch.Tensor] = None,
        properties: Optional[List[str]] = None,
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of output module

        Parameters
        ----------
        sfeatures: torch.tensor(N_atoms, n_atombasis)
            Scalar atomic feature vectors
        vfeatures: torch.tensor(N_atoms, 3, n_atombasis)
            Vector atomic feature vectors
        atomic_numbers: torch.Tensor(N_atoms), optional, default None
            List of atomic numbers
        properties: list(str), optional, default None
            List of properties to compute by the model. If None, all properties
            are predicted.

        Returns
        -------
        dict(str, torch.Tensor)
            Dictionary of predicted properties

        """"""
        
        # Initialize predicted properties dictionary
        output_prediction = {}
        
        # Check requested properties
        if properties is None:
            predict_all = True
        else:
            predict_all = False

        # Iterate over scalar output blocks
        for prop, output_block in self.output_property_scalar_block.items():
            
            # Skip if property not requested
            if not predict_all and prop not in properties:
                continue
            
            # Compute prediction
            output_prediction[prop] = output_block(sfeatures)

            # Flatten prediction for scalar properties
            if self.output_n_property[prop] == 1:
                output_prediction[prop] = torch.flatten(
                    output_prediction[prop], start_dim=0)

        # Iterate over tensor output blocks
        for prop, output_block in self.output_property_tensor_block.items():
            
            # Get scalar and tensor property tags
            (sprop, vprop) = self.output_tag_properties[prop]
            
            # Skip if property not requested
            if (
                not predict_all 
                and not any([propi in properties for prop_i in (sprop, vprop)])
            ):
                continue
            
            # Compute prediction
            output_prediction[sprop], output_prediction[vprop] = (
                output_block(sfeatures, vfeatures))

            # Flatten prediction for scalar properties
            if self.output_n_property[sprop] == 1:
                output_prediction[sprop] = torch.flatten(
                    output_prediction[sprop], start_dim=0)
                
            # Flatten prediction for single vector properties
            if self.output_n_property[vprop] == 1:
                output_prediction[vprop] = (
                    output_prediction[vprop].reshape(-1, 3))

        # Apply property and atomic properties scaling
        for prop, scaling in self.output_scaling.items():
            if scaling.dim() == 1:
                (shift, scale) = scaling
            else:
                (shift, scale) = scaling[atomic_numbers].T
                if output_prediction[prop].dim() > 1:
                    compatible_shape = (
                        [atomic_numbers.shape[0]]
                        + (output_prediction[prop].dim() - 1)*[1])
                    shift = shift.reshape(compatible_shape)
                    scale = scale.reshape(compatible_shape)
            output_prediction[prop] = (
                output_prediction[prop]*scale + shift)

        return output_prediction",./Asparagus/asparagus/module/painn_modules.py
D3_dispersion,"class D3_dispersion(torch.nn.Module):
    """"""
    Torch implementation of Grimme's D3 method (only Becke-Johnson damping is
    implemented)

    Grimme, Stefan, et al. ""A consistent and accurate ab initio parametrization
    of density functional dispersion correction (DFT-D) for the 94 elements
    H-Pu."" The Journal of Chemical Physics 132, 15 (2010): 154104.

    Update of the implementation according with respect to the tad-dftd3 module
    on git: https://github.com/dftd3/tad-dftd3 (15.11.2024)

    Parameters
    ----------
    cutoff: float
        Upper cutoff distance
    cuton: float
        Lower cutoff distance starting switch-off function
    trainable: bool, optional, default True
        If True the dispersion parameters are trainable
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    unit_properties: dict, optional, default {}
        Dictionary with the units of the model properties to initialize
        correct conversion factors.
    truncation: str, optional, default 'force'
        Truncation method of the Dispersion potential at the cutoff range:
            None, 'None':
                No Dispersion potential shift applied
            'potential':
                Apply shifted Dispersion potential method
                    V_shifted(r) = V_Coulomb(r) - V_Coulomb(r_cutoff)
            'force', 'forces':
                Apply shifted Dispersion force method
                    V_shifted(r) = V_Dispersion(r) - V_Dispersion(r_cutoff)
                        - (dV_Dispersion/dr)|r_cutoff  * (r - r_cutoff)
    d3_s6: float, optional, default 1.0000
        d3_s6 dispersion parameter
    d3_s8: float, optional, default 0.9171
        d3_s8 dispersion parameter
    d3_a1: float, optional, default 0.3385
        d3_a1 dispersion parameter
    d3_a2: float, optional, default 2.8830
        d3_a2 dispersion parameter

    """"""

    def __init__(
        self,
        cutoff: float,
        cuton: float,
        trainable: bool,
        device: str,
        dtype: 'dtype',
        unit_properties: Optional[Dict[str, str]] = None,
        truncation: Optional[str] = 'force',
        d3_s6: Optional[float] = None,
        d3_s8: Optional[float] = None,
        d3_a1: Optional[float] = None,
        d3_a2: Optional[float] = None,
        **kwargs
    ):
        """"""
        Initialize Grimme D3 dispersion model.

        """"""

        super(D3_dispersion, self).__init__()

        # Relative filepath to package folder
        package_directory = os.path.dirname(os.path.abspath(__file__))

        # Assign variables
        self.dtype = dtype
        self.device = device

        # Load tables with reference values
        self.d3_rcov = torch.from_numpy(
            np.load(os.path.join(package_directory, ""grimme_d3"", ""rcov.npy""))
            ).to(dtype).to(device)
        self.d3_rcn = torch.from_numpy(
            np.genfromtxt(
                os.path.join(package_directory, ""grimme_d3"", ""refcn.csv""),
                delimiter=',')
            ).to(dtype).to(device)
        self.d3_rc6 = torch.from_numpy(
            np.load(os.path.join(package_directory, ""grimme_d3"", ""rc6.npy""))
            ).to(dtype).to(device)
        self.d3_r2r4 = torch.from_numpy(
            np.load(os.path.join(package_directory, ""grimme_d3"", ""r2r4.npy""))
            ).to(dtype).to(device)

        # Assign truncation method
        if truncation is None or truncation.lower() == 'none':
            self.potential_fn = self.dispersion_fn
        elif truncation.lower() == 'potential':
            self.potential_fn = self.dispersion_sp_fn
        elif truncation.lower() in ['force', 'forces']:
            self.potential_fn = self.dispersion_sf_fn
        else:
            raise SyntaxError(
                ""Truncation method of the Dispersion potential ""
                + f""'{truncation:}' is unknown!\n""
                + ""Available are 'None', 'potential', 'force'."")

        # Initialize global dispersion correction parameters
        # (default values for HF)
        if d3_s6 is None:
            d3_s6 = 1.0000
        if d3_s8 is None:
            d3_s8 = 0.9171
        if d3_a1 is None:
            d3_a1 = 0.3385
        if d3_a2 is None:
            d3_a2 = 2.8830

        if trainable:
            self.d3_s6 = torch.nn.Parameter(
                torch.tensor([d3_s6], device=device, dtype=dtype))
            self.d3_s8 = torch.nn.Parameter(
                torch.tensor([d3_s8], device=device, dtype=dtype))
            self.d3_a1 = torch.nn.Parameter(
                torch.tensor([d3_a1], device=device, dtype=dtype))
            self.d3_a2 = torch.nn.Parameter(
                torch.tensor([d3_a2], device=device, dtype=dtype))
        else:
            self.register_buffer(
                ""d3_s6"", torch.tensor([d3_s6], device=device, dtype=dtype))
            self.register_buffer(
                ""d3_s8"", torch.tensor([d3_s8], device=device, dtype=dtype))
            self.register_buffer(
                ""d3_a1"", torch.tensor([d3_a1], device=device, dtype=dtype))
            self.register_buffer(
                ""d3_a2"", torch.tensor([d3_a2], device=device, dtype=dtype))
        self.d3_k1 = torch.tensor([16.000], device=device, dtype=dtype)
        self.d3_k2 = torch.tensor([4./3.], device=device, dtype=dtype)
        self.d3_k3 = torch.tensor([-4.000], device=device, dtype=dtype)

        # Assign cutoff radii
        self.cutoff = cutoff
        self.cuton = cuton

        # Unit conversion factors
        self.set_unit_properties(unit_properties)

        # Prepare interaction switch-off range
        self.set_switch_of_range(cutoff, cuton)

        # Auxiliary parameter
        self.d3_rcn_max = torch.max(self.d3_rcn, dim=-1, keepdim=True)[0]
        self.zero_dtype = torch.tensor(0.0, device=device, dtype=dtype)
        self.zero_double = torch.tensor(
            0.0, device=device, dtype=torch.float64)
        self.small_double = torch.tensor(
            1e-300, device=device, dtype=torch.float64)
        self.one_dtype = torch.tensor(1.0, device=device, dtype=dtype)
        self.max_dtype = torch.tensor(
            torch.finfo(dtype).max, device=device, dtype=dtype)

        return

    def __str__(self):
        return ""D3 Dispersion""

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {}

    def set_unit_properties(
        self,
        unit_properties: Dict[str, str],
    ):
        """"""
        Set unit conversion factors for compatibility between requested
        property units and applied property units (for physical constants)
        of the module.

        Parameters
        ----------
        unit_properties: dict
            Dictionary with the units of the model properties to initialize
            correct conversion factors.

        """"""

        # Get conversion factors
        if unit_properties is None:
            unit_energy = settings._default_units.get('energy')
            unit_positions = settings._default_units.get('positions')
            factor_energy, _ = utils.check_units(unit_energy, 'Hartree')
            factor_positions, _ = utils.check_units('Bohr', unit_positions)
        else:
            factor_energy, _ = utils.check_units(
                unit_properties.get('energy'), 'Hartree')
            factor_positions, _ = utils.check_units(
                'Bohr', unit_properties.get('positions'))

        # Convert
        # Distances: model to Bohr
        # Energies: Hartree to model
        self.register_buffer(
            ""distances_model2Bohr"",
            torch.tensor(
                [factor_positions], device=self.device, dtype=self.dtype))
        self.register_buffer(
            ""energies_Hatree2model"",
            torch.tensor(
                [factor_energy], device=self.device, dtype=self.dtype))

        # Update interaction switch-off range units
        self.set_switch_of_range(self.cutoff, self.cuton)

        return

    def set_switch_of_range(
        self,
        cutoff: float,
        cuton: float,
    ):
        """"""
        Prepare switch-off parameters

        """"""

        self.cutoff = (
            torch.tensor([cutoff], device=self.device, dtype=self.dtype)
            * self.distances_model2Bohr)
        if cuton is None or cuton == cutoff:
            self.cuton = None
            self.switchoff_range = None
            self.use_switch = False
        else:
            self.cuton = (
                torch.tensor([cuton], device=self.device, dtype=self.dtype)
                * self.distances_model2Bohr)
            self.switchoff_range = (
                torch.tensor(
                    [cutoff - cuton], device=self.device, dtype=self.dtype)
                * self.distances_model2Bohr)
            self.use_switch = True

        return

    def switch_fn(
        self,
        distances: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Computes a smooth switch factors from 1 to 0 in the range from 'cuton'
        to 'cutoff'.

        """"""

        x = (self.cutoff - distances) / self.switchoff_range

        return torch.where(
            distances < self.cuton,
            torch.ones_like(x),
            torch.where(
                distances >= self.cutoff,
                torch.zeros_like(x),
                ((6.0*x - 15.0)*x + 10.0)*x**3
                )
            )

    def get_cn(
        self,
        atomic_numbers: torch.Tensor,
        atomic_numbers_i: torch.Tensor,
        atomic_numbers_j: torch.Tensor,
        distances: torch.Tensor,
        switch_off: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Compute coordination numbers by adding an inverse damping function.

        """"""

        # Compute atom pairs covalent radii
        rcov_ij = (
            torch.gather(self.d3_rcov, 0, atomic_numbers_i)
            + torch.gather(self.d3_rcov, 0, atomic_numbers_j))

        cn_ij = (
            1.0/(1.0 + torch.exp(-self.d3_k1 * (rcov_ij/distances - 1.0))))
        if self.use_switch:
            cn_ij = cn_ij*switch_off

        return utils.scatter_sum(
            cn_ij, idx_i, dim=0, shape=atomic_numbers.shape)

    def get_weights(
        self,
        atomic_numbers: torch.Tensor,
        cn: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute dispersion weights

        """"""

        # Get reference atomic coordination numbers of atom pairs ij
        rcn = self.d3_rcn[atomic_numbers]

        # Selection of non-zero reference coordination numbers
        mask_rcn = rcn >= 0

        # Compute deviation between reference coordination number and actual
        # coordination number
        dcn = (rcn - cn.unsqueeze(-1)).type(torch.double)

        # Compute and normalize coordination number Gaussian weights in double
        # precision and convert back to dtype
        gaussian_weights = torch.where(
            mask_rcn,
            torch.exp(self.d3_k3*dcn**2),
            self.zero_double)
        norm = torch.where(
            mask_rcn,
            torch.sum(gaussian_weights, dim=-1, keepdim=True),
            self.small_double)
        mask_norm = norm == 0
        norm = torch.where(
            mask_norm,
            self.small_double,
            norm)
        gaussian_weights = (gaussian_weights/norm).type(self.dtype)

        # Prevent exceptional values in the Gaussian weights, either because
        # the norm was zero or the weight is to large.
        exceptional = torch.logical_or(
            mask_norm, gaussian_weights > self.max_dtype)
        if torch.any(exceptional):
            rcn_max = self.d3_rcn_max[atomic_numbers]
            gaussian_weights = torch.where(
                exceptional,
                torch.where(rcn == rcn_max, self.one_dtype, self.zero_dtype),
                gaussian_weights)
        gaussian_weights = torch.where(
            mask_rcn,
            gaussian_weights,
            self.zero_dtype)

        return gaussian_weights

    def get_c6(
        self,
        atomic_numbers_i: torch.Tensor,
        atomic_numbers_j: torch.Tensor,
        weigths: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute atomic c6 dispersion coefficients

        """"""

        # Collect reference c6 dispersion coefficients of atom pairs ij
        rc6 = self.d3_rc6[atomic_numbers_i, atomic_numbers_j]

        # Collect atomic weights of atom pairs ij
        weights_i = weigths[idx_i]
        weights_j = weigths[idx_j]
        weights_ij = weights_i.unsqueeze(-1)*weights_j.unsqueeze(-2)

        # Compute atomic c6 dispersion coefficients
        c6 = torch.sum(torch.sum(torch.mul(weights_ij, rc6), dim=-1), dim=-1)

        return c6

    def dispersion_fn(
        self,
        distances: torch.Tensor,
        distances6: torch.Tensor,
        distances8: torch.Tensor,
        switch_off: torch.Tensor,
        c6: torch.Tensor,
        c8: torch.Tensor,
        fct6: torch.Tensor,
        fct8: torch.Tensor,
        damp_c6: torch.Tensor,
        damp_c8: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute atomic D3 dispersion energy

        """"""

        # Compute atomic dispersion energy contributions
        e6 = -0.5*self.d3_s6*c6*damp_c6
        e8 = -0.5*self.d3_s8*c8*damp_c8

        # Apply switch-off function
        edisp = switch_off*(e6 + e8)

        return edisp

    def dispersion_sp_fn(
        self,
        distances: torch.Tensor,
        distances6: torch.Tensor,
        distances8: torch.Tensor,
        switch_off: torch.Tensor,
        c6: torch.Tensor,
        c8: torch.Tensor,
        fct6: torch.Tensor,
        fct8: torch.Tensor,
        damp_c6: torch.Tensor,
        damp_c8: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute shifted potential atomic D3 dispersion energy

        """"""

        # Compute all required powers of the cutoff distance
        cutoff2 = self.cutoff**2
        cutoff6 = cutoff2**3
        cutoff8 = cutoff6*cutoff2
        denominator6 = cutoff6 + fct6
        denominator8 = cutoff8 + fct8

        # Compute force shifted atomic dispersion energy contributions
        e6 = -0.5*self.d3_s6*c6*(damp_c6 - 1.0/denominator6)
        e8 = -0.5*self.d3_s8*c8*(damp_c8 - 1.0/denominator8)

        # Apply switch-off function
        edisp = switch_off*(e6 + e8)

        return edisp

    def dispersion_sf_fn(
        self,
        distances: torch.Tensor,
        distances6: torch.Tensor,
        distances8: torch.Tensor,
        switch_off: torch.Tensor,
        c6: torch.Tensor,
        c8: torch.Tensor,
        fct6: torch.Tensor,
        fct8: torch.Tensor,
        damp_c6: torch.Tensor,
        damp_c8: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute shifted force  atomic D3 dispersion energy

        """"""

        # Compute all required powers of the cutoff distance
        cutoff2 = self.cutoff**2
        cutoff6 = cutoff2**3
        cutoff8 = cutoff6*cutoff2
        denominator6 = cutoff6 + fct6
        denominator8 = cutoff8 + fct8

        # Compute force shifted atomic dispersion energy contributions
        e6 = -0.5*self.d3_s6*c6*(
            damp_c6 - 1.0/denominator6
            + 6.0*cutoff6/denominator6**2*(distances/self.cutoff - 1.0))
        e8 = -0.5*self.d3_s8*c8*(
            damp_c8 - 1.0/denominator8
            + 8.0*cutoff8/denominator8**2*(distances/self.cutoff - 1.0))

        # Apply switch-off function
        edisp = switch_off*(e6 + e8)

        return edisp

    def forward(
        self,
        atomic_numbers: torch.Tensor,
        distances: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute Grimme's D3 dispersion energy in Hartree with atom pair
        distances in Bohr.

        Parameters
        ----------
        atomic_numbers : torch.Tensor
            Atomic numbers of all atoms in the batch.
        distances : torch.Tensor
            Distances between all atom pairs in the batch.
        idx_i : torch.Tensor
            Indices of the first atom of each pair.
        idx_j : torch.Tensor
            Indices of the second atom of each pair.

        Returns
        -------
        torch.Tensor
            Dispersion atom energy contribution

        """"""

        # Convert distances from model unit to Bohr
        distances_d3 = distances*self.distances_model2Bohr

        # Compute switch-off function
        if self.use_switch:
            switch_off = self.switch_fn(distances_d3)
        else:
            switch_off = torch.where(
                distances_d3 < self.cutoff,
                torch.ones_like(distances_d3),
                torch.zeros_like(distances_d3),
            )

        # Gather atomic numbers of atom pairs ij
        atomic_numbers_i = torch.gather(atomic_numbers, 0, idx_i)
        atomic_numbers_j = torch.gather(atomic_numbers, 0, idx_j)

        # Compute coordination numbers and of atom pairs ij
        cn = self.get_cn(
            atomic_numbers,
            atomic_numbers_i,
            atomic_numbers_j,
            distances_d3,
            switch_off,
            idx_i,
            idx_j)

        # Compute atomic weights
        weights = self.get_weights(
            atomic_numbers,
            cn)

        # Compute atomic C6 and C8 coefficients
        c6 = self.get_c6(
            atomic_numbers_i,
            atomic_numbers_j,
            weights,
            idx_i,
            idx_j)
        qq = (
            3.0
            * torch.gather(self.d3_r2r4, 0, atomic_numbers_i)
            * torch.gather(self.d3_r2r4, 0, atomic_numbers_j))
        c8 = qq*c6

        # Compute the powers of the atom pair distances
        distances2 = distances_d3**2
        distances6 = distances2**3
        distances8 = distances6*distances2

        # Apply rational Becke-Johnson damping.
        fct = self.d3_a1*torch.sqrt(qq) + self.d3_a2
        fct2 = fct**2
        fct6 = fct2**3
        fct8 = fct6*fct2
        damp_c6 = 1.0/(distances6 + fct6)
        damp_c8 = 1.0/(distances8 + fct8)

        # Compute atomic dispersion energy contributions
        Edisp = self.potential_fn(
            distances_d3,
            distances6,
            distances8,
            switch_off,
            c6,
            c8,
            fct6,
            fct8,
            damp_c6,
            damp_c8)

        # Return system dispersion energies and convert to model energy unit
        return self.energies_Hatree2model*utils.scatter_sum(
            Edisp, idx_i, dim=0, shape=atomic_numbers.shape)",./Asparagus/asparagus/module/dispersion.py
Input_PhysNet,"class Input_PhysNet(torch.nn.Module):
    """"""
    PhysNet input module class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    input_n_atombasis: int, optional, default 128
        Number of atomic features (length of the atomic feature vector)
    input_radial_fn: (str, callable), optional, default 'GaussianRBF'
        Type of the radial basis function.
    input_n_radialbasis: int, optional, default 64
        Number of input radial basis centers
    input_cutoff_fn: (str, callable), optional, default 'Poly6'
        Cutoff function type for radial basis function scaling
    input_radial_cutoff: float, optional, default 8.0
        Cutoff distance radial basis function
    input_rbf_center_start: float, optional, default 1.0
        Lowest radial basis center distance.
    input_rbf_center_end: float, optional, default None (input_radial_cutoff)
        Highest radial basis center distance. If None, radial basis cutoff
        distance is used.
    input_rbf_trainable: bool, optional, default True
        If True, radial basis function parameter are optimized during training.
        If False, radial basis function parameter are fixed.
    input_n_maxatom: int, optional, default 94 (Plutonium)
        Highest atom order number to initialize atom feature vector library.
    input_atom_features_range: float, optional, default sqrt(3)
        Range for uniform distribution randomizer for initial atom feature
        vector.
    **kwargs: dict, optional
        Additional arguments for parameter initialization

    """"""
    
    # Default arguments for input module
    _default_args = {
        'input_n_atombasis':            128,
        'input_radial_fn':              'GaussianRBF',
        'input_n_radialbasis':          64,
        'input_cutoff_fn':              'Poly6',
        'input_radial_cutoff':          8.0,
        'input_rbf_center_start':       1.0,
        'input_rbf_center_end':         None,
        'input_rbf_trainable':          True,
        'input_n_maxatom':              94,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'input_n_atombasis':            [utils.is_integer],
        'input_radial_fn':              [utils.is_string, utils.is_callable],
        'input_n_radialbasis':          [utils.is_integer],
        'input_cutoff_fn':              [utils.is_string, utils.is_callable],
        'input_radial_cutoff':          [utils.is_numeric],
        'input_rbf_center_start':       [utils.is_numeric],
        'input_rbf_center_end':         [utils.is_None, utils.is_numeric],
        'input_rbf_trainable':          [utils.is_bool],
        'input_n_maxatom':              [utils.is_integer],
        }

    # Input type module
    _input_type = 'PhysNet'

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        input_n_atombasis: Optional[int] = None,
        input_radial_fn: Optional[Union[str, object]] = None,
        input_n_radialbasis: Optional[int] = None,
        input_cutoff_fn: Optional[Union[str, object]] = None,
        input_radial_cutoff: Optional[float] = None,
        input_rbf_center_start: Optional[float] = None,
        input_rbf_center_end: Optional[float] = None,
        input_rbf_trainable: Optional[bool] = None,
        input_n_maxatom: Optional[int] = None,
        device: Optional[str] = None,
        dtype: Optional['device'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PhysNet input module.

        """"""

        super(Input_PhysNet, self).__init__()

        ####################################
        # # # Check Module Class Input # # #
        ####################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, module),
            check_dtype=utils.get_dtype_args(self, module)
        )
            
        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)
        
        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Check general model cutoff with radial basis cutoff
        if config.get('model_cutoff') is None:
            raise ValueError(
                ""No general model interaction cutoff 'model_cutoff' is yet ""
                + ""defined for the model calculator!"")
        elif config['model_cutoff'] < self.input_radial_cutoff:
            raise ValueError(
                ""The model interaction cutoff distance 'model_cutoff' ""
                + f""({self.model_cutoff:.2f}) must be larger or equal ""
                + ""the descriptor range 'input_radial_cutoff' ""
                + f""({config.get('input_radial_cutoff'):.2f})!"")
        
        ####################################
        # # # Input Module Class Setup # # #
        ####################################
        
        # Initialize atomic feature vectors
        self.atom_features = torch.nn.Embedding(
            self.input_n_maxatom + 1,
            self.input_n_atombasis,
            padding_idx=0,
            max_norm=self.input_n_atombasis,
            device=self.device, 
            dtype=self.dtype)

        # Initialize radial cutoff function
        self.cutoff = layer.get_cutoff_fn(self.input_cutoff_fn)(
            self.input_radial_cutoff, device=self.device, dtype=self.dtype)
        
        # Get upper RBF center range
        if self.input_rbf_center_end is None:
            self.input_rbf_center_end = self.input_radial_cutoff
        
        # Initialize Radial basis function
        radial_fn = layer.get_radial_fn(self.input_radial_fn)
        self.radial_fn = radial_fn(
            self.input_n_radialbasis,
            self.input_rbf_center_start, self.input_rbf_center_end,
            self.input_rbf_trainable, 
            self.device,
            self.dtype)

        return

    def __str__(self):
        return self._input_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""
        
        return {
            'input_type': self._input_type,
            'input_n_atombasis': self.input_n_atombasis,
            'input_radial_fn': str(self.input_radial_fn),
            'input_n_radialbasis': self.input_n_radialbasis,
            'input_radial_cutoff': self.input_radial_cutoff,
            'input_cutoff_fn': str(self.input_cutoff_fn),
            'input_rbf_trainable': self.input_rbf_trainable,
            'input_n_maxatom': self.input_n_maxatom,
            }

    def forward(
        self, 
        atomic_numbers: torch.Tensor,
        positions: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
        pbc_offset_ij: Optional[torch.Tensor] = None,
        idx_u: Optional[torch.Tensor] = None,
        idx_v: Optional[torch.Tensor] = None,
        pbc_offset_uv: Optional[torch.Tensor] = None,
    ) -> List[torch.Tensor]:
        """"""
        Forward pass of the input module.

        Parameters
        ----------
        atomic_numbers : torch.Tensor(N_atoms)
            Atomic numbers of the system
        positions : torch.Tensor(N_atoms, 3)
            Atomic positions of the system
        idx_i : torch.Tensor(N_pairs)
            Atom i pair index
        idx_j : torch.Tensor(N_pairs)
            Atom j pair index
        pbc_offset_ij : torch.Tensor(N_pairs, 3), optional, default None
            Position offset from periodic boundary condition
        idx_u : torch.Tensor(N_pairs), optional, default None
            Long-range atom u pair index
        idx_v : torch.Tensor(N_pairs), optional, default None
            Long-range atom v pair index
        pbc_offset_uv : torch.Tensor(N_pairs, 3), optional, default None
            Long-range position offset from periodic boundary condition

        Returns
        -------
        features: torch.tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        distances: torch.tensor(N_pairs)
            Atom pair distances
        cutoffs: torch.tensor(N_pairs)
            Atom pair distance cutoffs
        rbfs: torch.tensor(N_pairs, n_radialbasis)
            Atom pair radial basis functions
        distances_uv: torch.tensor(N_pairs_uv)
            Long-range atom pair distances

        """"""
        
        # Collect atom feature vectors
        features = self.atom_features(atomic_numbers)

        # Compute atom pair distances
        if pbc_offset_ij is None:
            distances = torch.norm(
                positions[idx_j] - positions[idx_i],
                dim=-1)
        else:
            distances = torch.norm(
                positions[idx_j] - positions[idx_i] + pbc_offset_ij,
                dim=-1)

        # Compute long-range cutoffs
        if pbc_offset_uv is None and idx_u is not None:
            distances_uv = torch.norm(
                positions[idx_u] - positions[idx_v], dim=-1)
        elif idx_u is not None:
            distances_uv = torch.norm(
                positions[idx_v] - positions[idx_u] + pbc_offset_uv, dim=-1)
        else:
            distances_uv = distances

        # Compute distance cutoff values
        cutoffs = self.cutoff(distances)

        # Compute radial basis functions
        rbfs = self.radial_fn(distances)

        return features, distances, cutoffs, rbfs, distances_uv",./Asparagus/asparagus/module/physnet_modules.py
Graph_PhysNet,"class Graph_PhysNet(torch.nn.Module): 
    """"""
    PhysNet message passing module class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    graph_n_blocks: int, optional, default 5
        Number of information processing cycles
    graph_n_residual_interaction: int, optional, default 3
        Number of residual layers for atomic feature and radial basis vector
        interaction.
    graph_n_residual_features: int, optional, default 2
        Number of residual layers for atomic feature interactions.
    graph_activation_fn: (str, object), optional, default 'shifted_softplus'
        Residual layer activation function.

    """"""
    
    # Default arguments for graph module
    _default_args = {
        'graph_n_blocks':               5,
        'graph_n_residual_interaction': 3,
        'graph_n_residual_features':    2,
        'graph_activation_fn':          'shifted_softplus',
        }

    # Expected data types of input variables
    _dtypes_args = {
        'graph_n_blocks':               [utils.is_integer],
        'graph_n_residual_interaction': [utils.is_integer],
        'graph_n_residual_features':    [utils.is_integer],
        'graph_activation_fn':          [utils.is_string, utils.is_callable],
        }

    # Graph type module
    _graph_type = 'PhysNet'

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        graph_n_blocks: Optional[int] = None,
        graph_n_residual_interaction: Optional[int] = None,
        graph_n_residual_features: Optional[int] = None,
        graph_activation_fn: Optional[Union[str, object]] = None,
        device: Optional[str] = None,
        dtype: Optional['device'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PhysNet message passing module.

        """"""
        
        super(Graph_PhysNet, self).__init__()
        
        ####################################
        # # # Check Module Class Input # # #
        ####################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, module),
            check_dtype=utils.get_dtype_args(self, module)
        )

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Get input to graph module interface parameters 
        self.n_atombasis = config.get('input_n_atombasis')
        self.n_radialbasis = config.get('input_n_radialbasis')
        
        ####################################
        # # # Graph Module Class Setup # # #
        ####################################
        
        # Initialize activation function
        self.activation_fn = layer.get_activation_fn(
            self.graph_activation_fn)

        # Initialize message passing blocks
        self.interaction_blocks = torch.nn.ModuleList([
            layers_physnet.InteractionBlock(
                self.n_atombasis, 
                self.n_radialbasis, 
                self.graph_n_residual_interaction,
                self.graph_n_residual_features,
                self.activation_fn,
                device=self.device,
                dtype=self.dtype)
            for _ in range(self.graph_n_blocks)
            ])

        return

    def __str__(self):
        return self._graph_type

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""
        
        return {
            'graph_type': self._graph_type,
            'graph_n_blocks': self.graph_n_blocks,
            'graph_n_residual_interaction': self.graph_n_residual_interaction,
            'graph_n_residual_features': self.graph_n_residual_features,
            'graph_activation_fn': self.graph_activation_fn,
            }

    def forward(
        self, 
        features: torch.Tensor,
        distances: torch.Tensor,
        cutoffs: torch.Tensor,
        rbfs: torch.Tensor, 
        idx_i: torch.Tensor, 
        idx_j: torch.Tensor,
    ) -> List[torch.Tensor]:
        """"""
        Forward pass of the graph module.
        
        Parameters
        ----------
        features: torch.tensor(N_atoms, n_atombasis)
            Atomic feature vectors
        distances: torch.tensor(N_pairs)
            Atom pair distances
        cutoffs: torch.tensor(N_pairs)
            Atom pair distance cutoffs
        rbfs: torch.tensor(N_pairs, n_radialbasis)
            Atom pair radial basis functions
        idx_i : torch.Tensor(N_pairs)
            Atom i pair index
        idx_j : torch.Tensor(N_pairs)
            Atom j pair index

        Returns
        -------
        features_list: [torch.tensor(N_atoms, n_atombasis)]*n_blocks
            List of modified atomic feature vectors

        """"""
        
        # Compute descriptor vectors
        descriptors = cutoffs[..., None]*rbfs
        
        # Initialize refined feature vector list
        x_list = []
        
        # Apply message passing model
        x = features
        for interaction_block in self.interaction_blocks:
            x = interaction_block(x, descriptors, idx_i, idx_j)
            x_list.append(x)

        return x_list",./Asparagus/asparagus/module/physnet_modules.py
Output_PhysNet,"class Output_PhysNet(torch.nn.Module): 
    """"""
    PhysNet output module class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to json file (str)
    output_properties: list(str), optional '['energy', 'forces']'
        List of output properties to compute by the model
        e.g. ['energy', 'forces', 'atomic_charges']
    output_n_residual: int, optional, default 1
        Number of residual layers for transformation from atomic feature vector
        to output results.
    output_activation_fn: (str, callable), optional, default 'shifted_softplus'
        Residual layer activation function.
    output_property_scaling: dictionary, optional, default None
        Property average and standard deviation for the use as scaling factor 
        (standard deviation) and shift term (average) parameter pairs
        for each property.
    **kwargs: dict, optional
        Additional arguments

    """"""

    # Default arguments for graph module
    _default_args = {
        'output_properties':            None,
        'output_n_residual':            1,
        'output_activation_fn':         'shifted_softplus',
        'output_property_scaling':      None,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'output_properties':            [utils.is_string_array, utils.is_None],
        'output_n_residual':            [utils.is_integer],
        'output_activation_fn':         [utils.is_string, utils.is_callable],
        'output_property_scaling':      [utils.is_dictionary, utils.is_None],
        }
    
    # Output type module
    _output_type = 'PhysNet'

    # Property exclusion lists for properties (keys) derived from other 
    # properties (items) but in the model class
    _property_exclusion = {
        'energy': ['atomic_energies'],
        'forces': [], 
        'hessian': [],
        'dipole': ['atomic_charges']}
    
    # Output module properties that are handled specially
    _property_special = ['energy', 'atomic_energies', 'atomic_charges']
    
    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        output_properties: Optional[List[str]] = None,
        output_n_residual: Optional[int] = None,
        output_activation_fn: Optional[Union[str, object]] = None,
        output_property_scaling: Optional[
            Dict[str, Union[List[float], Dict[int, List[float]]]]] = None,
        device: Optional[str] = None,
        dtype: Optional['device'] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize PhysNet output module.

        """"""

        super(Output_PhysNet, self).__init__()

        ####################################
        # # # Check Module Class Input # # #
        ####################################
        
        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, module),
            check_dtype=utils.get_dtype_args(self, module)
        )

        # Update global configuration dictionary
        config.update(
            config_update,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        # Get input and graph to output module interface parameters 
        self.n_maxatom = config.get('input_n_maxatom')
        n_atombasis = config.get('input_n_atombasis')
        graph_n_blocks = config.get('graph_n_blocks')

        ##########################################
        # # # Check Output Module Properties # # #
        ##########################################

        # Get model properties to check with output module properties
        model_properties = config.get('model_properties')

        # Initialize output module properties
        properties_list = []
        if self.output_properties is not None:
            for prop in self.output_properties:
                if prop in self._property_exclusion:
                    for prop_der in self._property_exclusion[prop]:
                        properties_list.append(prop_der)
                else:
                    properties_list.append(prop)

        # Check output module properties with model properties
        for prop in model_properties:
            if (
                prop not in properties_list
                and prop in self._property_exclusion
            ):
                for prop_der in self._property_exclusion[prop]:
                    if prop_der not in properties_list:
                        properties_list.append(prop_der)
            elif prop not in properties_list:
                properties_list.append(prop)

        # Update output property list and global configuration dictionary
        self.output_properties = properties_list
        config_update = {
            'output_properties': self.output_properties}
        config.update(
            config_update,
            verbose=verbose)

        #####################################
        # # # Output Module Class Setup # # #
        #####################################

        # Initialize activation function
        self.activation_fn = layer.get_activation_fn(
            self.output_activation_fn)

        # Initialize property to output block dictionary
        self.output_property_block = torch.nn.ModuleDict({})

        # Initialize property to number of output block predictions dictionary
        self.output_n_property = {}

        # Check special case: atom energies and charges from one output block
        if all([
            prop in self.output_properties
            for prop in ['atomic_energies', 'atomic_charges']]
        ):
            # Set case flag for output module predicting atomic energies and
            # charges
            self.output_energies_charges = True
            self.output_n_property['atomic_energies_charges'] = 2

            # PhysNet energy and atom charges output block
            output_block = torch.nn.ModuleList([
                layers_physnet.OutputBlock(
                    n_atombasis,
                    self.output_n_property['atomic_energies_charges'],
                    self.output_n_residual,
                    self.activation_fn,
                    device=self.device,
                    dtype=self.dtype)
                for _ in range(graph_n_blocks)]
                )

            # Assign output block to dictionary
            self.output_property_block['atomic_energies_charges'] = (
                output_block)

        elif any([
            prop in self.output_properties
            for prop in ['atomic_energies', 'atomic_charges']]
        ):
            
            # Get property label
            if 'atomic_energies' in self.output_properties:
                prop = 'atomic_energies'
            else:
                prop = 'atomic_charges'

            # Set case flag for output module predicting just atomic energies
            # or charges
            self.output_energies_charges = False
            self.output_n_property[prop] = 1

            # PhysNet energy only output block
            output_block = torch.nn.ModuleList([
                layers_physnet.OutputBlock(
                    n_atombasis,
                    self.output_n_property[prop],
                    self.output_n_residual,
                    self.activation_fn,
                    device=self.device,
                    dtype=self.dtype)
                for _ in range(graph_n_blocks)
                ])

            # Assign output block to dictionary
            self.output_property_block[prop] = output_block

        else:

            # Set case flag for output module predicting just atomic energies
            # or charges
            self.output_energies_charges = False

        # Create further output blocks for properties with certain exceptions
        for prop in self.output_properties:

            # Skip deriving properties
            if prop in self._property_exclusion:
                continue

            # No output_block for already covered special properties
            if prop in self._property_special:
                continue

            # Initialize output block
            self.output_n_property[prop] = 1
            output_block = torch.nn.ModuleList([
                layers_physnet.OutputBlock(
                    n_atombasis,
                    self.output_n_property[prop],
                    self.output_n_residual,
                    self.activation_fn,
                    device=self.device,
                    dtype=self.dtype)
                for _ in range(graph_n_blocks)
                ])

            # Assign output block to dictionary
            self.output_property_block[prop] = output_block

        # Initialize property and atomic properties scaling dictionary
        self.output_scaling = torch.nn.ParameterDict()

        # Assign property and atomic properties scaling parameters
        self.set_property_scaling(self.output_property_scaling)

        return

    def __str__(self):
        return self._output_type
    
    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {
            'output_type': self._output_type,
            'output_properties': self.output_properties,
            'output_n_residual': self.output_n_residual,
            'output_activation_fn': self.output_activation_fn,
            }

    def set_property_scaling(
        self,
        scaling_parameters:
            Dict[str, Union[List[float], Dict[int, List[float]]]],
        trainable: Optional[bool] = True,
        set_shift_term: Optional[bool] = True,
        set_scaling_factor: Optional[bool] = True,
    ):
        """"""
        Update output atomic property scaling factor and shift terms.

        Parameters
        ----------
        scaling_parameters: dict(str, (list(float), dict(int, float))
            Dictionary of shift term and scaling factor for a model property
            ({'property': [shift, scaling]}) or shift term and scaling factor
            of a model property for each atom type
            ({'atomic property': {'atomic number': [shift, scaling]}}).
        trainable: bool, optional, default True
            If True the scaling factor and shift term parameters are trainable.
        set_shift_term: bool, optional, default True
            If True, set or update the shift term. Else, keep previous
            value.
        set_scaling_factor: bool, optional, default True
            If True, set or update the scaling factor. Else, keep previous
            value.

        """"""

        # Set scaling factor and shifts for output properties
        for prop in self.output_properties:

            # Initialize or get output property scaling parameter list
            if self.output_scaling.get(prop) is None:
                if 'atomic' in prop:
                    prop_scaling = np.array(
                        [[0.0, 1.0] for _ in range(self.n_maxatom + 1)],
                        dtype=float)
                else:
                    prop_scaling = np.array([0.0, 1.0], dtype=float)
            else:
                prop_scaling = (
                    self.output_scaling[prop].detach().cpu().numpy())

            # Assign scaling factor and shift
            if (
                scaling_parameters is not None
                and scaling_parameters.get(prop) is not None
            ):

                if utils.is_dictionary(scaling_parameters.get(prop)):

                    # Iterate over atom type specific scaling factor and shifts
                    for ai, pars in scaling_parameters.get(prop).items():
                    
                        if set_shift_term:
                            prop_scaling[ai, 0] = pars[0]
                        if set_scaling_factor:
                            prop_scaling[ai, 1] = pars[1]

                else:

                    pars = scaling_parameters.get(prop)
                    if set_shift_term:
                        prop_scaling[0] = pars[0]
                    if set_scaling_factor:
                        prop_scaling[1] = pars[1]

            # Add list to property scaling dictionary
            prop_scaling = torch.tensor(
                prop_scaling,
                device=self.device,
                dtype=self.dtype)
            if trainable:
                if self.output_scaling.get(prop) is None:
                    self.output_scaling[prop] = (
                        torch.nn.Parameter(prop_scaling))
                else:
                    with torch.no_grad():
                        self.output_scaling[prop][:] = prop_scaling
            else:
                if self.output_scaling.get(prop) is None:
                    self.output_scaling[prop] = (
                        self.register_buffer(
                            f""output_scaling:{prop:s}"",
                            prop_scaling)
                        )
                else:
                    self.output_scaling[prop][:] = prop_scaling

        return

    def get_property_scaling(
        self,
    ) -> Dict[str, Union[List[float], Dict[int, List[float]]]]:
        """"""
        Get atomic property scaling factor and shift terms.

        Returns
        -------
        dict(str, (list(float), dict(int, float))
            Dictionary of shift term and scaling factor for a model property
            ({'property': [shift, scaling]}) or shift term and scaling factor
            of a model property for each atom type
            ({'atomic property': {'atomic number': [shift, scaling]}}).

        """"""

        # Get scaling factor and shifts for output properties
        scaling_parameters = {}
        for prop in self.output_properties:

            # Get output property scaling
            prop_scaling = (
                self.output_scaling[prop].detach().cpu().numpy())

            # Check for atom or system resolved scaling
            if prop_scaling.ndim == 1:
                scaling_parameters[prop] = prop_scaling
            else:
                scaling_parameters[prop] = {}
                for ai, pars in enumerate(prop_scaling):
                    scaling_parameters[prop][ai] = pars

        return scaling_parameters

    def forward(
        self,
        features_list: List[torch.Tensor],
        atomic_numbers: Optional[torch.Tensor] = None,
        properties: Optional[List[str]] = None,
    ) -> Dict[str, torch.Tensor]:

        """"""
        Forward pass of output module

        Parameters
        ----------
        features_list : [torch.Tensor(N_atoms, n_atombasis)]*n_blocks
            List of atom feature vectors
        atomic_numbers: torch.Tensor(N_atoms), optional, default None
            List of atomic numbers
        properties: list(str), optional, default None
            List of properties to compute by the model. If None, all properties
            are predicted.

        Returns
        -------
        dict(str, torch.Tensor)
            Dictionary of predicted properties

        """"""

        # Initialize predicted properties dictionary
        output_prediction = {}
        
        # Check requested properties
        if properties is None:
            predict_all = True
        else:
            predict_all = False

        # Initialize training properties
        if self.training:
            nhloss = 0.0
            last_prediction_squared = 0.0
        
        # Iterate over output blocks
        for prop, output_block in self.output_property_block.items():

            # Skip if property not requested
            if not predict_all and prop not in properties:
                continue

            # Compute prediction and loss function contribution
            for iblock, (features, output) in enumerate(
                zip(features_list, output_block)
            ):

                prediction = output(features)
                if iblock:
                    output_prediction[prop] = (
                        output_prediction[prop] + prediction)
                else:
                    output_prediction[prop] = prediction
                
                # If training mode is active, compute nhloss contribution
                if self.training:
                    prediction_squared = prediction**2
                    if iblock:
                        nhloss = nhloss + torch.mean(
                            prediction_squared
                            / (
                                prediction_squared 
                                + last_prediction_squared 
                                + 1.0e-7)
                            )
                    last_prediction_squared = prediction_squared
            
            # Flatten prediction for scalar properties
            if self.output_n_property[prop] == 1:
                output_prediction[prop] = torch.flatten(
                    output_prediction[prop], start_dim=0)
            
        # Save nhloss if training mode is active
        if self.training:
            output_prediction['nhloss'] = nhloss
        
        # Post-process atomic energies/charges case
        if self.output_energies_charges:

            output_prediction['atomic_energies'], \
                output_prediction['atomic_charges'] = (
                    output_prediction['atomic_energies_charges'][:, 0],
                    output_prediction['atomic_energies_charges'][:, 1])

        # Apply property and atomic properties scaling
        for prop, scaling in self.output_scaling.items():
            if scaling.dim() == 1:
                (shift, scale) = scaling
            else:
                (shift, scale) = scaling[atomic_numbers].T
            output_prediction[prop] = (
                output_prediction[prop]*scale + shift)

        return output_prediction",./Asparagus/asparagus/module/physnet_modules.py
PC_shielded_electrostatics,"class PC_shielded_electrostatics(torch.nn.Module):
    """"""
    Torch implementation of a shielded point charge electrostatic model that
    avoids singularities at very close atom pair distances.

    Parameters
    ----------
    cutoff: float
        interaction cutoff distance.
    cutoff_short_range: float
        Short range cutoff distance.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    unit_properties: dict, optional, default None
        Dictionary with the units of the model properties to initialize correct
        conversion factors.
    switch_fn: (str, callable), optional, default None
        Switch function for the short range cutoff.
    **kwargs
        Additional keyword arguments.

    """"""

    def __init__(
        self,
        cutoff: float,
        cutoff_short_range: float,
        device: str,
        dtype: 'dtype',
        unit_properties: Optional[Dict[str, str]] = None,
        switch_fn: Optional[Union[str, object]] = 'Poly6',
        **kwargs
    ):

        super(PC_shielded_electrostatics, self).__init__()

        # Assign variables
        self.cutoff = cutoff
        if cutoff_short_range is None or cutoff == cutoff_short_range:
            self.cutoff_short_range = cutoff
            self.split_distance = False
        else:
            self.cutoff_short_range = cutoff_short_range
            self.split_distance = True
        self.cutoff_squared = cutoff**2
        
        # Assign module variable parameters from configuration
        self.dtype = dtype
        self.device = device

        # Assign switch function
        switch_class = layer.get_cutoff_fn(switch_fn)
        self.switch_fn = switch_class(
            self.cutoff_short_range, device=self.device, dtype=self.dtype)

        # Set property units for parameter scaling
        self.set_unit_properties(unit_properties)

        return

    def __str__(self):
        return ""Shielded Point Charge Electrostatics""

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {}

    def set_unit_properties(
        self,
        unit_properties: Dict[str, str],
    ):
        """"""
        Set unit conversion factors for compatibility between requested
        property units and applied property units (for physical constants)
        of the module.
        
        Parameters
        ----------
        unit_properties: dict
            Dictionary with the units of the model properties to initialize 
            correct conversion factors.
        
        """"""

        # Get conversion factors
        if unit_properties is None:
            unit_energy = settings._default_units.get('energy')
            unit_positions = settings._default_units.get('positions')
            unit_charge = settings._default_units.get('charge')
            factor_energy, _ = utils.check_units(unit_energy)
            factor_positions, _ = utils.check_units(unit_positions)
            factor_charge, _ = utils.check_units(unit_charge)
        else:
            factor_energy, _ = utils.check_units(
                unit_properties.get('energy'))
            factor_positions, _ = utils.check_units(
                unit_properties.get('positions'))
            factor_charge, _ = utils.check_units(
                unit_properties.get('charge'))
    
        # Convert 1/(2*4*pi*epsilon) from e**2/eV/Ang to model units
        kehalf_ase = 7.199822675975274
        kehalf = torch.tensor(
            [kehalf_ase*factor_charge**2/factor_energy/factor_positions],
            device=self.device, dtype=self.dtype)
        self.register_buffer(
            'kehalf', kehalf)

        return

    def forward(
        self,
        properties: Dict[str, torch.Tensor],
        distances: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        """"""
        Compute shielded electrostatic interaction between atom center point 
        charges.

        Parameters
        ----------
        properties: dict
            system properties including atomic charges
        distances : torch.Tensor
            Distances between all atom pairs in the batch.
        idx_i : torch.Tensor
            Indices of the first atom of each pair.
        idx_j : torch.Tensor
            Indices of the second atom of each pair.

        Returns
        -------
        torch.Tensor
            Dispersion atom energy contribution
        
        """"""

        # Grep atomic charges
        atomic_charges = properties['atomic_charges']

        # Gather atomic charge pairs
        atomic_charges_i = torch.gather(atomic_charges, 0, idx_i)
        atomic_charges_j = torch.gather(atomic_charges, 0, idx_j)

        # Compute shielded distances
        distances_shielded = torch.sqrt(distances**2 + 1.0)

        # Compute switch weights
        switch_off_weights = self.switch_fn(distances)
        switch_on_weights = 1.0 - switch_off_weights

        # Compute electrostatic potential
        if self.split_distance:

            # Shifted Force Coulomb potential method
            # Compute ordinary (unshielded) and shielded contributions
            E_ordinary = (
                1.0/distances
                + distances/self.cutoff_squared
                - 2.0/self.cutoff)
            E_shielded = (
                1.0/distances_shielded
                + distances_shielded/self.cutoff_squared
                - 2.0/self.cutoff)

            # Combine electrostatic contributions
            E = (
                self.kehalf*atomic_charges_i*atomic_charges_j*(
                    switch_off_weights*E_shielded
                    + switch_on_weights*E_ordinary))
            
        else:

            # Compute ordinary (unshielded) and shielded contributions
            E_ordinary = 1.0/distances
            E_shielded = 1.0/distances_shielded

            # Combine electrostatic contributions
            E = (
                self.kehalf*atomic_charges_i*atomic_charges_j
                * (
                    switch_off_weights*E_shielded 
                    + switch_on_weights*E_ordinary)
                )

        # Apply interaction cutoff
        E = torch.where(
            distances <= self.cutoff,
            E,                      # distance <= cutoff
            torch.zeros_like(E))    # distance > cutoff

        # Sum up electrostatic atom pair contribution of each atom
        return utils.scatter_sum(
             E, idx_i, dim=0, shape=atomic_charges.shape)",./Asparagus/asparagus/module/electrostatics.py
PC_damped_electrostatics,"class PC_damped_electrostatics(torch.nn.Module):
    """"""
    Torch implementation of a damped point charge electrostatic model that
    avoids singularities at very close atom pair distances.

    Parameters
    ----------
    cutoff: float
        interaction cutoff distance.
    cutoff_short_range: float
        Short range cutoff distance.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    unit_properties: dict, optional, default None
        Dictionary with the units of the model properties to initialize correct
        conversion factors.
    switch_fn: (str, callable), optional, default None
        Switch function for the short range cutoff.
    truncation: str, optional, default 'force'
        Truncation method of the Coulomb potential at the cutoff range:
            None, 'None': 
                No Coulomb potential shift applied
            'potential':
                Apply shifted Coulomb potential method
                    V_shifted(r) = V_Coulomb(r) - V_Coulomb(r_cutoff)
            'force', 'forces':
                Apply shifted Coulomb force method
                    V_shifted(r) = V_Coulomb(r) - V_Coulomb(r_cutoff)
                        - (dV_Coulomb/dr)|r_cutoff  * (r - r_cutoff)
                    
    **kwargs
        Additional keyword arguments.

    """"""

    def __init__(
        self,
        cutoff: float,
        cutoff_short_range: float,
        device: str,
        dtype: 'dtype',
        unit_properties: Optional[Dict[str, str]] = None,
        switch_fn: Optional[Union[str, object]] = 'Poly6',
        truncation: Optional[str] = 'force',
        **kwargs
    ):

        super(PC_damped_electrostatics, self).__init__()

        # Assign variables
        self.cutoff = cutoff
        if cutoff_short_range is None or cutoff == cutoff_short_range:
            self.cutoff_short_range = cutoff
        else:
            self.cutoff_short_range = cutoff_short_range
        self.cutoff_squared = cutoff**2
        
        # Assign module variable parameters from configuration
        self.dtype = dtype
        self.device = device

        # Assign switch function
        switch_class = layer.get_cutoff_fn(switch_fn)
        self.switch_fn = switch_class(
            self.cutoff_short_range, device=self.device, dtype=self.dtype)

        # Assign truncation method
        if truncation is None or truncation.lower() == 'none':
            self.potential_fn = self.damped_coulomb_fn
        elif truncation.lower() == 'potential':
            self.potential_fn = self.damped_coulomb_sp_fn
        elif truncation.lower() in ['force', 'forces']:
            self.potential_fn = self.damped_coulomb_sf_fn
        else:
            raise SyntaxError(
                ""Truncation method of the Coulomb potential ""
                + f""'{truncation:}' is unknown!\n""
                + ""Available are 'None', 'potential', 'force'."")

        # Set property units for parameter scaling
        self.set_unit_properties(unit_properties)

        return

    def __str__(self):
        return ""Shielded Point Charge Electrostatics""

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {}

    def damped_coulomb_fn(
        self,
        atomic_charges_i: torch.Tensor,
        atomic_charges_j: torch.Tensor,
        distances: torch.Tensor,
        distances_damped: torch.Tensor,
        switch_damped: torch.Tensor,
        switch_ordinary: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Damped Coulomb potential

        """"""
        
        # Compute ordinary and damped contributions
        E_ordinary = 1.0/distances
        E_damped = 1.0/distances_damped

        # Compute damped electrostatics
        E = (
            self.kehalf*atomic_charges_i*atomic_charges_j
            * (switch_damped*E_damped + switch_ordinary*E_ordinary)
            )

        return E

    def damped_coulomb_sp_fn(
        self,
        atomic_charges_i: torch.Tensor,
        atomic_charges_j: torch.Tensor,
        distances: torch.Tensor,
        distances_damped: torch.Tensor,
        switch_damped: torch.Tensor,
        switch_ordinary: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Damped & shifted potential Coulomb interaction

        """"""
        
        # Compute ordinary and damped contributions
        E_ordinary = 1.0/distances
        E_damped = 1.0/distances_damped
        E_shift = -1.0/self.cutoff

        # Compute damped electrostatics
        E = (
            self.kehalf*atomic_charges_i*atomic_charges_j
            * (switch_damped*E_damped + switch_ordinary*E_ordinary + E_shift)
            )

        return E

    def damped_coulomb_sf_fn(
        self,
        atomic_charges_i: torch.Tensor,
        atomic_charges_j: torch.Tensor,
        distances: torch.Tensor,
        distances_damped: torch.Tensor,
        switch_damped: torch.Tensor,
        switch_ordinary: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Damped & shifted forces Coulomb interaction

        """"""
        
        # Compute ordinary and damped contributions
        E_ordinary = 1.0/distances
        E_damped = 1.0/distances_damped
        E_shift = distances/self.cutoff_squared - 2.0/self.cutoff

        # Compute damped electrostatics
        E = (
            self.kehalf*atomic_charges_i*atomic_charges_j
            * (switch_damped*E_damped + switch_ordinary*E_ordinary + E_shift)
            )

        return E

    def set_unit_properties(
        self,
        unit_properties: Dict[str, str],
    ):
        """"""
        Set unit conversion factors for compatibility between requested
        property units and applied property units (for physical constants)
        of the module.
        
        Parameters
        ----------
        unit_properties: dict
            Dictionary with the units of the model properties to initialize 
            correct conversion factors.
        
        """"""

        # Get conversion factors
        if unit_properties is None:
            unit_energy = settings._default_units.get('energy')
            unit_positions = settings._default_units.get('positions')
            unit_charge = settings._default_units.get('charge')
            factor_energy, _ = utils.check_units(unit_energy)
            factor_positions, _ = utils.check_units(unit_positions)
            factor_charge, _ = utils.check_units(unit_charge)
        else:
            factor_energy, _ = utils.check_units(
                unit_properties.get('energy'))
            factor_positions, _ = utils.check_units(
                unit_properties.get('positions'))
            factor_charge, _ = utils.check_units(
                unit_properties.get('charge'))
    
        # Convert 1/(2*4*pi*epsilon) from e**2/eV/Ang to model units
        # Coulomb factor ke is halfed to avoid double counting as atom pairs
        # are handled in both order i<->j and j<->i.
        kehalf_ase = 7.199822675975274
        kehalf = torch.tensor(
            [kehalf_ase*factor_charge**2/factor_energy/factor_positions],
            device=self.device, dtype=self.dtype)
        self.register_buffer(
            'kehalf', kehalf)

        return

    def forward(
        self,
        properties: Dict[str, torch.Tensor],
        distances: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        """"""
        Compute shielded electrostatic interaction between atom center point 
        charges.

        Parameters
        ----------
        properties: dict
            system properties including atomic charges
        distances : torch.Tensor
            Distances between all atom pairs in the batch.
        idx_i : torch.Tensor
            Indices of the first atom of each pair.
        idx_j : torch.Tensor
            Indices of the second atom of each pair.

        Returns
        -------
        torch.Tensor
            Dispersion atom energy contribution
        
        """"""

        # Grep atomic charges
        atomic_charges = properties['atomic_charges']

        # Gather atomic charge pairs
        atomic_charges_i = torch.gather(atomic_charges, 0, idx_i)
        atomic_charges_j = torch.gather(atomic_charges, 0, idx_j)

        # Compute shielded distances
        distances_damped = torch.sqrt(distances**2 + 1.0)

        # Compute switch weights
        switch_damped = self.switch_fn(distances)
        switch_ordinary = 1.0 - switch_damped

        # Compute damped electrostatic Coulomb potential
        E = self.potential_fn(
            atomic_charges_i,
            atomic_charges_j,
            distances,
            distances_damped,
            switch_damped,
            switch_ordinary)

        # Apply interaction cutoff
        E = torch.where(
            distances <= self.cutoff,
            E,                      # distance <= cutoff
            torch.zeros_like(E))    # distance > cutoff

        # Sum up electrostatic atom pair contribution of each atom
        return utils.scatter_sum(
             E, idx_i, dim=0, shape=atomic_charges.shape)",./Asparagus/asparagus/module/electrostatics.py
PC_Dipole_damped_electrostatics,"class PC_Dipole_damped_electrostatics(torch.nn.Module):
    """"""
    Torch implementation of a damped point charge and atomic dipole 
    electrostatic model that avoids singularities at very close atom pair
    distances.

    Parameters
    ----------
    cutoff: float
        interaction cutoff distance.
    cutoff_short_range: float
        Short range cutoff distance.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    unit_properties: dict, optional, default None
        Dictionary with the units of the model properties to initialize correct
        conversion factors.
    switch_fn: (str, callable), optional, default None
        Switch function for the short range cutoff.
    truncation: str, optional, default 'force'
        Truncation method of the Coulomb potential at the cutoff range:
            None, 'None': 
                No Coulomb potential shift applied
            'potential':
                Apply shifted Coulomb potential method
                    V_shifted(r) = V_Coulomb(r) - V_Coulomb(r_cutoff)
            'force', 'forces':
                Apply shifted Coulomb force method
                    V_shifted(r) = V_Coulomb(r) - V_Coulomb(r_cutoff)
                        - (dV_Coulomb/dr)|r_cutoff  * (r - r_cutoff)
                    
    **kwargs
        Additional keyword arguments.

    """"""

    def __init__(
        self,
        cutoff: float,
        cutoff_short_range: float,
        device: str,
        dtype: 'dtype',
        unit_properties: Optional[Dict[str, str]] = None,
        switch_fn: Optional[Union[str, object]] = 'Poly6',
        truncation: Optional[str] = 'force',
        **kwargs
    ):

        super(PC_Dipole_damped_electrostatics, self).__init__()

        # Assign variables
        self.cutoff = cutoff
        if cutoff_short_range is None or cutoff == cutoff_short_range:
            self.cutoff_short_range = cutoff
        else:
            self.cutoff_short_range = cutoff_short_range
        self.cutoff_squared = cutoff**2
        self.cutoff_cubed = self.cutoff_squared*cutoff
        self.cutoff_quartic = self.cutoff_squared*self.cutoff_squared

        # Assign module variable parameters from configuration
        self.dtype = dtype
        self.device = device

        # Assign switch function
        switch_class = layer.get_cutoff_fn(switch_fn)
        self.switch_fn = switch_class(
            self.cutoff_short_range, device=self.device, dtype=self.dtype)

        # Assign truncation method
        if truncation is None or truncation.lower() == 'none':
            self.potential_fn = self.damped_coulomb_fn
        elif truncation.lower() == 'potential':
            self.potential_fn = self.damped_coulomb_sp_fn
        elif truncation.lower() in ['force', 'forces']:
            self.potential_fn = self.damped_coulomb_sf_fn
        else:
            raise SyntaxError(
                ""Truncation method of the Coulomb potential ""
                + f""'{truncation:}' is unknown!\n""
                + ""Available are 'None', 'potential', 'force'."")

        # Set property units for parameter scaling
        self.set_unit_properties(unit_properties)

        return

    def __str__(self):
        return ""Shielded Point Charge and Atomic Dipole Electrostatics""

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""

        return {}

    def damped_coulomb_fn(
        self,
        atomic_charges_i: torch.Tensor,
        atomic_charges_j: torch.Tensor,
        atomic_dipoles_i: torch.Tensor,
        atomic_dipoles_j: torch.Tensor,
        distances: torch.Tensor,
        distances_damped: torch.Tensor,
        switch_damped: torch.Tensor,
        switch_ordinary: torch.Tensor,
        vectors: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Damped Coulomb potential

        """"""

        # Compute ordinary and damped contributions
        E_ordinary = 1.0/distances
        E_damped = 1.0/distances_damped

        # Compute damped reciprocal distances
        chi = (switch_damped*E_damped + switch_ordinary*E_ordinary)

        # Compute damped charge-charge electrostatics
        E = atomic_charges_i*atomic_charges_j*chi

        # Compute damped charge-dipole and dipole-dipole electrostatics
        if vectors is not None:
            
            # Compute powers of damped reciprocal distances
            chi2 = chi**2
            chi3 = chi2*chi

            # Normalize atom pair vectors (vec/chi is canceled within formula)
            chi_vectors = vectors/distances[:, None]

            # Compute dot products of atom pair vector and atomic dipole
            dot_ij = torch.sum(chi_vectors*atomic_dipoles_j, axis=1)
            dot_ji = torch.sum(chi_vectors*atomic_dipoles_i, axis=1)

            # Compute damped charge-dipole electrostatics (times 2 to counter
            # kehalf = ke/2, as charge(i)-dipole(j) != charge(j)-dipole(i))
            E = E + 2.0*atomic_charges_i*dot_ij*chi2

            # Compute damped dipole-dipole electrostatics
            E = E + (
                torch.sum(atomic_dipoles_i*atomic_dipoles_j, axis=1)
                - 3*dot_ij*dot_ji
                )*chi3

        # Sum electrostatic contributions
        E = self.kehalf*E

        return E

    def damped_coulomb_sp_fn(
        self,
        atomic_charges_i: torch.Tensor,
        atomic_charges_j: torch.Tensor,
        atomic_dipoles_i: torch.Tensor,
        atomic_dipoles_j: torch.Tensor,
        distances: torch.Tensor,
        distances_damped: torch.Tensor,
        switch_damped: torch.Tensor,
        switch_ordinary: torch.Tensor,
        vectors: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Damped & shifted potential Coulomb interaction

        """"""
        
        # Compute ordinary and damped contributions
        E_ordinary = 1.0/distances
        E_damped = 1.0/distances_damped

        # Compute damped reciprocal distances
        chi = (switch_damped*E_damped + switch_ordinary*E_ordinary)
        chi_shift = 1.0/self.cutoff

        # Compute damped charge-charge electrostatics
        E = atomic_charges_i*atomic_charges_j*(chi - chi_shift)

        # Compute damped charge-dipole and dipole-dipole electrostatics
        if vectors is not None:
            
            # Compute powers of damped reciprocal distances
            chi2 = chi**2
            chi3 = chi2*chi
            chi2_shift = chi_shift**2
            chi3_shift = chi2_shift*chi_shift

            # Adjust atom pair vectors
            chi_vectors = vectors/distances[:, None]

            # Compute dot products of atom pair vector and atomic dipole
            dot_ij = torch.sum(chi_vectors*atomic_dipoles_j, axis=1)
            dot_ji = torch.sum(chi_vectors*atomic_dipoles_i, axis=1)

            # Compute damped charge-dipole electrostatics (times 2 to counter
            # kehalf = ke/2, as charge(i)-dipole(j) != charge(j)-dipole(i))
            E = E + 2.0*atomic_charges_i*dot_ij*(chi2 - chi2_shift)

            # Compute damped dipole-dipole electrostatics
            E = E + (
                torch.sum(atomic_dipoles_i*atomic_dipoles_j, axis=1)
                - 3*dot_ij*dot_ji
                )*(chi3 - chi3_shift)

        # Sum electrostatic contributions
        E = self.kehalf*E

        return E

    def damped_coulomb_sf_fn(
        self,
        atomic_charges_i: torch.Tensor,
        atomic_charges_j: torch.Tensor,
        atomic_dipoles_i: torch.Tensor,
        atomic_dipoles_j: torch.Tensor,
        distances: torch.Tensor,
        distances_damped: torch.Tensor,
        switch_damped: torch.Tensor,
        switch_ordinary: torch.Tensor,
        vectors: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Damped & shifted forces Coulomb interaction

        """"""
        
        # Compute ordinary and damped contributions
        E_ordinary = 1.0/distances
        E_damped = 1.0/distances_damped

        # Compute damped reciprocal distances
        chi = (switch_damped*E_damped + switch_ordinary*E_ordinary)
        chi_shift = 2.0/self.cutoff - distances/self.cutoff_squared

        # Compute damped charge-charge electrostatics
        E = atomic_charges_i*atomic_charges_j*(chi - chi_shift)

        # Compute damped charge-dipole and dipole-dipole electrostatics
        if vectors is not None:
            
            # Compute powers of damped reciprocal distances
            chi2 = chi**2
            chi3 = chi2*chi
            chi2_shift = (
                3.0/self.cutoff_squared - 2.0*distances/self.cutoff_cubed)
            chi3_shift = (
                4.0/self.cutoff_cubed - 3.0*distances/self.cutoff_quartic)

            # Adjust atom pair vectors
            chi_vectors = vectors/distances[:, None]

            # Compute dot products of atom pair vector and atomic dipole
            dot_ij = torch.sum(chi_vectors*atomic_dipoles_j, axis=1)
            dot_ji = torch.sum(chi_vectors*atomic_dipoles_i, axis=1)

            # Compute damped charge-dipole electrostatics (times 2 to counter
            # kehalf = ke/2, as charge(i)-dipole(j) != charge(j)-dipole(i))
            E = E + 2.0*atomic_charges_i*dot_ij*(chi2 - chi2_shift)

            # Compute damped dipole-dipole electrostatics
            E = E + (
                torch.sum(atomic_dipoles_i*atomic_dipoles_j, axis=1)
                - 3*dot_ij*dot_ji
                )*(chi3 - chi3_shift)

        # Sum electrostatic contributions
        E = self.kehalf*E

        return E

    def set_unit_properties(
        self,
        unit_properties: Dict[str, str],
    ):
        """"""
        Set unit conversion factors for compatibility between requested
        property units and applied property units (for physical constants)
        of the module.
        
        Parameters
        ----------
        unit_properties: dict
            Dictionary with the units of the model properties to initialize 
            correct conversion factors.
        
        """"""

        # Get conversion factors
        if unit_properties is None:
            unit_energy = settings._default_units.get('energy')
            unit_positions = settings._default_units.get('positions')
            unit_charge = settings._default_units.get('charge')
            factor_energy, _ = utils.check_units(unit_energy)
            factor_positions, _ = utils.check_units(unit_positions)
            factor_charge, _ = utils.check_units(unit_charge)
        else:
            factor_energy, _ = utils.check_units(
                unit_properties.get('energy'))
            factor_positions, _ = utils.check_units(
                unit_properties.get('positions'))
            factor_charge, _ = utils.check_units(
                unit_properties.get('charge'))
    
        # Convert 1/(2*4*pi*epsilon) from e**2/eV/Ang to model units
        kehalf_ase = 7.199822675975274
        kehalf = torch.tensor(
            [kehalf_ase*factor_charge**2/factor_energy/factor_positions],
            device=self.device, dtype=self.dtype)
        self.register_buffer(
            'kehalf', kehalf)

        return

    def forward(
        self,
        properties: Dict[str, torch.Tensor],
        distances: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
        vectors: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> torch.Tensor:
        """"""
        Compute shielded electrostatic interaction between atom center point 
        charges and dipoles.

        Parameters
        ----------
        properties: dict
            system properties including atomic charges
        distances : torch.Tensor
            Distances between all atom pairs in the batch.
        idx_i : torch.Tensor
            Indices of the first atom of each pair.
        idx_j : torch.Tensor
            Indices of the second atom of each pair.
        vectors: torch.Tensor, optional, default None
            Connection vectors i->j between all atom pairs in the batch.
            If None, charge-dipole and dipole-dipole interaction are ignored.

        Returns
        -------
        torch.Tensor
            Dispersion atom energy contribution
        
        """"""

        # Grep atomic charges and dipoles
        atomic_charges = properties.get('atomic_charges')
        atomic_dipoles = properties.get('atomic_dipoles')

        # Gather atomic charge and dipole pairs
        atomic_charges_i = torch.gather(atomic_charges, 0, idx_i)
        atomic_charges_j = torch.gather(atomic_charges, 0, idx_j)
        if atomic_dipoles is None or vectors is None:
            atomic_dipoles_i = None
            atomic_dipoles_j = None
        else:
            atomic_dipoles_i = atomic_dipoles[idx_i]
            atomic_dipoles_j = atomic_dipoles[idx_j]

        # Compute shielded distances
        distances_damped = torch.sqrt(distances**2 + 1.0)

        # Compute switch weights
        switch_damped = self.switch_fn(distances)
        switch_ordinary = 1.0 - switch_damped

        # Compute damped electrostatic Coulomb potential
        E = self.potential_fn(
            atomic_charges_i,
            atomic_charges_j,
            atomic_dipoles_i,
            atomic_dipoles_j,
            distances,
            distances_damped,
            switch_damped,
            switch_ordinary,
            vectors)

        # Apply interaction cutoff
        E = torch.where(
            distances <= self.cutoff,
            E,                      # distance <= cutoff
            torch.zeros_like(E))    # distance > cutoff

        # Sum up electrostatic atom pair contribution of each atom
        return utils.scatter_sum(
             E, idx_i, dim=0, shape=atomic_charges.shape)",./Asparagus/asparagus/module/electrostatics.py
ZBL_repulsion,"class ZBL_repulsion(torch.nn.Module):
    """"""
    Torch implementation of a Ziegler-Biersack-Littmark style nuclear
    repulsion model.

    Parameters
    ----------
    cutoff: float
        Upper cutoff distance
    cuton: float
        Lower cutoff distance starting switch-off function
    trainable: bool
        If True, repulsion parameter are trainable. Else, default parameter
        values are fixed.
    device: str
        Device type for model variable allocation
    dtype: dtype object
        Model variables data type
    unit_properties: dict, optional, default {}
        Dictionary with the units of the model properties to initialize correct
        conversion factors.

    """"""

    def __init__(
        self,
        cutoff: float,
        cuton: float,
        trainable: bool,
        device: str,
        dtype: 'dtype',
        unit_properties: Optional[Dict[str, str]] = None,
        **kwargs
    ):
        """"""
        Initialize Ziegler-Biersack-Littmark style nuclear repulsion model.

        """"""

        super(ZBL_repulsion, self).__init__()

        # Assign variables
        self.dtype = dtype
        self.device = device

        # Assign cutoff radii and prepare switch-off parameters
        self.cutoff = torch.tensor(
            [cutoff], device=self.device, dtype=self.dtype)
        if cuton is None:
            self.cuton = torch.tensor(
                [0.0], device=self.device, dtype=self.dtype)
            self.switchoff_range = torch.tensor(
                [cutoff], device=self.device, dtype=self.dtype)
            self.use_switch = True
        elif cuton < cutoff:
            self.cuton = torch.tensor(
                [cuton], device=self.device, dtype=self.dtype)
            self.switchoff_range = torch.tensor(
                [cutoff - cuton], device=self.device, dtype=self.dtype)
            self.use_switch = True
        else:
            self.cuton = None
            self.switchoff_range = None
            self.use_switch = False

        # Initialize repulsion model parameters
        a_coefficient = 0.8854 # Bohr
        a_exponent = 0.23
        phi_coefficients = [0.18175, 0.50986, 0.28022, 0.02817]
        phi_exponents = [3.19980, 0.94229, 0.40290, 0.20162]

        if trainable:
            self.a_coefficient = torch.nn.Parameter(
                torch.tensor(
                    [a_coefficient], device=self.device, dtype=self.dtype))
            self.a_exponent = torch.nn.Parameter(
                torch.tensor(
                    [a_exponent], device=self.device, dtype=self.dtype))
            self.phi_coefficients = torch.nn.Parameter(
                torch.tensor(
                    phi_coefficients, device=self.device, dtype=self.dtype))
            self.phi_exponents = torch.nn.Parameter(
                torch.tensor(
                    phi_exponents, device=self.device, dtype=self.dtype))
        else:
            self.register_buffer(
                ""a_coefficient"",
                torch.tensor(
                    [a_coefficient], device=self.device, dtype=self.dtype))
            self.register_buffer(
                ""a_exponent"",
                torch.tensor(
                    [a_exponent], device=self.device, dtype=self.dtype))
            self.register_buffer(
                ""phi_coefficients"",
                torch.tensor(
                    phi_coefficients, device=self.device, dtype=self.dtype))
            self.register_buffer(
                ""phi_exponents"",
                torch.tensor(
                    phi_exponents, device=self.device, dtype=self.dtype))

        # Unit conversion factors
        self.set_unit_properties(unit_properties)

        return

    def __str__(self):
        return ""Ziegler-Biersack-Littmark style nuclear repulsion model""

    def get_info(self) -> Dict[str, Any]:
        """"""
        Return class information
        """"""
        return {}

    def set_unit_properties(
        self,
        unit_properties: Dict[str, str],
    ):
        """"""
        Set unit conversion factors for compatibility between requested
        property units and applied property units (for physical constants)
        of the module.

        Parameters
        ----------
        unit_properties: dict
            Dictionary with the units of the model properties to initialize
            correct conversion factors.

        """"""

        # Get conversion factors
        if unit_properties is None:
            unit_energy = settings._default_units.get('energy')
            unit_positions = settings._default_units.get('positions')
            factor_energy, _ = utils.check_units(unit_energy, 'Hartree')
            factor_positions, _ = utils.check_units('Bohr', unit_positions)
        else:
            factor_energy, _ = utils.check_units(
                unit_properties.get('energy'), 'Hartree')
            factor_positions, _ = utils.check_units(
                'Bohr', unit_properties.get('positions'))

        # Convert
        # Distances: model to Bohr
        # Energies: Hartree to model
        self.register_buffer(
            ""distances_model2Bohr"",
            torch.tensor(
                [factor_positions], device=self.device, dtype=self.dtype))
        self.register_buffer(
            ""energies_Hatree2model"",
            torch.tensor(
                [factor_energy], device=self.device, dtype=self.dtype))

        # Convert e**2/(4*pi*epsilon) = 1 from 1/Hartree/Bohr to model units
        ke_au = 1.
        ke = torch.tensor(
            [ke_au*self.energies_Hatree2model/self.distances_model2Bohr],
            device=self.device, dtype=self.dtype)
        self.register_buffer('ke', ke)

        return

    def switch_fn(
        self,
        distances: torch.Tensor
    ) -> torch.Tensor:
        """"""
        Computes a smooth switch factors from 1 to 0 in the range from 'cuton'
        to 'cutoff'.

        """"""

        x = (self.cutoff - distances) / self.switchoff_range

        return torch.where(
            distances < self.cuton,
            torch.ones_like(x),
            torch.where(
                distances >= self.cutoff,
                torch.zeros_like(x),
                ((6.0*x - 15.0)*x + 10.0)*x**3
                )
            )

    def forward(
        self,
        atomic_numbers: torch.Tensor,
        distances: torch.Tensor,
        idx_i: torch.Tensor,
        idx_j: torch.Tensor,
    ) -> torch.Tensor:
        """"""
        Compute Ziegler-Biersack-Littmark style nuclear repulsion potential
        in Hartree with atom pair distances in Angstrom.

        Parameters
        ----------
        atomic_numbers : torch.Tensor
            Atomic numbers of all atoms in the batch.
        distances : torch.Tensor
            Distances between all atom pairs in the batch.
        idx_i : torch.Tensor
            Indices of the first atom of each pair.
        idx_j : torch.Tensor
            Indices of the second atom of each pair.

        Returns
        -------
        torch.Tensor
            Nuclear repulsion atom energy contribution

        """"""

        # Compute switch-off function
        if self.use_switch:
            switch_off = self.switch_fn(distances)
        else:
            switch_off = torch.where(
                distances < self.cutoff,
                torch.ones_like(distances),
                torch.zeros_like(distances),
            )

        # Compute atomic number dependent function
        za = atomic_numbers**torch.abs(self.a_exponent)
        a_ij = (
            torch.abs(self.a_coefficient/self.distances_model2Bohr)
            / (za[idx_i] + za[idx_j]))

        # Compute screening function phi
        arguments = distances/a_ij
        coefficients = torch.nn.functional.normalize(
            torch.abs(self.phi_coefficients), p=1.0, dim=0)
        exponents = torch.abs(self.phi_exponents)
        phi = torch.sum(
            coefficients[None, ...]*torch.exp(
                -exponents[None, ...]*arguments[..., None]),
            dim=1)

        # Compute nuclear repulsion potential in model energy unit
        repulsion = (
            0.5*self.ke
            * atomic_numbers[idx_i]*atomic_numbers[idx_j]/distances
            * phi
            * switch_off)

        # Summarize and convert repulsion potential
        Erep = utils.scatter_sum(
            repulsion, idx_i, dim=0, shape=atomic_numbers.shape)

        return Erep",./Asparagus/asparagus/module/repulsion.py
TorchNeighborListRangeSeparated,"class TorchNeighborListRangeSeparated(torch.nn.Module):
    """"""
    Environment provider making use of neighbor lists as implemented in
    TorchAni. Modified to provide neighbor lists for a set of cutoff radii.

    Supports cutoffs and PBCs and can be performed on either CPU or GPU.

    References:
        https://github.com/aiqm/torchani/blob/master/torchani/aev.py

    Parameters
    ----------
    cutoff: list(float)
        List of Cutoff distances

    """"""

    def __init__(
        self,
        cutoff: List[float],
        device: str,
        dtype: object,
    ):
        """"""
        Initialize neighbor list computation class
        """"""

        super().__init__()

        # Assign module variable parameters
        self.device = device
        self.dtype = dtype

        # Check cutoffs
        if utils.is_numeric(cutoff):
            self.cutoff = torch.tensor(
                [cutoff], device=self.device, dtype=self.dtype)
        else:
            self.cutoff = torch.tensor(
                cutoff, device=self.device, dtype=self.dtype)

        self.max_cutoff = torch.max(self.cutoff)

        return

    def forward(
        self,
        coll_batch: Dict[str, torch.Tensor],
        atomic_numbers_cumsum: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """"""
        Build neighbor list for a batch of systems.
        Parameters
        ----------
        coll_batch: dict
            System property batch
        atomic_numbers_cumsum: torch.Tensor, optional, default None
            Cumulative atomic number sum serving as starting index for atom
            length system data lists.

        Returns
        -------
        dict(str, torch.Tensor)
            Updated system batch with atom pair information

        """"""

        # Extract system data
        atomic_numbers = coll_batch['atomic_numbers']
        positions = coll_batch['positions']
        cell = coll_batch['cell']
        pbc = coll_batch['pbc']

        # Check system indices
        if coll_batch.get(""sys_i"") is None:
            sys_i = torch.zeros_like(atomic_numbers)
        else:
            sys_i = coll_batch[""sys_i""]

        # Check for system batch or single system input
        # System batch:
        if coll_batch[""atoms_number""].dim():

            # Compute, eventually, cumulative atomic number list
            if atomic_numbers_cumsum is None:
                atomic_numbers_cumsum = torch.cat(
                    [
                        torch.zeros((1,), dtype=sys_i.dtype),
                        torch.cumsum(coll_batch[""atoms_number""][:-1], dim=0)
                    ],
                    dim=0)

        # Single system
        else:

            # Assign cumulative atomic number list and system index
            atomic_numbers_cumsum = torch.zeros((1,), dtype=sys_i.dtype)
            sys_i = torch.zeros_like(atomic_numbers)

            # Extend periodic system data
            cell = cell[None, ...]
            pbc = pbc[None, ...]

        # Compute atom pair neighbor list
        idcs_i, idcs_j, pbc_offsets = self._build_neighbor_list(
            self.cutoff,
            atomic_numbers,
            positions,
            cell,
            pbc,
            sys_i,
            atomic_numbers_cumsum)

        # Add neighbor lists to batch data
        # 1: Neighbor list of first cutoff (usually short range)
        coll_batch['idx_i'] = idcs_i[0].detach()
        coll_batch['idx_j'] = idcs_j[0].detach()
        if pbc_offsets is not None:
            coll_batch['pbc_offset_ij'] = pbc_offsets[0].detach()
        # 2: If demanded, neighbor list of second cutoff (usually long range)
        if len(idcs_i) > 1:
            coll_batch['idx_u'] = idcs_i[1].detach()
            coll_batch['idx_v'] = idcs_j[1].detach()
            if pbc_offsets is not None:
                coll_batch['pbc_offset_uv'] = pbc_offsets[1].detach()
        # 3+: If demanded, list of neighbor lists of further cutoffs
        if len(idcs_i) > 2:
            coll_batch['idcs_k'] = [idx_i.detach() for idx_i in idcs_i]
            coll_batch['idcs_l'] = [idx_j.detach() for idx_j in idcs_j]
            if pbc_offsets is not None:
                coll_batch['pbc_offsets_l'] = [
                    pbc_offset.detach() for pbc_offset in pbc_offsets]

        return coll_batch

    def _build_neighbor_list(
        self,
        cutoff: List[float],
        atomic_numbers: torch.Tensor,
        positions: torch.Tensor,
        cell: torch.Tensor,
        pbc: torch.Tensor,
        sys_i: torch.Tensor,
        atomic_numbers_cumsum: torch.Tensor,
    ) -> (List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]):

        # Initialize result lists
        idcs_i = [[] for _ in cutoff]
        idcs_j = [[] for _ in cutoff]
        offsets = [[] for _ in cutoff]

        # Iterate over system segments
        for iseg, idx_off in enumerate(atomic_numbers_cumsum):

            # Atom system selection
            select = sys_i == iseg

            # Check if shifts are needed for periodic boundary conditions
            if cell[iseg].dim() == 1:
                if cell[iseg].shape[0] == 3:
                    cell_seg = cell[iseg].diag()
                else:
                    cell_seg = cell[iseg].reshape(3, 3)
            else:
                cell_seg = cell[iseg]

            if torch.any(pbc[iseg]):
                seg_offsets = self._get_shifts(
                    cell_seg, pbc[iseg], self.max_cutoff)
            else:
                seg_offsets = torch.zeros(
                    0, 3, device=positions.device, dtype=positions.dtype)

            # Compute pair indices
            sys_idcs_i, sys_idcs_j, seg_offsets = self._get_neighbor_pairs(
                positions[select], cell_seg, seg_offsets, cutoff)

            # Create bidirectional id arrays, similar to what the ASE
            # neighbor list returns
            bi_idcs_i = [
                torch.cat((sys_idx_i, sys_idx_j), dim=0)
                for sys_idx_i, sys_idx_j in zip(sys_idcs_i, sys_idcs_j)]
            bi_idcs_j = [
                torch.cat((sys_idx_j, sys_idx_i), dim=0)
                for sys_idx_j, sys_idx_i in zip(sys_idcs_j, sys_idcs_i)]

            # Sort along first dimension (necessary for atom-wise pooling)
            for ic, (bi_idx_i, bi_idx_j, seg_offset) in enumerate(
                zip(bi_idcs_i, bi_idcs_j, seg_offsets)
            ):
                sorted_idx = torch.argsort(bi_idx_i)
                sys_idx_i = bi_idx_i[sorted_idx]
                sys_idx_j = bi_idx_j[sorted_idx]

                bi_offset = torch.cat((-seg_offset, seg_offset), dim=0)
                seg_offset = bi_offset[sorted_idx]
                seg_offset = torch.mm(seg_offset.to(cell.dtype), cell_seg)

                # Append pair indices and position offsets
                idcs_i[ic].append(sys_idx_i + idx_off)
                idcs_j[ic].append(sys_idx_j + idx_off)
                offsets[ic].append(seg_offset)

        idcs_i = [
            torch.cat(idx_i, dim=0).to(dtype=atomic_numbers.dtype)
            for idx_i in idcs_i]
        idcs_j = [
            torch.cat(idx_j, dim=0).to(dtype=atomic_numbers.dtype)
            for idx_j in idcs_j]
        offsets = [
            torch.cat(offset, dim=0).to(dtype=positions.dtype)
            for offset in offsets]

        return idcs_i, idcs_j, offsets

    def _get_neighbor_pairs(
        self,
        positions: torch.Tensor,
        cell: torch.Tensor,
        shifts: torch.Tensor,
        cutoff: torch.Tensor,
    ) -> (torch.Tensor, torch.Tensor, torch.Tensor):
        """"""
        Compute pairs of atoms that are neighbors.

        Copyright 2018- Xiang Gao and other ANI developers
        (https://github.com/aiqm/torchani/blob/master/torchani/aev.py)

        Arguments:
            positions (:class:`torch.Tensor`): tensor of shape
                (molecules, atoms, 3) for atom coordinates.
            cell (:class:`torch.Tensor`): tensor of shape (3, 3) of the
                three vectors defining unit cell:
                tensor([[x1, y1, z1], [x2, y2, z2], [x3, y3, z3]])
            shifts (:class:`torch.Tensor`): tensor of shape (?, 3) storing
                shifts
            cutoff (:class:`torch.Tensor`): tensor of shape (?) storing
                cutoff radii
        """"""

        num_atoms = positions.shape[0]
        all_atoms = torch.arange(num_atoms, device=cell.device)

        # 1) Central cell
        pi_center, pj_center = torch.combinations(all_atoms).unbind(-1)
        shifts_center = shifts.new_zeros(pi_center.shape[0], 3)

        # 2) cells with shifts
        # shape convention (shift index, molecule index, atom index, 3)
        num_shifts = shifts.shape[0]
        all_shifts = torch.arange(num_shifts, device=cell.device)
        shift_index, pi, pj = torch.cartesian_prod(
            all_shifts, all_atoms, all_atoms
        ).unbind(-1)
        shifts_outside = shifts.index_select(0, shift_index)

        # 3) combine results for all cells
        shifts_all = torch.cat([shifts_center, shifts_outside])
        pi_all = torch.cat([pi_center, pi])
        pj_all = torch.cat([pj_center, pj])

        # 4) Compute shifts and distance vectors
        shift_values = torch.mm(shifts_all.to(cell.dtype), cell)
        Rij_all = positions[pi_all] - positions[pj_all] + shift_values

        # 5) Compute distances, and find all pairs within cutoff
        # torch.norm(Rij_all, dim=1)
        distances2 = torch.sum(Rij_all**2, dim=1)
        in_cutoffs = [
            torch.nonzero(distances2 < cutoff_i**2, as_tuple=True)
            for cutoff_i in cutoff]

        # 6) Reduce tensors to relevant components
        atom_indices_i, atom_indices_j, offsets = [], [], []
        for in_cutoff in in_cutoffs:
            pair_index = in_cutoff
            atom_indices_i.append(pi_all[pair_index])
            atom_indices_j.append(pj_all[pair_index])
            offsets.append(shifts_all[pair_index])

        return atom_indices_i, atom_indices_j, offsets

    def _get_shifts(
        self,
        cell,
        pbc,
        cutoff
    ) -> torch.Tensor:
        """"""
        Compute the shifts of unit cell along the given cell vectors to make it
        large enough to contain all pairs of neighbor atoms with PBC under
        consideration.

        Copyright 2018- Xiang Gao and other ANI developers
        (https://github.com/aiqm/torchani/blob/master/torchani/aev.py)

        Arguments:
            cell (:class:`torch.Tensor`): tensor of shape (3, 3)
                of the three vectors defining unit cell:
                    tensor([[x1, y1, z1], [x2, y2, z2], [x3, y3, z3]])
            pbc (:class:`torch.Tensor`): boolean vector of size 3 storing
                if pbc is enabled for that direction.
            cutoff (:class:`torch.Tensor`): tensor of shape (1) storing
                cutoff radius

        Returns:
            :class:`torch.Tensor`: long tensor of shifts. the center cell and
                symmetric cells are not included.
        """"""
        reciprocal_cell = cell.inverse().t()
        inverse_lengths = torch.norm(reciprocal_cell, dim=1)

        num_repeats = torch.ceil(cutoff*inverse_lengths).to(cell.dtype)
        num_repeats = torch.where(
            pbc.flatten(),
            num_repeats,
            torch.Tensor([0], device=cell.device).to(cell.dtype)
        )

        r1 = torch.arange(
            1, num_repeats[0] + 1, dtype=cell.dtype, device=cell.device)
        r2 = torch.arange(
            1, num_repeats[1] + 1, dtype=cell.dtype, device=cell.device)
        r3 = torch.arange(
            1, num_repeats[2] + 1, dtype=cell.dtype, device=cell.device)
        o = torch.zeros(1, dtype=cell.dtype, device=cell.device)

        return torch.cat(
            [
                torch.cartesian_prod(r1, r2, r3),
                torch.cartesian_prod(r1, r2, o),
                torch.cartesian_prod(r1, r2, -r3),
                torch.cartesian_prod(r1, o, r3),
                torch.cartesian_prod(r1, o, o),
                torch.cartesian_prod(r1, o, -r3),
                torch.cartesian_prod(r1, -r2, r3),
                torch.cartesian_prod(r1, -r2, o),
                torch.cartesian_prod(r1, -r2, -r3),
                torch.cartesian_prod(o, r2, r3),
                torch.cartesian_prod(o, r2, o),
                torch.cartesian_prod(o, r2, -r3),
                torch.cartesian_prod(o, o, r3),
            ]
        )",./Asparagus/asparagus/module/neighborlist.py
TorchNeighborListRangeSeparatedFragments,"class TorchNeighborListRangeSeparatedFragments(torch.nn.Module):
    """"""
    Environment provider making use of neighbor lists as implemented in
    TorchAni. Modified to provide neighbor lists for a set of cutoff radii and atom system fragment definition.

    Supports cutoffs and PBCs and can be performed on either CPU or GPU.

    References:
        https://github.com/aiqm/torchani/blob/master/torchani/aev.py

    Parameters
    ----------
    cutoff: list(float)
        List of Cutoff distances

    """"""

    def __init__(
        self,
        cutoff: List[float],
        device: str,
        dtype: object,
    ):
        """"""
        Initialize neighbor list computation class
        """"""

        super().__init__()

        # Assign module variable parameters
        self.device = device
        self.dtype = dtype

        # Check cutoffs
        if utils.is_numeric(cutoff):
            self.cutoff = torch.tensor(
                [cutoff], device=self.device, dtype=self.dtype)
        else:
            self.cutoff = torch.tensor(
                cutoff, device=self.device, dtype=self.dtype)
        self.max_cutoff = torch.max(self.cutoff)

        return

    def forward(
        self,
        coll_batch: Dict[str, torch.Tensor],
        atomic_numbers_cumsum: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """"""
        Build neighbor list for a batch of systems.
        Parameters
        ----------
        coll_batch: dict
            System property batch
        atomic_numbers_cumsum: torch.Tensor, optional, default None
            Cumulative atomic number sum serving as starting index for atom
            length system data lists.

        Returns
        -------
        dict(str, torch.Tensor)
            Updated system batch with atom pair information

        """"""

        # Extract system data
        atomic_numbers = coll_batch['atomic_numbers']
        positions = coll_batch['positions']
        cell = coll_batch['cell']
        pbc = coll_batch['pbc']

        # Check system indices
        if coll_batch.get(""sys_i"") is None:
            sys_i = torch.zeros_like(atomic_numbers)
        else:
            sys_i = coll_batch[""sys_i""]

        # Check system fragments
        if coll_batch.get(""fragments"") is None:
            fragments = torch.zeros_like(atomic_numbers)
        else:
            fragments = coll_batch[""fragments""]

        # Check for system batch or single system input
        # System batch:
        if coll_batch[""atoms_number""].dim():

            # Compute, eventually, cumulative atomic number list
            if atomic_numbers_cumsum is None:
                atomic_numbers_cumsum = torch.cat(
                    [
                        torch.zeros((1,), dtype=sys_i.dtype),
                        torch.cumsum(coll_batch[""atoms_number""][:-1], dim=0)
                    ],
                    dim=0)

        # Single system
        else:

            # Assign cumulative atomic number list and system index
            atomic_numbers_cumsum = torch.zeros((1,), dtype=sys_i.dtype)
            sys_i = torch.zeros_like(atomic_numbers)

            # Extend periodic system data
            cell = cell[None, ...]
            pbc = pbc[None, ...]

        # Compute atom pair neighbor list
        idcs_i, idcs_j, pbc_offsets, fidcs_i = self._build_neighbor_list(
            self.cutoff,
            atomic_numbers,
            positions,
            cell,
            pbc,
            sys_i,
            fragments,
            atomic_numbers_cumsum)

        # Add neighbor lists to batch data
        # 1: Neighbor list of first cutoff (usually short range)
        coll_batch['idx_i'] = idcs_i[0].detach()
        coll_batch['idx_j'] = idcs_j[0].detach()
        if pbc_offsets is not None:
            coll_batch['pbc_offset_ij'] = pbc_offsets[0].detach()
        coll_batch['fidx_i'] = fidcs_i[0].detach()
        # 2: If demanded, neighbor list of second cutoff (usually long range)
        if len(idcs_i) > 1:
            coll_batch['idx_u'] = idcs_i[1].detach()
            coll_batch['idx_v'] = idcs_j[1].detach()
            if pbc_offsets is not None:
                coll_batch['pbc_offset_uv'] = pbc_offsets[1].detach()
            coll_batch['fidx_u'] = fidcs_i[1].detach()
        # 3+: If demanded, list of neighbor lists of further cutoffs
        if len(idcs_i) > 2:
            coll_batch['idcs_k'] = [idx_i.detach() for idx_i in idcs_i[2:]]
            coll_batch['idcs_l'] = [idx_j.detach() for idx_j in idcs_j[2:]]
            if pbc_offsets is not None:
                coll_batch['pbc_offsets_l'] = [
                    pbc_offset.detach() for pbc_offset in pbc_offsets]
            coll_batch['fidx_k'] = [fidx_i.detach() for fidx_i in fidcs_i[2:]]

        return coll_batch

    def _build_neighbor_list(
        self,
        cutoff: List[float],
        atomic_numbers: torch.Tensor,
        positions: torch.Tensor,
        cell: torch.Tensor,
        pbc: torch.Tensor,
        sys_i: torch.Tensor,
        fragments: torch.Tensor,
        atomic_numbers_cumsum: torch.Tensor,
    ) -> (
            List[torch.Tensor], List[torch.Tensor],
            List[torch.Tensor], List[torch.Tensor]
        ):

        # Initialize result lists
        idcs_i = [[] for _ in cutoff]
        idcs_j = [[] for _ in cutoff]
        offsets = [[] for _ in cutoff]

        # Iterate over system segments
        for iseg, idx_off in enumerate(atomic_numbers_cumsum):

            # Atom system selection
            select = sys_i == iseg

            # Check if shifts are needed for periodic boundary conditions
            if cell[iseg].dim() == 1:
                if cell[iseg].shape[0] == 3:
                    cell_seg = cell[iseg].diag()
                else:
                    cell_seg = cell[iseg].reshape(3, 3)
            else:
                cell_seg = cell[iseg]

            if torch.any(pbc[iseg]):
                seg_offsets = self._get_shifts(
                    cell_seg, pbc[iseg], self.max_cutoff)
            else:
                seg_offsets = torch.zeros(
                    0, 3, device=positions.device, dtype=positions.dtype)

            # Compute pair indices
            sys_idcs_i, sys_idcs_j, seg_offsets = self._get_neighbor_pairs(
                positions[select], cell_seg, seg_offsets, cutoff)

            # Create bidirectional id arrays, similar to what the ASE
            # neighbor list returns
            bi_idcs_i = [
                torch.cat((sys_idx_i, sys_idx_j), dim=0)
                for sys_idx_i, sys_idx_j in zip(sys_idcs_i, sys_idcs_j)]
            bi_idcs_j = [
                torch.cat((sys_idx_j, sys_idx_i), dim=0)
                for sys_idx_j, sys_idx_i in zip(sys_idcs_j, sys_idcs_i)]

            # Sort along first dimension (necessary for atom-wise pooling)
            for ic, (bi_idx_i, bi_idx_j, seg_offset) in enumerate(
                zip(bi_idcs_i, bi_idcs_j, seg_offsets)
            ):
                sorted_idx = torch.argsort(bi_idx_i)
                sys_idx_i = bi_idx_i[sorted_idx]
                sys_idx_j = bi_idx_j[sorted_idx]

                bi_offset = torch.cat((-seg_offset, seg_offset), dim=0)
                seg_offset = bi_offset[sorted_idx]
                seg_offset = torch.mm(seg_offset.to(cell.dtype), cell_seg)

                # Append pair indices and position offsets
                idcs_i[ic].append(sys_idx_i + idx_off)
                idcs_j[ic].append(sys_idx_j + idx_off)
                offsets[ic].append(seg_offset)

        idcs_i = [
            torch.cat(idx_i, dim=0).to(dtype=atomic_numbers.dtype)
            for idx_i in idcs_i]
        idcs_j = [
            torch.cat(idx_j, dim=0).to(dtype=atomic_numbers.dtype)
            for idx_j in idcs_j]
        offsets = [
            torch.cat(offset, dim=0).to(dtype=positions.dtype)
            for offset in offsets]
        fidcs_i = [
            fragments[idx_i].to(dtype=fragments.dtype)
            for idx_i in idcs_i]

        return idcs_i, idcs_j, offsets, fidcs_i

    def _get_neighbor_pairs(
        self,
        positions: torch.Tensor,
        cell: torch.Tensor,
        shifts: torch.Tensor,
        cutoff: torch.Tensor,
    ) -> (torch.Tensor, torch.Tensor, torch.Tensor):
        """"""
        Compute pairs of atoms that are neighbors.

        Copyright 2018- Xiang Gao and other ANI developers
        (https://github.com/aiqm/torchani/blob/master/torchani/aev.py)

        Arguments:
            positions (:class:`torch.Tensor`): tensor of shape
                (molecules, atoms, 3) for atom coordinates.
            cell (:class:`torch.Tensor`): tensor of shape (3, 3) of the
                three vectors defining unit cell:
                tensor([[x1, y1, z1], [x2, y2, z2], [x3, y3, z3]])
            shifts (:class:`torch.Tensor`): tensor of shape (?, 3) storing
                shifts
            cutoff (:class:`torch.Tensor`): tensor of shape (?) storing
                cutoff radii
        """"""

        num_atoms = positions.shape[0]
        all_atoms = torch.arange(num_atoms, device=cell.device)

        # 1) Central cell
        pi_center, pj_center = torch.combinations(all_atoms).unbind(-1)
        shifts_center = shifts.new_zeros(pi_center.shape[0], 3)

        # 2) cells with shifts
        # shape convention (shift index, molecule index, atom index, 3)
        num_shifts = shifts.shape[0]
        all_shifts = torch.arange(num_shifts, device=cell.device)
        shift_index, pi, pj = torch.cartesian_prod(
            all_shifts, all_atoms, all_atoms
        ).unbind(-1)
        shifts_outside = shifts.index_select(0, shift_index)

        # 3) combine results for all cells
        shifts_all = torch.cat([shifts_center, shifts_outside])
        pi_all = torch.cat([pi_center, pi])
        pj_all = torch.cat([pj_center, pj])

        # 4) Compute shifts and distance vectors
        shift_values = torch.mm(shifts_all.to(cell.dtype), cell)
        Rij_all = positions[pi_all] - positions[pj_all] + shift_values

        # 5) Compute distances, and find all pairs within cutoff
        # torch.norm(Rij_all, dim=1)
        distances2 = torch.sum(Rij_all**2, dim=1)
        in_cutoffs = [
            torch.nonzero(distances2 < cutoff_i**2, as_tuple=True)
            for cutoff_i in cutoff]

        # 6) Reduce tensors to relevant components
        atom_indices_i, atom_indices_j, offsets = [], [], []
        for in_cutoff in in_cutoffs:
            pair_index = in_cutoff
            atom_indices_i.append(pi_all[pair_index])
            atom_indices_j.append(pj_all[pair_index])
            offsets.append(shifts_all[pair_index])

        return atom_indices_i, atom_indices_j, offsets

    def _get_shifts(
        self,
        cell,
        pbc,
        cutoff
    ) -> torch.Tensor:
        """"""
        Compute the shifts of unit cell along the given cell vectors to make it
        large enough to contain all pairs of neighbor atoms with PBC under
        consideration.

        Copyright 2018- Xiang Gao and other ANI developers
        (https://github.com/aiqm/torchani/blob/master/torchani/aev.py)

        Arguments:
            cell (:class:`torch.Tensor`): tensor of shape (3, 3)
                of the three vectors defining unit cell:
                    tensor([[x1, y1, z1], [x2, y2, z2], [x3, y3, z3]])
            pbc (:class:`torch.Tensor`): boolean vector of size 3 storing
                if pbc is enabled for that direction.
            cutoff (:class:`torch.Tensor`): tensor of shape (1) storing
                cutoff radius

        Returns:
            :class:`torch.Tensor`: long tensor of shifts. the center cell and
                symmetric cells are not included.
        """"""
        reciprocal_cell = cell.inverse().t()
        inverse_lengths = torch.norm(reciprocal_cell, dim=1)

        num_repeats = torch.ceil(cutoff*inverse_lengths).to(cell.dtype)
        num_repeats = torch.where(
            pbc.flatten(),
            num_repeats,
            torch.Tensor([0], device=cell.device).to(cell.dtype)
        )

        r1 = torch.arange(
            1, num_repeats[0] + 1, dtype=cell.dtype, device=cell.device)
        r2 = torch.arange(
            1, num_repeats[1] + 1, dtype=cell.dtype, device=cell.device)
        r3 = torch.arange(
            1, num_repeats[2] + 1, dtype=cell.dtype, device=cell.device)
        o = torch.zeros(1, dtype=cell.dtype, device=cell.device)

        return torch.cat(
            [
                torch.cartesian_prod(r1, r2, r3),
                torch.cartesian_prod(r1, r2, o),
                torch.cartesian_prod(r1, r2, -r3),
                torch.cartesian_prod(r1, o, r3),
                torch.cartesian_prod(r1, o, o),
                torch.cartesian_prod(r1, o, -r3),
                torch.cartesian_prod(r1, -r2, r3),
                torch.cartesian_prod(r1, -r2, o),
                torch.cartesian_prod(r1, -r2, -r3),
                torch.cartesian_prod(o, r2, r3),
                torch.cartesian_prod(o, r2, o),
                torch.cartesian_prod(o, r2, -r3),
                torch.cartesian_prod(o, o, r3),
            ]
        )",./Asparagus/asparagus/module/neighborlist.py
MEP,"class MEP:

    '''

    This class calculates the minimum energy path from the transtion state to the most favorable part of the PES.

    It follows the gradient of the vector of the Transition state, therefore it is necessary to calculate the hessian of the transition state.


    Parameters:
    -----------

    atoms: ase.Atoms object
        It is the transition state geometry.
    model_calculator: asparagus.Asparagus object
       An asparagus model.
    eps: float (optional)
        Step size to follow the gradient.
    number_of_steps: int (optional)
        Number of steps to follow the gradient.
    output: str (optional)
        Name of the output trajectory if not defined it is equal to mep.traj.
    output_file: str (optional)
        Name of the output file with the energy and steps of the MEP.
    '''

    def __init__(self,atoms=None,
                 model_calculator=None,
                 eps=0.001,
                 number_of_steps=4000,
                 output='mep.traj',
                 output_file=None,
                 **kwargs):

            #ASE object atoms
            self.atoms = atoms
            #Set up the dynamics
            self.eps = eps
            self.number_of_steps = number_of_steps
            #Output files
            self.output = output
            self.output_file = output_file

            if self.output_file is None:
                self.output_file = self.output + '.txt'
            elif self.output_file is False:
                pass
            else:
                self.output_file = output_file

            if self.atoms is type(str):
                self.atoms = read(self.atoms)

            if self.atoms is None:
                raise ValueError('The transition state geometry is required')

            if model_calculator is None:
                raise ValueError('The model calculator is required')

            # Get the ASE calculator
            self.ase_calculator = model_calculator.get_ase_calculator()

            # Check the implemented properties
            self.implemented_properties =self.ase_calculator.implemented_properties

            if 'forces' not in self.implemented_properties:
                self.implemented_properties.append('forces')

            if 'hessian' not in self.implemented_properties:
                self.implemented_properties.append('hessian')

            #Initialize the trajectory file
            self.mep_trajectory = Trajectory(self.output, 'w', self.atoms,properties=['energy','forces'])

    def get_MEP(self):
        """"""
        This function calculates the minimum energy path from the transition state to the most favorable part of the PES.

        It saves the trajectory in a trajectory file and the energy and steps in a txt file.

        """"""

        initial_system = self.atoms.copy()
        initial_system.set_calculator(self.ase_calculator)

        n_atom = len(initial_system)

        # Calculate the hessian of the transition state
        initial_system.get_potential_energy()
        hessian = np.reshape(initial_system.calc.results['hessian'],(n_atom*3,n_atom*3))

        # Calculate the eigenvalues and eigenvectors of the hessian

        eigenvalues, eigenvectors = np.linalg.eigh(hessian)
        eigenvectors = eigenvectors.T
        v_to_follow = eigenvectors[0].reshape(n_atom,3)

        # get initial positions
        pos = initial_system.get_positions() - self.eps * v_to_follow
        initial_system.set_positions(pos)
        grad_old = v_to_follow

        self.ase_calculator.implemented_properties.remove('hessian')
        steps = []
        energies = []

        for i in range(self.number_of_steps):
            pos = initial_system.get_positions()

            grad = -self.ase_calculator.get_forces()
            grad /= np.linalg.norm(grad)
            pos = pos - self.eps * grad
            initial_system.set_positions(pos)
            ener_i = initial_system.get_potential_energy()
            energies.append(ener_i)
            steps.append(i)
            self.mep_trajectory.write(initial_system)
            if np.linalg.norm(grad - grad_old) < 1e-4:
                break
            grad_old = grad

        if self.output_file is not False:
            np.savetxt(self.output_file, np.c_[steps,energies],header='Step,Energy')



    def write_trajectory(self, system):
        """"""
        Write current image to trajectory file but without constraints
        """"""

        system_noconstraint = system.copy()
        system_noconstraint.set_constraint()
        self.mep_trajectory.write(system_noconstraint)",./Asparagus/asparagus/tools/MEP.py
MEP_NEB,"class MEP_NEB:

    '''

    TO BE FINISHED

    This class calculates the minimum energy path between two geometries in the PES using the nudged elastic band method.

    '''

    def __init__(self,react,
                 prod,
                 atoms_charge=None,
                 config=None,
                 config_file=None,
                 model_checkpoint=None,
                 implemented_properties=None,
                 use_neighbor_list=False,
                 label='mep_neb',
                 nimages=5,
                 fixed_atoms=None,
                 k=0.1,
                 climb=True,
                 fmax=0.05,
                 output='mep_neb.traj',
                 **kwargs
                 ):

        if react is type(str):
            react = read(react)
        if prod is type(str):
            prod = read(prod)

        self.react = react
        self.prod = prod

        if self.fixed_atoms is None:
            self.fixed_atoms = []
        else:
            self.fixed_atoms = fixed_atoms

        self.nimages = nimages
        self.k = k
        self.climb = climb
        self.fmax = fmax
        self.output = output

        ######################################
        # # # Check ASE Calculator Input # # #
        ######################################

        # Assign model parameter configuration library
        if config is None:
            config_data = settings.get_config(
                self.config, config_file, **kwargs)
        else:
            config_data = settings.get_config(
                config, config_file, **kwargs)

        # Check model parameter configuration and set default
        config_data.check()

        # Check for empty config dictionary
        if ""model_directory"" not in config_data:
            raise SyntaxError(
                ""Configuration does not provide information for a model ""
                + ""calculator. Please check the input in 'config'."")

        ##################################
        # # # Prepare NNP Calculator # # #
        ##################################

        # Assign NNP calculator
        if self.model_calculator is None:
            # Assign NNP calculator model
            self.model_calculator = self._get_Calculator(
                config_data,
                **kwargs)

        # Add calculator info to configuration dictionary
        if hasattr(self.model_calculator, ""get_info""):
            config_data.update(
                self.model_calculator.get_info(),
                verbose=False)

        # Initialize checkpoint file manager and load best model
        filemanager = utils.FileManager(config_data, **kwargs)
        if model_checkpoint is None:
            latest_checkpoint = filemanager.load_checkpoint(best=True)
        elif utils.is_integer(model_checkpoint):
            latest_checkpoint = filemanager.load_checkpoint(
                num_checkpoint=model_checkpoint)
        else:
            raise ValueError(
                ""Input 'model_checkpoint' must be either None to load best ""
                + ""model checkpoint or an integer of a respective checkpoint ""
                + ""file."")
        self.model_calculator.load_state_dict(
            latest_checkpoint['model_state_dict'])

        ##################################
        # # # Prepare ASE Calculator # # #
        ##################################
        if implemented_properties is not None:
            self.implemented_properties = implemented_properties
        else:
            self.implemented_properties = ['energy', 'forces']

        if 'energy' not in self.implemented_properties:
            self.implemented_properties.append('energy')

        if 'forces' not in self.implemented_properties:
            self.implemented_properties.append('forces')


        # Create images

        self.images = [self.react]

        self.images += [self.react.copy() for i in range(self.nimages)]

        self.images += [self.prod]

        self.neb = NEB(self.images, k=self.k, climb=self.climb)

        self.neb.interpolate()

        # set_calculators
        self.calcs = [interface.ASE_Calculator(self.model_calculator,
                                                atoms=self.images[i],
                                                atoms_charge=atoms_charge,
                                                implemented_properties=self.implemented_properties,
                                                use_neighbor_list=use_neighbor_list,
                                                label=label) for i in range(self.nimages+2)]


    def get_mep_neb(self):

        # Setting up the constraints and the calculators

        for index, image in enumerate(self.images):
            image.calc = self.calcs[index]
            image.set_constraint(FixAtoms(mask=self.fixed_atoms))

        # Run optimization
        optimizer = FIRE(self.neb, trajectory=self.output)
        optimizer.run(fmax=self.fmax)

    def analyze_neb(self,output=None,show_plot=True,save_plot=False):

        '''
        This function analyzes the NEB calculation and returns the energy barrier and the reaction energy and a NEB plot.

        Parameters
        ----------
        output: The output trajectory from the NEB calculation.
        show_plot: bool (optional) show the plot of the NEB calculation.
        save_plot: bool (optional) save the plot of the NEB calculation.

        Returns
        -------

        '''
        if output is None:
            raise ValueError('The output trajectory is required')

        traj = Trajectory(output)

        # It takes the last trajectory of the NEB calculation, Note that ASE does not read the extreme images of the NEB calculation.
        traj_last = traj[-self.nimages+1:-2]

        nebtools = NEBTools(traj_last)

        Ef, dE = nebtools.get_barrier()
        print('Energy barrier:{} eV'.format(Ef))
        print('Reaction energy:{} eV'.format(dE))

        if show_plot:
            #Plotting set up from ASE NEB tutorial
            fig = plt.figure(figsize=(5.5, 4.0))
            ax = fig.add_axes((0.15, 0.15, 0.8, 0.75))
            nebtools.plot_band(ax)
            if save_plot:
                plt.savefig('neb_plot.png',dpi=300)

        if save_plot and not show_plot:
            fig = nebtools.plot_band()
            fig.savefig('neb_plot.png',dpi=300)",./Asparagus/asparagus/tools/MEP.py
DMC,"class DMC:
    """"""
    This code adapt the diffusion Monte Carlo code from 
    https://github.com/MMunibas/dmc_gpu_PhysNet/tree/main to the asparagus 
    framework.

    As a difference with the original implementation, this code reads
    the initial coordinates and the equilibrium coordinates from the atoms
    object in ASE.

    **Original Message**
    DMC code for the calculation of zero-point energies using PhysNet based 
    PESs on GPUs/CPUs. The calculation is performed in Cartesian coordinates.
    See e.g. American Journal of Physics 64, 633 (1996);
    https://doi.org/10.1119/1.18168 for DMC implementation and 
    https://scholarblogs.emory.edu/bowman/diffusion-monte-carlo/ for Fortran 90 
    implementation.

    Parameters
    ----------
    atoms: (str, ase.Atoms)
        File path to a structure file or an ase.Atoms object of the system
    model_calculator: (str, Callable)
        Either a file path to an Asparagus config file ('.json') or an
        Asparagus main class instance or initialized calculator.
    charge: float, optional, default None
        Total charge of the system. If the charge is required for the
        calculator initialization but 'None', a charge of 0 is assumed.
    optimize: bool, optional, default True
        Perform a structure optimization to determine the minimum potential.
    nwalker: int, optional, default 100
        Number of walkers for the DMC
    nsteps: int, optional, default 1000
        Number of steps for each of the DMC walkers
    eqsteps: int, optional, default 100
        Number of initial equilibration steps for the DMC walkers
    stepsize: float, optional, default 0.1
        Step size for the DMC in imaginary time
    alpha: float, optional, default 1.0
        Alpha parameter for the DMC feed-back parameter, usually proportional
        to 1/stepsize
    max_batch: int, optional, default 128
        Maximum size of the batch to compute walker energies
    seed: int, optional, default: np.random.randint(1E6)
        Specify random seed for atom positions shuffling
    initial_positions: (str, list(float), ase.Atoms), optional, default None
        Initial coordinates for the DMC system as a list of the correct 
        shape in Angstrom, a file path to the respective ASE Atoms file or
        even an ASE Atoms object at initial coordinates itself.
        If None, 'atoms' positions are taken as initial coordinates.
    filename: str, optional, default 'dmc'
        Name tag of the output file where results are going to be stored. 
        The DMC code create 4 files with the same name but different 
        extensions: '.pot', '.log' and '.xyz'. 
        '.pot': Potential energies of the DMC runs.
        '.log': Log information of the runs.
        '.xyz': Two '.xyz' files are generated where the last 10 steps of the
            simulation and the defective geometries are saved respectively.

    """"""

    def __init__(self,
        atoms: Union[str, ase.Atoms],
        model_calculator: Union[str, Callable] = None,
        charge: Optional[float] = None,
        optimize: Optional[bool] = True,
        nwalker: Optional[int] = 100,
        nsteps: Optional[int] = 1000,
        eqsteps: Optional[int] = 100,
        stepsize: Optional[float] = 0.1,
        alpha: Optional[float] = 1.0,
        max_batch: Optional[int] = 128,
        seed: Optional[int] = np.random.randint(1E6),
        initial_positions: Optional[Union[str, List[float], ase.Atoms]] = None,
        filename: Optional[str] = None,
        **kwargs
    ):

        # Check atoms input
        if utils.is_string(atoms):
            self.atoms = io.read(atoms)
        elif utils.is_ase_atoms(atoms):
            self.atoms = atoms
        else:
            raise ValueError(
                f""Invalid 'atoms' input of type '{type(atoms)}'.\n""
                + ""Required is either a path to a structure file ""
                + ""(e.g. '.xyz') or an ASE Atoms object."")

        # Check model calculator input
        if model_calculator is None:
            raise ValueError(
                ""A model calculator must be assigned!\nEither as a file path""
                + ""to an Asparagus config file, as a Asparagus instance or an ""
                + ""Aspargus calculator itself."")
        if utils.is_string(model_calculator):
            model = Asparagus(config=model_calculator)
            self.model_calculator = model.get_model_calculator(
                model_compile=False)
            self.model_unit_properties = model.model_unit_properties
        else:
            if hasattr(model_calculator, 'get_model_calculator'):
                self.model_calculator = model_calculator.get_model_calculator(
                    model_compile=False)
                self.model_unit_properties = (
                    self.model_calculator.model_unit_properties)
            elif hasattr(model_calculator, 'model_unit_properties'):
                self.model_calculator = model_calculator
                self.model_unit_properties = (
                    model_calculator.model_unit_properties)
            else:
                raise ValueError(
                    f""The model calculator is of unknown type!\n"")

        # Read device and dtype information
        self.device = self.model_calculator.device
        self.dtype = self.model_calculator.dtype

        # Check system charge
        if charge is None:
            try:
                atomic_charges = atoms.get_charges()
            except RuntimeError:
                atomic_charges = atoms.get_initial_charges()
            self.charge = np.sum(atomic_charges)
        elif utils.is_numeric(charge):
            self.charge = charge
        else:
            raise ValueError(
                f""Invalid 'charge' input of type '{type(charge)}'.\n""
                + ""Required is either a numeric system charge input or no ""
                + ""input (None) to assign a charge from the ASE Atoms object."")

        # Check optimization flag
        self.optimize = bool(optimize)

        # Check DMC input
        # Number of walkers for the DMC
        if utils.is_numeric(nwalker):
            self.nwalker = int(nwalker)
        else:
            raise ValueError(
                f""Invalid 'nwalker' input of type '{type(nwalker)}'.\n""
                + ""Required is a numeric input!"")
        # Number of steps for the DMC
        if utils.is_numeric(nsteps):
            self.nsteps = int(nsteps)
        else:
            raise ValueError(
                f""Invalid 'nsteps' input of type '{type(nsteps)}'.\n""
                + ""Required is a numeric input!"")
        # Number of equilibration steps for the DMC
        if utils.is_numeric(eqsteps):
            self.eqsteps = eqsteps
        else:
            raise ValueError(
                f""Invalid 'eqsteps' input of type '{type(eqsteps)}'.\n""
                + ""Required is a numeric input!"")
        # Step size for the DMC in imaginary time
        if utils.is_numeric(stepsize):
            self.stepsize = stepsize
        else:
            raise ValueError(
                f""Invalid 'stepsize' input of type '{type(stepsize)}'.\n""
                + ""Required is a numeric input!"")
        # Alpha parameter for the DMC
        if utils.is_numeric(alpha):
            self.alpha = alpha
        else:
            raise ValueError(
                f""Invalid 'alpha' input of type '{type(alpha)}'.\n""
                + ""Required is a numeric input!"")
        # Alpha parameter for the DMC
        if utils.is_numeric(alpha):
            self.alpha = alpha
        else:
            raise ValueError(
                f""Invalid 'alpha' input of type '{type(alpha)}'.\n""
                + ""Required is a numeric input!"")
        # Size of the batch
        if utils.is_numeric(max_batch):
            self.max_batch = int(max_batch)
        else:
            raise ValueError(
                f""Invalid 'max_batch' input of type '{type(max_batch)}'.\n""
                + ""Required is a numeric input!"")
        # Seed for random generator
        if utils.is_numeric(seed):
            self.seed = int(seed)
        else:
            raise ValueError(
                f""Invalid 'seed' input of type '{type(seed)}'.\n""
                + ""Required is a numeric input!"")

        # Check initial coordinates for the DMC runs
        if initial_positions is None:
            self.atoms_initial = self.atoms.copy()
        elif utils.is_string(initial_positions):
            self.atoms_initial = io.read(initial_positions)
        elif utils.is_numeric_array(initial_positions):
            if initial_positions.shape == (len(self.atoms), 3,):
                initial_positions = np.array(
                    initial_positions, dtype=float)
                self.atoms_initial = self.atoms.copy()
                self.atoms_initial.set_positions(initial_positions)
            else:
                raise ValueError(
                    ""Invalid 'initial_positions' input shape ""
                    + f""{initial_positions.shape:} but {(len(atoms), 3,):} is""
                    + ""expected!"")
        elif utils.is_ase_atoms(initial_positions):
            self.atoms_initial = initial_positions
        else:
            raise ValueError(
                ""Invalid 'initial_positions' input of type ""
                + f""'{type(initial_positions)}'!"")

        # Check model properties and get unit conversion from model units to
        # atomic units.
        self.prepare_dmc()
        
        # Check output file name tag
        if filename is None:
            self.filename = ""dmc""
        elif utils.is_string(filename):
            self.filename = filename
        else:
            raise ValueError(
                f""Invalid 'filename' input of type '{type(filename)}'!\n""
                + ""Required is a string for a filename tag."")

        # Initialize DMC logger
        self.logger = Logger_DMC(
            self.filename,
            maxsteps=self.nsteps,
            maxwalker=3*self.nwalker)

        return

    def prepare_dmc(
        self,
    ):
        """"""
        Finalize the DMC preparation

        """"""

        # Check implemented properties
        if 'energy' not in self.model_calculator.model_properties:
            raise ValueError(
                ""The model property 'energy ' is not predicted but required!"")

        # ASE to model position and charge conversion
        self.ase_conversion = {}
        self.ase_conversion['positions'], _ = utils.check_units(
            None, self.model_unit_properties.get('positions'))
        self.ase_conversion['charge'], _ = utils.check_units(
            None, self.model_unit_properties.get('charge'))

        # Initialize conversion dictionary
        self.dmc_conversion = {}
        
        # Get positions and energy conversion factors from model to atomic
        # units
        self.dmc_conversion['positions'], _ = utils.check_units(
            'Bohr', self.model_unit_properties.get('positions'))
        self.dmc_conversion['energy'], _ = utils.check_units(
            'Hartree', self.model_unit_properties.get('energy'))

        # Get mass conversion factor from u to atomic units (electron mass)
        self.dmc_conversion['mass'] = 1822.88848

        # Prepare DMC related system information
        self.natoms = len(self.atoms)
        self.atomic_numbers = self.atoms.get_atomic_numbers()
        self.atomic_symbols = self.atoms.get_chemical_symbols()
        self.atomic_masses = self.atoms.get_masses()

        return

    def run(
        self,
        optimize: Optional[bool] = None,
        optimize_method: Optional[Callable] = BFGS,
        optimize_fmax: Optional[float] = 0.01,
        nwalker: Optional[int] = None,
        nsteps: Optional[int] = None,
        eqsteps: Optional[int] = None,
        stepsize: Optional[float] = None,
        alpha: Optional[float] = None,
        seed: Optional[int] = None,
        store_steps: Optional[int] = 10,
        verbose: Optional[bool] = True,
    ):
        """"""
        Run the diffusion Monte-Carlo simulation.

        Parameters
        ----------
        optimize: bool, optional, default None
            Perform a structure optimization to determine the minimum 
            potential. If None, initial option is taken.
        optimize_method: ase.optimizer, optional, default BFGS
            ASE optimizer to use for the structure optimization.
        optmize_fmax: float, optional, default 0.01
            Maximum force component value used as convergence threshold for
            the optimization.
        nwalker: int, optional, default None
            Number of walkers for the DMC
        nsteps: int, optional, default None
            Number of steps for each of the DMC walkers
        eqsteps: int, optional, default None
            Number of initial equilibration steps for the DMC walkers
        stepsize: float, optional, default None
            Step size for the DMC in imaginary time
        alpha: float, optional, default None
            Alpha parameter for the DMC feed-back parameter, usually proportional
            to 1/stepsize
        seed: int, optional, default None
            Specify random seed for atom positions shuffling
        store_steps: int, optional, default 10
            Number of last DMC steps for which the walkers atom positions are
            stored in a file.
        verbose: bool, optional, default False
            Print DMC simulation progress

        """"""

        # Check input parameter
        if nwalker is None:
            nwalker = self.nwalker
        if nsteps is None:
            nsteps = self.nsteps
        if eqsteps is None:
            eqsteps = self.eqsteps
        if stepsize is None:
            stepsize = self.stepsize
        if alpha is None:
            alpha = self.alpha
        if seed is None:
            seed = self.seed

        # Write log file header
        self.logger.log_begin(
            nwalker,
            nsteps,
            eqsteps,
            stepsize,
            alpha,
            seed)

        # Set random generator seed
        np.random.seed(seed)

        # If requested, perform structure optimization
        if optimize is None:
            optimize = self.optimize
        if optimize:
            self.atoms = self.optimize_atoms(
                self.atoms,
                self.charge,
                optimize_method,
                optimize_fmax)

        # Create initial system batch information
        batch_initial = self.model_calculator.create_batch(
            self.atoms_initial,
            charge=self.charge,
            conversion=self.ase_conversion)
        batch_minimum = self.model_calculator.create_batch(
            self.atoms,
            charge=self.charge,
            conversion=self.ase_conversion)

        # First evaluation of the potential energy
        results_initial = self.model_calculator(
            batch_initial,
            no_derivation=True)
        energy_initial = (
            results_initial['energy'].cpu().detach().numpy()
            * self.dmc_conversion['energy'])
        results_minimum = self.model_calculator(
            batch_minimum,
            no_derivation=True)
        energy_minimum = (
            results_minimum['energy'].cpu().detach().numpy()
            * self.dmc_conversion['energy'])

        # Initialize the DMC
        w_positions, dmc_energy, w_status, w_stepsize = self.init_walker(
            energy_initial,
            energy_minimum,
            self.atoms_initial.get_positions(),
            nwalker,
            nsteps,
            eqsteps,
            stepsize)

        # Write initial state to log file
        self.logger.write_pot(np.sum(w_status), dmc_energy, step=0)

        # Run main DMC loop
        dmc_avg_energy = 0.0
        dmc_total_energy = 0.0
        w_positions_step = np.zeros_like(w_positions)
        for istep in range(nsteps):

            # Start timer
            if verbose:
                start_time = time.time()

            # Execute walk step
            w_positions_step = self.walk(
                w_positions,
                w_status,
                w_stepsize,
                w_positions_step)

            # Execute branching step
            w_positions, w_status, dmc_energy = self.branch(
                w_positions,
                w_positions_step,
                dmc_energy,
                w_status,
                dmc_total_energy,
                energy_minimum,
                nwalker,
                stepsize,
                alpha,
                store_positions=(istep > nsteps - store_steps))

            # Add progression to log file
            self.logger.write_pot(
                np.sum(w_status),
                dmc_energy,
                step=(istep + 1))

            # Add DMC energy contribution to average energy for steps later
            # then the equilibration run
            if istep > eqsteps:
                dmc_avg_energy += dmc_energy

            if verbose and not (istep%10):
                self.logger.log_write(
                    f""Step:  {istep:d}  ""
                    + f""time/step  {time.time() - start_time:.6f}s  ""
                    + f""Number of active walker {np.sum(w_status):d}\n"")

        # Finalize average DMC energy
        dmc_avg_energy = dmc_avg_energy/(nsteps - eqsteps)
        self.logger.write_avg(dmc_avg_energy)

        # Terminate DMC simulation and close files
        self.logger.log_end()
        self.logger.close_files()

        return

    def optimize_atoms(
        self,
        atoms: ase.Atoms,
        charge: float,
        optimize_method: Callable,
        optimize_fmax: float,
    ) -> ase.Atoms:
        """"""
        Perform a structure optimization of the atoms object using the
        model calculator
        
        Parameters
        ----------
        atoms: ase.Atoms
            ASE Atoms object to optimize
        charge: float
            System charge
        optimize_method: ase.optimizer
            ASE optimizer to use for the structure optimization.
        optimize_fmax: float
            Maximum force component value used as convergence threshold for
            the optimization.

        Returns
        -------
        ase.Atoms
            Optimized ASE atoms object

        """"""

        # Check for forces in model properties
        if not 'forces' in self.model_calculator.model_properties:
            self.logger.log_write(
                ""Requested structure optimization are not possible!\n""
                + ""The model calculator does not predict forces.\n"")
            return atoms

        # Prepare ASE calculator of the model calculator
        ase_calculator = interface.ASE_Calculator(
            self.model_calculator,
            atoms=atoms,
            charge=charge)

        # Assign ASE calculator
        atoms.calc = ase_calculator
        
        # Initialize optimizer
        dyn = optimize_method(atoms)
        
        # Perform structure optimization
        dyn.run(fmax=optimize_fmax)

        return atoms

    def init_walker(
        self,
        energy_initial: float,
        energy_minimum: float,
        positions_initial: List[float],
        nwalker: int,
        nsteps: int,
        eqsteps: int,
        stepsize: float,
    ):
        """"""
        Initialize DMC simulation variables such as the atoms positions for
        each walker (w_positions) and the status of the walker (w_status).

        Parameters
        ----------
        energy_initial: float
            Potential energy of the initial atom configuration
        energy_minimum:
            Minimum potential energy of the atom configuration
        positions_initial: list(float)
            Initial atom positions assigned to the walkers
        nwalker: int
            Number of walkers for the DMC
        nsteps: int
            Number of steps for each of the DMC walkers
        eqsteps: int
            Number of initial equilibration steps for the DMC walkers
        stepsize: float
            Step size for the DMC in imaginary time

        Returns
        -------

        """"""

        # Prepare atom mass weighted step size
        w_stepsize = np.sqrt(
            (stepsize/self.dmc_conversion['positions'])
            / (self.atomic_masses*self.dmc_conversion['mass']))

        # Initialize walker positions and walker status
        w_status = np.ones(3*nwalker, dtype=bool)
        w_status[nwalker:] = False
        w_positions = np.zeros([3*nwalker, self.natoms, 3], dtype=float)

        # Set initial atom positions to each walker
        w_positions[:] = positions_initial

        # Assign DMC energy shifted by the minimum potential energy
        dmc_energy = energy_initial - energy_minimum

        return w_positions, dmc_energy, w_status, w_stepsize

    def walk(
        self,
        w_positions: List[float],
        w_status: List[bool],
        w_stepsize: List[float],
        w_positions_step: List[float],
    ) -> List[float]:
        """"""
        Walk routine performs the diffusion process of the replicas/walkter by
        adding random displacement sqrt(deltatau)*rho to the atom positions of
        the alive replicas, where rho is random number from a Gaussian
        distribution

        Parameters
        ----------
        w_positions: list(float)
            Atom coordinates of the walkers
        w_status: list(bool)
            Walker status
        w_stepsize: list(float)
            Atom mass weighted step size
        w_positions_step: list(float)
            Array for the processed atom coordinates of the walkers

        Returns
        -------
        list(float)
            Processed atom coordinates of the walker

        """"""

        # Get size of the random number array
        size = w_positions[w_status].shape

        # Get random Gaussian distributed numbers
        rho = np.random.normal(size=size)

        # Get processed atom positions
        w_positions_step[w_status] = (
            w_positions[w_status]
            + rho*w_stepsize.reshape(1, -1, 1))

        return w_positions_step

    def branch(
        self,
        w_positions: List[float],
        w_positions_step: List[float],
        dmc_energy: float,
        w_status: List[bool],
        dmc_total_energy: float,
        energy_minimum: float,
        nwalker: int,
        stepsize: float,
        alpha: float,
        defective_threshold: Optional[float] = -1.e-5,
        store_positions: Optional[bool] = False,
    ):
        """"""
        Perform The birth-death (branching) process, which follows the
        diffusion step

        Parameters
        ----------
        w_positions: list(float)
            Atom coordinates of the walkers
        w_positions_step: list(float)
            Array for the processed atom coordinates of the walkers
        dmc_energy: float
            Current DMC reference energy to determine walker dying probability
        w_status: list(bool)
            Walker status
        dmc_total_energy:
            Total DMC run energy
        energy_minimum: float
            Atoms system minimum potential energy
        nwalker: int, optional, default None
            Number of walkers for the DMC
        stepsize: float, optional, default None
            Step size for the DMC in imaginary time
        alpha: float, optional, default None
            Alpha parameter for the DMC feed-back parameter, usually proportional
            to 1/stepsize
        defective_threshold: float, optional, default -1.0e-5
            Negative threshold of potential energy below the minimum potential
            energy to decide if walker atom positions predict wrong/defective
            model calculator results.
        store_positions: bool, optional, default False
            Flag if the walker atom positions will be stored to a file.

        """"""

        # Compute number of active walkers
        w_nactive = np.sum(w_status)

        # Update model calculator batch for new walker atom systems
        # and compute walker potential energies
        w_energies = self.compute_energies(
            w_positions_step,
            w_status,
            w_nactive=w_nactive)

        # Shift walker energies by the minimum potential energy
        w_energies = w_energies - energy_minimum

        # Check for energies that are lower than the minimum potential energy
        selection_defective = w_energies < defective_threshold
        flag_defective = np.any(selection_defective)

        # Write defective walkers to file
        if flag_defective:
            self.logger.write_error(
                w_positions_step[w_status][selection_defective],
                w_energies[selection_defective],
                self.atomic_symbols)

        # DMC step acceptance criteria
        probability_threshold = (
            1.0 - np.exp((dmc_energy - w_energies)*stepsize))

        # Test whether one of the walkers has to die, most likely due to
        # high potential energy atom configuration
        probability_dicerole = np.random.uniform(size=w_nactive)
        selection_failed = probability_dicerole < probability_threshold
        flag_failed = np.any(selection_failed)

        # Update DMC walker status
        if flag_defective or flag_failed:

            # Select walker to kill
            selection = np.logical_or(selection_defective, selection_failed)

            # Set walker status to dead and walker energies to zero
            w_status[w_status][selection] = False
            w_energies[selection] = 0.0

        # Compute new total DMC energy
        dmc_total_energy = dmc_total_energy + np.sum(w_energies)

        # If requested, store walker atom positions to file
        if store_positions:
            self.logger.write_last(
                w_positions_step,
                w_status,
                w_energies,
                self.atomic_symbols)

        # Give birth to new walkers if walker energies are lower than the
        # reference DMC energy shown by negative probability threshold.
        probability_birth = probability_threshold < 0.0
        if np.any(probability_birth):

            for iw in np.where(probability_birth)[0]:

                # Skip defective walkers
                if selection_defective[iw]:
                    continue

                # Walker birth criteria
                threshold = -1.0*probability_threshold[iw]
                nbirth = int(threshold)

                # Test wether new walker(s) is(are) born
                dicerole = np.random.uniform()
                if dicerole < (threshold - nbirth):
                    nbirth += 1

                # Initialize new walker eventually
                for ib in range(nbirth):

                    w_positions[w_nactive] = w_positions[iw]
                    w_positions_step[w_nactive] = w_positions_step[iw]
                    w_status[w_nactive] = True
                    dmc_total_energy = dmc_total_energy + w_energies[iw]
                    w_nactive += 1

        # Assign accepted atom positions of active walkers as new walker
        # positions
        for iw, positions_new in enumerate(w_positions_step[w_status]):
            w_positions[iw] = positions_new
            w_status[iw] = True
        w_nactive = iw + 1
        w_positions[iw:] = 0.0
        w_status[iw:] = False
        w_positions_step[:] = 0.0

        # Update DMC reference energy
        dmc_energy = (
            dmc_total_energy/w_nactive
            + alpha*(1.0 - float(w_nactive)/float(nwalker)))

        return w_positions, w_status, dmc_energy

    def compute_energies(
        self,
        w_positions: List[float],
        w_status: List[bool],
        w_nactive: Optional[int] = None,
    ) -> List[float]:
        """"""
        Compute walker potential energies

        Parameters
        ----------
        w_positions: list(float)
            Atom coordinates of the walkers
        w_status: list(bool)
            Walker status
        w_nactive: int, optional, default None
            Number of active walkers

        Returns
        -------
        list(float)
            Walker potential energies

        """"""

        # If not given, compute number of walkers which are active
        if w_nactive is None:
            w_nactive = np.sum(w_status)

        if w_nactive < self.max_batch:

            # If number of walkers is below the maximum batch size limit,
            # compute the walker energies in one batch
            w_batch = self.model_calculator.create_batch_copies(
                self.atoms,
                ncopies=w_nactive,
                positions=w_positions[w_status],
                charge=self.charge,
                conversion=self.ase_conversion)

            w_results = self.model_calculator(
                w_batch,
                no_derivation=True)
            w_energies = (
                w_results['energy'].cpu().detach().numpy()
                * self.dmc_conversion['energy'])

        else:

            # If number of walkers is larger than maximum batch size limit,
            # compute the walker energies in multiple batches
            w_energies = None
            w_end = 0
            while w_end < w_nactive:

                # Prepare batch start and end point parameters
                w_start = w_end
                w_end = w_end + self.max_batch
                if w_end > w_nactive:
                    w_end = w_nactive
                w_nbatch = w_end - w_start

                # Prepare and run batch calculation
                w_batch = self.model_calculator.create_batch_copies(
                    self.atoms,
                    ncopies=w_nbatch,
                    positions=w_positions[w_status][w_start:w_end],
                    charge=self.charge,
                    conversion=self.ase_conversion)

                w_results_batch = self.model_calculator(
                                w_batch,
                                no_derivation=True)
                w_energies_batch = (
                    w_results_batch['energy'].cpu().detach().numpy()
                    * self.dmc_conversion['energy'])

                # Assign batch energies
                if w_energies is None:
                    w_energies = torch.zeros(
                        w_nactive,
                        dtype=w_energies_batch.dtype,
                        device=w_energies_batch.device)
                w_energies[w_start:w_end] = w_energies_batch

        return w_energies",./Asparagus/asparagus/tools/DMC.py
Logger_DMC,"class Logger_DMC:
    """"""
    Class to write the log files of the DMC simulation.

    Parameters
    ----------
    filename: str, optional, default 'dmc'
        Name tag of the output file where results are going to be stored. 
        The DMC code create 4 files with the same name but different 
        extensions: '.pot', '.log' and '.xyz'. 
        '.log': Log information of the runs.
        '.pot': Potential energies of the DMC runs.
        '.xyz': Two '.xyz' files are generated where the last 10 steps of the
            simulation and the defective geometries are saved respectively.
    maxsteps: int, optional, default None
        Expected maximum number of DMC steps just column formatting
    maxwalker: int, optional, default None
        Expected maximum number of DMC walkers just column formatting
    write_interval: int, optional, default 10
        Interval of writing log and potential output to the respective file

    """"""

    def __init__(
        self,
        filename: str,
        maxsteps: Optional[int] = None,
        maxwalker: Optional[int] = None,
        write_interval: Optional[int] = 10,
    ):

        # File name tag
        self.filename = filename
        
        # Logger files
        self.logfile = open(self.filename + "".log"", 'w')
        self.potfile = open(self.filename + "".pot"", 'w')
        self.errorfile = open(""defective_"" + self.filename + "".xyz"", 'w')
        self.lastfile = open(""configs_"" + self.filename + "".xyz"", 'w')

        # Check maximum DMC step and walker input and get number of expected
        # digits for the step counter and alive walkers
        if maxsteps is None:
            self.dimstep = 6
        else:
            self.dimstep = len(str(maxsteps))
        if maxwalker is None:
            self.dimwalker = 4
        else:
            self.dimwalker = len(str(maxwalker))

        # Check write interval parameter
        self.write_interval = int(write_interval)
        if self.write_interval < 1:
            self.write_interval = self.write_interval

        # Initialize log and potential file output message variables
        self.log_message = """"
        self.pot_message = """"

        # Units conversion
        self.au2ang = 0.5291772083
        self.au2cm = 219474.6313710

        return

    def log_begin(
        self,
        nwalker: int,
        nstep: int,
        eqstep: int,
        stepsize: float,
        alpha: float,
        seed: int,
    ):
        """"""
        Subroutine to write header of log file
        logging all job details and the initial parameters of the DMC simulation

        Parameters
        ----------
        nwalker: int
            Number of walkers for the DMC
        nsteps: int
            Number of steps for each of the DMC walkers
        eqsteps: int
            Number of initial equilibration steps for the DMC walkers
        stepsize: float
            Step size for the DMC in imaginary time
        alpha: float
            Alpha parameter for the DMC feed-back parameter, usually 
            proportional to 1/stepsize
        seed: int
            Random seed for atom positions shuffling

        """"""

        # Write log file header
        message = (
            ""  Diffusion Monte-Carlo Run\n""
            + f""    stored in {self.filename:s}\n\n""
            + f""DMC Simulation started at {str(datetime.now()):s}\n""
            + f""Number of random walkers: {nwalker:d}\n""
            + f""Number of total steps: {nstep:d}\n""
            + f""Number of steps before averaging: {eqstep:d}\n""
            + f""Stepsize: {stepsize:.6e}\n""
            + f""Alpha: {alpha:.6e}\n""
            + f""Random seed: {seed:d}\n\n"")
        self.logfile.write(message)

        return

    def log_write(
        self,
        message: str
    ):
        """"""
        Function to write custom message to log file

        """"""
        self.logfile.write(message)
        return

    def log_end(self):
        """"""
        Function to write footer of logfile

        """"""
        
        # Write log file footer
        message = (
            f""DMC Simulation terminated at {str(datetime.now()):s}\n""
            + ""DMC calculation terminated successfully\n"")
        self.logfile.write(message)

        return

    def write_error(
        self,
        w_positions: List[float],
        w_energies: List[float],
        symbols: List[str],
    ):
        """"""
        Subroutine to write '.xyz' file of defective configurations

        Parameters
        ----------
        w_positions: list(float)
            Defective atom positions
        w_energies: list(float)
            Defective potential energies
        symbols: list(str)
            Element symbols of the atom system

        """"""

        # Iteration over defective configurations
        message = """"
        for ip, (positions, energies) in enumerate(
            zip(w_positions, w_energies)
        ):
            message += (
                f""{positions.shape[0]:d}\n""
                + f""{energies*self.au2cm:8.2f}\n"")
            for ia, (pi, si) in enumerate(zip(positions, symbols)):
                message += (
                    f""{si:s}  ""
                    + f""{pi[0]:.8f}  ""
                    + f""{pi[1]:.8f}  ""
                    + f""{pi[2]:.8f}\n"")

        # Write defective configurations
        self.errorfile.write(message)

        return

    def write_last(
        self,
        w_positions: List[float],
        w_status: List[bool],
        w_energies: List[float],
        symbols: List[str],
    ):
        """"""
        Subroutine to write '.xyz' file of DMC walker atom positions for the
        last 'n' steps

        Parameters
        ----------
        w_positions: list(float)
            Defective atom positions
        w_status: list(bool)
            Walker status
        w_energies: list(float)
            Defective potential energies
        symbols: list(str)
            Element symbols of the atom system

        """"""

        # Iteration over walkers
        message = """"
        natoms = len(symbols)
        for iw, status in enumerate(w_status):
            if not status:
                continue
            message += (
                f""{natoms:d}\n""
                + f""{w_energies[iw]*self.au2cm:8.2f}\n"")
            for ia, (pi, si) in enumerate(zip(w_positions[iw], symbols)):
                message += (
                    f""{si:s}  ""
                    + f""{pi[0]:.8f}  ""
                    + f""{pi[1]:.8f}  ""
                    + f""{pi[2]:.8f}\n"")

        # Write last configurations
        self.lastfile.write(message)

        return

    def write_pot(
        self,
        w_alive: int,
        dmc_energy: float,
        step: int,
    ):
        """"""
        Write potential file

        Parameters
        ----------
        w_alive: int
            Number of walker which are alive
        dmc_energy: float
            DMC potential energy
        step: int
            DMC progression step

        """"""

        # Add step potential information
        self.pot_message += (
            f""{step:{self.dimstep:d}d}  ""
            + f""{w_alive:{self.dimwalker:d}d}  ""
            + f""{dmc_energy:8.7f} Hartree  ""
            + f""{dmc_energy*self.au2cm:8.2f} cm**-1\n"")

        # If write interval is reached, write potential information to file
        if step % self.write_interval == 0:
            self.potfile.write(self.pot_message)
            self.pot_message = """"

        return

    def write_avg(
        self,
        w_avg_energy
    ):
        """"""
        Write average DMC energy to log file

        Parameters
        ----------
        dmc_avg_energy: float
            Average DMC energy of the trajectory

        """"""
        
        self.logfile.write(
            ""\nAverage energy of trajectory:  ""
            + f""{w_avg_energy:8.7f} Hartree  ""
            + f""{w_avg_energy*self.au2cm:8.2f} cm**-1\n"")

        return

    def close_files(self):
        """"""
        Close all files

        """"""
        self.potfile.close()
        self.logfile.close()
        self.errorfile.close()
        self.lastfile.close()
        return",./Asparagus/asparagus/tools/DMC.py
AdaptiveSampling,"class AdaptiveSampling:
    def __init__(self, initial_dataset = None, 
                 num_epochs=100, 
                 model_name='adaptive_sampling_test', 
                 properties=['energy', 'forces'], 
                 split_ratio=0.3, 
                 initial_train_size=0.9, 
                 initial_validation_size=0.1, 
                 initial_test_size=0.0, 
                 num_iterations=4):
        self.initial_dataset = None
        self.num_epochs = num_epochs
        self.model_name = model_name
        self.properties = properties
        self.split_ratio = split_ratio
        self.initial_train_size = initial_train_size
        self.initial_validation_size = initial_validation_size
        self.initial_test_size = initial_test_size
        self.num_iterations = num_iterations

    def dataset_split(self):
        data = np.load(self.initial_dataset)
        data_1 = {}
        data_2 = {}
        for key in data:
            array = data[key]
            split_idx = int(len(array) * self.split_ratio)
            array_1 = array[:split_idx]
            array_2 = array[split_idx:]
            data_1[key] = array_1
            data_2[key] = array_2
        np.savez('initial_train.npz', **data_1)
        np.savez('initial_test.npz', **data_2)
        print(f""Splitting initial data into two separate files was performed successfully."")

    def process_npz_to_dat(self, npz_file, dat_file, en_file):
        data = np.load(npz_file)
        N = data['N']
        R = data['R']
        Z = data['Z']
        E = data['E']

        with open(dat_file, 'w') as file:
            for idx in range(len(N)):
                N_line = ' '.join(map(str, N[idx])) if isinstance(N[idx], np.ndarray) else str(N[idx])
                file.write(N_line + '\n\n')

                for j in range(R[idx].shape[0]):
                    Z_str = {1: 'H', 6: 'C', 8: 'O', 0: 'X'}.get(Z[idx, j], 'X')
                    row_str = ' '.join(map(str, [Z_str] + list(R[idx][j])))
                    if '0.0 0.0 0.0' not in row_str:
                        file.write(row_str + '\n')

        with open(en_file, 'w') as file:
            for energy in E:
                E_line = ' '.join(map(str, energy)) if isinstance(energy, np.ndarray) else str(energy)
                file.write(E_line + '\n')

    def calc_energy_for_testing(self, model_config, struct_file, out_file):
        model = Asparagus(config=model_config)
        calc = model.get_ase_calculator()
        structures = read(struct_file, index=':')
        print(""Calculating energies"")
        with open(out_file, 'w') as out_f:
            for atom in structures:
                atom.calc = calc
                e = atom.get_potential_energy()
                calc.reset()
                out_f.write(str(e) + '\n')

    def estimation_extraction(self, train_npz, test_npz, calc_data, ref_data, N, i):
        npz1 = np.load(train_npz)
        npz2 = np.load(test_npz)
        val1 = np.loadtxt(calc_data)
        val2 = np.loadtxt(ref_data)
        diff = val1 - val2
        mean_diff = np.mean(diff)
        indices = np.argsort(np.abs(diff))[-N:]

        combined_data = {key: np.concatenate((npz2[key][indices], npz1[key]), axis=0) for key in npz2.files}
        np.savez(f""train_set_{i}_iteration.npz"", **combined_data)

        new_test_data = {key: np.delete(npz2[key], indices, axis=0) for key in npz2.files}
        np.savez(f""test_set_{i}_iteration.npz"", **new_test_data)
        print(f""Mean difference for iteration {i}: {mean_diff}"")

    def run(self):
        if not os.path.exists('initial_train.npz'):
            self.dataset_split()

        if not os.path.exists(f""{self.model_name}.json""):
            model = Asparagus(
                config=f""{self.model_name}.json"",
                data_source=""initial_train.npz"",
                data_file=""initial_train.db"",
                model_directory=f""{self.model_name}_0_iteration"",
                model_properties=self.properties,
                data_num_train=self.initial_train_size,
                data_num_valid=self.initial_validation_size,
                data_num_test=self.initial_test_size,
                trainer_max_epochs=self.num_epochs,
                data_seed=np.random.randint(1E6),
                trainer_evaluate_testset=False,
            )
            model.train()

        with open(f""{self.model_name}.json"", 'r') as file:
            for line in file:
                if '""data_seed"":' in line:
                    match = re.search(r'""data_seed"":\s*(\d+)', line)
                    if match:
                        data_seed_value = int(match.group(1))
                    break

        if not os.path.exists(""ref_en_0_iteration.dat""):
            self.process_npz_to_dat('initial_test.npz', 'structures_0_iteration.xyz', 'ref_en_0_iteration.dat')

        if not os.path.exists(""pred_energies_0_iteration.dat""):
            self.calc_energy_for_testing(f""{self.model_name}.json"", 'structures_0_iteration.xyz', 'pred_energies_0_iteration.dat')

        self.estimation_extraction('initial_train.npz', 'initial_test.npz', 'pred_energies_0_iteration.dat', 'ref_en_0_iteration.dat', 1000, 1)

        for i in range(1, self.num_iterations + 1):
            if not os.path.exists(f""{self.model_name}_{i}_iteration.json""):
                model = Asparagus(
                    config=f""{self.model_name}_{i}_iteration.json"",
                    data_file=f""{self.model_name}_{i}_iteration.db"",
                    data_source=f""train_set_{i}_iteration.npz"",
                    model_directory=f""{self.model_name}_{i}_iteration"",
                    model_properties=['energy', 'forces'],
                    data_num_train=0.9,
                    data_num_valid=0.1,
                    trainer_evaluate_testset=False,
                    model_checkpoint=f""{self.model_name}_{i-1}_iteration/best/best_model.pt"",
                    trainer_max_epochs=100,
                    data_seed=data_seed_value,
                )
                model.train(reset_best_loss=True)

                self.process_npz_to_dat(f""test_set_{i}_iteration.npz"", f""structures_{i}_iteration.xyz"", f""ref_en_{i}_iteration.dat"")
                self.calc_energy_for_testing(f""{self.model_name}_{i}_iteration.json"", f""structures_{i}_iteration.xyz"", f""pred_energies_{i}_iteration.dat"")
                self.estimation_extraction(f""train_set_{i}_iteration.npz"", f""test_set_{i}_iteration.npz"", f""pred_energies_{i}_iteration.dat"", f""ref_en_{i}_iteration.dat"", 1000, i + 1)",./Asparagus/asparagus/tools/AdaptiveSampling.py
HyperParameterTuning,"class HyperParameterTuning:
    """"""
    Hyperparameter tuning using Ray Tune
    
    """"""
    
    def __init__(
        self,
        model_trainer: Callable,
        #parameter_labels: Union[str, List[str]]:
    ):
        
        config = {
            ""batch_size"": tune.choice([4, 8, 16, 32]),
        }
        
        scheduler = ray.tune.schedulers.ASHAScheduler(
            metric=""loss"",
            mode=""min"",
            max_t=100,
            grace_period=1,
            reduction_factor=2,
        )

        result = ray.tune.run(
            #partial(train_cifar, data_dir=data_dir),
            resources_per_trial={""cpu"": 2, ""gpu"": gpus_per_trial},
            config=config,
            num_samples=10,
            scheduler=scheduler,
        )

    return",./Asparagus/asparagus/tools/HyperParameterTuning.py
MDP,"class MDP:
    """"""
    This class calculates the Minimum Dynamic Path (MDP) [J. Chem. Phys. 150,
    074107 (2019)] for a given molecule starting from the Transition state.

    Parameters:
    -----------
    atoms: ase.Atoms
        Transition state geometry
    model_calculator: asparagus.Asparagus object
       An asparagus model.
    time_step: opt(float)
        Time step for the MD simulation
    langevin: opt(bool)
        If True use Langevin dynamics, if False use Velocity Verlet
    friction: opt(float)
        Friction coefficient for Langevin dynamics only if langevin=True
    temperature: opt(float)
        Temperature for Langevin dynamics only if langevin=True
    eps: opt(float)
        Initial displacement for the MDP
    number_of_steps: opt(int)
        Number of steps for the MDP
    output: opt(str)
        Output file name for the trajectory
    output_file: opt(str)
        Output file name for the log file of energies
    
    """"""

    def __init__(self,atoms=None,
                 model_calculator=None,
                 forward=True,
                 time_step=0.1,
                 langevin=False,
                 friction=0.02,
                 temperature=300,
                 eps=0.0005,
                 number_of_steps=4000,
                 output='mdp.traj',
                 output_file=None,
                 **kwargs):

        # ASE object atoms
        self.atoms = atoms
        # Options for the dynamivs
        self.eps = eps
        self.number_of_steps = number_of_steps
        self.forward = forward
        self.langevin = langevin
        self.time_step = time_step
        # Output files
        self.output = output
        self.output_file = output_file

        if self.langevin:
            self.friction = friction
            self.temperature = temperature
            print('Using Langevin dynamics with friction %s and temperature %s' % (self.friction, self.temperature))
        elif self.langevin is True and (friction is None or temperature is None):
            raise ValueError('The friction and temperature are required for Langevin dynamics')
        else:
            self.friction = None
            self.temperature = None

        if self.output_file is None:
            self.output_file = self.output + '.txt'
        elif self.output_file is False:
            pass
        else:
            self.output_file = output_file

        if self.atoms is type(str):
            self.atoms = read(self.atoms)

        if self.atoms is None:
            raise ValueError('The transition state geometry is required')

        if model_calculator is None:
            raise ValueError('The model calculator is required')

        # Get the ASE calculator
        self.ase_calculator = model_calculator.get_ase_calculator()

        # Check the implemented properties
        self.implemented_properties = self.ase_calculator.implemented_properties

        if 'forces' not in self.implemented_properties:
            self.implemented_properties.append('forces')

        if 'hessian' not in self.implemented_properties:
            self.implemented_properties.append('hessian')


        # Initialize the trajectory file
        self.mdp_trajectory = Trajectory(self.output, 'w', self.atoms, properties=['energy', 'forces'])


    def run_mdp(self,forward=None):
        """"""
        This function runs the MDP calculation.

        Parameters
        ----------
        forward: opt(bool) If True forward, if False backward. This defines the direction of the momentum of the system.
                           Default is None because it is defined in the __init__ function.

        Returns
        -------


        """"""
        if forward is not None:
            warnings.WarningMessage('The forward parameter is rewritten to %s' % forward)
            self.forward = forward

        initial_system = self.atoms.copy()
        initial_system.set_calculator(self.ase_calculator)

        n_atom = len(initial_system)

        # Calculate the hessian of the transition state
        initial_system.get_potential_energy()
        hessian = np.reshape(initial_system.calc.results['hessian'], (n_atom * 3, n_atom * 3))

        # Calculate the eigenvalues and eigenvectors of the hessian

        eigenvalues, eigenvectors = np.linalg.eigh(hessian)
        print('Eigenvalues of the Hessian:', eigenvalues)
        eigenvectors = eigenvectors.T
        normdisp = eigenvectors[0].reshape(n_atom, 3)

        if self.forward:
            normdisp = -normdisp
        else:
            normdisp = normdisp

        initial_system.set_momenta(self.eps * normdisp)

        pos = initial_system.get_positions()

        new_pos = pos + self.eps * normdisp
        initial_system.set_positions(new_pos)

        if self.langevin:
            dyn = Langevin(initial_system, self.time_step * units.fs, self.temperature * units.kB, self.friction)
        else:
            dyn = VelocityVerlet(initial_system, self.time_step * units.fs)

        self.write_trajectory(initial_system)
        self.ase_calculator.implemented_properties.remove('hessian')

        step = []
        eners_pot = []
        eners_kin = []
        eners_tot = []
        for i in range(self.number_of_steps):
            dyn.run(1)
            ener_i = initial_system.get_potential_energy()
            epot = ener_i/n_atom
            ekin = initial_system.get_kinetic_energy()/n_atom
            etot = epot + ekin
            self.write_trajectory(initial_system)
            eners_tot.append(etot)
            eners_pot.append(epot)
            eners_kin.append(ekin)
            step.append(i)

        if self.output_file is not False:
            np.savetxt(self.output_file, np.c_[step,eners_pot,eners_kin,eners_tot],
                       header='Step,Energy Potential,Energy Kinetic,Energy Total')


    def write_trajectory(self, system):
        """"""
        Write current image to trajectory file but without constraints
        """"""

        system_noconstraint = system.copy()
        system_noconstraint.set_constraint()
        self.mdp_trajectory.write(system_noconstraint)",./Asparagus/asparagus/tools/MDP.py
Tester,"class Tester:
    """"""
    Model Prediction Tester Class

    Parameters
    ----------
    config: (str, dict, object), optional, default None
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    data_container: data.DataContainer, optional
        Data container object of the reference test data set.
        If not provided, the data container will be initialized according
        to config input.
    test_datasets: (str, list(str)) optional, default ['test']
        A string or list of strings to define the data sets ('train',
        'valid', 'test') of which the evaluation will be performed.
        By default it is just the test set of the data container object.
        Inputs 'full' or 'all' requests the evaluation of all sets.
    test_properties: (str, list(str)), optional, default None
        Model properties to evaluate which must be available in the
        model prediction and the reference test data set. If None, all
        model properties will be evaluated if available in the test set.
    test_batch_size: int, optional, default None
        Reference dataloader batch size
    test_num_batch_workers: int, optional, default 1
        Number of data loader workers.
    test_directory: str, optional, default '.'
        Directory to store evaluation graphics and data.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for tester class
    _default_args = {
        'test_datasets':                ['test'],
        'test_properties':              None,
        'test_batch_size':              128,
        'test_num_batch_workers':       1,
        'test_directory':               '.',
        }

    # Expected data types of input variables
    _dtypes_args = {
        'test_datasets':                [
            utils.is_string, utils.is_string_array],
        'test_properties':            [
            utils.is_string, utils.is_string_array, utils.is_None],
        'test_batch_size':              [utils.is_integer],
        'test_num_batch_workers':       [utils.is_integer],
        'test_directory':               [utils.is_string],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        data_container: Optional[data.DataContainer] = None,
        test_datasets: Optional[Union[str, List[str]]] = None,
        test_properties: Optional[Union[str, List[str]]] = None,
        test_batch_size: Optional[int] = None,
        test_num_batch_workers: Optional[int] = None,
        test_directory: Optional[str] = None,
        device: Optional[str] = None,
        dtype: Optional[object] = None,
        verbose: Optional[bool] = True,
        **kwargs
    ):
        """"""
        Initialize model tester.

        """"""

        ####################################
        # # # Check Model Tester Input # # #
        ####################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, training),
            check_dtype=utils.get_dtype_args(self, training)
        )

        # Update global configuration dictionary
        config.update(
            config_update,
            config_from=self,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        ################################
        # # # Check Data Container # # #
        ################################

        # Assign DataContainer and test data loader
        if self.data_container is None:
            self.data_container = data.DataContainer(
                config=config,
                **kwargs)

        # Get reference data properties
        self.data_properties = self.data_container.data_properties
        self.data_units = self.data_container.data_unit_properties

        #########################################
        # # # Prepare Reference Data Loader # # #
        #########################################

        # Initialize training, validation and test data loader
        self.data_container.init_dataloader(
            self.test_batch_size,
            self.test_batch_size,
            self.test_batch_size,
            num_workers=self.test_num_batch_workers,
            device=self.device,
            dtype=self.dtype)

        # Prepare list of data set definition for evaluation
        if utils.is_string(self.test_datasets):
            self.test_datasets = [self.test_datasets]
        if 'full' in self.test_datasets or 'all' in self.test_datasets:
            self.test_datasets = self.data_container.get_datalabels()

        # Collect requested data loader
        self.test_data = {
            label: self.data_container.get_dataloader(label)
            for label in self.test_datasets}

        ##########################
        # # # Prepare Tester # # #
        ##########################

        # Check test properties if defined
        self.test_properties = self.check_test_properties(
            self.test_properties,
            self.data_properties)

        # Check test directory
        if not os.path.exists(self.test_directory):
            os.makedirs(self.test_directory)

        return

    def check_test_properties(
        self,
        test_properties: Union[str, List[str]],
        data_properties: List[str],
    ) -> List[str]:
        """"""
        Check availability of 'test_properties' in 'data_properties' and
        return eventually corrected test_properties as list.

        Parameters
        ----------
        test_properties: (str, list(str)), optional, default None
            Model properties to evaluate which must be available in the
            model prediction and the reference test data set. If None, model
            properties will be evaluated as initialized.
        data_properties: list(str), optional, default None
            List of properties available in the reference data set.

        Returns
        -------
        List[str]
            Test properties

        """"""

        # If not defined, take all reference properties, else check
        # availability
        if test_properties is None:
            test_properties = data_properties
        else:
            if utils.is_string(test_properties):
                test_properties = [test_properties]
            checked_properties = []
            for prop in test_properties:
                if prop not in data_properties:
                    self.logger.warning(
                        f""Requested property '{prop}' in "" +
                        ""'test_properties' for the model evaluation "" +
                        ""is not avaible in the reference data set and "" +
                        ""will be ignored!"")
                else:
                    checked_properties.append(prop)
            test_properties = checked_properties

        return test_properties

    def test(
        self,
        model_calculator: torch.nn.Module,
        model_conversion: Optional[Dict[str, float]] = None,
        test_properties: Optional[Union[str, List[str]]] = None,
        test_directory: Optional[str] = None,
        test_plot_correlation: Optional[bool] = True,
        test_plot_histogram: Optional[bool] = False,
        test_plot_residual: Optional[bool] = False,
        test_plot_format: Optional[str] = 'png',
        test_plot_dpi: Optional[int] = 300,
        test_save_csv: Optional[bool] = False,
        test_csv_file: Optional[str] = 'results.csv',
        test_save_npz: Optional[bool] = False,
        test_npz_file: Optional[str] = 'results.npz',
        test_scale_per_atom: Optional[Union[str, List[str]]] = ['energy'],
        verbose: Optional[bool] = True,
        **kwargs,
    ):
        """"""

        Main function to evaluate the model prediction on the test data set.

        Parameters
        ----------
        model_calculator: torch.nn.Module
            NNP model calculator to predict test properties. The prediction
            are done with the given state of parametrization, no checkpoint
            files will be loaded.
        model_conversion: dict(str, float), optional, default None
            Model prediction to reference data unit conversion.
        test_properties: (str, list(str)), optional, default None
            Model properties to evaluate which must be available in the
            model prediction and the reference test data set. If None, model
            properties will be evaluated as initialized.
        test_directory: str, optional, default '.'
            Directory to store evaluation graphics and data.
        test_plot_correlation: bool, optional, default True
            Show evaluation in property correlation plots
            (x-axis: reference property; y-axis: predicted property).
        test_plot_histogram: bool, optional, default False
            Show prediction error spread in histogram plots.
        test_plot_residual: bool, optional, default False
            Show evaluation in residual plots.
            (x-axis: reference property; y-axis: prediction error).
        test_plot_format: str, optional, default 'png'
            Plot figure format (for options see matplotlib.pyplot.savefig()).
        test_plot_dpi: int, optional, default 300
            Plot figure dpi.
        test_save_csv: bool, optional, default False
            Save all model prediction results and respective reference values
            in a csv file.
        test_csv_file: str, optional, default 'results.csv'
            Name tag of the csv file. The respective data set label and 
            property will be added as prefix to the tag:
            ""{label:s}_{property:s}_{test_csv_file:s}""
        test_save_npz: bool, optional, default False
            Save all model prediction results and respective reference values
            in a binary npz file.
        test_npz_file: str, optional, default 'results.npz'
            Name tag of the npz file. The respective data set label and 
            property will be added as prefix to the tag:
            ""{label:s}_{property:s}_{test_csv_file:s}""
        test_scale_per_atom: (str list(str), optional, default ['energy']
            List of properties where the results will be scaled by the number
            of atoms in the particular system.
        verbose: bool, optional, default True
            Print test metrics.

        """"""

        #################################
        # # # Check Test Properties # # #
        #################################

        # Get model properties
        if hasattr(model_calculator, ""model_properties""):
            model_properties = model_calculator.model_properties
        else:
            raise AttributeError(
                ""Model calculator has no 'model_properties' attribute"")
        if hasattr(model_calculator, ""model_ensemble""):
            model_ensemble = model_calculator.model_ensemble
            model_ensemble_num = model_calculator.model_ensemble_num
        else:
            model_ensemble = False
            model_ensemble_num = None

        # Check test properties if defined or take initialized ones
        if test_properties is None:
            test_properties = self.test_properties
        else:
            test_properties = self.check_test_properties(
                test_properties,
                self.data_properties)

        # Check test properties model to reference data conversion
        test_conversion = {}
        for prop in test_properties:
            if model_conversion is None or model_conversion.get(prop) is None:
                test_conversion[prop] = 1.0
            else:
                test_conversion[prop] = model_conversion.get(prop)

        # Check test output directory
        if test_directory is None:
            test_directory = self.test_directory
        elif utils.is_string(test_directory):
            if not os.path.exists(test_directory):
                os.makedirs(test_directory)
        else:
            raise SyntaxError(
                ""Test results output directory input 'test_directory' is not ""
                + ""a string of a valid file path."")

        # Compare model properties with test properties and store properties
        # to evaluate
        eval_properties = []
        for prop in test_properties:
            if prop in model_properties:
                eval_properties.append(prop)
            else:
                self.logger.warning(
                    f""Requested property '{prop}' in "" +
                    ""'test_properties' is not predicted by the "" +
                    ""model calculator and will be ignored!"")

        ##############################
        # # # Compute Properties # # #
        ##############################

        # Get reference atomic energy shifts
        metadata = self.data_container.get_metadata()
        if 'data_atomic_energies_scaling' in metadata:
            data_atomic_energies_scaling_str = metadata.get(
                'data_atomic_energies_scaling')
            data_atomic_energies_scaling = {}
            for key, item in data_atomic_energies_scaling_str.items():
                data_atomic_energies_scaling[int(key)] = item
            max_atomic_number = max([
                int(atomic_number)
                for atomic_number in data_atomic_energies_scaling.keys()])
            atomic_energies_shift = np.zeros(
                max_atomic_number + 1, dtype=float)
            for atomic_number in range(max_atomic_number + 1):
                if atomic_number in data_atomic_energies_scaling:
                    atomic_energies_shift[atomic_number] = (
                        data_atomic_energies_scaling[atomic_number][0])
        else:
            atomic_energies_shift = None

        # Loop over all requested data set
        for label, datasubset in self.test_data.items():

            # Get model cutoffs
            cutoffs = model_calculator.get_cutoff_ranges()
            
            # Set maximum model cutoff for neighbor list calculation
            datasubset.init_neighbor_list(
                cutoff=cutoffs,
                device=self.device,
                dtype=self.dtype)

            # Prepare dictionary for property values, number of atoms per
            # system, and reference energy shifts
            test_prediction = {prop: [] for prop in eval_properties}
            if model_ensemble:
                test_prediction.update(
                    {
                        imodel: {prop: [] for prop in eval_properties}
                        for imodel in range(model_ensemble_num)
                    })
            test_reference = {prop: [] for prop in eval_properties}
            test_prediction['atoms_number'] = []
            test_shifts = {prop: [] for prop in ['energy', 'atomic_energies']}

            # Reset property metrics
            metrics_test = self.reset_metrics(
                eval_properties, model_ensemble, model_ensemble_num)

            # Loop over data batches
            for batch in datasubset:

                # Predict model properties from data batch
                prediction = model_calculator(
                    batch,
                    verbose_results=True)

                # Compute metrics for test properties
                metrics_batch = self.compute_metrics(
                    prediction,
                    batch,
                    eval_properties,
                    test_conversion,
                    model_ensemble,
                    model_ensemble_num)

                # Update average metrics
                self.update_metrics(
                    metrics_test,
                    metrics_batch,
                    eval_properties,
                    model_ensemble,
                    model_ensemble_num)

                # Store prediction and reference data system resolved
                Nsys = len(batch['atoms_number'])
                Natoms = len(batch['atomic_numbers'])
                Npairs = len(batch['idx_i'])
                for prop in eval_properties:
                    
                    # Detach prediction and reference data
                    data_prediction = prediction[prop].detach().cpu().numpy()
                    data_reference = batch[prop].detach().cpu().numpy()

                    # Apply unit conversion of model prediction
                    data_prediction *= test_conversion[prop]

                    # If data are atom resolved
                    if not data_prediction.shape:
                        data_prediction = [data_prediction]
                        data_reference = list(data_reference)
                    elif data_prediction.shape[0] == Natoms:
                        sys_i = batch['sys_i'].cpu().numpy()
                        data_prediction = [
                            list(data_prediction[sys_i == isys])
                            for isys in range(Nsys)]
                        data_reference = [
                            list(data_reference[sys_i == isys])
                            for isys in range(Nsys)]
                    # If data are atom pair resolved
                    elif data_prediction.shape[0] == Npairs:
                        sys_pair_i = (
                            batch['sys_i'][batch['idx_i']].cpu().numpy())
                        data_prediction = [
                            list(data_prediction[sys_pair_i == isys])
                            for isys in range(Nsys)]
                        data_reference = [
                            list(data_reference[sys_pair_i == isys])
                            for isys in range(Nsys)]
                    # Else, it is already system resolved
                    else:
                        data_prediction = list(data_prediction)
                        data_reference = list(data_reference)

                    # Assign prediction and reference data
                    test_prediction[prop] += data_prediction
                    test_reference[prop] += data_reference

                    if model_ensemble:
                        
                        for imodel in range(model_ensemble_num):
                            
                            # Detach prediction and reference data
                            data_prediction = (
                                prediction[imodel][prop].detach().cpu().numpy()
                                )

                            # Apply unit conversion of model prediction
                            data_prediction *= test_conversion[prop]

                            # If data are atom resolved
                            if not data_prediction.shape:
                                data_prediction = [data_prediction]
                            elif data_prediction.shape[0] == Natoms:
                                sys_i = batch['sys_i'].cpu().numpy()
                                data_prediction = [
                                    list(data_prediction[sys_i == isys])
                                    for isys in range(Nsys)]
                            # If data are atom pair resolved
                            elif data_prediction.shape[0] == Npairs:
                                sys_pair_i = (
                                    batch['sys_i'][
                                        batch['idx_i']].cpu().numpy())
                                data_prediction = [
                                    list(data_prediction[sys_pair_i == isys])
                                    for isys in range(Nsys)]
                            # Else, it is already system resolved
                            else:
                                data_prediction = list(data_prediction)

                            # Assign prediction data
                            test_prediction[imodel][prop] += data_prediction

                # Store atom numbers
                test_prediction['atoms_number'] += list(
                    batch['atoms_number'].cpu().numpy())

                # Compute energy and atomic energies shifts
                test_shifts_energy = np.zeros(Nsys, dtype=float)
                test_shifts_atomic_energies = np.zeros(Natoms, dtype=float)
                if atomic_energies_shift is None:
                    test_shifts['energy'] += list(test_shifts_energy)
                    test_shifts['atomic_energies'] += list(
                        test_shifts_atomic_energies)
                else:
                    atomic_numbers = batch['atomic_numbers'].cpu().numpy()
                    sys_i = batch['sys_i'].cpu().numpy()
                    test_shifts_atomic_energies = (
                        atomic_energies_shift[atomic_numbers])
                    np.add.at(
                        test_shifts_energy,
                        sys_i,
                        test_shifts_atomic_energies)
                    test_shifts['energy'] += list(test_shifts_energy)
                    test_shifts['atomic_energies'] += list(
                        test_shifts_atomic_energies)

            # Print metrics
            if verbose:
                self.print_metric(
                    metrics_test,
                    eval_properties,
                    label,
                    model_ensemble,
                    model_ensemble_num)

            ###########################
            # # # Save Properties # # #
            ###########################

            # Save test prediction to files
            if test_save_csv:
                self.save_csv(
                    test_prediction,
                    test_reference,
                    test_shifts,
                    label,
                    test_directory,
                    test_csv_file)
                if model_ensemble:
                    for imodel in range(model_ensemble_num):
                        test_directory_model = os.path.join(
                            test_directory, f""{imodel:d}"")
                        if not os.path.exists(test_directory_model):
                            os.makedirs(test_directory_model)
                        self.save_csv(
                            test_prediction[imodel],
                            test_reference,
                            test_shifts,
                            label,
                            test_directory_model,
                            test_csv_file,
                            imodel=imodel)
            if test_save_npz:
                self.save_npz(
                    test_prediction,
                    test_reference,
                    test_shifts,
                    label,
                    test_directory,
                    test_npz_file)
                if model_ensemble:
                    for imodel in range(model_ensemble_num):
                        test_directory_model = os.path.join(
                            test_directory, f""{imodel:d}"")
                        if not os.path.exists(test_directory_model):
                            os.makedirs(test_directory_model)
                        self.save_npz(
                            test_prediction[imodel],
                            test_reference,
                            test_shifts,
                            label,
                            test_directory_model,
                            test_npz_file,
                            imodel=imodel)

            ###########################
            # # # Plot Properties # # #
            ###########################

            # Check input for scaling per atom and prepare atom number scaling
            if utils.is_string(test_scale_per_atom):
                test_scale_per_atom = [test_scale_per_atom]
            test_property_scaling = {}
            for prop in eval_properties:
                if prop in test_scale_per_atom:
                    test_property_scaling[prop] = (
                        1./np.array(
                            test_prediction['atoms_number'], dtype=float)
                        )
                else:
                    test_property_scaling[prop] = None

            # Plot correlation between model and reference properties
            if test_plot_correlation:
                for prop in eval_properties:
                    self.plot_correlation(
                        label,
                        prop,
                        self.plain_data(test_prediction[prop]),
                        self.plain_data(test_reference[prop]),
                        self.data_units[prop],
                        metrics_test[prop],
                        test_property_scaling[prop],
                        test_directory,
                        test_plot_format,
                        test_plot_dpi)
                    if model_ensemble:
                        for imodel in range(model_ensemble_num):
                            test_directory_model = os.path.join(
                                test_directory, f""{imodel:d}"")
                            if not os.path.exists(test_directory_model):
                                os.makedirs(test_directory_model)
                            self.plot_correlation(
                                label,
                                prop,
                                self.plain_data(test_prediction[imodel][prop]),
                                self.plain_data(test_reference[prop]),
                                self.data_units[prop],
                                metrics_test[prop][imodel],
                                test_property_scaling[prop],
                                test_directory_model,
                                test_plot_format,
                                test_plot_dpi)

            # Plot histogram of the prediction error
            if test_plot_histogram:
                for prop in eval_properties:
                    self.plot_histogram(
                        label,
                        prop,
                        self.plain_data(test_prediction[prop]),
                        self.plain_data(test_reference[prop]),
                        self.data_units[prop],
                        metrics_test[prop],
                        test_directory,
                        test_plot_format,
                        test_plot_dpi)
                    if model_ensemble:
                        for imodel in range(model_ensemble_num):
                            test_directory_model = os.path.join(
                                test_directory, f""{imodel:d}"")
                            if not os.path.exists(test_directory_model):
                                os.makedirs(test_directory_model)
                            self.plot_histogram(
                                label,
                                prop,
                                self.plain_data(test_prediction[imodel][prop]),
                                self.plain_data(test_reference[prop]),
                                self.data_units[prop],
                                metrics_test[prop][imodel],
                                test_directory_model,
                                test_plot_format,
                                test_plot_dpi)

            # Plot histogram of the prediction error
            if test_plot_residual:
                for prop in eval_properties:
                    self.plot_residual(
                        label,
                        prop,
                        self.plain_data(test_prediction[prop]),
                        self.plain_data(test_reference[prop]),
                        self.data_units[prop],
                        metrics_test[prop],
                        test_property_scaling[prop],
                        test_directory,
                        test_plot_format,
                        test_plot_dpi)
                    if model_ensemble:
                        for imodel in range(model_ensemble_num):
                            test_directory_model = os.path.join(
                                test_directory, f""{imodel:d}"")
                            if not os.path.exists(test_directory_model):
                                os.makedirs(test_directory_model)
                            self.plot_residual(
                                label,
                                prop,
                                self.plain_data(test_prediction[imodel][prop]),
                                self.plain_data(test_reference[prop]),
                                self.data_units[prop],
                                metrics_test[prop][imodel],
                                test_property_scaling[prop],
                                test_directory_model,
                                test_plot_format,
                                test_plot_dpi)

        return

    @staticmethod
    def is_imported(
        module: str
    ) -> bool:
        """"""
        Check if a module is imported.
        
        Parameters
        ----------
        module: str
            Module name
            
        Returns
        -------
        bool
            Module availability flag on the system

        """"""

        return module in sys.modules

    def save_npz(
        self,
        prediction: Dict[str, np.ndarray],
        reference: Dict[str, np.ndarray],
        shifts: Dict[str, np.ndarray],
        label: str,
        test_directory: str,
        npz_file: str,
        imodel: Optional[int] = None,
    ):
        """"""
        Save results of the test set to a binary npz file.

        Parameters
        ----------
        prediction: dict
            Dictionary of the property predictions to save.
        prediction: dict
            Dictionary of the reference property values to save.
        shifts: dict
            Dictionary of the reference property shifts.
        label: str
            Dataset label for the npz file prefix.
        test_directory: str
            Directory to save the npz file.
        npz_file: str
            Name tag of the npz file.
        imodel: int, optional, default None
            Model index for the respective model in the model ensemble.

        """"""

        # Check for .npz file extension
        if 'npz' != npz_file.split('.')[-1]:
            npz_file += '.npz'

        # Iterate over properties
        for prop, pred in prediction.items():
            
            # Check property in reference data, if not skip
            if not prop in reference:
                continue
            
            # Prepare npz file name
            npz_file_prop = os.path.join(
                test_directory, f""{label:s}_{prop:s}_{npz_file:s}"")

            # Prepare data
            if prop in shifts:
                results_np = np.column_stack((
                    np.array(pred).reshape(-1),
                    np.array(reference[prop]).reshape(-1),
                    np.array(shifts[prop]).reshape(-1))
                )
                columns_np=[
                    ""prediction"", ""reference"", ""shift""]
            else:
                results_np = np.column_stack((
                    np.array(pred).reshape(-1),
                    np.array(reference[prop]).reshape(-1))
                )
                columns_np=[
                    ""prediction"", ""reference""]
            
            # Store data in npz format generated via the pandas data frame
            if self.is_imported(""pandas""):
                results = pd.DataFrame(
                    results_np,
                    columns=columns_np
                    )
                np.savez(
                    npz_file_prop,
                    **{
                        column: results[column].values
                        for column in results.columns}
                    )
            else:
                self.logger.warning(
                    ""Module 'pandas' is not available. ""
                    + ""Test properties are not written to a npz file!"")

            # Print info
            if imodel is None:
                addition = """"
            else:
                addition = f"" of model {imodel:d}""
            self.logger.info(
                f""Prediction results{addition:s} and reference data for the ""
                + f""dataset '{label:s}' and property '{prop:s}' are saved in:""
                + f""\n'{npz_file_prop:s}'."")

        return

    def save_csv(
        self,
        prediction: Dict[str, np.ndarray],
        reference: Dict[str, np.ndarray],
        shifts: Dict[str, np.ndarray],
        label: str,
        test_directory: str,
        csv_file: str,
        imodel: Optional[int] = None,
    ):
        """"""
        Save results of the data set to a csv file.

        Parameters
        ----------
        prediction: dict
            Dictionary of the property predictions to save.
        prediction: dict
            Dictionary of the reference property values to save.
        shifts: dict
            Dictionary of the reference property shifts.
        label: str
            Dataset label for the csv file prefix.
        test_directory: str
            Directory to save the csv file.
        csv_file: str
            Name tag of the csv file.
        imodel: int, optional, default None
            Model index for the respective model in the model ensemble.

        """"""

        # Check for .csv file extension
        if 'csv' != csv_file.split('.')[-1]:
            csv_file += '.csv'

        # Iterate over properties
        for prop, pred in prediction.items():
            
            # Check property in reference data, if not skip
            if not prop in reference:
                continue
            
            # Prepare csv file name
            csv_file_prop = os.path.join(
                test_directory, f""{label:s}_{prop:s}_{csv_file:s}"")

            # Prepare data
            if prop in shifts:
                results_np = np.column_stack((
                    np.array(pred).reshape(-1),
                    np.array(reference[prop]).reshape(-1),
                    np.array(shifts[prop]).reshape(-1))
                )
                columns_np=[
                    f""{prop:s} prediction"", "" reference"", "" shift""]
            else:
                results_np = np.column_stack((
                    np.array(pred).reshape(-1),
                    np.array(reference[prop]).reshape(-1))
                )
                columns_np=[
                    f""{prop:s} prediction"", "" reference""]
            
            # Store data in csv format generated via the pandas data frame
            if self.is_imported(""pandas""):
                results = pd.DataFrame(
                    results_np,
                    columns=columns_np
                    )
                results.to_csv(csv_file_prop, index=False)
            else:
                self.logger.warning(
                    ""Module 'pandas' is not available. ""
                    + ""Test properties are not written to a csv file!"")

            # Print info
            if imodel is None:
                addition = """"
            else:
                addition = f"" of model {imodel:d}""
            self.logger.info(
                f""Prediction results{addition:s} and reference data for the ""
                + f""dataset '{label:s}' and property '{prop:s}' are saved in:""
                + f""\n'{csv_file_prop:s}'."")

        return

    def reset_metrics(
        self,
        test_properties: List[str],
        model_ensemble: bool,
        model_ensemble_num: int,
    ) -> Dict[str, float]:
        """"""
        Reset the metrics dictionary.

        Parameters
        ----------
        test_properties: list(str)
            List of properties to restart
        model_ensemble: bool
            Model calculator or model ensemble flag
        model_ensemble_num: int
            Model ensemble calculator number

        Returns
        -------
        dict(str, dict(str, float))
            Property metrics dictionary

        """"""

        # Initialize metrics dictionary
        metrics = {}

        # Add data counter
        metrics['Ndata'] = 0

        # Add training property metrics
        for prop in test_properties:
            metrics[prop] = {
                'mae': 0.0,
                'mse': 0.0}
            if model_ensemble:
                metrics[prop]['std'] = 0.0
                for imodel in range(model_ensemble_num):
                    metrics[prop][imodel] = {
                        'mae': 0.0,
                        'mse': 0.0}

        return metrics

    def compute_metrics(
        self,
        prediction: Dict[str, Any],
        reference: Dict[str, Any],
        test_properties: List[str],
        test_conversion: Dict[str, float],
        model_ensemble: bool,
        model_ensemble_num: int,
    ) -> Dict[str, float]:
        """"""
        Compute the metrics mean absolute error (MAE) and mean squared error
        (MSE) for the test set.

        Parameters
        ----------
        prediction: dict
            Dictionary of the model predictions
        reference: dict
            Dictionary of the reference data
        test_properties: list(str)
            List of properties to evaluate.
        test_conversion: dict(str, float)
            Model prediction to test data unit conversion.
        model_ensemble: bool
            Model calculator or model ensemble flag
        model_ensemble_num: int
            Model ensemble calculator number

        Returns
        -------
        dict(str, dict(str, float))
            Property metrics dictionary

        """"""

        # Initialize metrics dictionary
        metrics = {}

        # Add batch size
        metrics['Ndata'] = reference[
            test_properties[0]].size()[0]

        # Iterate over test properties
        mae_fn = torch.nn.L1Loss(reduction=""mean"")
        mse_fn = torch.nn.MSELoss(reduction=""mean"")
        for ip, prop in enumerate(test_properties):

            # Initialize single property metrics dictionary
            metrics[prop] = {}

            # Compute MAE and MSE
            metrics[prop]['mae'] = mae_fn(
                torch.flatten(prediction[prop])
                * test_conversion[prop],
                torch.flatten(reference[prop]))
            metrics[prop]['mse'] = mse_fn(
                torch.flatten(prediction[prop])
                * test_conversion[prop],
                torch.flatten(reference[prop]))

            # For model ensembles
            if model_ensemble:
                
                # Compute mean standard deviation
                prop_std = f""std_{prop:s}""
                metrics[prop]['std'] = torch.mean(prediction[prop_std])
                
                #  Compute single calculator statistics
                for imodel in range(model_ensemble_num):
                    metrics[prop][imodel] = {}
                    metrics[prop][imodel]['mae'] = mae_fn(
                        torch.flatten(prediction[imodel][prop])
                        * test_conversion[prop],
                        torch.flatten(reference[prop]))
                    metrics[prop][imodel]['mse'] = mse_fn(
                        torch.flatten(prediction[imodel][prop])
                        * test_conversion[prop],
                        torch.flatten(reference[prop]))

        return metrics

    def update_metrics(
        self,
        metrics: Dict[str, float],
        metrics_update: Dict[str, float],
        test_properties: List[str],
        model_ensemble: bool,
        model_ensemble_num: int,
    ) -> Dict[str, float]:
        """"""
        Update the metrics dictionary.

        Parameters
        ----------
        metrics: dict
            Dictionary of the new metrics
        metrics_update: dict
            Dictionary of the metrics to update
        test_properties: list(str)
            List of properties to evaluate
        model_ensemble: bool
            Model calculator or model ensemble flag
        model_ensemble_num: int
            Model ensemble calculator number

        Returns
        -------
        dict(str, dict(str, float))
            Property metrics dictionary

        """"""

        # Get data sizes and metric ratio
        Ndata = metrics['Ndata']
        Ndata_update = metrics_update['Ndata']
        fdata = float(Ndata)/float((Ndata + Ndata_update))
        fdata_update = 1. - fdata

        # Update metrics
        metrics['Ndata'] = metrics['Ndata'] + metrics_update['Ndata']
        for prop in test_properties:
            for metric in ['mae', 'mse']:
                metrics[prop][metric] = (
                    fdata*metrics[prop][metric]
                    + fdata_update*metrics_update[prop][metric].detach().item()
                    )
            if model_ensemble:
                metrics[prop]['std'] = (
                    fdata*metrics[prop]['std']
                    + fdata_update*metrics_update[prop]['std'].detach().item()
                    )
                for imodel in range(model_ensemble_num):
                    for metric in ['mae', 'mse']:
                        metrics[prop][imodel][metric] = (
                            fdata*metrics[prop][imodel][metric]
                            + fdata_update
                            * (metrics_update
                               )[prop][imodel][metric].detach().item()
                            )

        return metrics

    def print_metric(
        self,
        metrics: Dict[str, float],
        test_properties: List[str],
        test_label: str,
        model_ensemble: bool,
        model_ensemble_num: int,
    ):
        """"""
        Print the values of MAE and RMSE for the test set.

        Parameters
        ----------
        metrics: dict
            Dictionary of the property metrics
        test_properties: list(str)
            List of properties to evaluate
        test_label: str
            Label of the reference data set
        model_ensemble: bool
            Model calculator or model ensemble flag
        model_ensemble_num: int
            Model ensemble calculator number

        """"""

        # Prepare label input
        if len(test_label):
            msg_label = f"" for {test_label:s} set""
        else:
            msg_label = """"

        # Prepare header
        message = f""Summary{msg_label:s}:\n""
        message += f""   {'Property Metrics':<18s} ""
        message += f""{'MAE':<8s}   ""
        message += f""{'RMSE':<8s}""
        if model_ensemble:
            message += f""   {'Std':<8s}\n""
        else:
            message += ""\n""
        
        # Add property metrics
        for prop in test_properties:
            
            # Mean metrics
            message += f""   {prop:<18s} ""
            message += f""{metrics[prop]['mae']:3.2e},  ""
            message += f""{np.sqrt(metrics[prop]['mse']):3.2e}""
            if model_ensemble:
                message += f"",  {metrics[prop]['std']:3.2e}""
            message += f"" {self.data_units[prop]:s}\n""
            
            # For model ensemble, single model metrics
            if model_ensemble:
                for imodel in range(model_ensemble_num):
                    if imodel:
                        message += f""     {f'      {imodel:d}':<16s}  ""
                    else:
                        message += f""     {f'Model {imodel:d}':<16s}  ""
                    message += f""{metrics[prop][imodel]['mae']:3.2e},  ""
                    message += f""{np.sqrt(metrics[prop][imodel]['mse']):3.2e}""
                    message += f"" {self.data_units[prop]:s}\n""

        # Print metrics
        self.logger.info(message)

        return

    def plain_data(
        self,
        data_nd: List[Any],
    ) -> List[Any]:

        return np.array([
            data_i
            for data_sys in data_nd
            for data_i in np.array(data_sys).reshape(-1)])

    def plot_correlation(
        self,
        label_dataset: str,
        label_property: str,
        data_prediction: List[float],
        data_reference: List[float],
        unit_property: str,
        data_metrics: Dict[str, float],
        test_scaling: List[float],
        test_directory: str,
        test_plot_format: str,
        test_plot_dpi: int,
    ):
        """"""
        Plot property data correlation data.
        (x-axis: reference data; y-axis: predicted data)

        Some pre-defined plot properties are:
        figsize = (6, 6)
        fontsize = 12

        Parameters
        ----------
        label_dataset: str
            Label of the data set
        label_property: str
            Label of the property
        data_prediction: list(float)
            List of the predicted data
        data_reference: list(float)
            List of the reference data
        unit_property: str
            Unit of the property
        data_metrics: dict
            Dictionary of the metrics
        test_scaling: list(float)
            List of the scaling factors
        test_directory: str
            Directory to save the plot
        test_plot_format: str
            Format of the plot
        test_plot_dpi: int
            DPI of the plot

        """"""

        # Plot property: Fontsize
        SMALL_SIZE = 12
        MEDIUM_SIZE = 14
        BIGGER_SIZE = 16
        plt.rc('font', size=SMALL_SIZE, weight='bold')
        plt.rc('xtick', labelsize=SMALL_SIZE)
        plt.rc('ytick', labelsize=SMALL_SIZE)
        plt.rc('legend', fontsize=SMALL_SIZE)
        plt.rc('axes', titlesize=MEDIUM_SIZE)
        plt.rc('axes', labelsize=MEDIUM_SIZE)
        plt.rc('figure', titlesize=BIGGER_SIZE)

        # Plot property: Figure size and arrangement
        figsize = (6, 6)
        sfig = float(figsize[0])/float(figsize[1])
        left = 0.20
        bottom = 0.15
        column = [0.75, 0.00]
        row = [column[0]*sfig]

        # Initialize figure
        fig = plt.figure(figsize=figsize)
        axs1 = fig.add_axes(
            [left + 0.*np.sum(column), bottom, column[0], row[0]])

        # Data label
        metrics_label = (
            f""{label_property:s} ({label_dataset:s})\n""
            + f""MAE = {data_metrics['mae']:3.2e} {unit_property:s}\n""
            + f""RMSE = {np.sqrt(data_metrics['mse']):3.2e} {unit_property:s}"")
        if self.is_imported(""scipy""):
            r2 = stats.pearsonr(data_prediction, data_reference).statistic
            metrics_label += (
                ""\n"" + r""1 - $R^2$ = "" + f""{1.0 - r2:3.2e}"")
        if 'std' in data_metrics:
            metrics_label += (
                f""\nStd = {data_metrics['std']:3.2e} {unit_property:s}"")

        # Scale data if requested
        if test_scaling is not None:
            data_prediction = data_prediction*test_scaling
            data_reference = data_reference*test_scaling
            scale_label = ""per atom ""
        else:
            scale_label = """"

        # Plot data
        data_min = np.min(
            (np.nanmin(data_reference), np.nanmin(data_prediction)))
        data_max = np.max(
            (np.nanmax(data_reference), np.nanmax(data_prediction)))
        data_dif = data_max - data_min
        axs1.plot(
            [data_min - data_dif*0.05, data_max + data_dif*0.05],
            [data_min - data_dif*0.05, data_max + data_dif*0.05],
            color='black',
            marker='None', linestyle='--')
        axs1.plot(
            data_reference,
            data_prediction,
            color='blue', markerfacecolor='None',
            marker='o', linestyle='None',
            label=metrics_label)

        # Axis range
        axs1.set_xlim(data_min - data_dif*0.05, data_max + data_dif*0.05)
        axs1.set_ylim(data_min - data_dif*0.05, data_max + data_dif*0.05)

        # Figure title
        title = f""Correlation plot\n{label_property:s} ({label_dataset:s})""
        if 'std' in data_metrics:
            title = title[:-1] + "", ensemble average)""
        axs1.set_title(title, fontweight='bold')

        # Axis labels
        axs1.set_xlabel(
            f""Reference {label_property:s} {scale_label:s}({unit_property:s})"",
            fontweight='bold')
        axs1.get_xaxis().set_label_coords(0.5, -0.12)
        axs1.set_ylabel(
            f""Model {label_property:s} {scale_label:s}({unit_property:s})"",
            fontweight='bold')
        axs1.get_yaxis().set_label_coords(-0.18, 0.5)

        # Figure legend
        axs1.legend(loc='upper left')

        # Save figure
        plt.savefig(
            os.path.join(
                test_directory,
                f""{label_dataset:s}_correlation_{label_property:s}""
                + f"".{test_plot_format:s}""),
            format=test_plot_format,
            dpi=test_plot_dpi)
        plt.close()

        return

    def plot_histogram(
        self,
        label_dataset: str,
        label_property: str,
        data_prediction: List[float],
        data_reference: List[float],
        unit_property: str,
        data_metrics: Dict[str, float],
        test_directory: str,
        test_plot_format: str,
        test_plot_dpi: int,
        test_binnum: Optional[int] = 101,
        test_histlog: Optional[bool] = False,
    ):
        """"""
        Plot prediction error spread as histogram.

        Parameters
        ----------
        label_dataset: str
            Label of the data set
        label_property: str
            Label of the property
        data_prediction: list(float)
            List of the predicted data
        data_reference: list(float)
            List of the reference data
        unit_property: str
            Unit of the property
        data_metrics: dict
            Dictionary of the metrics
        test_directory: str
            Directory to save the plot
        test_plot_format: str
            Format of the plot
        test_plot_dpi: int
            DPI of the plot

        """"""

        # Plot property: Fontsize
        SMALL_SIZE = 12
        MEDIUM_SIZE = 14
        BIGGER_SIZE = 16
        plt.rc('font', size=SMALL_SIZE, weight='bold')
        plt.rc('xtick', labelsize=SMALL_SIZE)
        plt.rc('ytick', labelsize=SMALL_SIZE)
        plt.rc('legend', fontsize=SMALL_SIZE)
        plt.rc('axes', titlesize=MEDIUM_SIZE)
        plt.rc('axes', labelsize=MEDIUM_SIZE)
        plt.rc('figure', titlesize=BIGGER_SIZE)

        # Plot property: Figure size and arrangement
        figsize = (6, 6)
        sfig = float(figsize[0])/float(figsize[1])
        left = 0.20
        bottom = 0.15
        column = [0.75, 0.00]
        row = [column[0]*sfig]

        # Initialize figure
        fig = plt.figure(figsize=figsize)
        axs1 = fig.add_axes(
            [left + 0.*np.sum(column), bottom, column[0], row[0]])

        # Data label
        metrics_label = (
            f""{label_property:s} ({label_dataset:s})\n""
            + f""MAE = {data_metrics['mae']:3.2e} {unit_property:s}\n""
            + f""RMSE = {np.sqrt(data_metrics['mse']):3.2e} {unit_property:s}"")
        if self.is_imported(""scipy""):
            r2 = stats.pearsonr(data_prediction, data_reference).statistic
            metrics_label += (
                ""\n"" + r""1 - $R^2$ = "" + f""{1.0 - r2:3.2e}"")
        if 'std' in data_metrics:
            metrics_label += (
                f""\nStd = {data_metrics['std']:3.2e} {unit_property:s}"")

        # Plot data
        data_dif = data_reference - data_prediction
        data_min = np.nanmin(data_dif)
        data_max = np.nanmax(data_dif)
        data_absmax = np.max((np.abs(data_min), np.abs(data_max)))
        data_absmax += data_absmax/(2.0*test_binnum)
        data_bin = np.linspace(-data_absmax, data_absmax, num=test_binnum)
        axs1.hist(
            data_reference - data_prediction,
            bins=data_bin,
            density=True,
            color='red',
            log=test_histlog,
            label=metrics_label)

        # Axis range
        axs1.set_xlim(-data_absmax, data_absmax)

        # Figure title
        title = (
            ""Prediction error distribution\n""
            + f""{label_property:s} ({label_dataset:s})"")
        if 'std' in data_metrics:
            title = title[:-1] + "", ensemble average)""
        axs1.set_title(title, fontweight='bold')

        # Axis labels
        axs1.set_xlabel(
            f""Error in {label_property:s} ({unit_property:s})"",
            fontweight='bold')
        axs1.get_xaxis().set_label_coords(0.5, -0.12)
        if test_histlog:
            ylabel = ""log(Probability)""
        else:
            ylabel = ""Probability""
        axs1.set_ylabel(
            ylabel,
            fontweight='bold')
        axs1.get_yaxis().set_label_coords(-0.18, 0.5)

        # Figure legend
        axs1.legend(loc='upper left')

        # Save figure
        plt.savefig(
            os.path.join(
                test_directory,
                f""{label_dataset:s}_histogram_{label_property:s}""
                + f"".{test_plot_format:s}""),
            format=test_plot_format,
            dpi=test_plot_dpi)
        plt.close()

        return

    def plot_residual(
        self,
        label_dataset: str,
        label_property: str,
        data_prediction: List[float],
        data_reference: List[float],
        unit_property: str,
        data_metrics: Dict[str, float],
        test_scaling: List[float],
        test_directory: str,
        test_plot_format: str,
        test_plot_dpi: int,
    ):
        """"""
        Plot property data residual data.
        (x-axis: reference data; y-axis: prediction error)

        Parameters
        ----------
        label_dataset: str
            Label of the data set
        label_property: str
            Label of the property
        data_prediction: list(float)
            List of the predicted data
        data_reference: list(float)
            List of the reference data
        unit_property: str
            Unit of the property
        data_metrics: dict
            Dictionary of the metrics
        test_scaling: list(float)
            List of the scaling factors
        test_directory: str
            Directory to save the plot
        test_plot_format: str
            Format of the plot
        test_plot_dpi: int
            DPI of the plot

        """"""

        # Plot property: Fontsize
        SMALL_SIZE = 12
        MEDIUM_SIZE = 14
        BIGGER_SIZE = 16
        plt.rc('font', size=SMALL_SIZE, weight='bold')
        plt.rc('xtick', labelsize=SMALL_SIZE)
        plt.rc('ytick', labelsize=SMALL_SIZE)
        plt.rc('legend', fontsize=SMALL_SIZE)
        plt.rc('axes', titlesize=MEDIUM_SIZE)
        plt.rc('axes', labelsize=MEDIUM_SIZE)
        plt.rc('figure', titlesize=BIGGER_SIZE)

        # Plot property: Figure size and arrangement
        figsize = (12, 6)
        left = 0.10
        bottom = 0.15
        column = [0.85, 0.00]
        row = [0.75, 0.00]

        # Initialize figure
        fig = plt.figure(figsize=figsize)
        axs1 = fig.add_axes(
            [left + 0.*np.sum(column), bottom, column[0], row[0]])

        # Data label
        metrics_label = (
            f""{label_property:s} ({label_dataset:s})\n""
            + f""MAE = {data_metrics['mae']:3.2e} {unit_property:s}\n""
            + f""RMSE = {np.sqrt(data_metrics['mse']):3.2e} {unit_property:s}"")
        if self.is_imported(""scipy""):
            r2 = stats.pearsonr(data_prediction, data_reference).statistic
            metrics_label += (
                ""\n"" + r""1 - $R^2$ = "" + f""{1.0 - r2:3.2e}"")
        if 'std' in data_metrics:
            metrics_label += (
                f""\nStd = {data_metrics['std']:3.2e} {unit_property:s}"")

        # Scale data if requested
        if test_scaling is not None:
            data_prediction = data_prediction*test_scaling
            data_reference = data_reference*test_scaling
            scale_label = ""per atom ""
        else:
            scale_label = """"

        # Plot data
        data_min = np.nanmin(data_reference)
        data_max = np.nanmax(data_reference)
        data_dif = data_max - data_min
        axs1.plot(
            [data_min - data_dif*0.05, data_max + data_dif*0.05],
            [0.0, 0.0],
            color='black',
            marker='None', linestyle='--')
        data_deviation = data_reference - data_prediction
        data_devmin = np.nanmin(data_deviation)
        data_devmax = np.nanmax(data_deviation)
        data_devdif = data_devmax - data_devmin
        axs1.plot(
            data_reference,
            data_deviation,
            color='darkgreen', markerfacecolor='None',
            marker='o', linestyle='None',
            label=metrics_label)

        # Axis range
        axs1.set_xlim(
            data_min - data_dif*0.05, data_max + data_dif*0.05)
        axs1.set_ylim(
            data_devmin - data_devdif*0.05, data_devmax + data_devdif*0.05)

        # Figure title
        title = f""Residual plot\n{label_property:s} ({label_dataset:s})""
        if 'std' in data_metrics:
            title = title[:-1] + "", ensemble average)""
        axs1.set_title(title, fontweight='bold')

        # Axis labels
        axs1.set_xlabel(
            f""Reference {label_property:s} {scale_label:s}({unit_property:s})"",
            fontweight='bold')
        axs1.get_xaxis().set_label_coords(0.5, -0.12)
        axs1.set_ylabel(
            f""Prediction error {label_property:s} {scale_label:s}""
            + f""({unit_property:s})"",
            fontweight='bold')
        axs1.get_yaxis().set_label_coords(-0.08, 0.5)

        # Figure legend
        axs1.legend(loc='upper left')

        # Save figure
        plt.savefig(
            os.path.join(
                test_directory,
                f""{label_dataset:s}_residual_{label_property:s}""
                + f"".{test_plot_format:s}""),
            format=test_plot_format,
            dpi=test_plot_dpi)
        plt.close()

        return",./Asparagus/asparagus/training/tester.py
Trainer,"class Trainer:
    """"""
    NNP model Trainer class

    Parameters
    ----------
    config: (str, dict, object)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to config json file (str)
    data_container: data.DataContainer, optional, default None
        Reference data container object providing training, validation and
        test data for the model training.
    model_calculator: torch.nn.Module, optional, default None
        Model calculator to train matching model properties with the reference
        data set. If not provided, the model calculator will be initialized
        according to config input.
    trainer_restart: bool, optional, default False
        Restart the model training from state in config['model_directory']
    trainer_max_epochs: int, optional, default 10000
        Maximum number of training epochs
    trainer_properties: list, optional, default None
        Properties contributing to the prediction quality value.
        If the list is empty or None, all properties which are both predicted
        by the model calculator and available in the data container will be
        considered for the loss function.
    trainer_properties_metrics: dict, optional, default 'MSE' for all
        Quantification of the property prediction quality only for
        properties in the reference data set.
        Can be given for each property individually and by keyword 'all'
        for every property else wise.
    trainer_properties_weights: dict, optional, default {...}
        Weighting factors for the combination of single property loss
        values to total loss value.
    trainer_batch_size: int, optional, default None
        Default dataloader batch size
    trainer_train_batch_size: int, optional, default None
        Training dataloader batch size. If None, default batch size is used.
    trainer_valid_batch_size: int, optional, default None
        Validation dataloader batch size. If None, default batch size is used.
    trainer_test_batch_size:  int, optional, default None
        Test dataloader batch size. If None, default batch size is used.
    trainer_num_batch_workers: int, optional, default 1
        Number of dataloader workers.
    trainer_optimizer: (str, object), optional, default 'AMSgrad'
        Optimizer class for the NNP model training
    trainer_optimizer_args: dict, optional, default {}
        Additional optimizer class arguments
    trainer_scheduler: (str, object), optional, default 'ExponentialLR'
        Learning rate scheduler class for the NNP model training
    trainer_scheduler_args: dict, optional, default {}
        Additional learning rate scheduler class arguments
    trainer_ema: bool, optional, default True
        Apply exponential moving average scheme for NNP model training
    trainer_ema_decay: float, optional, default 0.999
        Exponential moving average decay rate
    trainer_max_gradient_norm: float, optional, default 1.0
        Maximum model parameter gradient norm to clip its step size.
        If None, parameter gradient norm clipping is deactivated.
    trainer_max_gradient_value: float, optional, default 10.0
        Maximum model parameter gradient value to clip its step size.
        If None, parameter gradient value clipping is deactivated.
    trainer_save_interval: int, optional, default 5
        Interval between epoch to save current and best set of model
        parameters.
    trainer_validation_interval: int, optional, default 5
        Interval between epoch to evaluate model performance on
        validation data.
    trainer_evaluate_testset: bool, optional, default True
        Each validation interval and in case of a new best loss function,
        apply Tester class on the test set.
    trainer_max_checkpoints: int, optional, default 1
        Maximum number of checkpoint files stored before deleting the
        oldest ones up to the number threshold.
    trainer_summary_writer: bool, optional, default False
        Write training process to a tensorboard summary writer instance
    trainer_print_progress_bar: bool, optional, default True
        Print progress bar to stout.
    trainer_debug_mode: bool, optional, default False
        Perform model training in debug mode, which check repeatedly for
        'NaN' results.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for trainer class
    _default_args = {
        'trainer_restart':              False,
        'trainer_max_epochs':           10_000,
        'trainer_properties':           None,
        'trainer_properties_metrics':   {'else': 'mse'},
        'trainer_properties_weights':   {
            'energy': 1., 'forces': 50., 'dipole': 25., 'else': 1.},
        'trainer_batch_size':           32,
        'trainer_train_batch_size':     None,
        'trainer_valid_batch_size':     None,
        'trainer_test_batch_size':      None,
        'trainer_num_batch_workers':    0,
        'trainer_optimizer':            'Adam',
        'trainer_optimizer_args':       {'lr': 0.001},
        'trainer_scheduler':            'ReduceLROnPlateau',
        'trainer_scheduler_args':       {},
        'trainer_ema':                  True,
        'trainer_ema_decay':            0.99,
        'trainer_max_gradient_norm':    1.0,
        'trainer_max_gradient_value':   10.0,
        'trainer_save_interval':        5,
        'trainer_validation_interval':  5,
        'trainer_evaluate_testset':     True,
        'trainer_max_checkpoints':      1,
        'trainer_summary_writer':       True,
        'trainer_print_progress_bar':   True,
        'trainer_debug_mode':           False,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'trainer_restart':              [utils.is_bool],
        'trainer_max_epochs':           [utils.is_integer],
        'trainer_properties':           [utils.is_string_array],
        'trainer_properties_metrics':   [utils.is_dictionary],
        'trainer_properties_weights':   [utils.is_dictionary],
        'trainer_batch_size':           [utils.is_integer],
        'trainer_train_batch_size':     [utils.is_integer, utils.is_None],
        'trainer_valid_batch_size':     [utils.is_integer, utils.is_None],
        'trainer_test_batch_size':      [utils.is_integer, utils.is_None],
        'trainer_num_batch_workers':    [utils.is_integer],
        'trainer_optimizer':            [utils.is_string, utils.is_callable],
        'trainer_optimizer_args':       [utils.is_dictionary],
        'trainer_scheduler':            [utils.is_string, utils.is_callable],
        'trainer_scheduler_args':       [utils.is_dictionary],
        'trainer_ema':                  [utils.is_bool],
        'trainer_ema_decay':            [utils.is_numeric],
        'trainer_max_gradient_norm':    [utils.is_numeric, utils.is_None],
        'trainer_max_gradient_value':   [utils.is_numeric, utils.is_None],
        'trainer_save_interval':        [utils.is_integer],
        'trainer_validation_interval':  [utils.is_integer],
        'trainer_evaluate_testset':     [utils.is_bool],
        'trainer_max_checkpoints':      [utils.is_integer],
        'trainer_summary_writer':       [utils.is_bool],
        'trainer_print_progress_bar':   [utils.is_bool],
        'trainer_debug_mode':           [utils.is_bool],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        data_container: Optional[data.DataContainer] = None,
        model_calculator: Optional[torch.nn.Module] = None,
        trainer_restart: Optional[int] = None,
        trainer_max_epochs: Optional[int] = None,
        trainer_properties: Optional[List[str]] = None,
        trainer_properties_metrics: Optional[Dict[str, str]] = None,
        trainer_properties_weights: Optional[Dict[str, float]] = None,
        trainer_batch_size: Optional[int] = None,
        trainer_train_batch_size: Optional[int] = None,
        trainer_valid_batch_size: Optional[int] = None,
        trainer_test_batch_size: Optional[int] = None,
        trainer_num_batch_workers: Optional[int] = None,
        trainer_optimizer: Optional[Union[str, object]] = None,
        trainer_optimizer_args: Optional[Dict[str, float]] = None,
        trainer_scheduler: Optional[Union[str, object]] = None,
        trainer_scheduler_args: Optional[Dict[str, float]] = None,
        trainer_ema: Optional[bool] = None,
        trainer_ema_decay: Optional[float] = None,
        trainer_max_gradient_norm: Optional[float] = None,
        trainer_max_gradient_value: Optional[float] = None,
        trainer_save_interval: Optional[int] = None,
        trainer_validation_interval: Optional[int] = None,
        trainer_evaluate_testset: Optional[bool] = None,
        trainer_max_checkpoints: Optional[int] = None,
        trainer_summary_writer: Optional[bool] = None,
        trainer_print_progress_bar: Optional[bool] = None,
        trainer_debug_mode: Optional[bool] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        verbose: Optional[bool] = True,
        **kwargs,
    ):
        """"""
        Initialize Model Calculator Trainer.

        """"""

        ##########################################
        # # # Check Calculator Trainer Input # # #
        ##########################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, training),
            check_dtype=utils.get_dtype_args(self, training)
        )

        # Update global configuration dictionary
        config.update(
            config_update,
            config_from=self,
            verbose=verbose)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        ################################
        # # # Check Data Container # # #
        ################################

        # Assign DataContainer if not done already
        if data_container is None:
            self.data_container = data.DataContainer(
                config=config,
                **kwargs)

        # Check dataloader batch input
        if self.trainer_train_batch_size is None:
            self.trainer_train_batch_size = self.trainer_batch_size
        if self.trainer_valid_batch_size is None:
            self.trainer_valid_batch_size = self.trainer_batch_size
        if self.trainer_test_batch_size is None:
            self.trainer_test_batch_size = self.trainer_batch_size

        # Get reference data property units
        self.data_units = self.data_container.data_unit_properties

        #########################################
        # # # Prepare Reference Data Loader # # #
        #########################################

        # Initialize training, validation and test data loader
        self.data_container.init_dataloader(
            self.trainer_train_batch_size,
            self.trainer_valid_batch_size,
            self.trainer_test_batch_size,
            num_workers=self.trainer_num_batch_workers,
            device=self.device,
            dtype=self.dtype)

        # Get training and validation data loader
        self.data_train = self.data_container.get_dataloader('train')
        self.data_valid = self.data_container.get_dataloader('valid')

        ##################################
        # # # Check Model Calculator # # #
        ##################################

        # Assign model calculator model if not done already
        if self.model_calculator is None:
            self.model_calculator, _, _ = model.get_model_calculator(
                config=config,
                **kwargs)

        # Check for model ensemble calculator
        if hasattr(self.model_calculator, 'model_ensemble'):
            self.model_ensemble = self.model_calculator.model_ensemble
        else:
            self.model_ensemble = False
        if self.model_ensemble:
            raise SyntaxError(
                ""The Trainer class cannot handle model ensembles!"")

        # Get model properties
        self.model_properties = self.model_calculator.model_properties
        self.model_units = self.model_calculator.model_unit_properties

        ############################
        # # # Check Properties # # #
        ############################

        # Check property definition for the loss function evaluation
        self.trainer_properties = self.check_properties(
            self.trainer_properties,
            self.data_container.data_properties,
            self.model_properties)

        # Check property metrics and weights for the loss function
        self.trainer_properties_metrics, self.trainer_properties_weights = (
            self.check_properties_metrics_weights(
                self.trainer_properties,
                self.trainer_properties_metrics,
                self.trainer_properties_weights)
            )

        # Check model units and model to data unit conversion
        self.model_units, self.data_units, self.model_conversion = (
            self.check_model_units(
                self.trainer_properties,
                self.model_units,
                self.data_units)
            )

        # Show assigned properties, their units and contribution to loss value
        if verbose:
            self.print_trainer_info()

        # Assign potentially new property units to the model
        if hasattr(self.model_calculator, 'set_unit_properties'):
            self.model_calculator.set_unit_properties(self.model_units)

        ###################################################################
        # # # Prepare Optimizer, Scheduler and Gradient Normalization # # #
        ###################################################################

        # Assign model parameter optimizer
        self.trainer_optimizer, trainer_optimizer_args = get_optimizer(
            self.trainer_optimizer,
            self.model_calculator.get_trainable_parameters(),
            self.trainer_optimizer_args)

        # Assign learning rate scheduler
        self.trainer_scheduler, trainer_scheduler_args = get_scheduler(
            self.trainer_scheduler,
            self.trainer_optimizer,
            self.trainer_scheduler_args)

        # Check maximum gradient norm and value clipping input
        if self.trainer_max_gradient_norm is None:
            self.gradient_clipping_norm = False
        else:
            self.gradient_clipping_norm = True
        if self.trainer_max_gradient_value is None:
            self.gradient_clipping_value = False
        else:
            self.gradient_clipping_value = True

        # Update global configuration dictionary with optimizer and scheduler
        # options
        config.update(
            {
                ""trainer_optimizer_args"": trainer_optimizer_args,
                ""trainer_scheduler_args"": trainer_scheduler_args,
            },
            config_from=self,
            verbose=verbose
        )

        #######################
        # # # Prepare EMA # # #
        #######################

        # Assign Exponential Moving Average model
        if self.trainer_ema:
            from torch_ema import ExponentialMovingAverage
            self.trainer_ema_model = ExponentialMovingAverage(
                self.model_calculator.parameters(),
                decay=self.trainer_ema_decay)

        ################################
        # # # Prepare File Manager # # #
        ################################

        # Initialize checkpoint file manager
        self.filemanager = model.FileManager(
            config=config,
            model_calculator=self.model_calculator,
            max_checkpoints=self.trainer_max_checkpoints,
            verbose=verbose)

        # Initialize training summary writer
        if self.trainer_summary_writer:
            from torch.utils.tensorboard import SummaryWriter
            self.summary_writer = SummaryWriter(
                log_dir=self.filemanager.logs_dir)

        ##########################
        # # # Prepare Tester # # #
        ##########################

        # Assign model prediction tester if test set evaluation is requested
        if self.trainer_evaluate_testset:
            self.tester = Tester(
                config=config,
                data_container=self.data_container,
                test_datasets='test',
                test_batch_size=self.trainer_test_batch_size,
                test_num_batch_workers=self.trainer_num_batch_workers)

        #############################
        # # # Save Model Config # # #
        #############################

        # Save a copy of the current model configuration in the model directory
        self.filemanager.save_config(config)

        return

    def __str__(self):
        messgage = ""Trainer""
        if hasattr(self, 'model_calculator'):
            messgage += f"" of {str(self.model_calculator):s} model""
        if hasattr(self, 'filemanager'):
            messgage += f"" in {str(self.filemanager):s}""
        return messgage

    def run(
        self,
        checkpoint: Optional[Union[str, int]] = 'last',
        restart: Optional[bool] = True,
        reset_best_loss: Optional[bool] = False,
        reset_energy_shift: Optional[bool] = False,
        skip_property_scaling: Optional[bool] = False,
        skip_initial_testing: Optional[bool] = False,
        print_progress: Optional[bool] = True,
        ithread: Optional[int] = None,
        verbose: Optional[bool] = True,
        **kwargs,
    ):
        """"""
        Train model calculator.

        Parameters
        ----------
        checkpoint: (str, int), optional, default 'last'
            If string and a valid file path, load the respective checkpoint 
            file.
            If string 'best' or 'last', load respectively the best checkpoint 
            file (as with None) or the with the highest epoch number for each.
            If integer, load the checkpoint file of the respective epoch 
            If None, load checkpoint file with best loss function value.
        restart: bool, optional, default True
            If True, restart the model training from the last checkpoint file,
            if available. If False or no checkpoint file exist, start training
            from scratch.
        reset_best_loss: bool, optional, default False
            If False, continue model potential validation from stored best
            loss value. Else, reset best loss value to None.
        reset_energy_shift: bool, optional, default False
            If True and a model checkpoint file is successfully loaded, only
            the (atomic) energy shifts will be initially optimized to match
            best the reference training set. This function is used, e.g., for
            ""transfer learning"", when a trained model is retrained on a
            different reference data set with a changed total energy shift.
        skip_property_scaling: bool, optional, default False
            Skip the initial model properties scaling factor and shift term
            optimization to match best the reference training set.
        skip_initial_testing: bool, optional, default False
            Skip the initial model evaluation on the reference test set, if the
            model evaluation is enabled anyways (see trainer_evaluate_testset).
        print_progress: bool, optional, default True
            Show  progress report.
        ithread: int, optional, default None
            Thread number

        """"""

        #################################
        # # # Load Model Checkpoint # # #
        #################################

        # Load checkpoint file
        loaded_checkpoint, checkpoint_file = self.filemanager.load_checkpoint(
            checkpoint_label=checkpoint,
            return_name=True,
            verbose=verbose)

        trainer_epoch_start = 1
        best_loss = None
        if loaded_checkpoint is not None:

            # Assign model parameters
            self.model_calculator.load_state_dict(
                loaded_checkpoint['model_state_dict'])

            self.model_calculator.checkpoint_loaded = True
            self.model_calculator.checkpoint_file = checkpoint_file
            if verbose:
                self.logger.info(
                    f""Checkpoint file '{checkpoint_file:s}' loaded."")

            # If restart training enabled, assign optimizer, scheduler and
            # epoch parameter if available
            optimizer_state, scheduler_state = ""None"", ""None""
            if restart:
                if loaded_checkpoint.get('optimizer_state_dict') is not None:
                    self.trainer_optimizer.load_state_dict(
                        loaded_checkpoint['optimizer_state_dict'])
                    optimizer_state = ""Loaded""
                if loaded_checkpoint.get('scheduler_state_dict') is not None:
                    self.trainer_scheduler.load_state_dict(
                        loaded_checkpoint['scheduler_state_dict'])
                    scheduler_state = ""Loaded""
                if loaded_checkpoint.get('epoch') is not None:
                    trainer_epoch_start = loaded_checkpoint['epoch'] + 1

                # Initialize best total loss value of validation reference data
                if (
                    reset_best_loss
                    or loaded_checkpoint.get('best_loss') is None
                ):
                    best_loss = None
                elif loaded_checkpoint['best_loss'] == 0.0:
                    best_loss = None
                    self.logger.warning(
                        ""Loaded best loss value from the checkpoint file ""
                        + f""'{checkpoint_file:s}' is zero indicating an error ""
                        + ""during generation of the checkpoint file.\n""
                        + ""The best loss value is reset!"")
                else:
                    best_loss = loaded_checkpoint['best_loss']

            # Print checkpoint file info
            if verbose:               
                if restart:
                    message = ""Restart""
                else:
                    message = ""Start""
                message += (
                    ""training from checkpoint file ""
                    + f""'{checkpoint_file:s}':\n""
                    + f"" Current Epoch: {trainer_epoch_start:d}\n"")
                if best_loss is None:
                    message += "" Best validation loss: None\n""
                else:
                    message += f"" Best validation loss: {best_loss:.2E}\n""
                message += (
                    f"" Optimizer state: {optimizer_state:s}\n""
                    + f"" Scheduler state: {scheduler_state:s}"")
                self.logger.info(message)

        # Skip if max epochs are already reached
        if trainer_epoch_start > self.trainer_max_epochs:
            if verbose:
                self.logger.info(
                    f""Max Epochs ({self.trainer_max_epochs:d}) already ""
                    + f""reached ({trainer_epoch_start:d})."")
            return

        ################################
        # # # Prepare Model Cutoff # # #
        ################################

        # Get model and descriptor cutoffs
        cutoffs = self.model_calculator.get_cutoff_ranges()

        # Set model and descriptor cutoffs for neighbor list calculation
        self.data_train.init_neighbor_list(
            cutoff=cutoffs,
            device=self.device,
            dtype=self.dtype)
        self.data_valid.init_neighbor_list(
            cutoff=cutoffs,
            device=self.device,
            dtype=self.dtype)

        ####################################
        # # # Prepare Property Scaling # # #
        ####################################

        # Either, if a model checkpoint file is loaded, re-optimize energy or
        # atomic energy shifts if specifically requested.
        # If no model checkpoint file is loaded or it is still the first epoch,
        # run model property scaling for the model calculator.
        if self.model_calculator.checkpoint_loaded and reset_energy_shift:

            # Get loaded checkpoint file path
            checkpoint_file = self.model_calculator.checkpoint_file
            if checkpoint_file is None:
                checkpoint_file = ""unknown(?)""

            if verbose:
                self.logger.info(
                    ""Model calculator checkpoint file already loaded!\n""
                    + f""Checkpoint file: '{checkpoint_file:s}'\n""
                    + ""Model energy shifts will be re-evaluated."")
            
            # Get model energy properties
            properties_scaleable = []
            for prop in self.model_calculator.get_scaleable_properties():
                if prop in ['energy', 'atomic_energies']:
                    properties_scaleable.append(prop)

            # Get model energy scaling
            training.set_property_scaling_estimation(
                model_calculator=self.model_calculator,
                data_loader=self.data_train,
                properties=properties_scaleable,
                set_shift_term=True,
                set_scaling_factor=False,
                )

        elif (
            self.model_calculator.checkpoint_loaded
            and not trainer_epoch_start == 1
        ):

            # Get loaded checkpoint file path
            checkpoint_file = self.model_calculator.checkpoint_file
            if checkpoint_file is None:
                checkpoint_file = ""unknown(?)""

            if verbose:
                self.logger.info(
                    ""Model calculator checkpoint file already loaded!\n""
                    + f""Checkpoint file: '{checkpoint_file:s}'\n""
                    + ""No model property scaling parameter are set."")

        elif not skip_property_scaling:

            if verbose:
                self.logger.info(
                    ""No Model calculator checkpoint file loaded!\n""
                    ""Model property scaling parameter will be set."")

            # Get model property scaling
            training.set_property_scaling_estimation(
                model_calculator=self.model_calculator,
                data_loader=self.data_train,
                properties=self.model_calculator.get_scaleable_properties())

        #############################################
        # # # Prepare Model Training and Metric # # #
        #############################################

        # Initialize training mode for calculator
        self.model_calculator.train()
        torch.set_grad_enabled(True)
        if self.trainer_debug_mode:
            torch.autograd.set_detect_anomaly(True)

        # Reset best property metrics
        metrics_best = self.reset_metrics()
        metrics_best['loss'] = best_loss

        # Define loss function
        loss_fn = torch.nn.SmoothL1Loss(reduction='mean')

        # Get scheduler argument list for correct parameter passing
        scheduler_arguments = (
            self.trainer_scheduler.step.__code__.co_varnames)

        # Count number of training batches
        Nbatch_train = torch.tensor(len(self.data_train), dtype=torch.int64)

        # Initialize training time estimation per epoch
        train_time_estimation = torch.tensor(0.0, dtype=torch.float64)

        # Prepare progress status thread prefix
        if ithread is None:
            thread_prefix = """"
        else:
            thread_prefix = f""(Thread {ithread:d}) ""
                
        ##########################
        # # # Start Training # # #
        ##########################

        # Initial evaluation of the test set if requested
        if self.trainer_evaluate_testset and not skip_initial_testing:
            self.tester.test(
                self.model_calculator,
                model_conversion=self.model_conversion,
                test_directory=self.filemanager.best_dir,
                test_plot_correlation=True,
                test_plot_histogram=True,
                test_plot_residual=True,
                **kwargs)

        # Loop over epochs
        for epoch in torch.arange(
            trainer_epoch_start, self.trainer_max_epochs + 1
        ):

            # Start epoch train timer
            train_time_epoch_start = time.time()

            # Reset property metrics
            metrics_train = self.reset_metrics()

            # Loop over training batches
            for ib, batch in enumerate(self.data_train):

                # Start batch train timer
                train_time_batch_start = time.time()

                # Eventually show training progress
                if print_progress and self.trainer_print_progress_bar:
                    utils.print_ProgressBar(
                        ib, Nbatch_train,
                        prefix=f""{thread_prefix:s}Epoch {epoch: 5d}"",
                        suffix=(
                            ""Complete - Remaining Epoch Time: ""
                            + f""{train_time_estimation: 4.1f} s     ""
                            ),
                        length=42)

                # Reset optimizer gradients
                self.trainer_optimizer.zero_grad(
                    set_to_none=(
                        not (
                            self.gradient_clipping_norm
                            or self.gradient_clipping_value)
                        )
                    )

                # Predict model properties from data batch
                prediction = self.model_calculator(batch)

                # Check for NaN predictions
                if self.trainer_debug_mode:
                    for prop, item in prediction.items():
                        if torch.any(torch.isnan(item)):
                            raise SyntaxError(
                                f""Property prediction of '{prop:s}' contains ""
                                + f""{torch.sum(torch.isnan(item))} elements ""
                                + ""of value 'NaN'!"")

                # Compute total and single loss values for training properties
                metrics_batch = self.compute_metrics(
                    prediction, batch, loss_fn=loss_fn)
                loss = metrics_batch['loss']

                # Check for NaN loss value
                if torch.isnan(loss):
                    raise SyntaxError(
                        ""Loss value of training batch is 'NaN'!"")

                # Predict parameter gradients by backwards propagation
                loss.backward()
                
                # Clip parameter gradients norm and values
                if self.gradient_clipping_value:
                    torch.nn.utils.clip_grad_value_(
                        self.model_calculator.parameters(),
                        self.trainer_max_gradient_value)
                if self.gradient_clipping_norm:
                    torch.nn.utils.clip_grad_norm_(
                        self.model_calculator.parameters(),
                        self.trainer_max_gradient_norm)

                # Update model parameters
                self.trainer_optimizer.step()

                # Apply Exponential Moving Average
                if self.trainer_ema:
                    self.trainer_ema_model.update()

                # Update average metrics
                self.update_metrics(metrics_train, metrics_batch)

                # End batch train timer
                train_time_batch_end = time.time()

                # Eventually update training batch time estimation
                if verbose:
                    train_time_batch = (
                        train_time_batch_end - train_time_batch_start)
                    if ib:
                        train_time_estimation = (
                            0.5*(train_time_estimation - train_time_batch)
                            + 0.5*train_time_batch*(Nbatch_train - ib - 1))
                    else:
                        train_time_estimation = (
                            train_time_batch*(Nbatch_train - 1))

            # Increment scheduler step
            if 'metrics' in scheduler_arguments:
                self.trainer_scheduler.step(metrics_train['loss'])
            else:
                self.trainer_scheduler.step()

            # Stop epoch train timer
            train_time_epoch_end = time.time()
            train_time_epoch = train_time_epoch_end - train_time_epoch_start

            # Eventually show final training progress
            if print_progress and self.trainer_print_progress_bar:
                utils.print_ProgressBar(
                    Nbatch_train, Nbatch_train,
                    prefix=f""{thread_prefix:s}Epoch {epoch: 5d}"",
                    suffix=(
                        ""Done - Epoch Time: ""
                        + f""{train_time_epoch: 4.1f} s, ""
                        + f""Loss: {metrics_train['loss']: 4.4f}   ""),
                    length=42)
            elif print_progress:
                utils.print_Progress(
                    f""{thread_prefix:s}Done Epoch {epoch: 5d}, ""
                    + f""Epoch Time: {train_time_epoch: 4.1f} s, ""
                    + f""Loss: {metrics_train['loss']: 4.4f}   "")

            # Add process to training summary writer
            if self.trainer_summary_writer:
                for prop, value in metrics_train.items():
                    if utils.is_dictionary(value):
                        for metric, val in value.items():
                            self.summary_writer.add_scalar(
                                '_'.join(('train', prop, metric)),
                                metrics_train[prop][metric],
                                global_step=epoch)
                    else:
                        self.summary_writer.add_scalar(
                            '_'.join(('train', prop)),
                            metrics_train[prop],
                            global_step=epoch)

            # Save current model each interval
            if not (epoch % self.trainer_save_interval):
                self.filemanager.save_checkpoint(
                    model_calculator=self.model_calculator,
                    optimizer=self.trainer_optimizer,
                    scheduler=self.trainer_scheduler,
                    epoch=epoch,
                    best_loss=best_loss)

            # Perform model validation each interval
            if not (epoch % self.trainer_validation_interval):

                # Change to evaluation mode for calculator
                self.model_calculator.eval()

                # Reset property metrics
                metrics_valid = self.reset_metrics()

                # If EMA is active
                if self.trainer_ema:
                    # Store last model parameter set
                    self.trainer_ema_model.store(
                        self.model_calculator.parameters())
                    # Load EMA model parameter set
                    self.trainer_ema_model.copy_to(
                        self.model_calculator.parameters())
    
                # Loop over validation batches
                for batch in self.data_valid:

                    # Predict model properties from data batch
                    prediction = self.model_calculator(batch)

                    # Compute total and single loss values for
                    # validation properties
                    metrics_batch = self.compute_metrics(
                        prediction, batch,
                        loss_fn=loss_fn, loss_only=False)

                    # Update average metrics
                    self.update_metrics(metrics_valid, metrics_batch)

                # Change back to training mode for calculator
                self.model_calculator.train()

                # Add process to training summary writer
                if self.trainer_summary_writer:
                    for prop, value in metrics_valid.items():
                        if utils.is_dictionary(value):
                            for metric, val in value.items():
                                self.summary_writer.add_scalar(
                                    '_'.join(('valid', prop, metric)),
                                    metrics_valid[prop][metric],
                                    global_step=epoch)
                        else:
                            self.summary_writer.add_scalar(
                                '_'.join(('valid', prop)),
                                metrics_valid[prop],
                                global_step=epoch)

                # Check for model improvement and save as best model eventually
                if (
                    best_loss is None
                    or metrics_valid['loss'] < best_loss
                ):

                    # Store best metrics
                    metrics_best = metrics_valid

                    # Update best total loss value
                    best_loss = metrics_valid['loss']

                    # Save model calculator state
                    self.filemanager.save_checkpoint(
                        model_calculator=self.model_calculator,
                        optimizer=self.trainer_optimizer,
                        scheduler=self.trainer_scheduler,
                        epoch=epoch,
                        best=True,
                        best_loss=best_loss)

                    # Evaluation of the test set if requested
                    if self.trainer_evaluate_testset:
                        self.tester.test(
                            self.model_calculator,
                            model_conversion=self.model_conversion,
                            test_directory=self.filemanager.best_dir,
                            test_plot_correlation=True,
                            test_plot_histogram=True,
                            test_plot_residual=True,
                            verbose=verbose,
                            **kwargs)

                    # Add process to training summary writer
                    if self.trainer_summary_writer:
                        for prop, value in metrics_best.items():
                            if utils.is_dictionary(value):
                                for metric, val in value.items():
                                    self.summary_writer.add_scalar(
                                        '_'.join(('best', prop, metric)),
                                        metrics_best[prop][metric],
                                        global_step=epoch)
                            else:
                                self.summary_writer.add_scalar(
                                    '_'.join(('best', prop)),
                                    metrics_best[prop],
                                    global_step=epoch)

                    # If EMA is active
                    if self.trainer_ema:
                        # Restore last model parameter set
                        self.trainer_ema_model.restore(
                            self.model_calculator.parameters())

                # Print validation metrics summary
                if print_progress and verbose:

                    msg = (
                        f""{thread_prefix:s}Summary Epoch: {epoch:d}/""
                        + f""{self.trainer_max_epochs:d}\n""
                        + ""  Loss   train / valid: ""
                        + f"" {metrics_train['loss']:.2E} /""
                        + f"" {metrics_valid['loss']:.2E}""
                        + f""  Best Loss valid: {metrics_best['loss']:.2E}\n""
                        + f""  Property Metrics (valid):\n"")
                    for prop in self.trainer_properties:
                        msg += (
                            f""    {prop:10s}  MAE (Best) / RMSE (Best): ""
                            + f"" {metrics_valid[prop]['mae']:.2E}""
                            + f"" ({metrics_best[prop]['mae']:.2E}) /""
                            + f"" {np.sqrt(metrics_valid[prop]['mse']):.2E}""
                            + f"" ({np.sqrt(metrics_best[prop]['mse']):.2E})""
                            + f"" {self.model_units[prop]:s}\n"")
                    self.logger.info(msg)

                elif print_progress:

                    utils.print_Progress(
                        f""{thread_prefix:s}Summary Epoch: {epoch:d}/""
                        + f""{self.trainer_max_epochs:d}: ""
                        + ""  Loss   train / valid: ""
                        + f"" {metrics_train['loss']:.2E} /""
                        + f"" {metrics_valid['loss']:.2E}""
                        + f""  Best Loss valid: {metrics_best['loss']:.2E}"")

        return

    def predict_batch(self, batch):
        """"""
        Predict properties from data batch.

        Parameters
        ----------
        batch: dict
            Data batch dictionary

        Returns
        -------
        dict(str, torch.Tensor)
            Model Calculator prediction of properties

        """"""

        # Predict properties
        return self.model_calculator(
            batch['atoms_number'],
            batch['atomic_numbers'],
            batch['positions'],
            batch['idx_i'],
            batch['idx_j'],
            batch['charge'],
            batch['atoms_seg'],
            batch['pbc_offset'])

    def check_properties(
        self,
        trainer_properties: List[str],
        data_properties: List[str],
        model_properties: List[str],
    ) -> List[str]:
        """"""
        Check properties for the contribution to the loss function between
        predicted model properties and available properties in the reference
        data container.

        Parameter
        ---------
        trainer_properties: list(str)
            Properties contributing to the loss function. If empty or None,
            take all matching properties between model and data container.
        data_properties: list(str)
            Properties available in the reference data container
        model_properties: list(str)
            Properties predicted by the model calculator

        Returns:
        --------
        list(str)
            List of loss function property contributions.

        """"""

        # Check matching data and model properties
        matching_properties = []
        for prop in model_properties:
            if prop in data_properties:
                matching_properties.append(prop)

        # Check training properties are empty, use all matching properties
        if trainer_properties is None or not len(trainer_properties):
            trainer_properties = matching_properties
        else:
            for prop in trainer_properties:
                if prop not in matching_properties:
                    if prop in data_properties:
                        msg = ""model calculator!""
                    else:
                        msg = ""data container!""
                    raise SyntaxError(
                        f""Requested property '{prop:s}' as loss function ""
                        + ""contribution is not available in "" + msg)

        return trainer_properties

    def check_properties_metrics_weights(
        self,
        trainer_properties: List[str],
        trainer_properties_metrics: Dict[str, float],
        trainer_properties_weights: Dict[str, float],
        default_property_metrics: Optional[str] = 'mse',
        default_property_weights: Optional[float] = 1.0,
    ) -> (Dict[str, float], Dict[str, float]):
        """"""
        Prepare property loss metrics and weighting factors for the loss
        function contributions.

        Parameter
        ---------
        trainer_properties: list(str)
            Properties contributing to the loss function
        trainer_properties_metrics: dict(str, float)
            Metrics functions for property contribution in the loss function
        trainer_properties_weights: dict(str, float)
            Weighting factors for property metrics in the loss function
        default_property_metrics: str, optional, default 'mse'
            Default option, if the property not in metrics dictionary and no
            other default value is defined by key 'else'.
            Default: mean squared error (mse)
            Alternative: mean absolute error (mae)
        default_property_weights: str, optional, default 1.0
            Default option, if the property not in weights dictionary and no
            other default value is defined by key 'else'.

        Returns:
        --------
        dict(str, float)
            Prepared property metrics dictionary
        dict(str, float)
            Prepared property weighting factors dictionary

        """"""

        # Check property metrics
        for prop in trainer_properties:
            if (
                trainer_properties_metrics.get(prop) is None
                and trainer_properties_metrics.get('else') is None
            ):
                trainer_properties_metrics[prop] = default_property_metrics
            elif trainer_properties_metrics.get(prop) is None:
                trainer_properties_metrics[prop] = (
                    trainer_properties_metrics.get('else'))

        # Check property weights
        for prop in trainer_properties:
            if (
                trainer_properties_weights.get(prop) is None
                and trainer_properties_weights.get('else') is None
            ):
                trainer_properties_weights[prop] = default_property_weights
            elif trainer_properties_weights.get(prop) is None:
                trainer_properties_weights[prop] = (
                    trainer_properties_weights.get('else'))

        return trainer_properties_metrics, trainer_properties_weights

    def check_model_units(
        self,
        trainer_properties: List[str],
        model_units: Dict[str, str],
        data_units: Dict[str, str],
    ) -> ([Dict[str, str], Dict[str, str], Dict[str, float]]):
        """"""
        Check the definition of the model units or assign units from the
        reference dataset

        Parameter
        ---------
        trainer_properties: list(str)
            Properties contributing to the loss function
        model_units: dict(str, str)
            Dictionary of model property units.
        data_units: dict(str, str)
            Dictionary of data property units.

        Returns
        -------
        dict(str, str)
            Dictionary of model property units
        dict(str, str)
            Dictionary of data property units
        dict(str, float)
            Dictionary of model to data property unit conversion factors

        """"""

        # Initialize model to data unit conversion dictionary
        model_conversion = {}

        # Check basic properties - positions, charge
        for prop in ['positions', 'charge']:

            # Check model property unit
            if model_units.get(prop) is None:
                model_units[prop] = data_units.get(prop)
                model_conversion[prop] = 1.0
            else:
                model_conversion[prop], _ = utils.check_units(
                    model_units[prop], data_units.get(prop))

        # Iterate over training properties
        for prop in trainer_properties:

            # Check model property unit
            if model_units.get(prop) is None:
                model_units[prop] = data_units.get(prop)
                model_conversion[prop] = 1.0
            else:
                model_conversion[prop], _ = utils.check_units(
                    model_units[prop], data_units.get(prop))

        return model_units, data_units, model_conversion

    def reset_metrics(self):
        """"""
        Reset metrics dictionary.

        Returns
        -------
        dict(str, float)
            Metric values dictionary set to zero.

        """"""

        # Initialize metrics dictionary
        metrics = {}

        # Add loss total value
        metrics['loss'] = 0.0

        # Add data counter
        metrics['Ndata'] = 0

        # Add training property metrics
        for prop in self.trainer_properties:
            metrics[prop] = {
                'loss': 0.0,
                'mae': 0.0,
                'mse': 0.0}

        return metrics

    def update_metrics(
        self,
        metrics: Dict[str, float],
        metrics_update: Dict[str, float],
    ) -> Dict[str, float]:
        """"""
        Update metrics dictionary.

        Parameters
        ----------
        metrics: dict
            Metrics dictionary
        metrics_update: dict
            Metrics dictionary to update

        Returns
        -------
        dict(str, float)
            Updated metric values dictionary with new batch results

        """"""

        # Get data sizes and metric ratio
        Ndata = metrics['Ndata']
        Ndata_update = metrics_update['Ndata']
        fdata = float(Ndata)/float((Ndata + Ndata_update))
        fdata_update = 1. - fdata

        # Update metrics
        metrics['Ndata'] = metrics['Ndata'] + metrics_update['Ndata']
        metrics['loss'] = (
            fdata*metrics['loss']
            + fdata_update*metrics_update['loss'].detach().item())
        for prop in self.trainer_properties:
            for metric in metrics_update[prop].keys():
                metrics[prop][metric] = (
                    fdata*metrics[prop][metric]
                    + fdata_update*metrics_update[prop][metric].detach().item()
                    )

        return metrics

    def compute_metrics(
        self,
        prediction: Dict[str, Any],
        reference: Dict[str, Any],
        loss_fn: Optional[object] = None,
        loss_only: Optional[bool] = True,
    ) -> Dict[str, float]:
        """"""
        Compute metrics. This function evaluates the loss function.

        Parameters
        ----------
        prediction: dict
            Model prediction dictionary
        reference:
            Reference data dictionary
        loss_fn:
            Loss function if not defined it is set to torch.nn.L1Loss
        loss_only
            Compute only loss function or compute MAE and MSE as well

        Returns
        -------
        dict(str, float)
            Metric values dictionary

        """"""

        # Check loss function input
        if loss_fn is None:
            loss_fn = torch.nn.L1Loss(reduction=""mean"")

        # Initialize MAE calculator function if needed
        if not loss_only:
            mae_fn = torch.nn.L1Loss(reduction=""mean"")
            mse_fn = torch.nn.MSELoss(reduction=""mean"")

        # Initialize metrics dictionary
        metrics = {}

        # Add batch size
        metrics['Ndata'] = reference['atoms_number'].size()[0]

        # Iterate over training properties
        for ip, prop in enumerate(self.trainer_properties):

            # Initialize single property metrics dictionary
            metrics[prop] = {}

            # Compute loss value per atom
            metrics[prop]['loss'] = loss_fn(
                torch.flatten(prediction[prop])
                * self.model_conversion[prop],
                torch.flatten(reference[prop]))

            # Check for NaN loss value
            if torch.isnan(metrics[prop]['loss']):
                raise SyntaxError(
                    f""Loss value for property '{prop:s}' is 'NaN'!"")

            # Weight and add to total loss
            if ip:
                metrics['loss'] = metrics['loss'] + (
                    self.trainer_properties_weights[prop]
                    * metrics[prop]['loss'])
            else:
                metrics['loss'] = (
                    self.trainer_properties_weights[prop]
                    * metrics[prop]['loss'])

            # Compute MAE and MSE if requested
            if not loss_only:
                metrics[prop]['mae'] = mae_fn(
                    torch.flatten(prediction[prop])
                    * self.model_conversion[prop],
                    torch.flatten(reference[prop]))
                metrics[prop]['mse'] = mse_fn(
                    torch.flatten(prediction[prop])
                    * self.model_conversion[prop],
                    torch.flatten(reference[prop]))

        return metrics

    def print_trainer_info(self):
        """"""
        Print trainer properties summary

        """"""

        message = (
            f"" {'Property ':<17s} |""
            + f"" {'Model Unit':<12s} |""
            + f"" {'Data Unit':<12s} |""
            + f"" {'Conv. fact.':<12s} |""
            + f"" {'Loss Metric':<12s} |""
            + f"" {'Loss Weight':<12s}\n"")
        message += ""-""*len(message) + ""\n""
        for prop, model_unit in self.model_units.items():
            if self.data_units.get(prop) is None:
                data_unit = ""None""
            else:
                data_unit = self.data_units.get(prop)
            message += (
                f"" {prop:<17s} |""
                + f"" {model_unit:<12s} |""
                + f"" {data_unit:<12s} |"")
            if self.model_conversion.get(prop) is None:
                message += f"" {'None':<12s} |""
            else:
                message += f"" {self.model_conversion.get(prop):>12.4e} |""
            if prop in self.trainer_properties:
                message += f"" {self.trainer_properties_metrics[prop]:<12s} |""
            else:
                message += f"" {'':<12s} |""
            if prop in self.trainer_properties:
                message += f"" {self.trainer_properties_weights[prop]:> 11.4f}""
            message += ""\n""
        self.logger.info(
            ""Model and data properties, and model to data conversion factors ""
            + ""(model to data).""
            + ""\nError metric and weight are shown for the properties ""
            + ""included in the training loss function.\n""
            + message)

        return

    def reset_logger(
        self,
        level: Optional[Callable] = None,
        stream: Optional[Union[Callable, str]] = sys.stdout,
        verbose: Optional[bool] = True,
    ):
        """"""
        Reset Training class logger, e.g, in model ensemble training to stream
        model training to model specific output files.

        Parameters
        ----------
        level: callable, optional, default 'logging.INFO'
            Print level for output (e.g. logging.DEBUG, logging.INFO, ...)
        stream: (callable, str), optional, default 'sys.stdout'
            Output channel to print or file path to write
        verbose: bool, optional, default True
            Start logger output with header for information

        """"""

        # Set logger options
        self.logger = utils.set_logger(
            logging.getLogger(self.name),
            level=level,
            stream=stream,
            verbose=verbose)

        return",./Asparagus/asparagus/training/trainer.py
EnsembleTrainer,"class EnsembleTrainer:
    """"""
    Wrapper for the training of an model ensemble, either serial or in parallel
    for a certain number of epochs 'trainer_epochs_step' before cycling 
    to the next model calculator until maximum epochs is reached.

    Parameters
    ----------
    config: (str, dict, object)
        Either the path to json file (str), dictionary (dict) or
        settings.config class object of Asparagus parameters
    config_file: str, optional, default see settings.default['config_file']
        Path to config json file (str)
    trainer_epochs_step: int, optional, default None
        Number of epochs to train single NNP models each step before looping
        through the next until maximum number of epochs is reached.
    trainer_num_threads: int, optional, default 1
        Number of model training threads for parallel model training.
    trainer_max_epochs: int, optional, default 10000
        Maximum number of training epochs
    trainer_save_interval: int, optional, default 5
        Interval between epoch to save current and best set of model
        parameters.
    trainer_evaluate_testset: bool, optional, default True
        If True, run ensemble model test when each single model have been run
        through same 'trainer_epochs_step'.
    trainer_batch_size: int, optional, default None
        Default dataloader batch size
    trainer_test_batch_size:  int, optional, default None
        Test dataloader batch size. If None, default batch size is used.
    trainer_num_batch_workers: int, optional, default 1
        Number of test dataloader workers.

    """"""

    # Initialize logger
    name = f""{__name__:s} - {__qualname__:s}""
    logger = utils.set_logger(logging.getLogger(name))

    # Default arguments for ensemble trainer class
    _default_args = {
        'trainer_epochs_step':          100,
        'trainer_num_threads':          1,
        'trainer_max_epochs':           10_000,
        'trainer_save_interval':        5,
        'trainer_evaluate_testset':     True,
        'trainer_batch_size':           32,
        'trainer_test_batch_size':      None,
        'trainer_num_batch_workers':    0,
        }

    # Expected data types of input variables
    _dtypes_args = {
        'trainer_epochs_step':          [utils.is_integer],
        'trainer_num_threads':          [utils.is_integer],
        'trainer_max_epochs':           [utils.is_integer],
        'trainer_save_interval':        [utils.is_integer],
        'trainer_evaluate_testset':     [utils.is_bool],
        'trainer_batch_size':           [utils.is_integer],
        'trainer_test_batch_size':      [utils.is_integer, utils.is_None],
        'trainer_num_batch_workers':    [utils.is_integer],
        }

    def __init__(
        self,
        config: Optional[Union[str, dict, object]] = None,
        config_file: Optional[str] = None,
        data_container: Optional[data.DataContainer] = None,
        model_calculator: Optional[torch.nn.Module] = None,
        trainer_epochs_step: Optional[int] = None,
        trainer_num_threads: Optional[int] = None,
        trainer_max_epochs: Optional[int] = None,
        trainer_save_interval: Optional[int] = None,
        trainer_evaluate_testset: Optional[bool] = None,
        trainer_batch_size: Optional[int] = None,
        trainer_test_batch_size: Optional[int] = None,
        trainer_num_batch_workers: Optional[int] = None,
        device: Optional[str] = None,
        dtype: Optional['dtype'] = None,
        **kwargs
    ):
        """"""
        Initialize Ensemble Model Calculator training.

        """"""

        ########################################
        # # # Check Ensemble Trainer Input # # #
        ########################################

        # Get configuration object
        config = settings.get_config(
            config, config_file, config_from=self)

        # Check input parameter, set default values if necessary and
        # update the configuration dictionary
        config_update = config.set(
            instance=self,
            argitems=utils.get_input_args(),
            check_default=utils.get_default_args(self, training),
            check_dtype=utils.get_dtype_args(self, training)
        )

        # Update global configuration dictionary
        config.update(config_update)

        # Assign module variable parameters from configuration
        self.device = utils.check_device_option(device, config)
        self.dtype = utils.check_dtype_option(dtype, config)

        ################################
        # # # Check Data Container # # #
        ################################

        # Assign DataContainer if not done already
        if data_container is None:
            self.data_container = data.DataContainer(
                config=config,
                **kwargs)

        ##################################
        # # # Check Model Calculator # # #
        ##################################

        # Assign model calculator model if not done already
        if self.model_calculator is None:
            self.model_calculator, _, _ = model.get_model_calculator(
                config=config,
                **kwargs)

        # Check for model ensemble calculator and grep model calculator number
        if hasattr(self.model_calculator, 'model_ensemble'):
            self.model_ensemble = self.model_calculator.model_ensemble
        else:
            self.model_ensemble = False
        if self.model_ensemble:
            self.model_ensemble_num = self.model_calculator.model_ensemble_num
        else:
            raise SyntaxError(
                ""The Ensemble Trainer class only handles model ensembles!"")

        # Initialize checkpoint file manager
        self.filemanager = model.FileManager(
            config=config,
            model_calculator=self.model_calculator)

        # Get model directory as main directory in which to store multiple
        # models
        self.model_directory = self.filemanager.model_directory

        #############################################
        # # # Check Ensemble Trainer Parameters # # #
        #############################################

        # Check that 'trainer_epochs_step' is a multiple of
        # 'trainer_save_interval' and update in config if necessary.
        trainer_epochs_step = int(
            self.trainer_save_interval*
            (self.trainer_epochs_step//self.trainer_save_interval)
        )
        if trainer_epochs_step != self.trainer_epochs_step:
            self.logger.warning(
                ""Number of epochs per training step ""
                + f""({self.trainer_epochs_step:d}) is not a multiple of the ""
                + f""Trainer's checkpoint save interval ""
                + f""({self.trainer_save_interval:d})!\n""
                + ""Number of epochs per training step is changed to ""
                + f""{trainer_epochs_step:d}."")
            self.trainer_epochs_step = trainer_epochs_step

        ##########################################
        # # # Prepare Model Ensemble Configs # # #
        ##########################################

        # Get model subdirectories
        self.model_subdirectories = [
            os.path.join(self.model_directory, f""{imodel:d}"")
            for imodel in range(self.model_ensemble_num)]

        # Create model subdirectories
        for model_directory in self.model_subdirectories:
            if not os.path.exists(model_directory):
                os.makedirs(model_directory)

        # Generate model configurations
        self.model_configs = []
        for imodel, model_subdirectory in enumerate(
            self.model_subdirectories
        ):
            
            # Get a copy of the configuration dictionary
            model_config = config.config_dict.copy()

            # Reset ensemble flag in model configuration
            model_config['model_ensemble'] = False
            model_config['model_ensemble_num'] = None

            # Store single model configuration
            model_config_file = os.path.join(model_subdirectory, ""config.json"")
            self.model_configs.append(
                settings.get_config(
                    config=model_config,
                    config_file=model_config_file,
                    config_from=f""Ensemble model {imodel:d}"",
                    verbose=False,
                    )
                )

        #################################################
        # # # Initialize Ensemble Training Schedule # # #
        #################################################

        # Initialize step number list per model
        self.trainer_model_step = np.zeros(
            self.model_ensemble_num, dtype=int)

        # Initialize active training flag list per model
        self.trainer_model_is_training = np.zeros(
            self.model_ensemble_num, dtype=bool)

        # Assign training schedule steps
        self.trainer_epoch_steps_list = np.arange(
            self.trainer_epochs_step,
            self.trainer_max_epochs + self.trainer_epochs_step,
            self.trainer_epochs_step)
        self.trainer_epoch_steps_number = len(self.trainer_epoch_steps_list)

        # Initialize first test step
        # When all model steps are at current test step, run tester class next
        # time
        self.trainer_test_step = 1

        # Check number of model training threads
        if self.trainer_num_threads > self.model_ensemble_num:
            self.logger.warning(
                ""Number of model training threads ""
                + f""({self.trainer_num_threads:d}) cannot be larger than ""
                + "" the number of ensemble model calculators ""
                + f""({self.model_ensemble_num:d})!\n""
                + ""Number of model training threads is lowered to ""
                + f""({model_ensemble_num:d})."")
            self.trainer_num_threads = self.model_ensemble_num

        ##########################
        # # # Prepare Tester # # #
        ##########################

        # Assign model prediction tester if test set evaluation is requested
        if self.trainer_evaluate_testset:
            if self.trainer_test_batch_size is None:
                self.trainer_test_batch_size = self.trainer_batch_size
            self.tester = Tester(
                config=config,
                data_container=self.data_container,
                test_datasets='test',
                test_batch_size=self.trainer_test_batch_size,
                test_num_batch_workers=self.trainer_num_batch_workers)

        return

    def run(
        self,
        checkpoint: Optional[Union[str, int, List[str], List[int]]] = 'last',
        restart: Optional[bool] = True,
        reset_best_loss: Optional[bool] = False,
        reset_energy_shift: Optional[bool] = False,
        skip_property_scaling: Optional[bool] = False,
        skip_initial_testing: Optional[bool] = False,
        **kwargs,
    ):
        """"""
        Train ensemble of model calculators.

        Parameters
        ----------
        checkpoint: (str, int, list(str), list(int)), optional, default 'last'
            If string 'best' or 'last', load respectively the best checkpoint 
            file (as with None) or the with the highest epoch number for each.
            If a string of valid file path, load the checkpoint file in all
            models of the ensemble (not recommended for ensemble learning).
            If a list of strings of valid file paths of the same length as the
            number of models in the ensemble, load the respective checkpoint 
            files.
            If integer or list of integer of the same length as the number of
            models in the ensemble, load the checkpoint file of the respective
            epoch.
        restart: bool, optional, default True
            If True, restart the ensemble model training from each of the last
            checkpoint files, respectively, if available. If False or no
            checkpoint file exist, start each training from scratch.
        reset_best_loss: bool, optional, default False
            If False, continue each model potential validation from stored best
            loss value. Else, reset best loss value to None.
        reset_energy_shift: bool, optional, default False
            If True and a model checkpoint file is successfully loaded, only
            the (atomic) energy shifts will be initially optimized for each 
            model in the ensemble to match best the reference training set.
            This function is used, e.g., for ""transfer learning"", when a
            trained model is retrained on a different reference data set with a
            changed total energy shift.
        skip_property_scaling: bool, optional, default False
            Skip for each model the initial model properties scaling factor and
            shift term optimization to match best the reference training set.
        skip_initial_testing: bool, optional, default False
            Skip for each model  the initial model evaluation on the reference
            test set, if the model evaluation is enabled anyways (see
            trainer_evaluate_testset).

        """"""

        # Check checkpoint input
        if checkpoint is None:
            checkpoint_list = [None]*self.model_ensemble_num
        elif utils.is_string(checkpoint) or utils.is_integer(checkpoint):
            checkpoint_list = [checkpoint]*self.model_ensemble_num
        elif (
            utils.is_string_array(checkpoint)
            or utils.is_integer_array(checkpoint)
        ):
            if len(checkpoint) != self.model_ensemble_num:
                raise ValueError(
                    ""Checkpoint in 'checkpoint' is a list but of different ""
                    + f""length ({len(checkpoint):d}) than the number of ""
                    + f""ensemble models ({self.model_ensemble_num:d})!"")
            checkpoint_list = checkpoint
        else:
            raise ValueError(
                ""Checkpoint input 'checkpoint' is of unkown format.\n""
                + ""Provide a string, integer or list of strings or integers."")

        # Initialize the multithreading lock
        self.lock = threading.Lock()

        # Print ensemble training information
        self.logger.info(
            ""Start model ensemble training:\n""
            + f"" Number of models: {self.model_ensemble_num:d}\n""
            + f"" Models directory: {self.model_directory:s}\n""
            + f"" Epochs per training step: {self.trainer_epochs_step:d}\n""
            + f"" Number of training threads: {self.trainer_num_threads:d}"")

        # Run sampling over sample systems
        if self.trainer_num_threads == 1:

            self.run_training(
                checkpoint_list,
                restart,
                reset_best_loss,
                reset_energy_shift,
                skip_property_scaling,
                skip_initial_testing)
        
        else:

            # Create threads
            threads = [
                threading.Thread(
                    target=self.run_training, 
                    args=(
                        checkpoint_list,
                        restart,
                        reset_best_loss,
                        reset_energy_shift,
                        skip_property_scaling,
                        skip_initial_testing, ),
                    kwargs={
                        'ithread': ithread}
                    )
                for ithread in range(self.trainer_num_threads)]

            # Start threads
            for thread in threads:
                thread.start()

            # Wait for threads to finish
            for thread in threads:
                thread.join()
        
        return

    def run_training(
        self, 
        checkpoint_list: Union[List[str], List[int]],
        restart: bool,
        reset_best_loss: bool,
        reset_energy_shift: bool,
        skip_property_scaling: bool,
        skip_initial_testing: bool,
        ithread: Optional[int] = None,
    ):
        """"""
        Run training step for one model calculator in ensemble queue.
        
        Parameters
        ----------
        checkpoint_list: list(str, int)
            Checkpoint option only for the first training step of each model.
            For later steps, checkpoint option is 'last'.
        restart: bool
            Restart option only for the first training step of each model.
            For later steps, restart is True.
        reset_best_loss: bool
            Reset best lost option only for the first training step of each
            model. For later steps, parameter is False.
        reset_energy_shift: bool
            Reset energy shift option only for the first training step of each
            model. For later steps, parameter is False.
        skip_property_scaling: bool
            Skip property scaling option only for the first training step of 
            each model. For later steps, parameter is True.
        skip_initial_testing: bool
            Skip initial testing option only for the first training step of 
            each model. For later steps, parameter is True.
        ithread: int, optional, default None
            Thread number

        """"""

        while self.keep_going():
            
            # Select next model and step
            imodel, istep, flag_test = self.next_step()

            # Check last model training epoch
            checkpoint = self.filemanager.load_checkpoint(
                'last',
                return_name=False,
                verbose=False)
            if checkpoint[imodel] is None:
                last_epoch = 0
            else:
                last_epoch = checkpoint[imodel]['epoch']
            if last_epoch >= self.trainer_epoch_steps_list[istep]:
                # Skip training step
                self.increment_model_step(imodel)
                continue

            # Print training thread information
            if ithread is None:
                self.logger.info(
                    f""Start training of model {imodel:d} up to epoch ""
                    + f""{self.trainer_epoch_steps_list[istep]:d}."")
            else:
                self.logger.info(
                    f""Start training of model {imodel:d} ""
                    + f""(thread {ithread:d}) up to epoch ""
                    + f""{self.trainer_epoch_steps_list[istep]:d}."")

            # Initialize ensemble model potential
            model_calculator, _, _ = model.get_model_calculator(
                config=self.model_configs[imodel],
                model_directory=self.model_subdirectories[imodel],
                verbose=False)

            # Initialize Trainer instance, only in series one thread after
            # another, to avoid conflicts with data container (e.g. reference
            # atomic energy shifts).
            with self.lock:

                trainer = training.Trainer(
                    config=self.model_configs[imodel],
                    data_container=self.data_container,
                    model_calculator=model_calculator,
                    trainer_max_epochs=self.trainer_epoch_steps_list[istep],
                    trainer_evaluate_testset=False,
                    trainer_print_progress_bar=False,
                    verbose=False)

            # Run test if requested
            if flag_test:
                
                # Get best model checkpoint
                checkpoint, checkpoint_file = self.filemanager.load_checkpoint(
                    'best',
                    return_name=True,
                    verbose=False)

                # Load current model checkpoint file
                self.model_calculator.load(
                    checkpoint,
                    checkpoint_file=checkpoint_file,
                    verbose=False)

                # Run test
                self.tester.test(
                    self.model_calculator,
                    model_conversion=trainer.model_conversion,
                    test_directory=os.path.join(self.model_directory, 'best'),
                    test_plot_correlation=True,
                    test_plot_histogram=True,
                    test_plot_residual=True)

            # Run training
            if istep:
                trainer.run(
                    checkpoint='last',
                    restart=True,
                    reset_best_loss=False,
                    reset_energy_shift=False,
                    skip_property_scaling=True,
                    skip_initial_testing=True,
                    ithread=ithread,
                    verbose=False)
            else:
                trainer.run(
                    checkpoint=checkpoint_list[imodel],
                    restart=restart,
                    reset_best_loss=reset_best_loss,
                    reset_energy_shift=reset_energy_shift,
                    skip_property_scaling=skip_property_scaling,
                    skip_initial_testing=skip_initial_testing,
                    ithread=ithread,
                    verbose=False)

            # Set model training status to idle and increment model step
            with self.lock:
                self.increment_model_step(imodel)

            # Print training thread information
            if ithread is None:
                self.logger.info(
                    f""Done training of model {imodel:d} up to epoch ""
                    + f""{self.trainer_epoch_steps_list[istep]:d}."")
            else:
                self.logger.info(
                    f""Done training of model {imodel:d} ""
                    + f""(thread {ithread:d}) up to epoch ""
                    + f""{self.trainer_epoch_steps_list[istep]:d}."")

        return

    def increment_model_step(
        self,
        imodel: int,
    ):
        """"""
        Set model training status to idle and increment model step

        Parameters
        ----------
        imodel: int
            Model calculator index

        """"""
        self.trainer_model_is_training[imodel] = False
        self.trainer_model_step[imodel] += 1
        return

    def keep_going(self):
        """"""
        Check if training steps are left.
        
        """"""
        
        # Hold further threads until check is complete
        with self.lock:

            # Models, which are currently not actively training
            idle_models = np.logical_not(self.trainer_model_is_training)

            # Get training steps for idle models
            idle_model_steps = self.trainer_model_step[idle_models]

            if np.any(
                idle_model_steps < self.trainer_epoch_steps_number
            ):
                return True
            else:
                return False

    def next_step(self):
        """"""
        Select next training step
        
        """"""

        # Hold further threads until selection is done
        with self.lock:

            # Models, which are currently not actively training
            idle_models = np.logical_not(self.trainer_model_is_training)

            # Select model with lowest step number which is currently not
            # actively training
            min_step = np.min(self.trainer_model_step[idle_models])
            imodel = np.where(
                np.logical_and(
                    idle_models,
                    self.trainer_model_step == min_step
                )
            )[0]
            if len(imodel):
                imodel = imodel[0]
            else:
                raise SyntaxError(
                    ""No inactive model found!"")

            # Get next epoch step of the selected model
            istep = self.trainer_model_step[imodel]

            # Set model status to actively training
            self.trainer_model_is_training[imodel] = True

            # Check if model test run is due
            flag_test = all([
                self.trainer_test_step == test_istep
                for test_istep in self.trainer_model_step])
            if flag_test:
                self.trainer_test_step += 1

        return imodel, istep, flag_test",./Asparagus/asparagus/training/ensemble.py
