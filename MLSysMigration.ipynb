{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Class Extraction**"
      ],
      "metadata": {
        "id": "Z6C4g4AHxilE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast  # Module for parsing Python source into its Abstract Syntax Tree (AST)\n",
        "import csv  # Module for reading and writing CSV files\n",
        "import os   # Module for interacting with the operating system (directory traversal)\n",
        "\n",
        "# Function to extract all class definitions from a single Python file\n",
        "def extract_classes_from_file(file_path):\n",
        "    # Open the file and parse it into an AST\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        tree = ast.parse(f.read(), filename=file_path)\n",
        "\n",
        "    classes = []  # List to hold extracted class info\n",
        "\n",
        "    # Re-read the file to get the full content for code extraction\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_content = f.read()\n",
        "\n",
        "    # Walk through each node in the AST to find class definitions\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, ast.ClassDef):  # Check if the node is a class definition\n",
        "            class_name = node.name  # Get the class name\n",
        "            class_code = ast.get_source_segment(file_content, node)  # Extract full source code for the class\n",
        "            classes.append((class_name, class_code, file_path))  # Add a tuple with class info to the list\n",
        "\n",
        "    return classes  # Return list of extracted classes\n",
        "\n",
        "# Function to recursively extract classes from all Python files in a given directory\n",
        "def extract_classes_from_project(directory):\n",
        "    all_classes = []  # List to store all extracted classes from all files\n",
        "    for root, _, files in os.walk(directory):  # Traverse the directory tree\n",
        "        for file in files:\n",
        "            if file.endswith(\".py\"):  # Check if the file is a Python source file\n",
        "                file_path = os.path.join(root, file)  # Construct the full file path\n",
        "                all_classes.extend(extract_classes_from_file(file_path))  # Extract and append classes from this file\n",
        "    return all_classes  # Return all extracted classes\n",
        "\n",
        "# Function to save extracted class data to a CSV file\n",
        "def save_to_csv(data, output_file):\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
        "        writer = csv.writer(f)  # Create a CSV writer\n",
        "        writer.writerow([\"ClassName\", \"Code\", \"File\"])  # Write header row\n",
        "        writer.writerows(data)  # Write all class data rows\n",
        "\n",
        "# === Main Execution ===\n",
        "\n",
        "# Path to the root directory of the project to scan\n",
        "project_directory = \"\"  # <- Replace with your target directory path\n",
        "output_csv = \"extracted_classes_case_study.csv\"  # Output CSV file name\n",
        "\n",
        "# Extract classes from the project directory\n",
        "extracted_classes = extract_classes_from_project(project_directory)\n",
        "\n",
        "# Save extracted data to CSV\n",
        "save_to_csv(extracted_classes, output_csv)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Extracted classes saved to {output_csv}\")"
      ],
      "metadata": {
        "id": "6iokrXyxyUKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **First Vertical Decomposition:**"
      ],
      "metadata": {
        "id": "L0rI-1R5xQz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Groq*** refers to the Groq API client, which is used to access LLMs (Large Language Models) hosted on Groq's ultra-fast inference engine.\n",
        "\n",
        "\n",
        "***dotenv*** is used to load environment variables from a .env file into the system's environment so they can be accessed via os.environ"
      ],
      "metadata": {
        "id": "fhGhe67rxyXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv groq"
      ],
      "metadata": {
        "id": "AM5mmgDwxwUc",
        "outputId": "35b61df1-3c5f-45d8-950d-c6932d11f2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting groq\n",
            "  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading groq-0.24.0-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv, groq\n",
            "Successfully installed dotenv-0.9.9 groq-0.24.0 python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classify the classes into User, Logic and Data layers"
      ],
      "metadata": {
        "id": "bEu7lGLpx7IL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model:** llama-3.1-8b-instant"
      ],
      "metadata": {
        "id": "MvpSyP23yeO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IXOHsR-wnI0",
        "outputId": "2794f1e5-5c91-4de8-c05f-83e31307d2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3f1716e226df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For handling CSV and DataFrames\n",
        "import re  # For regular expression operations (used to extract the category number)\n",
        "from dotenv import load_dotenv  # To load environment variables from a .env file\n",
        "from groq import Groq  # Groq client to interact with the LLM API\n",
        "import os  # For operating system-related operations like environment variable access\n",
        "\n",
        "# Load environment variables from a .env file (e.g., API keys)\n",
        "load_dotenv()\n",
        "\n",
        "# Set the Groq API key (make sure to replace with your actual key or load it securely)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "# Initialize the Groq LLM client with the API key\n",
        "groq = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# Define a mapping from numeric category values to human-readable architectural layer names\n",
        "category_map = {\n",
        "    \"1\": \"User Layer\",\n",
        "    \"2\": \"Logic Layer\",\n",
        "    \"3\": \"Data Layer\"\n",
        "}\n",
        "\n",
        "def classify_with_llm(class_name, class_code, class_file):\n",
        "    \"\"\"\n",
        "    Sends the class code to the LLM for classification into one of the three architectural layers.\n",
        "    Returns the layer number (as a string) or \"Unclassified\" if classification fails.\n",
        "    \"\"\"\n",
        "    # Prompt for LLM with detailed instructions and class metadata\n",
        "    prompt = f'''\n",
        "    You are an AI assistant specialized in software architecture analysis of Python applications. Your task is to examine the following class definition and assign it to the most appropriate architectural layer in the system based on its responsibilities.\n",
        "\n",
        "    There are three architectural layers:\n",
        "\n",
        "    1. **User Layer**: Responsible for managing user interaction. Classes in this layer handle incoming requests, interface with the logic layer, and prepare/display responses. Typical examples include API endpoints, UI controllers, or request routers.\n",
        "\n",
        "    2. **Logic Layer**: Encapsulates core business rules and logic. This includes traditional application services and ML-specific functionalities like data preprocessing, model training, evaluation logic, or decision-making components.\n",
        "\n",
        "    3. **Data Layer**: Manages persistent data storage and retrieval. This layer includes classes that handle databases, file systems, or any data source—whether for storing system state or machine learning models.\n",
        "\n",
        "    Class Metadata:\n",
        "    - **Class Name**: {class_name}\n",
        "    - **Class File**: {class_file}\n",
        "\n",
        "    Class Code:\n",
        "    ```python\n",
        "    {class_code}\n",
        "    ```\n",
        "\n",
        "    Categorize the class and respond with the category number.\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        # Send the prompt to the LLM using the llama-3.1-8b-instant model\n",
        "        chat_completion = groq.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            temperature=1  # Moderate randomness\n",
        "        )\n",
        "\n",
        "        # Extract the response content\n",
        "        content = chat_completion.choices[0].message.content\n",
        "\n",
        "        # Extract the first digit found (expected to be 1, 2, or 3)\n",
        "        match = re.search(r'\\d', content)\n",
        "        if match:\n",
        "            return match.group(0)\n",
        "        else:\n",
        "            return \"Unclassified\"  # If no digit is found in the response\n",
        "    except Exception as e:\n",
        "        # Catch and print any errors, return \"Unclassified\" for failed attempts\n",
        "        print(f\"Error occurred while processing class: {class_name}. Error: {str(e)}\")\n",
        "        return \"Unclassified\"\n",
        "\n",
        "def map_category(category_number):\n",
        "    \"\"\"\n",
        "    Maps a numeric category (as returned by the LLM) to a human-readable architectural layer.\n",
        "    \"\"\"\n",
        "    return category_map.get(category_number, \"Unclassified\")\n",
        "\n",
        "def classify_classes(df):\n",
        "    \"\"\"\n",
        "    Applies LLM classification to each class in the DataFrame and maps results to readable labels.\n",
        "    Adds a new column 'Layer_Category' with the classification results.\n",
        "    \"\"\"\n",
        "    # First classify with raw LLM output (1, 2, 3, or Unclassified)\n",
        "    df[\"Layer_Category\"] = df.apply(lambda row: classify_with_llm(row[\"ClassName\"], row[\"Code\"], row[\"File\"]), axis=1)\n",
        "\n",
        "    # Then map the numeric result to descriptive category names\n",
        "    df[\"Layer_Category\"] = df[\"Layer_Category\"].apply(map_category)\n",
        "    return df\n",
        "\n",
        "def classify_csv(input_file):\n",
        "    \"\"\"\n",
        "    Reads a CSV file containing extracted class definitions,\n",
        "    classifies each class using an LLM, and writes the results to a new CSV file.\n",
        "    \"\"\"\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Apply classification to all rows\n",
        "    classified_df = classify_classes(df)\n",
        "\n",
        "    # Print the classified DataFrame to console\n",
        "    print(classified_df)\n",
        "\n",
        "    # Save the classification results to a new output file\n",
        "    output_file = \"output_layers_Asparagus_llama3.1-8b.csv\"\n",
        "    classified_df.to_csv(output_file, index=False)\n",
        "    return output_file\n",
        "\n",
        "# Entry point when the script is run directly\n",
        "if __name__ == '__main__':\n",
        "    # Classify the classes from the specified CSV file\n",
        "    classify_csv(\"extracted_classes_Asparagus.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Layer Classification"
      ],
      "metadata": {
        "id": "IkfVMypLzDfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the Groq API key (make sure to replace with your actual key or load it securely)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "# Initialize the Groq client\n",
        "groq = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "def classify_ai_layer(class_name, class_code, class_file):\n",
        "    \"\"\"\n",
        "    Sends a class definition to the LLM to classify it into an AI-specific architectural layer.\n",
        "    \"\"\"\n",
        "    prompt = f'''\n",
        "    You are an AI software analysis assistant specializing in Python code. Your task is to analyze the given class definition and categorize it into one of the following categories:\n",
        "\n",
        "    Categories:\n",
        "    1. Non-AI Layer: Handles general application logic, such as UI, APIs, logging, or database management.\n",
        "    2. AI Preprocessing Layer: Manages data preparation, feature extraction, or dataset transformations.\n",
        "    3. Model Training & Evaluation Layer: Responsible for training machine learning models, fine-tuning, and evaluating their performance.\n",
        "    4. Model Deployment & Monitoring Layer: Handles inference, model serving, and performance monitoring.\n",
        "\n",
        "    Class Information:\n",
        "    - Class Name: {class_name}\n",
        "    - Class File: {class_file}\n",
        "\n",
        "    Class Code:\n",
        "    ```python\n",
        "    {class_code}\n",
        "    ```\n",
        "\n",
        "    Please categorize the class and respond with the category number.\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        # Send the prompt to the LLM and get the response\n",
        "        chat_completion = groq.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            temperature=1\n",
        "        )\n",
        "\n",
        "        # Extract the numeric category from the model response\n",
        "        content = chat_completion.choices[0].message.content\n",
        "        match = re.search(r'\\d', content)\n",
        "        if match:\n",
        "            return match.group(0)\n",
        "        else:\n",
        "            return \"Unclassified\"\n",
        "    except Exception as e:\n",
        "        # Print any error that occurs and return a fallback label\n",
        "        print(f\"Error occurred while processing class: {class_name}. Error: {str(e)}\")\n",
        "        return \"Unclassified\"\n",
        "\n",
        "# Dictionary mapping category numbers to human-readable AI layers\n",
        "ai_category_map = {\n",
        "    \"1\": \"Non-AI Layer\",\n",
        "    \"2\": \"AI Preprocessing Layer\",\n",
        "    \"3\": \"Model Training & Evaluation Layer\",\n",
        "    \"4\": \"Model Deployment & Monitoring Layer\"\n",
        "}\n",
        "\n",
        "def map_ai_category(category_number):\n",
        "    \"\"\"\n",
        "    Converts a category number to a descriptive name.\n",
        "    \"\"\"\n",
        "    return ai_category_map.get(category_number, \"Unclassified\")\n",
        "\n",
        "def classify_ai_layers(df):\n",
        "    \"\"\"\n",
        "    Applies AI layer classification to each class in the DataFrame.\n",
        "    Adds a new column with the AI-related classification.\n",
        "    \"\"\"\n",
        "    df[\"AI_Layer_Category\"] = df.apply(\n",
        "        lambda row: classify_ai_layer(row[\"ClassName\"], row[\"Code\"], row[\"File\"]), axis=1\n",
        "    )\n",
        "    df[\"AI_Layer_Category\"] = df[\"AI_Layer_Category\"].apply(map_ai_category)\n",
        "    return df\n",
        "\n",
        "def process_ai_classification(input_file):\n",
        "    \"\"\"\n",
        "    Loads a CSV with previously classified classes, applies AI-specific classification,\n",
        "    and saves the results to a new CSV.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "    df = classify_ai_layers(df)\n",
        "\n",
        "    # Show the result in console\n",
        "    print(df)\n",
        "\n",
        "    # Save the new classification result\n",
        "    output_file = \"8B-output_ai_layers_Asparagus_llama3.1-8b.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    return output_file\n",
        "\n",
        "# Run the AI layer classification when this script is executed directly\n",
        "if __name__ == '__main__':\n",
        "    process_ai_classification(\"output_layers_PFEAPP_llama3.1-8b.csv\")\n"
      ],
      "metadata": {
        "id": "YunxdebuzD8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4512842-85e2-4212-9717-1a230b1e75a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred while processing class: NewsService. Error: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01jmyy0n05efgvvv3ttw9hppyx` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7027, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error occurred while processing class: RabbitMQService. Error: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01jmyy0n05efgvvv3ttw9hppyx` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6122, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "Error occurred while processing class: PredictionService. Error: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.1-8b-instant` in organization `org_01jmyy0n05efgvvv3ttw9hppyx` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 9253, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "                 ClassName                                               Code  \\\n",
            "0                  Startup  class Startup\\n{\\n    private readonly IConfig...   \n",
            "1                  Program  class Program\\n    {\\n        private static I...   \n",
            "2                AuthProxy  class AuthProxy : ProxyBase, IAuthProxy\\n{\\n  ...   \n",
            "3                ProxyBase  class ProxyBase\\n{\\n    private readonly strin...   \n",
            "4                TimeProxy  class TimeProxy : ProxyBase, ITimeProxy\\n{\\n  ...   \n",
            "..                     ...                                                ...   \n",
            "226          ModelMetadata  class ModelMetadata(BaseModel):\\n    \"\"\"Model ...   \n",
            "227              ModelInfo  class ModelInfo(BaseModel):\\n    \"\"\"Model info...   \n",
            "228      ModelListResponse  class ModelListResponse(BaseModel):\\n    \"\"\"Li...   \n",
            "229  ModelMetadataResponse  class ModelMetadataResponse(BaseModel):\\n    \"...   \n",
            "230  DirectDisplayResponse  class DirectDisplayResponse(BaseModel):\\n    \"...   \n",
            "\n",
            "                                                  File Language  \\\n",
            "0    ./BackendMicroservices/Services/Microservices/...       C#   \n",
            "1    ./BackendMicroservices/Services/Microservices/...       C#   \n",
            "2    ./BackendMicroservices/Services/Microservices/...       C#   \n",
            "3    ./BackendMicroservices/Services/Microservices/...       C#   \n",
            "4    ./BackendMicroservices/Services/Microservices/...       C#   \n",
            "..                                                 ...      ...   \n",
            "226                          ./stock-ai/api/schemas.py   Python   \n",
            "227                          ./stock-ai/api/schemas.py   Python   \n",
            "228                          ./stock-ai/api/schemas.py   Python   \n",
            "229                          ./stock-ai/api/schemas.py   Python   \n",
            "230                          ./stock-ai/api/schemas.py   Python   \n",
            "\n",
            "    Layer_Category       AI_Layer_Category  \n",
            "0      Logic Layer            Non-AI Layer  \n",
            "1       User Layer            Non-AI Layer  \n",
            "2      Logic Layer            Non-AI Layer  \n",
            "3      Logic Layer            Non-AI Layer  \n",
            "4       User Layer            Non-AI Layer  \n",
            "..             ...                     ...  \n",
            "226    Logic Layer            Non-AI Layer  \n",
            "227     Data Layer  AI Preprocessing Layer  \n",
            "228     User Layer            Non-AI Layer  \n",
            "229    Logic Layer            Non-AI Layer  \n",
            "230     User Layer            Non-AI Layer  \n",
            "\n",
            "[231 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping of the Class Classification into a CSV"
      ],
      "metadata": {
        "id": "ibs7hRtkZMOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original CSV file containing class layer predictions\n",
        "txt_file = 'output_ai_layers_PFEAPP-llama31-8b.csv'\n",
        "df = pd.read_csv(txt_file, header=0)\n",
        "\n",
        "# Strip any leading/trailing whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Ensure the file contains the required 'ClassName' column\n",
        "if 'ClassName' not in df.columns:\n",
        "    raise ValueError(\"The file does not contain 'ClassName' as a column.\")\n",
        "\n",
        "# Define mapping for traditional system layers\n",
        "layer_mapping = {\n",
        "    'User Layer': 'User Layer',\n",
        "    'Logic Layer': 'Logic Layer',\n",
        "    'Data Layer': 'Data Layer'\n",
        "}\n",
        "\n",
        "# Define mapping for AI-specific layers\n",
        "ai_layer_mapping = {\n",
        "    'Non-AI Layer': 'Non-AI Layer',\n",
        "    'AI Preprocessing Layer': 'AI Preprocessing Layer',\n",
        "    'Model Training & Evaluation Layer': 'Model Training & Evaluation Layer',\n",
        "    'Model Deployment & Monitoring Layer': 'Model Deployment & Monitoring Layer'\n",
        "}\n",
        "\n",
        "# Create a DataFrame to hold one-hot encoded system layer categories\n",
        "df_layers = pd.DataFrame()\n",
        "df_layers['ClassName'] = df['ClassName'].str.strip()  # Clean class names\n",
        "\n",
        "# One-hot encode each traditional layer category\n",
        "for category in layer_mapping:\n",
        "    df_layers[category] = (df['Layer_Category'].str.strip() == category).astype(int)\n",
        "\n",
        "# Add a column to mark unclassified system layers\n",
        "df_layers['Unclassified'] = ~df['Layer_Category'].str.strip().isin(layer_mapping.keys())\n",
        "df_layers['Unclassified'] = df_layers['Unclassified'].astype(int)\n",
        "\n",
        "# Create a DataFrame to hold one-hot encoded AI layer categories\n",
        "df_ai_layers = pd.DataFrame()\n",
        "\n",
        "# One-hot encode each AI-specific layer category\n",
        "for category in ai_layer_mapping:\n",
        "    df_ai_layers[category] = (df['AI_Layer_Category'].str.strip() == category).astype(int)\n",
        "\n",
        "# Add a column to mark unclassified AI layers\n",
        "df_ai_layers['Unclassified'] = ~df['AI_Layer_Category'].str.strip().isin(ai_layer_mapping.keys())\n",
        "df_ai_layers['Unclassified'] = df_ai_layers['Unclassified'].astype(int)\n",
        "\n",
        "# Merge both system and AI layer one-hot encodings into a single DataFrame\n",
        "df_transformed = pd.concat([df_layers, df_ai_layers], axis=1)\n",
        "\n",
        "# Sort the output by class name\n",
        "df_transformed = df_transformed.sort_values(by=['ClassName'])\n",
        "\n",
        "# Save the transformed results to a new CSV file\n",
        "output_file = 'LLama8B_PFEAPP.csv'\n",
        "df_transformed.to_csv(output_file, index=False)\n",
        "\n",
        "# Notify the user\n",
        "print(f'Transformed file saved to {output_file}')"
      ],
      "metadata": {
        "id": "kT3c97lCF2Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2: Class Vector Embedding Encoding**"
      ],
      "metadata": {
        "id": "CABDiUGrbScs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the pretrained CodeBERT tokenizer and model\n",
        "model_name = \"microsoft/codebert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def get_code_embedding(code_snippet):\n",
        "    \"\"\"\n",
        "    Generates an embedding vector for a given code snippet using CodeBERT.\n",
        "    The embedding is the mean of all token representations in the last hidden state.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "    embedding = output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return embedding\n",
        "\n",
        "# Load class data from CSV and drop the 'File' column (not needed for embedding)\n",
        "df = pd.read_csv(\"cleaned_Classes_LLama70B_PFEAPP.csv\").drop(columns=['File'])\n",
        "\n",
        "# Generate CodeBERT embeddings for each class's code snippet\n",
        "df['Embedding'] = df['Code'].apply(lambda code: get_code_embedding(code))\n",
        "\n",
        "# Stack all embedding vectors into a single matrix for PCA processing\n",
        "embeddings_matrix = np.vstack(df['Embedding'].values)\n",
        "\n",
        "# Reduce dimensionality of embeddings using PCA (from 768 → 100 dimensions)\n",
        "pca = PCA(n_components=100)\n",
        "dim_reduced_embeddings = pca.fit_transform(embeddings_matrix)\n",
        "\n",
        "# Create a DataFrame with the reduced embeddings\n",
        "embedding_columns = [f'feature_{i}' for i in range(dim_reduced_embeddings.shape[1])]\n",
        "embeddings_df = pd.DataFrame(dim_reduced_embeddings, columns=embedding_columns)\n",
        "\n",
        "# Combine reduced embeddings with the corresponding class names\n",
        "df_final = pd.concat([df[['ClassName']], embeddings_df], axis=1)\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "output_path = \"generated_reduced_code_embeddings_reduced.csv\"\n",
        "df_final.to_csv(output_path, index=False)\n",
        "\n",
        "# Notify completion\n",
        "print(f\"Reduced embeddings saved to {output_path}\")"
      ],
      "metadata": {
        "id": "TVzXTCsrbaeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the CSVs"
      ],
      "metadata": {
        "id": "qKSkoArNbzBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two CSV files\n",
        "df1 = pd.read_csv(\"generated_reduced_code_embeddings_reduced.csv\")\n",
        "df2 = pd.read_csv(\"cleaned_LLama70B_PFEAPP.csv\")\n",
        "\n",
        "# Find common class names\n",
        "common_classes = set(df1['ClassName']).intersection(set(df2['ClassName']))\n",
        "\n",
        "# Filter DataFrames to only keep rows with common class names\n",
        "df1_common = df1[df1['ClassName'].isin(common_classes)]\n",
        "df2_common = df2[df2['ClassName'].isin(common_classes)]\n",
        "\n",
        "# Sort both DataFrames by 'ClassName'\n",
        "df1_sorted = df1_common.sort_values(by='ClassName').reset_index(drop=True)\n",
        "df2_sorted = df2_common.sort_values(by='ClassName').reset_index(drop=True)\n",
        "df2_sorted = df2_sorted.drop(df2_sorted.columns[0], axis=1)\n",
        "\n",
        "# Merge both sorted DataFrames side by side\n",
        "merged_df = pd.concat([df1_sorted, df2_sorted], axis=1)\n",
        "\n",
        "# Drop the 7th column from the end\n",
        "\n",
        "# Save the merged DataFrame\n",
        "merged_df.to_csv(\"merged_classnames_LLama70B.csv\", index=False)"
      ],
      "metadata": {
        "id": "f_tmmSBQb229"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 3: Microservices Identification and Clustering**"
      ],
      "metadata": {
        "id": "_MLtZQFEb751"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import hdbscan\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def cluster_classes_by_layer(csv_path, output_csv_path):\n",
        "    \"\"\"\n",
        "    Clusters classes within each layer (Non-AI, AI Preprocessing, Model Training, Model Deployment)\n",
        "    using HDBSCAN and saves the cluster labels to a new CSV.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        layers = {\n",
        "            \"Non-AI Layer\": \"Non_AI_Cluster\",\n",
        "            \"AI Preprocessing Layer\": \"AI_Preprocessing_Cluster\",\n",
        "            \"Model Training & Evaluation Layer\": \"Model_Training_Cluster\",\n",
        "            \"Model Deployment & Monitoring Layer\": \"Model_Deployment_Cluster\"\n",
        "        }\n",
        "\n",
        "        # Initialize cluster columns with NaN\n",
        "        for cluster_col_name in layers.values():\n",
        "            df[cluster_col_name] = np.nan\n",
        "\n",
        "        for layer, cluster_col_name in layers.items():\n",
        "            layer_df = df[df[layer] == 1].copy()\n",
        "\n",
        "            if layer_df.empty:\n",
        "                print(f\"Skipping {layer}: No data found.\")\n",
        "                continue\n",
        "\n",
        "            # Extract feature columns (excluding layer columns & ClassName)\n",
        "            feature_cols = [col for col in df.columns if col not in layers.keys() and col != 'ClassName']\n",
        "\n",
        "            # Drop rows with NaN in feature columns\n",
        "            layer_df = layer_df.dropna(subset=feature_cols)\n",
        "\n",
        "            # Ensure there are valid feature columns\n",
        "            if layer_df.empty or len(feature_cols) == 0:\n",
        "                print(f\"Skipping {layer}: No valid features after removing NaN values.\")\n",
        "                continue\n",
        "\n",
        "            features = layer_df[feature_cols].values\n",
        "\n",
        "            # Check if there are enough samples\n",
        "            if features.shape[0] < 2:\n",
        "                print(f\"Skipping {layer}: Not enough samples for clustering (Found {features.shape[0]} samples).\")\n",
        "                continue\n",
        "\n",
        "            # Standardize features\n",
        "            scaler = StandardScaler()\n",
        "            scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "            # Apply HDBSCAN clustering\n",
        "            clusterer = hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True)\n",
        "            cluster_labels = clusterer.fit_predict(scaled_features)\n",
        "\n",
        "            # Assign cluster labels\n",
        "            df.loc[layer_df.index, cluster_col_name] = cluster_labels\n",
        "\n",
        "        # Save the DataFrame with cluster labels to a new CSV\n",
        "        df.to_csv(output_csv_path, index=False)\n",
        "        print(f\"Clustering results saved to {output_csv_path}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Input CSV file not found.\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing column in CSV - {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "csv_path = \"merged_classnames_LLama70B.csv\"\n",
        "output_csv_path = \"LayerBased_clustered_classes.csv\"\n",
        "\n",
        "cluster_classes_by_layer(csv_path, output_csv_path)"
      ],
      "metadata": {
        "id": "T2ACY1bcb84s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results Evaluation"
      ],
      "metadata": {
        "id": "BcrvWxkGcLVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "# file1 = './70Bmerged_Output_Llama3.1-8B_cleaned.csv'\n",
        "# file2 = './upupdated_feature_matrix_modified_cleaned.csv'\n",
        "# Load the CSV files\n",
        "df_true = pd.read_csv('') #Benchmark Labels\n",
        "df_pred = pd.read_csv('') #Predicted Labels\n",
        "# Sort both DataFrames alphabetically by 'ClassName'\n",
        "df_true = df_true.sort_values(by='ClassName').reset_index(drop=True)\n",
        "df_pred = df_pred.sort_values(by='ClassName').reset_index(drop=True)\n",
        "# Validate structure\n",
        "assert df_true.shape == df_pred.shape, \"Mismatch in shape\"\n",
        "assert all(df_true.columns == df_pred.columns), \"Mismatch in column names\"\n",
        "\n",
        "# Find start index after 'ClassName'\n",
        "start_index = df_true.columns.get_loc('ClassName') + 1\n",
        "target_columns = df_true.columns[start_index:]\n",
        "\n",
        "# Lists for overall metrics\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "print(\"\\n📊 Per-Column Evaluation Metrics:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for col in target_columns:\n",
        "    y_true = df_true[col]\n",
        "    y_pred = df_pred[col]\n",
        "\n",
        "    # Store for global metrics\n",
        "    all_true.extend(y_true)\n",
        "    all_pred.extend(y_pred)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"🔸 {col}\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall:    {recall:.4f}\")\n",
        "    print(f\"   F1 Score:  {f1:.4f}\")\n",
        "    print(f\"   Accuracy:  {accuracy:.4f}\\n\")\n",
        "\n",
        "# Overall metrics across all columns and rows\n",
        "overall_precision = precision_score(all_true, all_pred, average='binary', zero_division=0)\n",
        "overall_recall = recall_score(all_true, all_pred, average='binary', zero_division=0)\n",
        "overall_f1 = f1_score(all_true, all_pred, average='binary', zero_division=0)\n",
        "overall_accuracy = accuracy_score(all_true, all_pred)\n",
        "\n",
        "print(\"📈 Overall Evaluation Metrics (across all columns):\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"✅ Precision: {overall_precision:.4f}\")\n",
        "print(f\"✅ Recall:    {overall_recall:.4f}\")\n",
        "print(f\"✅ F1 Score:  {overall_f1:.4f}\")\n",
        "print(f\"✅ Accuracy:  {overall_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "NmKLK44KcNJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}