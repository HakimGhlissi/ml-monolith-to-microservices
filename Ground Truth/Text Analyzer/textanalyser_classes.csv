ClassName,Code,File
EntityMention,"public class EntityMention {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected EntityMention(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(EntityMention obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_EntityMention(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public EntityMention() {
    this(globalJNI.new_EntityMention__SWIG_0(), true);
  }

  public EntityMention(int start_, int end_) {
    this(globalJNI.new_EntityMention__SWIG_1(start_, end_), true);
  }

  public EntityMention(int start_, int end_, int tag_, double score_) {
    this(globalJNI.new_EntityMention__SWIG_2(start_, end_, tag_, score_), true);
  }

  public void setStart(int value) {
    globalJNI.EntityMention_start_set(swigCPtr, this, value);
  }

  public int getStart() {
    return globalJNI.EntityMention_start_get(swigCPtr, this);
  }

  public void setEnd(int value) {
    globalJNI.EntityMention_end_set(swigCPtr, this, value);
  }

  public int getEnd() {
    return globalJNI.EntityMention_end_get(swigCPtr, this);
  }

  public void setTag(int value) {
    globalJNI.EntityMention_tag_set(swigCPtr, this, value);
  }

  public int getTag() {
    return globalJNI.EntityMention_tag_get(swigCPtr, this);
  }

  public void setScore(double value) {
    globalJNI.EntityMention_score_set(swigCPtr, this, value);
  }

  public double getScore() {
    return globalJNI.EntityMention_score_get(swigCPtr, this);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/EntityMention.java
EntityMentionVector,"public class EntityMentionVector {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected EntityMentionVector(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(EntityMentionVector obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_EntityMentionVector(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public EntityMentionVector() {
    this(globalJNI.new_EntityMentionVector__SWIG_0(), true);
  }

  public EntityMentionVector(long n) {
    this(globalJNI.new_EntityMentionVector__SWIG_1(n), true);
  }

  public long size() {
    return globalJNI.EntityMentionVector_size(swigCPtr, this);
  }

  public long capacity() {
    return globalJNI.EntityMentionVector_capacity(swigCPtr, this);
  }

  public void reserve(long n) {
    globalJNI.EntityMentionVector_reserve(swigCPtr, this, n);
  }

  public boolean isEmpty() {
    return globalJNI.EntityMentionVector_isEmpty(swigCPtr, this);
  }

  public void clear() {
    globalJNI.EntityMentionVector_clear(swigCPtr, this);
  }

  public void add(EntityMention x) {
    globalJNI.EntityMentionVector_add(swigCPtr, this, EntityMention.getCPtr(x), x);
  }

  public EntityMention get(int i) {
    return new EntityMention(globalJNI.EntityMentionVector_get(swigCPtr, this, i), false);
  }

  public void set(int i, EntityMention val) {
    globalJNI.EntityMentionVector_set(swigCPtr, this, i, EntityMention.getCPtr(val), val);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/EntityMentionVector.java
TokenIndexPair,"public class TokenIndexPair {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected TokenIndexPair(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(TokenIndexPair obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_TokenIndexPair(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public void setIndex(long value) {
    globalJNI.TokenIndexPair_index_set(swigCPtr, this, value);
  }

  public long getIndex() {
    return globalJNI.TokenIndexPair_index_get(swigCPtr, this);
  }

  public void setToken(String value) {
    globalJNI.TokenIndexPair_token_set(swigCPtr, this, value);
  }

  public String getToken() {
    return globalJNI.TokenIndexPair_token_get(swigCPtr, this);
  }

  public TokenIndexPair() {
    this(globalJNI.new_TokenIndexPair(), true);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/TokenIndexPair.java
BinaryRelation,"public class BinaryRelation {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected BinaryRelation(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(BinaryRelation obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_BinaryRelation(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public void setItem(SWIGTYPE_p_mitie__binary_relation value) {
    globalJNI.BinaryRelation_item_set(swigCPtr, this, SWIGTYPE_p_mitie__binary_relation.getCPtr(value));
  }

  public SWIGTYPE_p_mitie__binary_relation getItem() {
    return new SWIGTYPE_p_mitie__binary_relation(globalJNI.BinaryRelation_item_get(swigCPtr, this), true);
  }

  public BinaryRelation() {
    this(globalJNI.new_BinaryRelation(), true);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/BinaryRelation.java
BinaryRelationDetector,"public class BinaryRelationDetector {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected BinaryRelationDetector(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(BinaryRelationDetector obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_BinaryRelationDetector(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public BinaryRelationDetector(String filename) {
    this(globalJNI.new_BinaryRelationDetector(filename), true);
  }

  public void saveToDisk(String filename) {
    globalJNI.BinaryRelationDetector_saveToDisk(swigCPtr, this, filename);
  }

  public String getNameString() {
    return globalJNI.BinaryRelationDetector_getNameString(swigCPtr, this);
  }

  public double classify(BinaryRelation rel) {
    return globalJNI.BinaryRelationDetector_classify(swigCPtr, this, BinaryRelation.getCPtr(rel), rel);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/BinaryRelationDetector.java
TokenIndexVector,"public class TokenIndexVector {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected TokenIndexVector(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(TokenIndexVector obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_TokenIndexVector(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public TokenIndexVector() {
    this(globalJNI.new_TokenIndexVector__SWIG_0(), true);
  }

  public TokenIndexVector(long n) {
    this(globalJNI.new_TokenIndexVector__SWIG_1(n), true);
  }

  public long size() {
    return globalJNI.TokenIndexVector_size(swigCPtr, this);
  }

  public long capacity() {
    return globalJNI.TokenIndexVector_capacity(swigCPtr, this);
  }

  public void reserve(long n) {
    globalJNI.TokenIndexVector_reserve(swigCPtr, this, n);
  }

  public boolean isEmpty() {
    return globalJNI.TokenIndexVector_isEmpty(swigCPtr, this);
  }

  public void clear() {
    globalJNI.TokenIndexVector_clear(swigCPtr, this);
  }

  public void add(TokenIndexPair x) {
    globalJNI.TokenIndexVector_add(swigCPtr, this, TokenIndexPair.getCPtr(x), x);
  }

  public TokenIndexPair get(int i) {
    return new TokenIndexPair(globalJNI.TokenIndexVector_get(swigCPtr, this, i), false);
  }

  public void set(int i, TokenIndexPair val) {
    globalJNI.TokenIndexVector_set(swigCPtr, this, i, TokenIndexPair.getCPtr(val), val);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/TokenIndexVector.java
global,"public class global {
  public static String loadEntireFile(String filename) {
    return globalJNI.loadEntireFile(filename);
  }

  public static StringVector tokenize(String str) {
    return new StringVector(globalJNI.tokenize(str), true);
  }

  public static TokenIndexVector tokenizeWithOffsets(String str) {
    return new TokenIndexVector(globalJNI.tokenizeWithOffsets(str), true);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/global.java
globalJNI,"public class globalJNI {

    static {
        try {
            System.loadLibrary(""javamitie"");
        } catch (UnsatisfiedLinkError e) {
        System.err.println(""Native code library failed to load. \n"" + e);
        System.exit(1);
        }
    }
    
  public final static native long new_StringVector__SWIG_0();
  public final static native long new_StringVector__SWIG_1(long jarg1);
  public final static native long StringVector_size(long jarg1, StringVector jarg1_);
  public final static native long StringVector_capacity(long jarg1, StringVector jarg1_);
  public final static native void StringVector_reserve(long jarg1, StringVector jarg1_, long jarg2);
  public final static native boolean StringVector_isEmpty(long jarg1, StringVector jarg1_);
  public final static native void StringVector_clear(long jarg1, StringVector jarg1_);
  public final static native void StringVector_add(long jarg1, StringVector jarg1_, String jarg2);
  public final static native String StringVector_get(long jarg1, StringVector jarg1_, int jarg2);
  public final static native void StringVector_set(long jarg1, StringVector jarg1_, int jarg2, String jarg3);
  public final static native void delete_StringVector(long jarg1);
  public final static native long new_TokenIndexVector__SWIG_0();
  public final static native long new_TokenIndexVector__SWIG_1(long jarg1);
  public final static native long TokenIndexVector_size(long jarg1, TokenIndexVector jarg1_);
  public final static native long TokenIndexVector_capacity(long jarg1, TokenIndexVector jarg1_);
  public final static native void TokenIndexVector_reserve(long jarg1, TokenIndexVector jarg1_, long jarg2);
  public final static native boolean TokenIndexVector_isEmpty(long jarg1, TokenIndexVector jarg1_);
  public final static native void TokenIndexVector_clear(long jarg1, TokenIndexVector jarg1_);
  public final static native void TokenIndexVector_add(long jarg1, TokenIndexVector jarg1_, long jarg2, TokenIndexPair jarg2_);
  public final static native long TokenIndexVector_get(long jarg1, TokenIndexVector jarg1_, int jarg2);
  public final static native void TokenIndexVector_set(long jarg1, TokenIndexVector jarg1_, int jarg2, long jarg3, TokenIndexPair jarg3_);
  public final static native void delete_TokenIndexVector(long jarg1);
  public final static native long new_EntityMentionVector__SWIG_0();
  public final static native long new_EntityMentionVector__SWIG_1(long jarg1);
  public final static native long EntityMentionVector_size(long jarg1, EntityMentionVector jarg1_);
  public final static native long EntityMentionVector_capacity(long jarg1, EntityMentionVector jarg1_);
  public final static native void EntityMentionVector_reserve(long jarg1, EntityMentionVector jarg1_, long jarg2);
  public final static native boolean EntityMentionVector_isEmpty(long jarg1, EntityMentionVector jarg1_);
  public final static native void EntityMentionVector_clear(long jarg1, EntityMentionVector jarg1_);
  public final static native void EntityMentionVector_add(long jarg1, EntityMentionVector jarg1_, long jarg2, EntityMention jarg2_);
  public final static native long EntityMentionVector_get(long jarg1, EntityMentionVector jarg1_, int jarg2);
  public final static native void EntityMentionVector_set(long jarg1, EntityMentionVector jarg1_, int jarg2, long jarg3, EntityMention jarg3_);
  public final static native void delete_EntityMentionVector(long jarg1);
  public final static native long new_SDPair__SWIG_0();
  public final static native long new_SDPair__SWIG_1(String jarg1, double jarg2);
  public final static native long new_SDPair__SWIG_2(long jarg1, SDPair jarg1_);
  public final static native void SDPair_first_set(long jarg1, SDPair jarg1_, String jarg2);
  public final static native String SDPair_first_get(long jarg1, SDPair jarg1_);
  public final static native void SDPair_second_set(long jarg1, SDPair jarg1_, double jarg2);
  public final static native double SDPair_second_get(long jarg1, SDPair jarg1_);
  public final static native void delete_SDPair(long jarg1);
  public final static native String loadEntireFile(String jarg1);
  public final static native long tokenize(String jarg1);
  public final static native void TokenIndexPair_index_set(long jarg1, TokenIndexPair jarg1_, long jarg2);
  public final static native long TokenIndexPair_index_get(long jarg1, TokenIndexPair jarg1_);
  public final static native void TokenIndexPair_token_set(long jarg1, TokenIndexPair jarg1_, String jarg2);
  public final static native String TokenIndexPair_token_get(long jarg1, TokenIndexPair jarg1_);
  public final static native long new_TokenIndexPair();
  public final static native void delete_TokenIndexPair(long jarg1);
  public final static native long tokenizeWithOffsets(String jarg1);
  public final static native long new_EntityMention__SWIG_0();
  public final static native long new_EntityMention__SWIG_1(int jarg1, int jarg2);
  public final static native long new_EntityMention__SWIG_2(int jarg1, int jarg2, int jarg3, double jarg4);
  public final static native void EntityMention_start_set(long jarg1, EntityMention jarg1_, int jarg2);
  public final static native int EntityMention_start_get(long jarg1, EntityMention jarg1_);
  public final static native void EntityMention_end_set(long jarg1, EntityMention jarg1_, int jarg2);
  public final static native int EntityMention_end_get(long jarg1, EntityMention jarg1_);
  public final static native void EntityMention_tag_set(long jarg1, EntityMention jarg1_, int jarg2);
  public final static native int EntityMention_tag_get(long jarg1, EntityMention jarg1_);
  public final static native void EntityMention_score_set(long jarg1, EntityMention jarg1_, double jarg2);
  public final static native double EntityMention_score_get(long jarg1, EntityMention jarg1_);
  public final static native void delete_EntityMention(long jarg1);
  public final static native void BinaryRelation_item_set(long jarg1, BinaryRelation jarg1_, long jarg2);
  public final static native long BinaryRelation_item_get(long jarg1, BinaryRelation jarg1_);
  public final static native long new_BinaryRelation();
  public final static native void delete_BinaryRelation(long jarg1);
  public final static native long new_NamedEntityExtractor__SWIG_0(String jarg1);
  public final static native long new_NamedEntityExtractor__SWIG_1(String jarg1, String jarg2);
  public final static native long NamedEntityExtractor_getPossibleNerTags(long jarg1, NamedEntityExtractor jarg1_);
  public final static native void NamedEntityExtractor_saveToDisk(long jarg1, NamedEntityExtractor jarg1_, String jarg2);
  public final static native long NamedEntityExtractor_extractEntities__SWIG_0(long jarg1, NamedEntityExtractor jarg1_, long jarg2, StringVector jarg2_);
  public final static native long NamedEntityExtractor_extractEntities__SWIG_1(long jarg1, NamedEntityExtractor jarg1_, long jarg2, TokenIndexVector jarg2_);
  public final static native long NamedEntityExtractor_extractBinaryRelation(long jarg1, NamedEntityExtractor jarg1_, long jarg2, StringVector jarg2_, long jarg3, EntityMention jarg3_, long jarg4, EntityMention jarg4_);
  public final static native void delete_NamedEntityExtractor(long jarg1);
  public final static native long new_BinaryRelationDetector(String jarg1);
  public final static native void BinaryRelationDetector_saveToDisk(long jarg1, BinaryRelationDetector jarg1_, String jarg2);
  public final static native String BinaryRelationDetector_getNameString(long jarg1, BinaryRelationDetector jarg1_);
  public final static native double BinaryRelationDetector_classify(long jarg1, BinaryRelationDetector jarg1_, long jarg2, BinaryRelation jarg2_);
  public final static native void delete_BinaryRelationDetector(long jarg1);
  public final static native long new_NerTrainingInstance(long jarg1, StringVector jarg1_);
  public final static native void NerTrainingInstance_addEntity(long jarg1, NerTrainingInstance jarg1_, long jarg2, long jarg3, String jarg4);
  public final static native long NerTrainingInstance_getSize(long jarg1, NerTrainingInstance jarg1_);
  public final static native void delete_NerTrainingInstance(long jarg1);
  public final static native long new_NerTrainer(String jarg1);
  public final static native void NerTrainer_add(long jarg1, NerTrainer jarg1_, long jarg2, NerTrainingInstance jarg2_);
  public final static native void NerTrainer_setThreadNum(long jarg1, NerTrainer jarg1_, long jarg2);
  public final static native void NerTrainer_train(long jarg1, NerTrainer jarg1_, String jarg2);
  public final static native void NerTrainer_trainSeparateModels(long jarg1, NerTrainer jarg1_, String jarg2);
  public final static native void delete_NerTrainer(long jarg1);
  public final static native long new_TextCategorizer__SWIG_0(String jarg1);
  public final static native long new_TextCategorizer__SWIG_1(String jarg1, String jarg2);
  public final static native long TextCategorizer_getPossibleNerTags(long jarg1, TextCategorizer jarg1_);
  public final static native void TextCategorizer_saveToDisk(long jarg1, TextCategorizer jarg1_, String jarg2);
  public final static native long TextCategorizer_categorizeDoc(long jarg1, TextCategorizer jarg1_, long jarg2, StringVector jarg2_);
  public final static native void delete_TextCategorizer(long jarg1);
  public final static native long new_TextCategorizerTrainer(String jarg1);
  public final static native void TextCategorizerTrainer_add(long jarg1, TextCategorizerTrainer jarg1_, long jarg2, StringVector jarg2_, String jarg3);
  public final static native void TextCategorizerTrainer_setThreadNum(long jarg1, TextCategorizerTrainer jarg1_, long jarg2);
  public final static native void TextCategorizerTrainer_train(long jarg1, TextCategorizerTrainer jarg1_, String jarg2);
  public final static native void TextCategorizerTrainer_trainSeparateModels(long jarg1, TextCategorizerTrainer jarg1_, String jarg2);
  public final static native void delete_TextCategorizerTrainer(long jarg1);
}",mitie_lib/java_src/edu/mit/ll/mitie/globalJNI.java
TextCategorizer,"public class TextCategorizer {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected TextCategorizer(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(TextCategorizer obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_TextCategorizer(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public TextCategorizer(String filename) {
    this(globalJNI.new_TextCategorizer__SWIG_0(filename), true);
  }

  public TextCategorizer(String pureModelName, String extractorName) {
    this(globalJNI.new_TextCategorizer__SWIG_1(pureModelName, extractorName), true);
  }

  public StringVector getPossibleNerTags() {
    return new StringVector(globalJNI.TextCategorizer_getPossibleNerTags(swigCPtr, this), true);
  }

  public void saveToDisk(String filename) {
    globalJNI.TextCategorizer_saveToDisk(swigCPtr, this, filename);
  }

  public SDPair categorizeDoc(StringVector words) {
    return new SDPair(globalJNI.TextCategorizer_categorizeDoc(swigCPtr, this, StringVector.getCPtr(words), words), true);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/TextCategorizer.java
NamedEntityExtractor,"public class NamedEntityExtractor {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected NamedEntityExtractor(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(NamedEntityExtractor obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_NamedEntityExtractor(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public NamedEntityExtractor(String filename) {
    this(globalJNI.new_NamedEntityExtractor__SWIG_0(filename), true);
  }

  public NamedEntityExtractor(String pureModelName, String extractorName) {
    this(globalJNI.new_NamedEntityExtractor__SWIG_1(pureModelName, extractorName), true);
  }

  public StringVector getPossibleNerTags() {
    return new StringVector(globalJNI.NamedEntityExtractor_getPossibleNerTags(swigCPtr, this), true);
  }

  public void saveToDisk(String filename) {
    globalJNI.NamedEntityExtractor_saveToDisk(swigCPtr, this, filename);
  }

  public EntityMentionVector extractEntities(StringVector tokens) {
    return new EntityMentionVector(globalJNI.NamedEntityExtractor_extractEntities__SWIG_0(swigCPtr, this, StringVector.getCPtr(tokens), tokens), true);
  }

  public EntityMentionVector extractEntities(TokenIndexVector tokens) {
    return new EntityMentionVector(globalJNI.NamedEntityExtractor_extractEntities__SWIG_1(swigCPtr, this, TokenIndexVector.getCPtr(tokens), tokens), true);
  }

  public BinaryRelation extractBinaryRelation(StringVector tokens, EntityMention arg1, EntityMention arg2) {
    return new BinaryRelation(globalJNI.NamedEntityExtractor_extractBinaryRelation(swigCPtr, this, StringVector.getCPtr(tokens), tokens, EntityMention.getCPtr(arg1), arg1, EntityMention.getCPtr(arg2), arg2), true);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/NamedEntityExtractor.java
NerTrainer,"public class NerTrainer {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected NerTrainer(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(NerTrainer obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_NerTrainer(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public NerTrainer(String filename) {
    this(globalJNI.new_NerTrainer(filename), true);
  }

  public void add(NerTrainingInstance item) {
    globalJNI.NerTrainer_add(swigCPtr, this, NerTrainingInstance.getCPtr(item), item);
  }

  public void setThreadNum(long num) {
    globalJNI.NerTrainer_setThreadNum(swigCPtr, this, num);
  }

  public void train(String filename) {
    globalJNI.NerTrainer_train(swigCPtr, this, filename);
  }

  public void trainSeparateModels(String filename) {
    globalJNI.NerTrainer_trainSeparateModels(swigCPtr, this, filename);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/NerTrainer.java
TextCategorizerTrainer,"public class TextCategorizerTrainer {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected TextCategorizerTrainer(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(TextCategorizerTrainer obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_TextCategorizerTrainer(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public TextCategorizerTrainer(String filename) {
    this(globalJNI.new_TextCategorizerTrainer(filename), true);
  }

  public void add(StringVector words, String label) {
    globalJNI.TextCategorizerTrainer_add(swigCPtr, this, StringVector.getCPtr(words), words, label);
  }

  public void setThreadNum(long num) {
    globalJNI.TextCategorizerTrainer_setThreadNum(swigCPtr, this, num);
  }

  public void train(String filename) {
    globalJNI.TextCategorizerTrainer_train(swigCPtr, this, filename);
  }

  public void trainSeparateModels(String filename) {
    globalJNI.TextCategorizerTrainer_trainSeparateModels(swigCPtr, this, filename);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/TextCategorizerTrainer.java
SWIGTYPE_p_mitie__binary_relation,"public class SWIGTYPE_p_mitie__binary_relation {
  private transient long swigCPtr;

  protected SWIGTYPE_p_mitie__binary_relation(long cPtr, @SuppressWarnings(""unused"") boolean futureUse) {
    swigCPtr = cPtr;
  }

  protected SWIGTYPE_p_mitie__binary_relation() {
    swigCPtr = 0;
  }

  protected static long getCPtr(SWIGTYPE_p_mitie__binary_relation obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }
}",mitie_lib/java_src/edu/mit/ll/mitie/SWIGTYPE_p_mitie__binary_relation.java
StringVector,"public class StringVector {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected StringVector(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(StringVector obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_StringVector(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public StringVector() {
    this(globalJNI.new_StringVector__SWIG_0(), true);
  }

  public StringVector(long n) {
    this(globalJNI.new_StringVector__SWIG_1(n), true);
  }

  public long size() {
    return globalJNI.StringVector_size(swigCPtr, this);
  }

  public long capacity() {
    return globalJNI.StringVector_capacity(swigCPtr, this);
  }

  public void reserve(long n) {
    globalJNI.StringVector_reserve(swigCPtr, this, n);
  }

  public boolean isEmpty() {
    return globalJNI.StringVector_isEmpty(swigCPtr, this);
  }

  public void clear() {
    globalJNI.StringVector_clear(swigCPtr, this);
  }

  public void add(String x) {
    globalJNI.StringVector_add(swigCPtr, this, x);
  }

  public String get(int i) {
    return globalJNI.StringVector_get(swigCPtr, this, i);
  }

  public void set(int i, String val) {
    globalJNI.StringVector_set(swigCPtr, this, i, val);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/StringVector.java
SDPair,"public class SDPair {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected SDPair(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(SDPair obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_SDPair(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public SDPair() {
    this(globalJNI.new_SDPair__SWIG_0(), true);
  }

  public SDPair(String first, double second) {
    this(globalJNI.new_SDPair__SWIG_1(first, second), true);
  }

  public SDPair(SDPair p) {
    this(globalJNI.new_SDPair__SWIG_2(SDPair.getCPtr(p), p), true);
  }

  public void setFirst(String value) {
    globalJNI.SDPair_first_set(swigCPtr, this, value);
  }

  public String getFirst() {
    return globalJNI.SDPair_first_get(swigCPtr, this);
  }

  public void setSecond(double value) {
    globalJNI.SDPair_second_set(swigCPtr, this, value);
  }

  public double getSecond() {
    return globalJNI.SDPair_second_get(swigCPtr, this);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/SDPair.java
NerTrainingInstance,"public class NerTrainingInstance {
  private transient long swigCPtr;
  protected transient boolean swigCMemOwn;

  protected NerTrainingInstance(long cPtr, boolean cMemoryOwn) {
    swigCMemOwn = cMemoryOwn;
    swigCPtr = cPtr;
  }

  protected static long getCPtr(NerTrainingInstance obj) {
    return (obj == null) ? 0 : obj.swigCPtr;
  }

  protected void finalize() {
    delete();
  }

  public synchronized void delete() {
    if (swigCPtr != 0) {
      if (swigCMemOwn) {
        swigCMemOwn = false;
        globalJNI.delete_NerTrainingInstance(swigCPtr);
      }
      swigCPtr = 0;
    }
  }

  public NerTrainingInstance(StringVector tokens) {
    this(globalJNI.new_NerTrainingInstance(StringVector.getCPtr(tokens), tokens), true);
  }

  public void addEntity(long start, long length, String label) {
    globalJNI.NerTrainingInstance_addEntity(swigCPtr, this, start, length, label);
  }

  public long getSize() {
    return globalJNI.NerTrainingInstance_getSize(swigCPtr, this);
  }

}",mitie_lib/java_src/edu/mit/ll/mitie/NerTrainingInstance.java
CilinDictionary,"public class CilinDictionary {

	protected static Logger logger = Logger.getLogger(CilinDictionary.class);
	private static Map<String, Set<String>> wordIndex = new HashMap<String, Set<String>>();
	private static Map<String, Set<String>> codeIndex = new HashMap<String, Set<String>>();
	private static CilinDictionary instance;
	private static String path = ""/cilin.txt"";

	private CilinDictionary() {
		try {
			InputStreamReader read = new InputStreamReader(this.getClass().getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			while ((line = bufferedReader.readLine()) != null) {
				String[] items = line.split("" "");
				Set<String> set = new HashSet<String>();
				for (int i = 1; i < items.length; i++) {
					String code = items[i].trim();
					if (!code.equals("""")) {
						set.add(code);
						Set<String> codeWords = codeIndex.get(code);
						if (codeWords == null) {
							codeWords = new HashSet<String>();
						}
						codeWords.add(items[0]);
						codeIndex.put(code, codeWords);
					}
				}
				wordIndex.put(items[0], set);
			}
		} catch (Exception e) {
			logger.error(""error occurs when loading cilin ...."", e);
		}
	}

	public Set<String> getCilinCoding(String word) {
		return codeIndex.get(word);
	}

	public Set<String> getCilinWords(String code) {
		return wordIndex.get(code);
	}

	public static CilinDictionary getInstance() {
		if (instance == null) {
			synchronized (CilinDictionary.class) {
				instance = new CilinDictionary();
			}
		}
		return instance;
	}
}",src/main/java/com/seaboat/text/analyzer/cilin/CilinDictionary.java
Word2Vec,"public class Word2Vec {
	static {
		String _path = System.getProperty(""word2vec.path"");
		if (_path != null)
			path = _path;
		else
			throw new RuntimeException(""word2vec path is not set"");
	}

	private HashMap<String, float[]> wordMap = new HashMap<String, float[]>();
	private static final int MAX_SIZE = 50;
	private int words;
	private int size;
	private int topNSize = 40;
	private static Word2Vec instance = null;
	private static String path;
	private boolean isGoogleModel = true;

	private Word2Vec() {
	}

	private void init() {
		try {
			if (isGoogleModel)
				instance.loadGoogleModel(path);
			else
				instance.loadJavaModel(path);
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	public static Word2Vec getInstance(boolean isGoogleModel) {
		if (instance == null)
			synchronized (Word2Vec.class) {
				if (instance != null)
					return instance;
				instance = new Word2Vec();
				instance.isGoogleModel = isGoogleModel;
				instance.init();
			}
		return instance;
	}

	public void loadJavaModel(String path) throws IOException {
		try (DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(path)))) {
			words = dis.readInt();
			size = dis.readInt();
			float vector = 0;
			String key = null;
			float[] value = null;
			for (int i = 0; i < words; i++) {
				double len = 0;
				key = dis.readUTF();
				value = new float[size];
				for (int j = 0; j < size; j++) {
					vector = dis.readFloat();
					len += vector * vector;
					value[j] = vector;
				}

				len = Math.sqrt(len);
				for (int j = 0; j < size; j++) {
					value[j] /= len;
				}
				wordMap.put(key, value);
			}

		}
	}

	public void loadGoogleModel(String path) throws IOException {
		DataInputStream dis = null;
		BufferedInputStream bis = null;
		double len = 0;
		float vector = 0;
		try {
			bis = new BufferedInputStream(new FileInputStream(path));
			dis = new DataInputStream(bis);
			words = Integer.parseInt(readString(dis));
			size = Integer.parseInt(readString(dis));
			String word;
			float[] vectors = null;
			for (int i = 0; i < words; i++) {
				word = readString(dis);
				vectors = new float[size];
				len = 0;
				for (int j = 0; j < size; j++) {
					vector = readFloat(dis);
					len += vector * vector;
					vectors[j] = (float) vector;
				}
				len = Math.sqrt(len);
				for (int j = 0; j < size; j++) {
					vectors[j] /= len;
				}
				wordMap.put(word, vectors);
				dis.read();
			}
		} finally {
			bis.close();
			dis.close();
		}
	}

	public float[] getWordVector(String word) {
		return wordMap.get(word);
	}

	private float calDist(float[] vec1, float[] vec2) {
		float dist = 0;
		for (int i = 0; i < vec1.length; i++) {
			dist += vec1[i] * vec2[i];
		}
		return dist;
	}

	public float wordSimilarity(String word1, String word2) {
		float[] word1Vec = getWordVector(word1);
		float[] word2Vec = getWordVector(word2);
		if (word1Vec == null || word2Vec == null) {
			return 0;
		}
		return calDist(word1Vec, word2Vec);
	}

	public Set<WordEntry> getSimilarWords(String word, int maxReturnNum) {
		float[] center = getWordVector(word);
		if (center == null) {
			return Collections.emptySet();
		}
		int resultSize = getWords() < maxReturnNum ? getWords() : maxReturnNum;
		TreeSet<WordEntry> result = new TreeSet<WordEntry>();
		double min = Double.MIN_VALUE;
		for (Map.Entry<String, float[]> entry : getWordMap().entrySet()) {
			float[] vector = entry.getValue();
			float dist = calDist(center, vector);
			if (result.size() <= resultSize) {
				result.add(new WordEntry(entry.getKey(), dist));
				min = result.last().score;
			} else {
				if (dist > min) {
					result.add(new WordEntry(entry.getKey(), dist));
					result.pollLast();
					min = result.last().score;
				}
			}
		}
		result.pollFirst();
		return result;
	}

	private float calMaxSimilarity(String centerWord, List<String> wordList) {
		float max = -1;
		if (wordList.contains(centerWord)) {
			return 1;
		} else {
			for (String word : wordList) {
				float temp = wordSimilarity(centerWord, word);
				if (temp == 0)
					continue;
				if (temp > max) {
					max = temp;
				}
			}
		}
		if (max == -1)
			return 0;
		return max;
	}

	public float sentenceSimilarity(List<String> sentence1Words, List<String> sentence2Words) {
		if (sentence1Words.isEmpty() || sentence2Words.isEmpty()) {
			return 0;
		}
		float[] vector1 = new float[sentence1Words.size()];
		float[] vector2 = new float[sentence2Words.size()];
		for (int i = 0; i < vector1.length; i++) {
			vector1[i] = calMaxSimilarity(sentence1Words.get(i), sentence2Words);
		}
		for (int i = 0; i < vector2.length; i++) {
			vector2[i] = calMaxSimilarity(sentence2Words.get(i), sentence1Words);
		}
		float sum1 = 0;
		for (int i = 0; i < vector1.length; i++) {
			sum1 += vector1[i];
		}
		float sum2 = 0;
		for (int i = 0; i < vector2.length; i++) {
			sum2 += vector2[i];
		}
		return (sum1 + sum2) / (sentence1Words.size() + sentence2Words.size());
	}

	private static String readString(DataInputStream dis) throws IOException {
		byte[] bytes = new byte[MAX_SIZE];
		byte b = dis.readByte();
		int i = -1;
		StringBuilder sb = new StringBuilder();
		while (b != 32 && b != 10) {
			i++;
			bytes[i] = b;
			b = dis.readByte();
			if (i == 49) {
				sb.append(new String(bytes));
				i = -1;
				bytes = new byte[MAX_SIZE];
			}
		}
		sb.append(new String(bytes, 0, i + 1));
		return sb.toString();
	}

	public static float readFloat(InputStream is) throws IOException {
		byte[] bytes = new byte[4];
		is.read(bytes);
		return getFloat(bytes);
	}

	public static float getFloat(byte[] b) {
		int accum = 0;
		accum = accum | (b[0] & 0xff) << 0;
		accum = accum | (b[1] & 0xff) << 8;
		accum = accum | (b[2] & 0xff) << 16;
		accum = accum | (b[3] & 0xff) << 24;
		return Float.intBitsToFloat(accum);
	}

	public int getTopNSize() {
		return topNSize;
	}

	public void setTopNSize(int topNSize) {
		this.topNSize = topNSize;
	}

	public HashMap<String, float[]> getWordMap() {
		return wordMap;
	}

	public int getWords() {
		return words;
	}

	public int getSize() {
		return size;
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/Word2Vec.java
MapCount,"public class MapCount<T> {
	private HashMap<T, Integer> hm = null;

	public MapCount() {
		this.hm = new HashMap<T, Integer>();
	}

	public MapCount(int initialCapacity) {
		this.hm = new HashMap<T, Integer>(initialCapacity);
	}

	public void add(T t, int n) {
		Integer integer = null;
		if ((integer = (Integer) this.hm.get(t)) != null) {
			this.hm.put(t, Integer.valueOf(integer.intValue() + n));
		} else {
			this.hm.put(t, Integer.valueOf(n));
		}

	}

	public void add(T t) {
		this.add(t, 1);
	}

	public int size() {
		return this.hm.size();
	}

	public void remove(T t) {
		this.hm.remove(t);
	}

	public HashMap<T, Integer> get() {
		return this.hm;
	}

	public String getDic() {
		Iterator<?> iterator = this.hm.entrySet().iterator();
		StringBuilder sb = new StringBuilder();
		Entry<?, ?> next = null;

		while (iterator.hasNext()) {
			next = (Entry<?, ?>) iterator.next();
			sb.append(next.getKey());
			sb.append(""\t"");
			sb.append(next.getValue());
			sb.append(""\n"");
		}

		return sb.toString();
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/MapCount.java
WordEntry,"public class WordEntry implements Comparable<WordEntry> {
	public String name;
	public float score;

	public WordEntry(String name, float score) {
		this.name = name;
		this.score = score;
	}

	public String toString() {
		return this.name + ""\t"" + score;
	}

	public int compareTo(WordEntry o) {
		if (this.score < o.score) {
			return 1;
		} else {
			return -1;
		}
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/WordEntry.java
WordNeuron,"public class WordNeuron extends Neuron {
	public String name;
	public double[] syn0 = null;
	public List<Neuron> neurons = null;
	public int[] codeArr = null;

	public List<Neuron> makeNeurons() {
		if (neurons != null) {
			return neurons;
		}
		Neuron neuron = this;
		neurons = new LinkedList<>();
		while ((neuron = neuron.parent) != null) {
			neurons.add(neuron);
		}
		Collections.reverse(neurons);
		codeArr = new int[neurons.size()];

		for (int i = 1; i < neurons.size(); i++) {
			codeArr[i - 1] = neurons.get(i).code;
		}
		codeArr[codeArr.length - 1] = this.code;

		return neurons;
	}

	public WordNeuron(String name, double freq, int layerSize) {
		this.name = name;
		this.freq = freq;
		this.syn0 = new double[layerSize];
		Random random = new Random();
		for (int i = 0; i < syn0.length; i++) {
			syn0[i] = (random.nextDouble() - 0.5) / layerSize;
		}
	}

	public WordNeuron(String name, double freq, int category, int layerSize) {
		this.name = name;
		this.freq = freq;
		this.syn0 = new double[layerSize];
		this.category = category;
		Random random = new Random();
		for (int i = 0; i < syn0.length; i++) {
			syn0[i] = (random.nextDouble() - 0.5) / layerSize;
		}
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/WordNeuron.java
Word2VecTrainer,"public class Word2VecTrainer {

	private Map<String, Neuron> wordMap = new HashMap<>();

	private int layerSize = 200;
	private int window = 5;
	private double sample = 1e-3;
	private double alpha = 0.025;
	private double startingAlpha = alpha;
	public int EXP_TABLE_SIZE = 1000;
	private Boolean isCbow = false;
	private double[] expTable = new double[EXP_TABLE_SIZE];
	private int trainWordsCount = 0;
	private int MAX_EXP = 6;

	public Word2VecTrainer(Boolean isCbow, Integer layerSize, Integer window, Double alpha, Double sample) {
		createExpTable();
		if (isCbow != null) {
			this.isCbow = isCbow;
		}
		if (layerSize != null)
			this.layerSize = layerSize;
		if (window != null)
			this.window = window;
		if (alpha != null)
			this.alpha = alpha;
		if (sample != null)
			this.sample = sample;
	}

	public Word2VecTrainer() {
		createExpTable();
	}

	private void trainModel(File file) throws IOException {
		try (BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
			String temp = null;
			long nextRandom = 5;
			int wordCount = 0;
			int lastWordCount = 0;
			int wordCountActual = 0;
			while ((temp = br.readLine()) != null) {
				if (wordCount - lastWordCount > 10000) {
					System.out.println(""alpha:"" + alpha + ""\tProgress: ""
							+ (int) (wordCountActual / (double) (trainWordsCount + 1) * 100) + ""%"");
					wordCountActual += wordCount - lastWordCount;
					lastWordCount = wordCount;
					alpha = startingAlpha * (1 - wordCountActual / (double) (trainWordsCount + 1));
					if (alpha < startingAlpha * 0.0001) {
						alpha = startingAlpha * 0.0001;
					}
				}
				String[] strs = temp.split("" "");
				wordCount += strs.length;
				List<WordNeuron> sentence = new ArrayList<WordNeuron>();
				for (int i = 0; i < strs.length; i++) {
					Neuron entry = wordMap.get(strs[i]);
					if (entry == null) {
						continue;
					}
					// The subsampling randomly discards frequent words while keeping the
					// ranking same
					if (sample > 0) {
						double ran = (Math.sqrt(entry.freq / (sample * trainWordsCount)) + 1)
								* (sample * trainWordsCount) / entry.freq;
						nextRandom = nextRandom * 25214903917L + 11;
						if (ran < (nextRandom & 0xFFFF) / (double) 65536) {
							continue;
						}
					}
					sentence.add((WordNeuron) entry);
				}

				for (int index = 0; index < sentence.size(); index++) {
					nextRandom = nextRandom * 25214903917L + 11;
					if (isCbow) {
						cbowGram(index, sentence, (int) nextRandom % window);
					} else {
						skipGram(index, sentence, (int) nextRandom % window);
					}
				}

			}
			System.out.println(""Vocab size: "" + wordMap.size());
			System.out.println(""Words in train file: "" + trainWordsCount);
			System.out.println(""sucess train over!"");
		}
	}

	private void skipGram(int index, List<WordNeuron> sentence, int b) {
		WordNeuron word = sentence.get(index);
		int a, c = 0;
		for (a = b; a < window * 2 + 1 - b; a++) {
			if (a == window) {
				continue;
			}
			c = index - window + a;
			if (c < 0 || c >= sentence.size()) {
				continue;
			}

			double[] neu1e = new double[layerSize];
			// HIERARCHICAL SOFTMAX
			List<Neuron> neurons = word.neurons;
			WordNeuron we = sentence.get(c);
			for (int i = 0; i < neurons.size(); i++) {
				HiddenNeuron out = (HiddenNeuron) neurons.get(i);
				double f = 0;
				// Propagate hidden -> output
				for (int j = 0; j < layerSize; j++) {
					f += we.syn0[j] * out.syn1[j];
				}
				if (f <= -MAX_EXP || f >= MAX_EXP) {
					continue;
				} else {
					f = (f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2);
					f = expTable[(int) f];
				}
				// 'g' is the gradient multiplied by the learning rate
				double g = (1 - word.codeArr[i] - f) * alpha;
				// Propagate errors output -> hidden
				for (c = 0; c < layerSize; c++) {
					neu1e[c] += g * out.syn1[c];
				}
				// Learn weights hidden -> output
				for (c = 0; c < layerSize; c++) {
					out.syn1[c] += g * we.syn0[c];
				}
			}

			// Learn weights input -> hidden
			for (int j = 0; j < layerSize; j++) {
				we.syn0[j] += neu1e[j];
			}
		}

	}

	private void cbowGram(int index, List<WordNeuron> sentence, int b) {
		WordNeuron word = sentence.get(index);
		int a, c = 0;

		List<Neuron> neurons = word.neurons;
		double[] neu1e = new double[layerSize];
		double[] neu1 = new double[layerSize];
		WordNeuron last_word;

		for (a = b; a < window * 2 + 1 - b; a++)
			if (a != window) {
				c = index - window + a;
				if (c < 0)
					continue;
				if (c >= sentence.size())
					continue;
				last_word = sentence.get(c);
				if (last_word == null)
					continue;
				for (c = 0; c < layerSize; c++)
					neu1[c] += last_word.syn0[c];
			}

		// HIERARCHICAL SOFTMAX
		for (int d = 0; d < neurons.size(); d++) {
			HiddenNeuron out = (HiddenNeuron) neurons.get(d);
			double f = 0;
			// Propagate hidden -> output
			for (c = 0; c < layerSize; c++)
				f += neu1[c] * out.syn1[c];
			if (f <= -MAX_EXP)
				continue;
			else if (f >= MAX_EXP)
				continue;
			else
				f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
			// 'g' is the gradient multiplied by the learning rate
			// double g = (1 - word.codeArr[d] - f) * alpha;
			// double g = f*(1-f)*( word.codeArr[i] - f) * alpha;
			double g = f * (1 - f) * (word.codeArr[d] - f) * alpha;
			//
			for (c = 0; c < layerSize; c++) {
				neu1e[c] += g * out.syn1[c];
			}
			// Learn weights hidden -> output
			for (c = 0; c < layerSize; c++) {
				out.syn1[c] += g * neu1[c];
			}
		}
		for (a = b; a < window * 2 + 1 - b; a++) {
			if (a != window) {
				c = index - window + a;
				if (c < 0)
					continue;
				if (c >= sentence.size())
					continue;
				last_word = sentence.get(c);
				if (last_word == null)
					continue;
				for (c = 0; c < layerSize; c++)
					last_word.syn0[c] += neu1e[c];
			}

		}
	}

	private void readVocab(File file) throws IOException {
		MapCount<String> mc = new MapCount<>();
		try (BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file)))) {
			String temp = null;
			while ((temp = br.readLine()) != null) {
				String[] split = temp.split("" "");
				trainWordsCount += split.length;
				for (String string : split) {
					mc.add(string);
				}
			}
		}
		for (Entry<String, Integer> element : mc.get().entrySet()) {
			wordMap.put(element.getKey(),
					new WordNeuron(element.getKey(), (double) element.getValue() / mc.size(), layerSize));
		}
	}

	private void readVocabWithSupervised(File[] files) throws IOException {
		for (int category = 0; category < files.length; category++) {
			MapCount<String> mc = new MapCount<>();
			try (BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(files[category])))) {
				String temp = null;
				while ((temp = br.readLine()) != null) {
					String[] split = temp.split("" "");
					trainWordsCount += split.length;
					for (String string : split) {
						mc.add(string);
					}
				}
			}
			for (Entry<String, Integer> element : mc.get().entrySet()) {
				double tarFreq = (double) element.getValue() / mc.size();
				if (wordMap.get(element.getKey()) != null) {
					double srcFreq = wordMap.get(element.getKey()).freq;
					if (srcFreq >= tarFreq) {
						continue;
					} else {
						Neuron wordNeuron = wordMap.get(element.getKey());
						wordNeuron.category = category;
						wordNeuron.freq = tarFreq;
					}
				} else {
					wordMap.put(element.getKey(), new WordNeuron(element.getKey(), tarFreq, category, layerSize));
				}
			}
		}
	}

	private void createExpTable() {
		for (int i = 0; i < EXP_TABLE_SIZE; i++) {
			expTable[i] = Math.exp(((i / (double) EXP_TABLE_SIZE * 2 - 1) * MAX_EXP));
			expTable[i] = expTable[i] / (expTable[i] + 1);
		}
	}

	public void learnFile(File file) throws IOException {
		readVocab(file);
		new Haffman(layerSize).make(wordMap.values());
		for (Neuron neuron : wordMap.values()) {
			((WordNeuron) neuron).makeNeurons();
		}

		trainModel(file);
	}

	public void learnFile(File summaryFile, File[] classifiedFiles) throws IOException {
		readVocabWithSupervised(classifiedFiles);
		new Haffman(layerSize).make(wordMap.values());
		for (Neuron neuron : wordMap.values()) {
			((WordNeuron) neuron).makeNeurons();
		}
		trainModel(summaryFile);
	}

	public void saveModel(File file) {

		try (DataOutputStream dataOutputStream = new DataOutputStream(
				new BufferedOutputStream(new FileOutputStream(file)))) {
			dataOutputStream.writeInt(wordMap.size());
			dataOutputStream.writeInt(layerSize);
			double[] syn0 = null;
			for (Entry<String, Neuron> element : wordMap.entrySet()) {
				dataOutputStream.writeUTF(element.getKey());
				syn0 = ((WordNeuron) element.getValue()).syn0;
				for (double d : syn0) {
					dataOutputStream.writeFloat(((Double) d).floatValue());
				}
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	public int getLayerSize() {
		return layerSize;
	}

	public void setLayerSize(int layerSize) {
		this.layerSize = layerSize;
	}

	public int getWindow() {
		return window;
	}

	public void setWindow(int window) {
		this.window = window;
	}

	public double getSample() {
		return sample;
	}

	public void setSample(double sample) {
		this.sample = sample;
	}

	public double getAlpha() {
		return alpha;
	}

	public void setAlpha(double alpha) {
		this.alpha = alpha;
		this.startingAlpha = alpha;
	}

	public Boolean getIsCbow() {
		return isCbow;
	}

	public void setIsCbow(Boolean isCbow) {
		this.isCbow = isCbow;
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/Word2VecTrainer.java
Neuron,"public abstract class Neuron implements Comparable<Neuron> {
	public double freq;
	public Neuron parent;
	public int code;
	public int category = -1;

	@Override
	public int compareTo(Neuron neuron) {
		if (this.category == neuron.category) {
			if (this.freq > neuron.freq) {
				return 1;
			} else {
				return -1;
			}
		} else if (this.category > neuron.category) {
			return 1;
		} else {
			return -1;
		}
	}
}",src/main/java/com/seaboat/text/analyzer/word2vec/Neuron.java
Haffman,"public class Haffman {
	private int layerSize;

	public Haffman(int layerSize) {
		this.layerSize = layerSize;
	}

	private TreeSet<Neuron> set = new TreeSet<>();

	public void make(Collection<Neuron> neurons) {
		set.addAll(neurons);
		while (set.size() > 1) {
			merger();
		}
	}

	private void merger() {
		HiddenNeuron hn = new HiddenNeuron(layerSize);
		Neuron min1 = set.pollFirst();
		Neuron min2 = set.pollFirst();
		hn.category = min2.category;
		hn.freq = min1.freq + min2.freq;
		min1.parent = hn;
		min2.parent = hn;
		min1.code = 0;
		min2.code = 1;
		set.add(hn);
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/Haffman.java
HiddenNeuron,"public class HiddenNeuron extends Neuron {

	public double[] syn1;

	public HiddenNeuron(int layerSize) {
		syn1 = new double[layerSize];
	}

}",src/main/java/com/seaboat/text/analyzer/word2vec/HiddenNeuron.java
HotWordExtractor,"public class HotWordExtractor implements IHotWordExtractor {

	protected static Logger logger = Logger.getLogger(HotWordExtractor.class);

	private IDF idf;

	private ScoreFactor factor;

	public HotWordExtractor() {
		this(new LuceneIDF(), new WordPopularityScore());
	}

	public HotWordExtractor(IDF idf, ScoreFactor factor) {
		this.idf = idf;
		this.factor = factor;
	}

	public List<Result> extract(int id, int topN) {
		return extract(id, topN, false);
	}

	@Override
	public List<Result> extract(int id) {
		return extract(id, 10);
	}

	@Override
	public List<Result> extract(int id, int topN, boolean useScore) {
		List<Result> list = new LinkedList<Result>();
		try {
			IndexReader reader = IndexUtil.getIndexReader();
			Terms terms = reader.getTermVector(id, ""content"");
			TermsEnum termsEnum = terms.iterator();
			BytesRef thisTerm = null;
			while ((thisTerm = termsEnum.next()) != null) {
				String term = thisTerm.utf8ToString();
				if ((term.length() > 1) && (!StringUtil.isNumericAndLetter(term)) && (!StringUtil.isMobile(term))
						&& (!StringUtil.isPhone(term)) && (!StringUtil.isContainNumber(term))
						&& (!StringUtil.isDate(term))) {
					float idfn = idf.getIDF(term);
					float scoreFactor = factor.getScoreFactor(term);
					int a = (int) termsEnum.totalTermFreq();
					long b = terms.size();
					float tf = (float) a / b;
					float score = idfn * tf + scoreFactor;
					list.add(new Result(term, (int) termsEnum.totalTermFreq(), score));
				}
			}
			// match the synonym
			List<Result> toRemove = new LinkedList<Result>();
			for (Result result : list) {
				Collection<String> synonyms;
				if ((synonyms = SynonymUtil.get(null).getSynonym(result.getTerm())) != null)
					for (Result r : list) {
						if (synonyms.contains(r.getTerm())) {
							r.setFrequency(r.getFrequency() + result.getFrequency());
							r.setScore(r.getScore() + result.getScore());
							toRemove.add(result);
						}
					}
			}
			for (Result r : toRemove)
				list.remove(r);
			if (useScore) {
				Collections.sort(list, new Comparator<Result>() {
					@Override
					public int compare(Result o1, Result o2) {
						Float f2 = Float.parseFloat(String.valueOf(o2.getScore()));
						Float f1 = Float.parseFloat(String.valueOf(o1.getScore()));
						return f2.compareTo(f1);
					}
				});
			} else {
				Collections.sort(list, new Comparator<Result>() {
					@Override
					public int compare(Result o1, Result o2) {
						return (o2.getFrequency() - o1.getFrequency());
					}
				});
			}
			if (list.size() > topN)
				return list.subList(0, topN);
			return list;
		} catch (IOException e) {
			logger.error(""IOException when getting reader. "", e);
			e.printStackTrace();
		}
		return null;
	}

	@Override
	public List<String> extract(String text) {
		// TODO Auto-generated method stub
		return null;
	}
}",src/main/java/com/seaboat/text/analyzer/hotword/HotWordExtractor.java
TextIndexer,"public class TextIndexer {

  protected static Logger logger = Logger.getLogger(TextIndexer.class);

  // only allow single thread
  public static synchronized long index(String text) {
    IndexWriter indexWriter = null;
    IndexReader reader = null;
    try {
      indexWriter = IndexUtil.getIndexWriter();
      reader = IndexUtil.getIndexReader();
    } catch (IOException e) {
      logger.error(""IOException when getting index writer. "", e);
    }
    FieldType type = new FieldType();
    type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
    type.setStored(true);
    type.setStoreTermVectors(true);
    type.setTokenized(true);
    Document doc = new Document();
    Field field = new Field(""content"", text, type);
    doc.add(field);
    try {
      indexWriter.addDocument(doc);
      indexWriter.commit();
      return reader.maxDoc();
    } catch (IOException e) {
      logger.error(""IOException when adding document. "", e);;
    }
    return -1;
  }

}",src/main/java/com/seaboat/text/analyzer/hotword/TextIndexer.java
LuceneMemoryIDF,"public class LuceneMemoryIDF implements IDF {

  @Override
  public float getIDF(String term) {
    try {
      IndexReader reader = MemoryIndexUtil.getIndexReader();
      float a = reader.numDocs() + 1;
      float b = reader.docFreq(new Term(""content"", term)) + 1;
      float idf = (float) Math.log(a / b);
      return idf;
    } catch (IOException e) {
      e.printStackTrace();
    }
    return 0;
  }

}",src/main/java/com/seaboat/text/analyzer/hotword/LuceneMemoryIDF.java
NormalIDF,"public class NormalIDF implements IDF{

  @Override
  public float getIDF(String term) {
    // TODO Auto-generated method stub
    return 0;
  }

}",src/main/java/com/seaboat/text/analyzer/hotword/NormalIDF.java
Synonym,"public class Synonym {

  private List<String> synonymList;

  public Synonym() {
    this.synonymList = new LinkedList<String>();
  }

  public void addSnonoym(String term) {
    this.synonymList.add(term);
  }

  public String getSynonym(String term) {
    for (int i = 1; i < synonymList.size(); i++) {
      if (synonymList.get(i).equals(term)) return synonymList.get(0);
    }
    return null;
  }
}",src/main/java/com/seaboat/text/analyzer/hotword/Synonym.java
Result,"public class Result {

  private float score;
  
  private int frequency;
  
  private String term;

  public Result(String term, int frequency, float score) {
   this.term=term;
   this.frequency = frequency;
   this.score = score;
  }

  public float getScore() {
    return score;
  }

  public void setScore(float score) {
    this.score = score;
  }

  public int getFrequency() {
    return frequency;
  }

  public void setFrequency(int frequency) {
    this.frequency = frequency;
  }

  public String getTerm() {
    return term;
  }

  public void setTerm(String term) {
    this.term = term;
  }
  
  
  
}",src/main/java/com/seaboat/text/analyzer/hotword/Result.java
WordPopularityScore,"public class WordPopularityScore implements ScoreFactor {
	protected static Logger logger = Logger.getLogger(WordPopularityScore.class);
	static Properties prop = new Properties();
	static {
		String path = System.getProperty(""user.dir"");
		InputStreamReader in;
		try {
			InputStream is = new FileInputStream(path + ""/resources/word-popularity.properties"");
			in = new InputStreamReader(is, ""UTF-8"");
			prop.load(in);
		} catch (Exception e) {
			try {
				InputStream is = new FileInputStream(""word-popularity.properties"");
				in = new InputStreamReader(is, ""UTF-8"");
				prop.load(in);
			} catch (Exception e1) {
				logger.error(e1);
			}
		}
	}

	@Override
	public float getScoreFactor(String term) {
		Object o = prop.get(term);
		if (o != null)
			return Float.parseFloat(String.valueOf(o));
		return 0;
	}

}",src/main/java/com/seaboat/text/analyzer/hotword/WordPopularityScore.java
LuceneIDF,"public class LuceneIDF implements IDF {

  @Override
  public float getIDF(String term) {
    try {
      IndexReader reader = IndexUtil.getIndexReader();
      float a = reader.numDocs() + 1;
      float b = reader.docFreq(new Term(""content"", term)) + 1;
      float idf = (float) Math.log(a / b);
      return idf;
    } catch (IOException e) {
      e.printStackTrace();
    }
    return 0;
  }

}",src/main/java/com/seaboat/text/analyzer/hotword/LuceneIDF.java
WordSegmentUtil,"public class WordSegmentUtil {

	public static List<String> seg(String text) {
		List<Term> termList = ToAnalysis.parse(text).getTerms();
		List<String> wordList = new ArrayList<String>();
		for (Term wordTerm : termList) {
			wordList.add(wordTerm.getName());
		}
		return wordList;
	}

	public static List<Term> segment(String text) {
		List<Term> termList = ToAnalysis.parse(text).getTerms();
		return termList;
	}
	
}",src/main/java/com/seaboat/text/analyzer/util/WordSegmentUtil.java
Segment,"public class Segment {

	public static List<String> getWords(String sentence) {
		List<Term> termList = ToAnalysis.parse(sentence).getTerms();
		List<String> wordList = new ArrayList<String>();
		for (Term wordTerm : termList) {
			wordList.add(wordTerm.getName());
		}
		return wordList;
	}
}",src/main/java/com/seaboat/text/analyzer/util/Segment.java
SynonymUtil,"public class SynonymUtil {
	protected static Logger logger = Logger.getLogger(SynonymUtil.class);
	private Multimap<String, String> multimap = ArrayListMultimap.create();
	private static SynonymUtil instance = null;

	private SynonymUtil(String path) {
		if (path == null)
			path = System.getProperty(""user.dir"") + ""/resources/synonym.dic"";
		FileInputStream in;
		try {
			in = new FileInputStream(path);
			BufferedReader bufferedReader = new BufferedReader(new UnicodeReader(in, ""utf-8""));
			String line;
			while ((line = bufferedReader.readLine()) != null) {
				String[] ss = line.split("";"");
				multimap.put(ss[0], ss[1]);
				multimap.put(ss[1], ss[0]);
			}
			bufferedReader.close();
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
	}

	public static SynonymUtil get(String path) {
		if (instance != null)
			return instance;
		synchronized (SynonymUtil.class) {
			if (instance == null)
				instance = new SynonymUtil(path);
			return instance;
		}
	}

	public Collection<String> getSynonym(String term) {
		return multimap.get(term);
	}

}",src/main/java/com/seaboat/text/analyzer/util/SynonymUtil.java
ClearDataUtil,"public class ClearDataUtil {

	public static void main(String[] args) {
		System.out.println(""Start... "");
		String pathname = ""D:\\TextAnalyzer\\src\\main\\resources\\synonym.dic"";
		String newPath = ""D:\\TextAnalyzer\\src\\main\\resources\\1.dic"";
		try {
			File file = new File(pathname);
			BufferedInputStream fis = new BufferedInputStream(new FileInputStream(file));
			BufferedReader buffer = new BufferedReader(new InputStreamReader(fis, ""utf-8""), 20 * 1024 * 1024);
			OutputStreamWriter out = new OutputStreamWriter(new FileOutputStream(new File(newPath)), ""utf-8"");

			Set<String> set = new HashSet<String>();
			String temp = """";
			int x = 0;
			while ((temp = buffer.readLine()) != null) {
				set.add(temp);
				if (x % 30000 == 0) {
					System.out.print("".."");
				}
				x++;
			}
			fis.close();
			buffer.close();
			for (String xxser : set) {
				out.write(xxser + ""\r\n"");

			}
			System.out.println("""");
			out.close();
			System.out.println(""size = "" + set.size());
			System.out.println(""End..."");
		} catch (Exception e) {
			System.out.println("",100MB.."");
		}
	}

}",src/main/java/com/seaboat/text/analyzer/util/ClearDataUtil.java
DataReader,"public class DataReader {

  public static List<String> readContent(String file) {
    List<String> lines = new ArrayList<String>();
    BufferedReader bufferedReader;
    try {
      bufferedReader = new BufferedReader(new FileReader(file));
      String line;
      while ((line = bufferedReader.readLine()) != null) {
        lines.add(line);
      }
      bufferedReader.close();
    } catch (FileNotFoundException e) {
      e.printStackTrace();
    } catch (IOException e) {
      e.printStackTrace();
    }
    return lines;
  }

}",src/main/java/com/seaboat/text/analyzer/util/DataReader.java
MemoryIndexUtil,"public class MemoryIndexUtil {

	private static IndexReader indexReader;

	private static IndexWriter indexWriter;

	private static IndexSearcher indexSearcher;

	private static Directory directory = new RAMDirectory();;

	protected static Logger logger = Logger.getLogger(MemoryIndexUtil.class);

	public static IndexReader getIndexReader() throws IOException {
		if (null != indexReader) {
			return indexReader;
		} else {
			if (directory == null)
				throw new IOException(""directory is null."");
			indexReader = DirectoryReader.open(directory);
			return indexReader;
		}
	}

	public static synchronized IndexWriter getIndexWriter() throws IOException {
		if (null != indexWriter) {
			return indexWriter;
		} else {
			Analyzer analyzer = new AnsjAnalyzer(TYPE.query_ansj);
			IndexWriterConfig config = new IndexWriterConfig(analyzer);
			config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
			indexWriter = new IndexWriter(directory, config);
			return indexWriter;
		}
	}

	public static IndexSearcher getIndexSearcher() throws IOException {

		if (null != indexSearcher) {
			return indexSearcher;
		} else {
			synchronized (MemoryIndexUtil.class) {
				indexSearcher = new IndexSearcher(getIndexReader());
			}
			return indexSearcher;
		}
	}

	public static List<String> getToken(String text) {
		List<Term> termList = ToAnalysis.parse(text).getTerms();
		List<String> wordList = new ArrayList<String>();
		for (Term wordTerm : termList) {
			wordList.add(wordTerm.getName());
		}
		return wordList;
	}

}",src/main/java/com/seaboat/text/analyzer/util/MemoryIndexUtil.java
StringUtil,"public class StringUtil {

  protected static Logger logger = Logger.getLogger(StringUtil.class);

  public static boolean isNumericAndLetter(String str) {
    if (str.matches(""^[a-zA-Z0-9]+$"")) return true;
    return false;
  }

  public static boolean isContainNumber(String str) {
    Pattern p = Pattern.compile("".*\\d+.*"");
    Matcher m = p.matcher(str);
    if (m.matches()) return true;
    return false;
  }

  public static boolean isMobile(String str) {
    Pattern p = null;
    Matcher m = null;
    boolean b = false;
    p = Pattern.compile(""^[1][3,4,5,8][0-9]{9}$"");
    m = p.matcher(str);
    b = m.matches();
    return b;
  }

  public static boolean isPhone(String str) {
    Pattern p1 = null, p2 = null;
    Matcher m = null;
    boolean b = false;
    p1 = Pattern.compile(""^[0][1-9]{2,3}-[0-9]{5,10}$"");
    p2 = Pattern.compile(""^[1-9]{1}[0-9]{5,8}$"");
    if (str.length() > 9) {
      m = p1.matcher(str);
      b = m.matches();
    } else {
      m = p2.matcher(str);
      b = m.matches();
    }
    return b;
  }

  public static String[] FILTER_1 = {"""", """", """", """", """", """", """", """", """", """", """", """",
      """", """", """", """", """", """", """", """", """", """", """", """", """", """",
      """", """", """", """", """"};;
  public static String[] FILTER_2 = {"""", """", """"};

  public static boolean isDate(String str) {
    for (String s1 : FILTER_1) {
      if (str.indexOf(s1) != -1) {
        for (String s2 : FILTER_2) {
          if (str.indexOf(s2) != -1) return true;
        }
      }
    }
    return false;
  }

  public static String[] FILTER_WORDS = {"""", """", """", """", """", """", """", """"};

  public static boolean isFilterWord(String term) {
    for (String s : FILTER_WORDS)
      if (s.equals(term)) return true;
    return false;
  }

  public static boolean hasDigit(String content) {
    boolean flag = false;
    Pattern p = Pattern.compile("".*\\d+.*"");
    Matcher m = p.matcher(content);
    if (m.matches()) flag = true;
    return flag;
  }
}",src/main/java/com/seaboat/text/analyzer/util/StringUtil.java
VectorUtil,"public class VectorUtil {

  protected static Logger logger = Logger.getLogger(VectorUtil.class);

  public static List<String> getVectorDimension(List<String> texts) {
    int docNum = 0;
    IndexWriter indexWriter = null;
    try {
      indexWriter = MemoryIndexUtil.getIndexWriter();
    } catch (IOException e) {
      logger.error(""IOException when getting index writer. "", e);
    }
    for (String line : texts) {
      FieldType type = new FieldType();
      type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
      type.setStored(true);
      type.setStoreTermVectors(true);
      type.setTokenized(true);
      Document doc = new Document();
      Field field = new Field(""content"", line, type);
      doc.add(field);
      try {
        docNum++;
        indexWriter.addDocument(doc);
        indexWriter.commit();
      } catch (IOException e) {
        logger.error(""IOException when adding document. "", e);
      }
    }
    // get a whole term vector
    IndexReader reader = null;
    Set<String> vector = null;
    try {
      reader = MemoryIndexUtil.getIndexReader();
      vector = new HashSet<String>();
      for (int docId = 0; docId < docNum; docId++) {
        Terms terms = reader.getTermVector(docId, ""content"");
        TermsEnum termsEnum = terms.iterator();
        BytesRef thisTerm = null;
        while ((thisTerm = termsEnum.next()) != null) {
          String term = thisTerm.utf8ToString();
          if ((term.length() > 1) && (!StringUtil.isNumericAndLetter(term))
              && (!StringUtil.isMobile(term)) && (!StringUtil.isPhone(term))
              && (!StringUtil.isContainNumber(term)) && (!StringUtil.isDate(term)))
            vector.add(term);
        }
      }
    } catch (IOException e) {
      logger.error(""IOException happens "", e);
    }
    List<String> vectorList = new ArrayList<String>(vector);
    return vectorList;
  }

  public static double[][] getVector(int row, List<String> vectorList, IDF idf) {
    Map<String, Double> tfidf = new HashMap<String, Double>();
    // calculating the vector of samples
    double[][] samples = new double[row][vectorList.size()];
    IndexReader reader = null;
    try {
      reader = MemoryIndexUtil.getIndexReader();
      for (int i = 0; i < row; i++) {
        try {
          int j = 0;
          for (int m = 0; m < vectorList.size(); m++) {
            String vTerm = vectorList.get(m);
            Terms terms = reader.getTermVector(i, ""content"");
            TermsEnum termsEnum = terms.iterator();
            BytesRef thisTerm = null;
            boolean isContain = false;
            while ((thisTerm = termsEnum.next()) != null) {
              String term = thisTerm.utf8ToString();
              if (term.equals(vTerm)) {
                float idfn = idf.getIDF(term);
                int a = (int) termsEnum.totalTermFreq();
                long b = terms.size();
                float tf = (float) a / b;
                samples[i][j] = idfn * tf;
                tfidf.put(term, samples[i][j]);
                isContain = true;
                break;
              }
            }
            // not contain means this vector value is 0
            if (!isContain) samples[i][j] = 0.0;
            j++;
          }
        } catch (IOException e) {
          logger.error(""IOException happens "", e);
        }
      }
    } catch (IOException e) {
      logger.error(""IOException happens "", e);
    }
    return samples;
  }

}",src/main/java/com/seaboat/text/analyzer/util/VectorUtil.java
DataWriter,"public class DataWriter {

  public static void writeContent(String f, List<String> contents) {
    File file = new File(f);
    FileWriter fw = null;
    BufferedWriter writer = null;
    try {
      fw = new FileWriter(file);
      writer = new BufferedWriter(fw);
      for (String s : contents) {
        writer.write(s);
        writer.newLine();
      }
      writer.flush();
    } catch (FileNotFoundException e) {
      e.printStackTrace();
    } catch (IOException e) {
      e.printStackTrace();
    } finally {
      try {
        writer.close();
        fw.close();
      } catch (IOException e) {
        e.printStackTrace();
      }
    }
  }

}",src/main/java/com/seaboat/text/analyzer/util/DataWriter.java
IndexUtil,"public class IndexUtil {

  private static IndexReader indexReader;

  private static IndexWriter indexWriter;

  private static IndexSearcher indexSearcher;

  private static String path = ""_indexdir"";

  private static Directory directory;

  protected static Logger logger = Logger.getLogger(IndexUtil.class);

  static {
    try {
      directory = FSDirectory.open(Paths.get(path));
    } catch (IOException e) {
      logger.error(e);
    }
  }

  public static IndexReader getIndexReader() throws IOException {
    if (null != indexReader) {
      IndexReader tr = DirectoryReader.openIfChanged((DirectoryReader) indexReader);
      if (tr != null) {
        indexReader.close();
        indexReader = tr;
      }
      return indexReader;
    } else {
      indexReader = DirectoryReader.open(directory);
      return indexReader;
    }
  }

  public static synchronized IndexWriter getIndexWriter() throws IOException {
    if (null != indexWriter) {
      return indexWriter;
    } else {
      Analyzer analyzer = new AnsjAnalyzer(TYPE.query_ansj);
      IndexWriterConfig config = new IndexWriterConfig(analyzer);
      config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
      indexWriter = new IndexWriter(directory, config);
      return indexWriter;
    }
  }

  public static IndexSearcher getIndexSearcher() throws IOException {

    if (null != indexSearcher) {
      return indexSearcher;
    } else {
      synchronized (IndexUtil.class) {
        indexSearcher = new IndexSearcher(getIndexReader());
      }
      return indexSearcher;
    }
  }

}",src/main/java/com/seaboat/text/analyzer/util/IndexUtil.java
ObjectUtil,"public class ObjectUtil {

  protected static Logger logger = Logger.getLogger(ObjectUtil.class);
  
  public static Object readObjectFromFile(String fileName) {
    File file = new File(fileName);
    FileInputStream in;
    Object temp = null;
    try {
      in = new FileInputStream(file);
      ObjectInputStream objIn = new ObjectInputStream(in);
      temp = objIn.readObject();
      objIn.close();
    } catch (IOException e) {
      logger.error(""IOException happens "", e);
    } catch (ClassNotFoundException e) {
      logger.error(""ClassNotFoundException happens "", e);
    }
    return temp;
  }
  

  public static void saveObjectToFile(Object o, String file) {
    FileOutputStream out;
    try {
      out = new FileOutputStream(new File(file));
      ObjectOutputStream objOut = new ObjectOutputStream(out);
      objOut.writeObject(o);
      objOut.flush();
      objOut.close();
    } catch (IOException e) {
      logger.error(""IOException happens "", e);
    }
  }
  
}",src/main/java/com/seaboat/text/analyzer/util/ObjectUtil.java
SequenceUtil,"public class SequenceUtil {

  private static long id;

  private static long maxId;

  private static long delta = 50;

  private static String FILE = ""sequence"";
  
  protected static Logger logger = Logger.getLogger(SequenceUtil.class);

  static {
    iniMaxId();
  }

  public static synchronized long getId() {
    if (id >= maxId) {
      maxId = id + delta;
      writeToSequenceFile(maxId);
    }
    return ++id ;
  }

  private static void writeToSequenceFile(long maxId) {
    try {
      FileWriter fw = new FileWriter(FILE);
      fw.write(String.valueOf(maxId));
      fw.close();
    } catch (IOException e) {
      logger.error(""IOException"", e);
    }
  }

  public static void iniMaxId() {
    File file = new File(FILE);
    InputStreamReader read;
    try {
      read = new InputStreamReader(new FileInputStream(file), ""utf-8"");
      BufferedReader bufferedReader = new BufferedReader(read);
      String lineTxt = bufferedReader.readLine() ;
      if(lineTxt != null) {
        id = Long.parseLong(lineTxt);
        maxId = id + delta;
      }
      bufferedReader.close();
      // avoid suffering the repeating id
      writeToSequenceFile(maxId);
    } catch (UnsupportedEncodingException | FileNotFoundException e) {
      logger.error(""Exception"", e);
    } catch (IOException e) {
      logger.error(""IOException"", e);
    }
  }

}",src/main/java/com/seaboat/text/analyzer/util/SequenceUtil.java
CharacterUtil,"public class CharacterUtil {

	private static final char[] connectors = new char[] { '+', '#', '&', '.', '_', '-' };

	public static char full2Half(char c) {
		if (c == 12288) {
			c = ' ';
		} else if (c >= ' ' && c <= 65374) {
			c = (char) (c - 65248);
		} else {
		}
		return c;
	}

	public static boolean isChinese(char ch) {
		if (ch >= 0x4E00 && ch <= 0x9FA5)
			return true;
		return false;
	}

	public static boolean isEnglish(char ch) {
		if ((ch >= 0x0041 && ch <= 0x005A) || (ch >= 0x0061 && ch <= 0x007A))
			return true;
		return false;
	}

	public static boolean isDigit(char ch) {
		if (ch >= 0x0030 && ch <= 0x0039)
			return true;
		return false;
	}

	public static boolean isConnector(char ch) {
		for (char connector : connectors)
			if (ch == connector)
				return true;
		return false;
	}

	public static StringBuilder preprocess(String s) {
		StringBuilder sb = new StringBuilder();
		char[] charArray = s.toCharArray();
		for (char c : charArray) {
			c = full2Half(c);
			if (isChinese(c) || isEnglish(c) || isDigit(c) || isConnector(c))
				sb.append(c);
		}
		return sb;
	}

	public static int bytesToInt(byte[] src, int offset) {
		int value;
		if (src.length == 1)
			value = (int) ((src[offset] & 0xFF) << 24);
		else if (src.length == 2)
			value = (int) (((src[offset] & 0xFF) << 24) | ((src[offset + 1] & 0xFF) << 16));
		else if (src.length == 3)
			value = (int) (((src[offset] & 0xFF) << 24) | ((src[offset + 1] & 0xFF) << 16)
					| ((src[offset + 2] & 0xFF) << 8));
		else
			value = (int) (((src[offset] & 0xFF) << 24) | ((src[offset + 1] & 0xFF) << 16)
					| ((src[offset + 2] & 0xFF) << 8) | (src[offset + 3] & 0xFF));
		return value;
	}
}",src/main/java/com/seaboat/text/analyzer/util/CharacterUtil.java
UnicodeReader,"public class UnicodeReader extends Reader {
	PushbackInputStream internalIn;
	InputStreamReader internalIn2 = null;
	String defaultEnc;

	private static final int BOM_SIZE = 4;

	/**
	 *
	 * @param in  inputstream to be read
	 * @param defaultEnc default encoding if stream does not have 
	 *                   BOM marker. Give NULL to use system-level default.
	 */
	UnicodeReader(InputStream in, String defaultEnc) {
		internalIn = new PushbackInputStream(in, BOM_SIZE);
		this.defaultEnc = defaultEnc;
	}

	public String getDefaultEncoding() {
		return defaultEnc;
	}

	/**
	 * Get stream encoding or NULL if stream is uninitialized.
	 * Call init() or read() method to initialize it.
	 */
	public String getEncoding() {
		if (internalIn2 == null)
			return null;
		return internalIn2.getEncoding();
	}

	/**
	 * Read-ahead four bytes and check for BOM marks. Extra bytes are
	 * unread back to the stream, only BOM bytes are skipped.
	 */
	protected void init() throws IOException {
		if (internalIn2 != null)
			return;

		String encoding;
		byte bom[] = new byte[BOM_SIZE];
		int n, unread;
		n = internalIn.read(bom, 0, bom.length);

		if ((bom[0] == (byte) 0x00) && (bom[1] == (byte) 0x00) && (bom[2] == (byte) 0xFE) && (bom[3] == (byte) 0xFF)) {
			encoding = ""UTF-32BE"";
			unread = n - 4;
		} else if ((bom[0] == (byte) 0xFF) && (bom[1] == (byte) 0xFE) && (bom[2] == (byte) 0x00)
				&& (bom[3] == (byte) 0x00)) {
			encoding = ""UTF-32LE"";
			unread = n - 4;
		} else if ((bom[0] == (byte) 0xEF) && (bom[1] == (byte) 0xBB) && (bom[2] == (byte) 0xBF)) {
			encoding = ""UTF-8"";
			unread = n - 3;
		} else if ((bom[0] == (byte) 0xFE) && (bom[1] == (byte) 0xFF)) {
			encoding = ""UTF-16BE"";
			unread = n - 2;
		} else if ((bom[0] == (byte) 0xFF) && (bom[1] == (byte) 0xFE)) {
			encoding = ""UTF-16LE"";
			unread = n - 2;
		} else {
			// Unicode BOM mark not found, unread all bytes
			encoding = defaultEnc;
			unread = n;
		}
		//System.out.println(""read="" + n + "", unread="" + unread);

		if (unread > 0)
			internalIn.unread(bom, (n - unread), unread);

		// Use given encoding
		if (encoding == null) {
			internalIn2 = new InputStreamReader(internalIn);
		} else {
			internalIn2 = new InputStreamReader(internalIn, encoding);
		}
	}

	public void close() throws IOException {
		init();
		internalIn2.close();
	}

	public int read(char[] cbuf, int off, int len) throws IOException {
		init();
		return internalIn2.read(cbuf, off, len);
	}

}",src/main/java/com/seaboat/text/analyzer/util/UnicodeReader.java
PinyinUtil,"public class PinyinUtil {
	private static Map<Character, Set<String>> pinyinDict = new HashMap<Character, Set<String>>();

	private static PinyinUtil instance = null;

	private PinyinUtil() throws IOException {
		InputStream input = this.getClass().getResourceAsStream(""/pinyin.txt"");
		BufferedReader in = new BufferedReader(new InputStreamReader(input, ""UTF-8""));
		String line = null;
		while ((line = in.readLine()) != null) {
			char hanzi = line.charAt(0);
			String pinyin = line.substring(2, line.length());
			Set<String> set = pinyinDict.get(hanzi);
			if (set == null) {
				set = new HashSet<String>();
			}
			set.add(pinyin);

			pinyinDict.put(hanzi, set);
		}
		input.close();
		in.close();
	}

	public static PinyinUtil getInstance() {
		if (instance == null) {
			synchronized (PinyinUtil.class) {
				if (instance == null)
					try {
						instance = new PinyinUtil();
					} catch (IOException e) {
						e.printStackTrace();
					}
			}
		}

		return instance;
	}

	public Set<String> getPinyin(Character hanzi) {
		Set<String> set = pinyinDict.get(hanzi);
		if (set == null || set.size() == 0) {
			set = new HashSet<String>();
			set.add(hanzi.toString());
		}
		return set;
	}

	public Set<String> getPinyin(String word) {
		Set<String> word_set = new HashSet<String>();
		for (int i = 0; i < word.length(); i++) {
			Set<String> hanzi_set = getPinyin(word.charAt(i));
			if (word_set == null || word_set.size() == 0) {
				word_set.addAll(hanzi_set);
				continue;
			}

			Set<String> tmp_set = new HashSet<String>();
			for (String w : word_set) {
				for (String h : hanzi_set) {
					tmp_set.add(w + h);
				}
			}

			word_set = tmp_set;
		}

		return word_set;
	}

	public String getPinyinSingle(String word) {
		StringBuffer sb = new StringBuffer();
		for (int i = 0; i < word.length(); i++) {
			sb.append(getPinyin(word.charAt(i)).iterator().next());
		}
		return sb.toString();
	}

	public String getPinyinString(String word) {
		StringBuffer sb = new StringBuffer();
		for (int i = 0; i < word.length(); i++) {
			Set<String> pinyin = getPinyin(word.charAt(i));
			sb.append(pinyin.toString());
		}
		return sb.toString();
	}

	public String getPinyinHead(String word) {
		StringBuffer sb = new StringBuffer();
		for (int i = 0; i < word.length(); i++) {
			sb.append(getPinyin(word.charAt(i)).iterator().next().charAt(0));
		}
		return sb.toString();
	}

}",src/main/java/com/seaboat/text/analyzer/util/PinyinUtil.java
CorpusUtil,"public class CorpusUtil {
	protected static Logger logger = Logger.getLogger(CorpusUtil.class);

	/**
	 * @return word list, pos list
	 */
	public static List<String[]>[] readPeopleDailyCorpus(String fileName) {
		BufferedReader reader = null;
		List<String[]> contents = new ArrayList<String[]>();
		List<String[]> characters = new ArrayList<String[]>();
		try {
			reader = new BufferedReader(new InputStreamReader(new FileInputStream(fileName), ""UTF-8""));
			String line;
			while ((line = reader.readLine()) != null) {
				line = line.replaceAll(""\\["", """");
				line = line.replaceAll(""\\]/nz"", """");
				line = line.replaceAll(""\\]/mq"", """");
				line = line.replaceAll(""\\]/nt"", """");
				String[] terms = line.split(""(/[a-z]*\\s{0,})"");
				terms = line.split(""\\s{1,}[^a-z]*"");
				terms[0] = terms[0].substring(terms[0].indexOf(""/"") + 1);
				contents.add(line.split(""(/[a-z]*\\s{0,})""));
				characters.add(terms);
			}
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			if (reader != null) {
				try {
					reader.close();
				} catch (IOException e) {
					logger.error(""close file error"");
				}
			}
		}
		@SuppressWarnings(""unchecked"")
		List<String[]>[] lists = new ArrayList[2];
		lists[0] = contents;
		lists[1] = characters;
		return lists;
	}

}",src/main/java/com/seaboat/text/analyzer/util/CorpusUtil.java
SVMTrainer,"public class SVMTrainer {

	protected static Logger logger = Logger.getLogger(SVMTrainer.class);

	private IDF idf = new LuceneMemoryIDF();

	public static String TRAIN_FILE = ""data/train.txt"";

	public static String TEST_FILE = ""data/test.txt"";

	public static String MODEL_FILE = ""model/svm-model"";

	public static String VECTOR_FILE = ""model/vector"";

	public static String TFIDF_FILE = ""model/tfidf"";

	public SVMTrainer() {

	}

	public SVMTrainer(String model_file, String vector_file, String tfidf_file) {
		MODEL_FILE = model_file;
		VECTOR_FILE = vector_file;
		TFIDF_FILE = tfidf_file;
	}

	public void train() {
		List<Integer> labels = new LinkedList<Integer>();
		int docNum = 0;
		List<String> list = DataReader.readContent(TRAIN_FILE);
		IndexWriter indexWriter = null;
		try {
			indexWriter = MemoryIndexUtil.getIndexWriter();
		} catch (IOException e) {
			logger.error(""IOException when getting index writer. "", e);
		}
		int DOCID = 0;
		int delta = 10000;
		for (String line : list) {
			labels.add(Integer.parseInt(line.split(""\\|"")[0]));
			String text = line.split(""\\|"")[1];
			FieldType type = new FieldType();
			type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
			type.setStored(true);
			type.setStoreTermVectors(true);
			type.setTokenized(true);
			Document doc = new Document();
			Field field = new Field(""content"", text, type);
			Field docIdField = new Field(""docId"", String.valueOf(DOCID + delta), type);
			doc.add(field);
			doc.add(docIdField);
			try {
				docNum++;
				indexWriter.addDocument(doc);
				indexWriter.commit();
			} catch (IOException e) {
				logger.error(""IOException when adding document. "", e);
			}
			DOCID++;
		}
		// get a whole term vector
		IndexReader reader = null;
		Set<String> vector = null;
		try {
			reader = MemoryIndexUtil.getIndexReader();
			vector = new HashSet<String>();
			for (int docId = 0; docId < reader.maxDoc(); docId++) {
				Term t = new Term(""docId"", String.valueOf(docId + delta));
				Query query = new TermQuery(t);
				IndexSearcher searcher = new IndexSearcher(reader);
				TopDocs topDocs = searcher.search(query, 1);
				Terms terms = reader.getTermVector(topDocs.scoreDocs[0].doc, ""content"");
				TermsEnum termsEnum = terms.iterator();
				BytesRef thisTerm = null;
				while ((thisTerm = termsEnum.next()) != null) {
					String term = thisTerm.utf8ToString();
					if ((term.length() > 1) && (!StringUtil.isNumericAndLetter(term)) && (!StringUtil.isMobile(term))
							&& (!StringUtil.isPhone(term)) && (!StringUtil.isContainNumber(term))
							&& (!StringUtil.isDate(term)) && (!StringUtil.isFilterWord(term)))
						vector.add(term);
				}
			}
		} catch (IOException e) {
			logger.error(""IOException happens "", e);
		}

		List<String> vectorList = new ArrayList<String>(vector);
		Map<String, Double> tfidf = new HashMap<String, Double>();
		// calculating the vector of samples
		double[][] samples = new double[labels.size()][vectorList.size()];
		for (int i = 0; i < docNum; i++) {
			try {
				Term t = new Term(""docId"", String.valueOf(i + delta));
				Query query = new TermQuery(t);
				IndexSearcher searcher = new IndexSearcher(reader);
				TopDocs topDocs;
				topDocs = searcher.search(query, 1);
				int j = 0;
				for (int m = 0; m < vectorList.size(); m++) {
					String vTerm = vectorList.get(m);
					Terms terms = reader.getTermVector(topDocs.scoreDocs[0].doc, ""content"");
					TermsEnum termsEnum = terms.iterator();
					BytesRef thisTerm = null;
					boolean isContain = false;
					while ((thisTerm = termsEnum.next()) != null) {
						String term = thisTerm.utf8ToString();
						if (term.equals(vTerm)) {
							float idfn = idf.getIDF(term);
							int a = (int) termsEnum.totalTermFreq();
							long b = terms.size();
							float tf = (float) a / b;
							samples[i][j] = idfn * tf;
							tfidf.put(term, samples[i][j]);
							isContain = true;
							break;
						}
					}
					// not contain means this vector value is 0
					if (!isContain)
						samples[i][j] = 0.0;
					j++;
				}
			} catch (IOException e) {
				logger.error(""IOException happens "", e);
			}
		}
		Integer tmpInteger[] = new Integer[labels.size()];
		int labelInt[] = new int[labels.size()];
		labels.toArray(tmpInteger);
		for (int i = 0; i < tmpInteger.length; i++) {
			labelInt[i] = tmpInteger[i].intValue();
		}

		SVM<double[]> svm = new SVM<double[]>(new GaussianKernel(8.0), 500.0, 13, SVM.Multiclass.ONE_VS_ALL);
		svm.learn(samples, labelInt);
		svm.finish();

		saveModel(svm);
		saveVector(vectorList);
		saveTFIDF(tfidf);
	}

	private void saveTFIDF(Map<String, Double> tfidf) {
		ObjectUtil.saveObjectToFile(tfidf, TFIDF_FILE);
	}

	private void saveVector(List<String> vectorList) {
		ObjectUtil.saveObjectToFile(vectorList, VECTOR_FILE);
	}

	public void saveModel(SVM<double[]> svm) {
		ObjectUtil.saveObjectToFile(svm, MODEL_FILE);
	}

	public int predict(double[] data) {
		@SuppressWarnings(""unchecked"")
		SVM<double[]> svm = (SVM<double[]>) ObjectUtil.readObjectFromFile(MODEL_FILE);
		return svm.predict(data);
	}

	public double[] getWordVector(String text) {
		List<String> termList = MemoryIndexUtil.getToken(text);
		@SuppressWarnings(""unchecked"")
		List<String> vectorList = (List<String>) ObjectUtil.readObjectFromFile(VECTOR_FILE);
		@SuppressWarnings(""unchecked"")
		Map<String, Double> tfidf = (Map<String, Double>) ObjectUtil.readObjectFromFile(TFIDF_FILE);
		double[] data = new double[vectorList.size()];
		int j = 0;
		for (String vTerm : vectorList) {
			if (termList.contains(vTerm)) {
				double value = tfidf.get(vTerm);
				if (value == 0)
					data[j] = 0.0;
				else
					data[j] = value;
			} else {
				// not contain means this vector value is 0
				data[j] = 0.0;
			}
			j++;
		}
		return data;
	}

}",src/main/java/com/seaboat/text/analyzer/training/SVMTrainer.java
DictSegment,"public class DictSegment implements Segment {

	public enum MODE {
		NORMAL, INDEX
	}

	private static Logger logger = Logger.getLogger(DictSegment.class);
	private static DictSegment instance = null;
	private static CoreWordDict dict = CoreWordDict.get();
	private static MODE mode = MODE.INDEX;

	public static DictSegment get() {
		if (instance != null)
			return instance;
		synchronized (DictSegment.class) {
			if (instance == null)
				instance = new DictSegment();
		}
		return instance;
	}

	public void setMode(MODE m) {
		DictSegment.mode = m;
	}

	private DictSegment() {
	}

	@Override
	public List<String> seg(String text) {
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = new ArrayList<String>();
		if (mode == MODE.INDEX) {
			String s = text;
			int index = 0;
			while (true) {
				List<Integer> temp_list = dict.prefixSearch(s.substring(index, s.length()));
				if (temp_list.size() == 0) {
					words.add(s.substring(index, index + 1) + ""/un"");
					index++;
					if (index == s.length())
						break;
					continue;
				} else {
					int len = dict.getStringByIndex(temp_list.get(0)).length();
					for (int i : temp_list) {
						if (dict.getStringByIndex(i).length() > len)
							len = dict.getStringByIndex(i).length();
						words.add(dict.getStringByIndex(i) + ""/"" + dict.getPostType(i));
					}
					index = index + len;
				}
				if (index >= s.length())
					break;
			}
		} else if (mode == MODE.NORMAL) {
			String s = text;
			int index = 0;
			while (true) {
				List<Integer> temp_list = dict.prefixSearch(s.substring(index, s.length()));
				if (temp_list.size() == 0) {
					words.add(s.substring(index, index + 1) + ""/un"");
					index++;
					if (index == s.length())
						break;
					continue;
				} else {
					//last is the longest
					String longest = dict.getStringByIndex(temp_list.get(temp_list.size() - 1));
					int len = longest.length();
					words.add(longest + ""/"" + dict.getPostType(temp_list.get(temp_list.size() - 1)));
					index = index + len;
				}
				if (index >= s.length())
					break;
			}
		}
		logger.debug(""segment elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
		return words;
	}

	@Override
	public List<String> Search(String text) {
		//		return tree.acSearch(text);
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/segment/DictSegment.java
RuleBasedSegment,"public class RuleBasedSegment implements Segment {

	enum TYPE {
		//forward maximum matching
		FMM,
		//reverse maximum matching
		RMM
	};

	private static Logger logger = Logger.getLogger(RuleBasedSegment.class);
	private static RuleBasedSegment instance = null;
	private static CoreWordDict dict = CoreWordDict.get();
	private static TYPE type = TYPE.FMM;

	public static RuleBasedSegment get() {
		if (instance != null)
			return instance;
		synchronized (RuleBasedSegment.class) {
			if (instance == null)
				instance = new RuleBasedSegment();
		}
		return instance;
	}

	public static void setType(TYPE type) {
		RuleBasedSegment.type = type;
	}

	private RuleBasedSegment() {
	}

	@Override
	public List<String> seg(String text) {
		switch (type) {
		case FMM:
			return fmm(text);
		case RMM:
			return rmm(text);
		}
		return null;
	}

	private List<String> rmm(String text) {
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = new ArrayList<String>();
		int LEN = 10;
		while (text.length() > 0) {
			String temp;
			if (text.length() < LEN) {
				temp = text;
			} else {
				temp = text.substring(text.length() - LEN);
			}

			while (temp.length() > 0) {
				int id = dict.exactlySearch(temp);
				if (id != -1 || temp.length() == 1) {
					if (temp.length() == 1 && id == -1)
						words.add(temp + ""/un"");
					else
						words.add(temp + ""/"" + dict.getPostType(id));
					text = text.substring(0, text.length() - temp.length());
					break;
				} else {
					temp = temp.substring(1);
				}
			}
		}
		Collections.reverse(words);
		logger.debug(""segment elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
		return words;
	}

	private List<String> fmm(String text) {
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = new ArrayList<String>();
		String s = text;
		int index = 0;
		while (true) {
			List<Integer> temp_list = dict.prefixSearch(s.substring(index, s.length()));
			if (temp_list.size() == 0) {
				words.add(s.substring(index, index + 1) + ""/un"");
				index++;
				if (index == s.length())
					break;
				continue;
			} else {
				//last is the longest
				String longest = dict.getStringByIndex(temp_list.get(temp_list.size() - 1));
				int len = longest.length();
				words.add(longest + ""/"" + dict.getPostType(temp_list.get(temp_list.size() - 1)));
				index = index + len;
			}
			if (index >= s.length())
				break;
		}
		logger.debug(""segment elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
		return words;
	}

	@Override
	public List<String> Search(String text) {
		//		return tree.acSearch(text);
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/segment/RuleBasedSegment.java
CharEditDistance,"public class CharEditDistance {

	public static int getEditDistance(String s, String t) {
		int d[][];
		int n;
		int m;
		int i;
		int j;
		char s_i;
		char t_j;
		int cost;

		n = s.length();
		m = t.length();
		if (n == 0) {
			return m;
		}
		if (m == 0) {
			return n;
		}
		d = new int[n + 1][m + 1];
		for (i = 0; i <= n; i++) {
			d[i][0] = i;
		}
		for (j = 0; j <= m; j++) {
			d[0][j] = j;
		}
		for (i = 1; i <= n; i++) {
			s_i = s.charAt(i - 1);
			for (j = 1; j <= m; j++) {
				t_j = t.charAt(j - 1);
				cost = (s_i == t_j) ? 0 : 1;
				d[i][j] = Math.min(d[i - 1][j] + 1, d[i][j - 1] + 1);
				d[i][j] = Math.min(d[i][j], d[i - 1][j - 1] + cost);
			}
		}
		return d[n][m];
	}

}",src/main/java/com/seaboat/text/analyzer/distance/CharEditDistance.java
StandardEditDistance,"public class StandardEditDistance {

	public static double getEditDistance(List<EditBlock> list1, List<EditBlock> list2) {
		if (list1 == null || list2 == null)
			throw new RuntimeException(""list1 or list2 is null"");
		double d[][];
		int n;
		int m;
		int i;
		int j;
		EditBlock s_i;
		EditBlock t_j;

		n = list1.size();
		m = list2.size();
		if (n == 0) {
			double distance = 0.0;
			for (int k = 0; k < m; k++) {
				distance += list2.get(k).getInsertionCost();
			}
			return distance;
		}
		if (m == 0) {
			double distance = 0.0;
			for (int k = 0; k < n; k++) {
				distance += list1.get(k).getDeletionCost();
			}
			return distance;
		}
		d = new double[n + 1][m + 1];
		for (i = 0; i <= n; i++) {
			d[i][0] = i;
		}
		for (j = 0; j <= m; j++) {
			d[0][j] = j;
		}
		for (i = 1; i <= n; i++) {
			s_i = list1.get(i - 1);
			for (j = 1; j <= m; j++) {
				t_j = list2.get(j - 1);
				double cost = s_i.getSubstitutionCost(t_j);
				d[i][j] = Math.min(d[i - 1][j] + 1, d[i][j - 1] + 1);
				d[i][j] = Math.min(d[i][j], d[i - 1][j - 1] + cost);
			}
		}
		return d[n][m];
	}

}",src/main/java/com/seaboat/text/analyzer/distance/StandardEditDistance.java
EditBlock,"public class EditBlock {

	private String word;

	private String pos;

	private static double THRESHOLD = 0.85;

	public EditBlock(String s, String pos) {
		this.word = s;
		this.pos = pos;
	}

	public double getDeletionCost() {
		return word.length();
	}

	public double getInsertionCost() {
		return word.length();
	}

	public String getWord() {
		return word;
	}

	public String getPos() {
		return pos;
	}

	public double getSubstitutionCost(EditBlock block) {
		if (!pos.equals(block.getPos()))
			return 1.0;
		double similarity = getSimilarity(word, block.getWord());
		if (pos.equals(block.getPos()) && similarity >= THRESHOLD)
			return 0.0;
		return 1.0 - similarity;
	}

	private double getSimilarity(String word1, String word2) {
		return Word2Vec.getInstance(false).wordSimilarity(word1, word2);
	}

	public String toString() {
		return word + pos;
	}

}",src/main/java/com/seaboat/text/analyzer/distance/EditBlock.java
XMeansCluster,"public class XMeansCluster {

  public static String DATA_FILE = ""cluster/data.txt"";

  private IDF idf = new LuceneMemoryIDF();

  public static int K = 10;

  public int[] learn(List<String> textList) {
    List<String> vectorList = VectorUtil.getVectorDimension(textList);
    double[][] datas = VectorUtil.getVector(textList.size(), vectorList, idf);
    XMeans xmeans = new XMeans(datas, K);
    return xmeans.getClusterLabel();
  }

}",src/main/java/com/seaboat/text/analyzer/clustering/XMeansCluster.java
VSMCluster,"public class VSMCluster {

	public static String DATA_FILE = ""cluster/data.txt"";

	private IDF idf = new LuceneMemoryIDF();

	private static double SIMILARITY = 0.01;

	public List<String> learn(List<String> textList) {
		List<String> vectorList = VectorUtil.getVectorDimension(textList);
		double[][] datas = VectorUtil.getVector(textList.size(), vectorList, idf);
		List<String> sortTexts = new ArrayList<String>();
		for (int i = 0; i < datas.length; i++) {
			if (i == 0) {
				sortTexts.add(""0"");
				continue;
			}
			sortBySimilarity(i, sortTexts, datas);
		}
		return sortTexts;
	}

	private void sortBySimilarity(int index, List<String> sortTexts, double[][] datas) {
		boolean flag = false;
		for (int i = 0; i < sortTexts.size(); i++) {
			String[] labels = sortTexts.get(i).split("","");
			for (int j = 0; j < labels.length; j++) {
				double[] textVector = datas[Integer.parseInt(labels[j])];
				double similarity = getSimilarity(textVector, datas[index]);
				// if the similarity is greater than SIMILARITY, it's a same class.
				if (similarity >= SIMILARITY) {
					sortTexts.set(i, sortTexts.get(i) + "","" + index);
					flag = true;
					break;
				}
			}
			// has got it,break;
			if (flag)
				return;
		}
		// it must be another class.
		sortTexts.add(String.valueOf(index));
	}

	private double getSimilarity(double[] textVector, double[] ds) {
		double sumA = 0;
		double sumB = 0;
		double sumC = 0;
		for (int i = 0; i < ds.length; i++) {
			sumA += Math.sqrt(textVector[i]);
			sumB += Math.sqrt(ds[i]);
			sumC += textVector[i] * ds[i];
		}
		return (double) sumC / ((Math.sqrt(sumA) * Math.sqrt(sumB)));
	}
}",src/main/java/com/seaboat/text/analyzer/clustering/VSMCluster.java
KMeansCluster,"public class KMeansCluster {

  public static String DATA_FILE = ""cluster/data.txt"";

  private IDF idf = new LuceneMemoryIDF();

  public static int K = 5;

  private static int ITERATE = 10000;

  public int[] learn(List<String> textList) {
    List<String> vectorList = VectorUtil.getVectorDimension(textList);
    double[][] datas = VectorUtil.getVector(textList.size(), vectorList, idf);
    KMeans kmeans = new KMeans(datas, K, ITERATE);
    return kmeans.getClusterLabel();
  }

}",src/main/java/com/seaboat/text/analyzer/clustering/KMeansCluster.java
HownetSimilarity,"public class HownetSimilarity implements ISimilarity {

	HownetSememeSimilarity sememeSimilarity = new HownetSememeSimilarity();

	private enum MODEL {
		UN_COMBINE_TERM, COMBINE_TERM
	};

	private MODEL model = MODEL.COMBINE_TERM;

	public enum SET_OPERATE_TYPE {
		AVERAGE, FUZZY
	};

	private SET_OPERATE_TYPE currentSetOperateType = SET_OPERATE_TYPE.AVERAGE;

	@Override
	public double getSimilarity(String s1, String s2) {
		if (model == MODEL.UN_COMBINE_TERM)
			return getSimilarityByModel1(s1, s2);
		else
			return getSimilarityByModel2(s1, s2);
	}

	private double getSimilarityByModel2(String s1, String s2) {
		double similarity = 0.0;
		if (s1.equals(s2)) {
			return 1.0;
		}
		Collection<Term> termColl1 = HownetGlossary.getInstance().getTermsWithCombining(s1);
		Collection<Term> termColl2 = HownetGlossary.getInstance().getTermsWithCombining(s2);
		if (isUnknownWord(termColl1) && !isUnknownWord(termColl2)) {
			termColl1 = HownetGlossary.getInstance().autoCombineTerms(s1, termColl2);
		} else if (isUnknownWord(termColl2) && !isUnknownWord(termColl1)) {
			termColl2 = HownetGlossary.getInstance().autoCombineTerms(s2, termColl1);
		} else if (isUnknownWord(termColl2) && isUnknownWord(termColl2)) {
			termColl1 = HownetGlossary.getInstance().autoCombineTerms(s1, termColl2);
			termColl2 = HownetGlossary.getInstance().autoCombineTerms(s2, termColl1);
			termColl1 = HownetGlossary.getInstance().autoCombineTerms(s1, termColl2);
			termColl2 = HownetGlossary.getInstance().autoCombineTerms(s2, termColl1);
		}
		for (Term c1 : termColl1) {
			for (Term c2 : termColl2) {
				double v = getSimilarity(c1, c2);
				if (v > similarity) {
					similarity = v;
				}
				if (similarity == 1.0) {
					break;
				}
			}
		}
		return similarity;
	}

	private double getSimilarityByModel1(String s1, String s2) {
		double similarity = 0.0;
		if (s1.equals(s2)) {
			return 1.0;
		}
		Collection<Term> termColl1 = HownetGlossary.getInstance().getTerms(s1);
		Collection<Term> termColl2 = HownetGlossary.getInstance().getTerms(s2);
		if (isUnknownWord(termColl1) || isUnknownWord(termColl2)) {
			return 0.0;
		}
		for (Term t1 : termColl1) {
			for (Term t2 : termColl2) {
				double v = getSimilarity(t1, t2);
				if (v > similarity) {
					similarity = v;
				}
				if (similarity == 1.0) {
					break;
				}
			}
		}
		return similarity;
	}

	public double getSimilarity(Term t1, Term t2) {
		double similarity = 0.0;
		if (t1 == null || t2 == null || !t1.getPos().equals(t2.getPos())) {
			return 0.0;
		}
		if (t1.equals(t2)) {
			return 1.0;
		}
		if (t1.isSubstantive() != t2.isSubstantive()) {
			return 0.0;
		} else if (t1.isSubstantive() == false) {
			similarity = sememeSimilarity.getSimilarity(t1.getMainSememe(), t2.getMainSememe());
		} else {
			double sim1 = sememeSimilarity.getSimilarity(t1.getMainSememe(), t2.getMainSememe());
			double sim2 = getSimilarity(t1.getSecondSememes(), t2.getSecondSememes());
			double sim3 = getSimilarity(t1.getRelationSememes(), t2.getRelationSememes());
			double sim4 = getSimilarity(t1.getSymbolSememes(), t2.getSymbolSememes());
			similarity = calculate(sim1, sim2, sim3, sim4);
		}
		return similarity;
	}

	protected double calculate(double sim_v1, double sim_v2, double sim_v3, double sim_v4) {
		return Constants.beta1 * sim_v1 + Constants.beta2 * sim_v1 * sim_v2 + Constants.beta3 * sim_v1 * sim_v2 * sim_v3
				+ Constants.beta4 * sim_v1 * sim_v2 * sim_v3 * sim_v4;
	}

	protected double getSimilarity(String[] sememes1, String[] sememes2) {
		if (currentSetOperateType == SET_OPERATE_TYPE.FUZZY) {
			return getSimilarity_Fuzzy(sememes1, sememes2);
		} else {
			return getSimilarity_AVG(sememes1, sememes2);
		}
	}

	private double getSimilarity_AVG(String[] sememes1, String[] sememes2) {
		double similarity = 0.0;
		double scoreArray[][];
		if (isBlank(sememes1) || isBlank(sememes2)) {
			if (isBlank(sememes1) && isBlank(sememes2)) {
				return 1.0;
			} else {
				return Constants.delta;
			}
		}
		double score = 0.0;
		int arrayLen = sememes1.length > sememes2.length ? sememes1.length : sememes2.length;
		scoreArray = new double[arrayLen][arrayLen];
		for (int i = 0; i < arrayLen; i++) {
			for (int j = 0; j < arrayLen; j++) {
				scoreArray[i][j] = 0;
			}
		}
		for (int i = 0; i < sememes1.length; i++) {
			for (int j = 0; j < sememes2.length; j++) {
				scoreArray[i][j] = sememeSimilarity.getSimilarity(sememes1[i], sememes2[j]);
			}
		}
		score = 0.0;
		while (scoreArray.length > 0) {
			double[][] tmp;
			int row = 0;
			int column = 0;
			double max = scoreArray[row][column];
			for (int i = 0; i < scoreArray.length; i++) {
				for (int j = 0; j < scoreArray[i].length; j++) {
					if (scoreArray[i][j] > max) {
						row = i;
						column = j;
						max = scoreArray[i][j];
					}
				}
			}
			score += max;
			tmp = new double[scoreArray.length - 1][scoreArray.length - 1];
			for (int i = 0; i < scoreArray.length; i++) {
				if (i == row) {
					continue;
				}
				for (int j = 0; j < scoreArray[i].length; j++) {
					if (j == column) {
						continue;
					}
					int tmprow = i;
					int tmpcol = j;
					if (i > row)
						tmprow--;
					if (j > column)
						tmpcol--;
					tmp[tmprow][tmpcol] = scoreArray[i][j];
				}
			}
			scoreArray = tmp;
		}
		similarity = score / arrayLen;
		return similarity;
	}

	protected double getSimilarity_Fuzzy(String[] sememes1, String[] sememes2) {
		// @ TODO
		return 0.0;
	}

	private boolean isUnknownWord(Collection<Term> term) {
		return term.size() == 0 || term == null;
	}

	private boolean isBlank(String[] str) {
		return str == null || str.length == 0;
	}

	public MODEL getModel() {
		return model;
	}

	public void setModel(MODEL model) {
		this.model = model;
	}

}",src/main/java/com/seaboat/text/analyzer/similarity/HownetSimilarity.java
WordLiteralValueSimilarity,"public class WordLiteralValueSimilarity implements ISimilarity {

	protected static Logger logger = Logger.getLogger(WordLiteralValueSimilarity.class);
	private double alpha = 0.6;
	private double beta = 0.4;

	@Override
	public double getSimilarity(String s1, String s2) {
		if (isBlank(s1) && isBlank(s2)) {
			return 1.0;
		}
		if (isBlank(s1) || isBlank(s2)) {
			return 0.0;
		}
		List<Character> sameHZ = new ArrayList<Character>();
		String longString = s1.length() >= s2.length() ? s1 : s2;
		String shortString = s1.length() < s2.length() ? s1 : s2;
		for (int i = 0; i < longString.length(); i++) {
			Character ch = longString.charAt(i);
			if (shortString.contains(ch.toString())) {
				sameHZ.add(ch);
			}
		}
		double dp = Math.min(1.0 * s1.length() / s2.length(), 1.0 * s2.length() / s1.length());
		double part1 = alpha * (1.0 * sameHZ.size() / s1.length() + 1.0 * sameHZ.size() / s2.length()) / 2.0;
		double part2 = beta * dp * (getWeightedResult(s1, sameHZ) + getWeightedResult(s2, sameHZ)) / 2.0;
		return part1 + part2;
	}

	private double getWeightedResult(String word1, List<Character> sameHZ) {
		double top = 0;
		double bottom = 0;
		for (int i = 0; i < word1.length(); i++) {
			if (sameHZ.contains(word1.charAt(i))) {
				top += (i + 1);
			}
			bottom += (i + 1);
		}
		return 1.0 * top / bottom;
	}

	private boolean isBlank(String str) {
		return str == null || str.trim().equals("""");
	}

}",src/main/java/com/seaboat/text/analyzer/similarity/WordLiteralValueSimilarity.java
SentenceSimilarity,"public class SentenceSimilarity implements ISimilarity {

	@Override
	public double getSimilarity(String s1, String s2) {
		List<EditBlock> list1 = new ArrayList<EditBlock>();
		List<EditBlock> list2 = new ArrayList<EditBlock>();
		for (String s : Segment.getWords(s1))
			list1.add(new EditBlock(s, """"));
		for (String s : Segment.getWords(s2))
			list2.add(new EditBlock(s, """"));
		double a = StandardEditDistance.getEditDistance(list1, list2);
		double b = Math.max(StandardEditDistance.getEditDistance(list1, new ArrayList<EditBlock>()),
				StandardEditDistance.getEditDistance(new ArrayList<EditBlock>(), list2));
		return 1 - ((a + 0.1) / (b + 0.1));
	}

}",src/main/java/com/seaboat/text/analyzer/similarity/SentenceSimilarity.java
PinyinSimilarity,"public class PinyinSimilarity implements ISimilarity {

	protected static Logger logger = Logger.getLogger(PinyinSimilarity.class);

	@Override
	public double getSimilarity(String s1, String s2) {
		Set<String> pinyinSet1 = PinyinUtil.getInstance().getPinyin(s1);
		Set<String> pinyinSet2 = PinyinUtil.getInstance().getPinyin(s2);

		double max = 0.0;
		for (String pinyin1 : pinyinSet1) {
			for (String pinyin2 : pinyinSet2) {
				double distance = CharEditDistance.getEditDistance(pinyin1, pinyin2);
				double similarity = 1
						- distance / ((pinyin1.length() > pinyin2.length()) ? pinyin1.length() : pinyin2.length());
				max = (max > similarity) ? max : similarity;
				if (max == 1.0) {
					return max;
				}
			}
		}
		return max;
	}

}",src/main/java/com/seaboat/text/analyzer/similarity/PinyinSimilarity.java
CilinSimilarity,"public class CilinSimilarity implements ISimilarity {

	protected static Logger logger = Logger.getLogger(CilinSimilarity.class);
	public static double[] WEIGHT = new double[] { 1.2, 1.2, 1.0, 1.0, 0.8, 0.4 };
	public static double TOTAL_WEIGHT = 5.6;

	@Override
	public double getSimilarity(String s1, String s2) {
		if (s1 == null && s2 == null) {
			return 1.0;
		} else if (s1 == null || s2 == null) {
			return 0.0;
		} else if (s1.equalsIgnoreCase(s2)) {
			return 1.0;
		}
		Set<String> codeSet1 = CilinDictionary.getInstance().getCilinCoding(s1);
		Set<String> codeSet2 = CilinDictionary.getInstance().getCilinCoding(s2);
		if (codeSet1 == null || codeSet2 == null) {
			return 0.0;
		}
		double similarity = 0.0;
		for (String code1 : codeSet1) {
			for (String code2 : codeSet2) {
				double s = sumWeight(code1, code2) / TOTAL_WEIGHT;
				logger.debug(code1 + ""-"" + code2 + ""-"" + sumWeight(code1, code2));
				if (similarity < s)
					similarity = s;
			}
		}
		return similarity;
	}

	public static double sumWeight(String code1, String code2) {
		double weight = 0.0;
		for (int i = 1; i <= 6; i++) {
			String c1 = getLevelCode(code1, i);
			String c2 = getLevelCode(code2, i);
			if (c1.equals(c2)) {
				weight += WEIGHT[i - 1];
			} else {
				break;
			}
		}
		return weight;
	}

	public static String getLevelCode(String code, int level) {
		switch (level) {
		case 1:
			return code.substring(0, 1);
		case 2:
			return code.substring(1, 2);
		case 3:
			return code.substring(2, 4);
		case 4:
			return code.substring(4, 5);
		case 5:
			return code.substring(5, 7);
		case 6:
			return code.substring(7);
		}
		return """";
	}
}",src/main/java/com/seaboat/text/analyzer/similarity/CilinSimilarity.java
HownetSememeSimilarity,"public class HownetSememeSimilarity implements ISimilarity {

	@Override
	public double getSimilarity(String s1, String s2) {
		if (isBlank(s1) && isBlank(s2)) {
			return 1.0;
		} else if ((isBlank(s1) && !isBlank(s2)) || (!isBlank(s1) && isBlank(s2))) {
			return 0.0;
		} else if (s1.equals(s2)) {
			return 1.0;
		}
		String key1 = s1.trim();
		String key2 = s2.trim();
		if ((key1.charAt(0) == '(') && (key1.charAt(key1.length() - 1) == ')')) {
			if (key2.charAt(0) == '(' && key2.charAt(key2.length() - 1) == ')') {
				key1 = key1.substring(1, key1.length() - 1);
				key2 = key2.substring(1, key2.length() - 1);
			} else {
				return 0.0;
			}
		}
		int pos = key1.indexOf('=');
		if (pos > 0) {
			int pos2 = key2.indexOf('=');
			if ((pos == pos2) && key1.substring(0, pos).equals(key2.substring(0, pos2))) {
				key1 = key1.substring(pos + 1);
				key2 = key2.substring(pos2 + 1);
			} else {
				return 0.0;
			}
		}
		String symbol1 = key1.substring(0, 1);
		String symbol2 = key2.substring(0, 1);
		for (int i = 0; i < Constants.Symbol_Descriptions.length; i++) {
			if (symbol1.equals(Constants.Symbol_Descriptions[i][0])) {
				if (symbol1.equals(symbol2)) {
					key1 = s1.substring(1);
					key2 = s2.substring(1);
					break;
				} else {
					return 0.0;
				}
			}
		}
		if ((pos = key1.indexOf(""|"")) >= 0) {
			key1 = key1.substring(pos + 1);
		}
		if ((pos = key2.indexOf(""|"")) >= 0) {
			key2 = key2.substring(pos + 1);
		}
		if (key1.equals(key2)) {
			return 1.0;
		}
		return getMaxSimilarity(key1, key2);
	}

	public double getMaxSimilarity(String s1, String s2) {
		double maxValue = 0.0;
		if (s1.equals(s2)) {
			return 1.0;
		}
		Collection<String> sememeIds1 = HownetSememe.getInstance().getSememeIds(s1);
		Collection<String> sememeIds2 = HownetSememe.getInstance().getSememeIds(s2);
		if (sememeIds1.size() == 0 || sememeIds1.size() == 0) {
			return 0.0;
		}
		for (String id1 : sememeIds1) {
			for (String id2 : sememeIds2) {
				double value = getSimilarityBySememeId(id1, id2);
				if (value > maxValue) {
					maxValue = value;
				}
			}
		}
		return maxValue;
	}

	private double getSimilarityBySememeId(final String id1, final String id2) {
		int position = 0;
		String[] array1 = id1.split(""-"");
		String[] array2 = id2.split(""-"");
		for (position = 0; position < array1.length && position < array2.length; position++) {
			if (!array1[position].equals(array2[position])) {
				break;
			}
		}
		return 2.0 * position / (array1.length + array2.length);
	}

	private boolean isBlank(String s1) {
		return s1.equals("""") || s1 == null;
	}

}",src/main/java/com/seaboat/text/analyzer/similarity/HownetSememeSimilarity.java
MitieEntityExtractor,"public class MitieEntityExtractor implements Extractor {

	protected static Logger logger = Logger.getLogger(MitieEntityExtractor.class);

	private static String TRAIN_FILE = ""data/tag_test.txt"";

	static {
		try {
			System.loadLibrary(""javamitie"");
		} catch (UnsatisfiedLinkError e) {
			System.err.println(""java.library.path="" + System.getProperty(""java.library.path""));
		}
	}

	@Override
	public void train(String samplesFile) {
		if (samplesFile == null)
			samplesFile = TRAIN_FILE;
		NerTrainer nerTrainer = new NerTrainer(""model/mitie_model/total_word_feature_extractor.dat"");
		List<String> texts = DataReader.readContent(samplesFile);
		for (String text : texts) {
			String[] line = text.split(""###"");
			String[] words = line[0].split(""/"");
			StringVector stringVector = new StringVector();
			for (String word : words)
				stringVector.add(word);
			NerTrainingInstance nerTrainingInstance = new NerTrainingInstance(stringVector);
			String[] ss = line[1].split(""/"");
			for (String s : ss) {
				String[] info = s.substring(1, s.length() - 1).split("","");
				nerTrainingInstance.addEntity(Long.parseLong(info[0]), Long.parseLong(info[1]), info[2]);
			}
			nerTrainer.add(nerTrainingInstance);
		}
		nerTrainer.setThreadNum(4);
		nerTrainer.train(""model/mitie_model/test_ner_model.dat"");
	}

	@Override
	public List<String> predict(String text) {
		NamedEntityExtractor ner = new NamedEntityExtractor(""model/mitie_model/test_ner_model.dat"");
		StringVector possibleTags = ner.getPossibleNerTags();
		List<String> words = WordSegmentUtil.seg(text);
		StringVector sVector = new StringVector();
		for (String word : words)
			sVector.add(word);
		try {
			EntityMentionVector entities = ner.extractEntities(sVector);
			logger.debug(""entities size : "" + entities.size());
			for (int i = 0; i < entities.size(); ++i) {
				EntityMention entity = entities.get(i);
				String tag = possibleTags.get(entity.getTag());
				Double score = entity.getScore();
				String scoreStr = String.format(""%1$,.3f"", score);
				logger.debug(""   Score: "" + scoreStr + "": "" + tag + "":"");
				for (int j = entity.getStart(); j < entity.getEnd(); ++j) {
					logger.debug(words.get(j) + "" "");
				}
				logger.debug("""");
			}
		} catch (Exception e) {
			e.printStackTrace();
		}

		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/extractor/MitieEntityExtractor.java
CrfInfoExtractor,"public class CrfInfoExtractor implements InfoExtractor {

	public static String dir = new File("""").getAbsolutePath();

	private static String script_path = dir + ""/python/crf_ner/crf_ner.py"";

	private static String model_path = dir + ""/model/crf/crf.model"";

	@Override
	public List<String> getIDs(String text) {
		String line;
		String string = """";
		try {
			String[] args = new String[] { ""python"", script_path, ""predict"", text, model_path };
			Process pr = Runtime.getRuntime().exec(args);
			BufferedReader in = new BufferedReader(new InputStreamReader(pr.getInputStream()));
			while ((line = in.readLine()) != null) {
				string += line;
			}
			in.close();
			pr.waitFor();
		} catch (Exception e) {
			e.printStackTrace();
		}
		List<String> IDs = new ArrayList<String>();
		if (string.length() > 0) {
			String[] labels = string.split("" "");
			List<Integer> idBegins = new ArrayList<Integer>();
			List<Integer> idEnds = new ArrayList<Integer>();
			for (int i = 0; i < labels.length; i++) {
				if (labels[i].equals(""IB"")) {
					idBegins.add(i);
				}
				if (labels[i].equals(""IE"")) {
					idEnds.add(i);
				}
			}
			for (int j = 0; j < idBegins.size(); j++)
				IDs.add(text.substring(idBegins.get(j), idEnds.get(j) + 1));
		}
		return IDs;
	}

	@Override
	public List<String> getNames(String text) {
		String line;
		String string = """";
		try {
			String[] args = new String[] { ""python"", script_path, ""predict"", text, model_path };
			Process pr = Runtime.getRuntime().exec(args);
			BufferedReader in = new BufferedReader(new InputStreamReader(pr.getInputStream()));
			while ((line = in.readLine()) != null) {
				string += line;
			}
			in.close();
			pr.waitFor();
		} catch (Exception e) {
			e.printStackTrace();
		}
		List<String> IDs = new ArrayList<String>();
		if (string.length() > 0) {
			String[] labels = string.split("" "");
			List<Integer> idBegins = new ArrayList<Integer>();
			List<Integer> idEnds = new ArrayList<Integer>();
			for (int i = 0; i < labels.length; i++) {
				if (labels[i].equals(""PB"")) {
					idBegins.add(i);
				}
				if (labels[i].equals(""PE"")) {
					idEnds.add(i);
				}
			}
			for (int j = 0; j < idBegins.size(); j++)
				IDs.add(text.substring(idBegins.get(j), idEnds.get(j) + 1));
		}
		return IDs;
	}

	@Override
	public List<String> getAddrs(String text) {
		String line;
		String string = """";
		try {
			String[] args = new String[] { ""python"", script_path, ""predict"", text, model_path };
			Process pr = Runtime.getRuntime().exec(args);
			BufferedReader in = new BufferedReader(new InputStreamReader(pr.getInputStream()));
			while ((line = in.readLine()) != null) {
				string += line;
			}
			in.close();
			pr.waitFor();
		} catch (Exception e) {
			e.printStackTrace();
		}
		List<String> IDs = new ArrayList<String>();
		if (string.length() > 0) {
			String[] labels = string.split("" "");
			List<Integer> idBegins = new ArrayList<Integer>();
			List<Integer> idEnds = new ArrayList<Integer>();
			for (int i = 0; i < labels.length; i++) {
				if (labels[i].equals(""LB"")) {
					idBegins.add(i);
				}
				if (labels[i].equals(""LE"")) {
					idEnds.add(i);
				}
			}
			for (int j = 0; j < idBegins.size(); j++)
				IDs.add(text.substring(idBegins.get(j), idEnds.get(j) + 1));
		}
		return IDs;
	}

}",src/main/java/com/seaboat/text/analyzer/extractor/CrfInfoExtractor.java
ObjectExtractor,"public class ObjectExtractor implements IObjectExtractor {

	@Override
	public List<String> extract(String text) {
		HashMap<String, String> map = new HashMap<String, String>();
		List<String> list = new ArrayList<String>();
		for (String key : map.keySet()) {
			map.get(key);
		}
		return list;
	}

}",src/main/java/com/seaboat/text/analyzer/extractor/ObjectExtractor.java
AddressExtractor,"public class AddressExtractor implements IAddressExtractor {
	static Properties prop = new Properties();

	@Override
	public List<String> extract(String text) {
		HashMap<String, String> map = new HashMap<String, String>();
		List<String> list = new ArrayList<String>();
		for (String key : map.keySet()) {
			String value = map.get(key);
			if (value.equals(""""))
				list.add(key);
		}
		return list;
	}

}",src/main/java/com/seaboat/text/analyzer/extractor/AddressExtractor.java
SegmentFile,"public class SegmentFile {

  private static String FILE = ""data/test.txt"";
  private static String segFILE = ""data/tag_test.txt"";

  public static void main(String[] args) {
    List<String> texts = DataReader.readContent(FILE);
    List<String> segTexts = new ArrayList<String>();
    for (String text : texts) {
      List<String> words = WordSegmentUtil.seg(text);
      String segmented = """";
      for (String word : words) {
        segmented += word + ""/"";
      }
      segmented = segmented.substring(0, segmented.length() - 1);
      segmented = segmented+""###"";
      segTexts.add(segmented);
    }
    DataWriter.writeContent(segFILE, segTexts);
  }

}",src/main/java/com/seaboat/text/analyzer/extractor/SegmentFile.java
WordSentimentTendency,"public class WordSentimentTendency implements Tendency {
	public static String[] POSITIVE_SEMEMES = new String[] { """", """", """", """", """", """", """", """", """", """",
			"""", """", """", """", """", """", """", """", """", """", """", """", """", """" };

	public static String[] NEGATIVE_SEMEMES = new String[] { """", """", """", """", """", """", """", """", """", """",
			"""", """", """", """", """", """", """", """", """", """", """", """", """" };

	HownetSememeSimilarity similarity = new HownetSememeSimilarity();

	@Override
	public double getTendency(String str) {
		double positive = getSentiment(str, POSITIVE_SEMEMES);
		double negative = getSentiment(str, NEGATIVE_SEMEMES);
		return positive - negative;
	}

	public double getSentiment(String word, String[] candidateSememes) {
		Collection<Term> terms = HownetGlossary.getInstance().getTerms(word);
		Set<String> sememes = new HashSet<String>();
		for (Term c : terms) {
			sememes.addAll(c.getAllSememeNames());
		}

		double max = 0.0;
		for (String item : sememes) {
			double total = 0.0;
			for (String positiveSememe : candidateSememes) {
				double value = similarity.getSimilarity(item, positiveSememe);
				if (value > 0.9) {
					return value;
				}
				total += value;
			}
			double sim = total / candidateSememes.length;
			if (sim > max) {
				max = sim;
			}
		}
		return max;
	}

}",src/main/java/com/seaboat/text/analyzer/tendency/WordSentimentTendency.java
GlossaryParser,"public class GlossaryParser implements Parser {

	private static Logger logger = Logger.getLogger(GlossaryParser.class);

	public Multimap<String, Term> parse(String file) {
		logger.debug(""loading hownet glossary..."");
		Multimap<String, Term> map = HashMultimap.create();
		long time = System.currentTimeMillis();
		try {
			InputStreamReader read = new InputStreamReader(this.getClass().getResourceAsStream(file), ""UTF-8"");
			XMLInputFactory inputFactory = XMLInputFactory.newInstance();
			XMLEventReader xmlEventReader = inputFactory.createXMLEventReader(read);
			while (xmlEventReader.hasNext()) {
				XMLEvent event = xmlEventReader.nextEvent();
				if (event.isStartElement()) {
					StartElement startElement = event.asStartElement();
					if (startElement.getName().toString().equals(""c"")) {
						String word = startElement.getAttributeByName(QName.valueOf(""w"")).getValue();
						String define = startElement.getAttributeByName(QName.valueOf(""d"")).getValue();
						String pos = startElement.getAttributeByName(QName.valueOf(""p"")).getValue();
						Term term = new Term(word, pos, define);
						map.put(word, term);
					}
				}
			}
			read.close();
		} catch (Exception e) {
			logger.error(""error occurs when parsing hownet, "", e);
		}
		time = System.currentTimeMillis() - time;
		logger.debug(""hownet is parsed completely!  time elapsed: "" + time + ""ms"");
		return map;
	}

}",src/main/java/com/seaboat/text/analyzer/hownet/GlossaryParser.java
HownetSememe,"public class HownetSememe {

	private static Multimap<String, Sememe> sememes = null;

	private static HownetSememe instance;

	private static String FILE = ""/hownet-sememe.xml"";

	private HownetSememe() {
		sememes = new SememeParser().parse(FILE);
	}

	public static HownetSememe getInstance() {
		if (instance == null) {
			synchronized (HownetSememe.class) {
				if (instance == null)
					instance = new HownetSememe();
			}
		}
		return instance;
	}

	public List<String> getDefine(String word) {
		Collection<Sememe> coll = sememes.get(word);
		List<String> list = Lists.newArrayList();
		for (Sememe s : coll)
			list.add(s.getDefine());
		return list;
	}

	public Collection<String> getSememeIds(String s1) {
		List<String> list = Lists.newArrayList();
		Collection<Sememe> coll = sememes.get(s1);
		for (Sememe s : coll)
			list.add(s.getId());
		return list;
	}

}",src/main/java/com/seaboat/text/analyzer/hownet/HownetSememe.java
Sememe,"public class Sememe {
	private String id;
	private String cnWord;
	private String enWord;
	private String define;

	public Sememe(String id, String en, String cn, String define) {
		this.id = id;
		this.cnWord = cn;
		this.enWord = en;
		this.define = define;
	}

	public String getId() {
		return id;
	}

	public void setId(String id) {
		this.id = id;
	}

	public String getCnWord() {
		return cnWord;
	}

	public void setCnWord(String cnWord) {
		this.cnWord = cnWord;
	}

	public String getEnWord() {
		return enWord;
	}

	public void setEnWord(String enWord) {
		this.enWord = enWord;
	}

	public String getDefine() {
		return define;
	}

	public void setDefine(String define) {
		this.define = define;
	}

	public int getType() {
		char ch = id.charAt(0);
		switch (ch) {
		case '1':
			return SememeType.Event;
		case '2':
			return SememeType.Entity;
		case '3':
			return SememeType.Attribute;
		case '4':
			return SememeType.Quantity;
		case '5':
			return SememeType.AValue;
		case '6':
			return SememeType.QValue;
		case '7':
			return SememeType.SecondaryFeature;
		case '8':
			return SememeType.Syntax;
		case '9':
			return SememeType.EventRoleAndFeature;
		default:
			return 0;
		}
	}

	@Override
	public String toString() {
		StringBuilder sb = new StringBuilder();
		sb.append(""id="");
		sb.append(id);
		sb.append(""; cnWord="");
		sb.append(cnWord);
		sb.append(""; enWord="");
		sb.append(enWord);
		sb.append(""; define="");
		sb.append(define);
		return sb.toString();
	}

}",src/main/java/com/seaboat/text/analyzer/hownet/Sememe.java
SememeParser,"public class SememeParser implements Parser {

	private static Logger logger = Logger.getLogger(SememeParser.class);

	public Multimap<String, Sememe> parse(String file) {
		logger.debug(""loading hownet sememe..."");
		long time = System.currentTimeMillis();
		Multimap<String, Sememe> map = HashMultimap.create();
		try {
			InputStreamReader read = new InputStreamReader(this.getClass().getResourceAsStream(file), ""UTF-8"");
			XMLInputFactory inputFactory = XMLInputFactory.newInstance();
			XMLEventReader xmlEventReader = inputFactory.createXMLEventReader(read);
			while (xmlEventReader.hasNext()) {
				XMLEvent event = xmlEventReader.nextEvent();
				if (event.isStartElement()) {
					StartElement startElement = event.asStartElement();
					if (startElement.getName().toString().equals(""sememe"")) {
						String en = startElement.getAttributeByName(QName.valueOf(""en"")).getValue();
						String cn = startElement.getAttributeByName(QName.valueOf(""cn"")).getValue();
						String id = startElement.getAttributeByName(QName.valueOf(""id"")).getValue();
						Attribute attr = startElement.getAttributeByName(QName.valueOf(""define""));
						String define = (attr == null ? null : attr.getValue());

						Sememe sememe = new Sememe(id, en, cn, define);
						map.put(cn, sememe);
					}
				}
			}
			read.close();
		} catch (Exception e) {
			logger.error(""error occurs when parsing hownet, "", e);
		}
		time = System.currentTimeMillis() - time;
		logger.debug(""hownet is parsed completely!  time elapsed: "" + time + ""ms"");
		return map;
	}

}",src/main/java/com/seaboat/text/analyzer/hownet/SememeParser.java
HownetGlossary,"public class HownetGlossary {

	private static Multimap<String, Term> glossary = null;

	private HownetSememeSimilarity sememeSimilarity = new HownetSememeSimilarity();

	private static HownetGlossary instance;

	private static String FILE = ""/hownet-glossary.xml"";

	private HownetGlossary() {
		glossary = new GlossaryParser().parse(FILE);
	}

	public static HownetGlossary getInstance() {
		if (instance == null) {
			synchronized (HownetGlossary.class) {
				if (instance == null)
					instance = new HownetGlossary();
			}
		}
		return instance;
	}

	public Collection<Term> getTerms(String key) {
		Collection<Term> terms = glossary.get(key);
		if (isUnknownWord(terms)) {
			terms = autoCombineTerms(key, null);
		}
		return terms;
	}

	public Collection<Term> getTermsWithCombining(String key) {
		return glossary.get(key);
	}

	/*
	 * calculating unknown word by segmenting and combining the word.
	 */
	public Collection<Term> autoCombineTerms(String u_word, Collection<Term> refTerms) {
		LinkedList<Term> u_list = Lists.newLinkedList();
		if (u_word == null) {
			return u_list;
		}
		for (String word : segmentUnknownWords(u_word, 3)) {
			Collection<Term> terms = getTerms(word);
			if (u_list.size() == 0) {
				u_list.addAll(terms);
				continue;
			}
			LinkedList<Term> tmp = Lists.newLinkedList();
			for (Term head : terms) {
				for (Term tail : u_list) {
					if (!isUnknownWord(refTerms)) {
						for (Term ref : refTerms) {
							Term term = autoCombineTerms(head, tail, ref);
							boolean exist = false;
							for (Term c : tmp) {
								if (c.getDefine().equals(term.getDefine())) {
									exist = true;
									break;
								}
							}
							if (!exist)
								tmp.add(term);
						}
					} else {
						Term term = autoCombineTerms(head, tail, null);
						boolean exist = false;
						for (Term c : tmp) {
							if (c.getDefine().equals(term.getDefine())) {
								exist = true;
								break;
							}
						}
						if (!exist)
							tmp.add(term);
					}
				}
			}
			u_list = tmp;
		}
		if ((u_list.size() > Constants.MAX_COMBINED_COUNT)) {
			int size = Constants.MAX_COMBINED_COUNT / 3;
			for (int i = 0; i < size; i++) {
				u_list.removeLast();
			}
		}
		return u_list;
	}

	private List<String> segmentUnknownWords(String u_word, int topN) {
		List<String> results = Lists.newLinkedList();
		int count = 0;
		while (u_word != null && !u_word.equals("""")) {
			String token = u_word;
			while (token.length() > 1 && isUnknownWord(glossary.get(token))) {
				token = token.substring(1);
			}
			results.add(token);
			count++;
			if (count >= topN)
				break;
			u_word = u_word.substring(0, (u_word.length() - token.length()));
		}

		return results;
	}

	public Term autoCombineTerms(Term head, Term tail, Term ref) {
		if (tail == null && head != null) {
			return new Term(head.getWord(), head.getPos(), head.getDefine());
		} else if (head == null && tail != null) {
			return new Term(tail.getWord(), tail.getPos(), tail.getDefine());
		}
		if (!tail.isSubstantive()) {
			return new Term(head.getWord() + tail.getWord(), head.getPos(), head.getDefine());
		}
		if (ref == null || !ref.isSubstantive()) {
			String define = tail.getDefine();
			List<String> sememeList = getAllSememes(head, true);
			for (String sememe : sememeList) {
				if (!define.contains(sememe)) {
					define = define + "","" + sememe;
				}
			}
			return new Term(head.getWord() + tail.getWord(), tail.getPos(), define);
		}

		String define = tail.getMainSememe();

		List<String> refSememes = getAllSememes(ref, false);
		List<String> headSememes = getAllSememes(head, true);
		List<String> tailSememes = getAllSememes(tail, false);

		double main_similarity = sememeSimilarity.getSimilarity(tail.getMainSememe(), ref.getMainSememe());
		if (main_similarity >= Constants.PARAM_THETA) {
			for (String tail_sememe : tailSememes) {
				double max_similarity = 0.0;
				String max_ref_sememe = null;
				for (String ref_sememe : refSememes) {
					double value = sememeSimilarity.getSimilarity(tail_sememe, ref_sememe);
					if (value > max_similarity) {
						max_similarity = value;
						max_ref_sememe = ref_sememe;
					}
				}
				if (max_similarity * main_similarity >= Constants.PARAM_XI) {
					define = define + "","" + tail_sememe;
					refSememes.remove(max_ref_sememe);
				}
			}
		} else {
			define = tail.getDefine();
		}
		for (String head_sememe : headSememes) {
			double max_similarity = 0.0;
			String max_ref_sememe = """";
			for (String ref_sememe : refSememes) {
				double value = sememeSimilarity.getSimilarity(getPureSememe(head_sememe), getPureSememe(ref_sememe));
				if (value > max_similarity) {
					max_similarity = value;
					max_ref_sememe = ref_sememe;
				}
			}

			if (main_similarity * max_similarity >= Constants.PARAM_OMEGA) {
				String sememe = max_ref_sememe.replace(getPureSememe(max_ref_sememe), getPureSememe(head_sememe));
				if (!define.contains(sememe)) {
					define = define + "","" + sememe;
				}
			} else if (!define.contains(head_sememe)) {
				define = define + "","" + head_sememe;
			}
		}
		return new Term(head.getWord() + tail.getWord(), tail.getPos(), define);
	}

	private List<String> getAllSememes(Term t, boolean includeMainSememe) {
		List<String> results = new ArrayList<String>();
		if (t != null) {
			if (includeMainSememe) {
				results.add(t.getMainSememe());
			}
			for (String sememe : t.getSecondSememes()) {
				results.add(sememe);
			}
			for (String sememe : t.getSymbolSememes()) {
				results.add(sememe);
			}
			for (String sememe : t.getRelationSememes()) {
				results.add(sememe);
			}
		}
		return results;
	}

	private String getPureSememe(String sememe) {
		String line = sememe.trim();
		if ((line.charAt(0) == '(') && (line.charAt(line.length() - 1) == ')')) {
			line = line.substring(1, line.length() - 1);
		}
		String symbol = line.substring(0, 1);
		for (int i = 0; i < Constants.Symbol_Descriptions.length; i++) {
			if (symbol.equals(Constants.Symbol_Descriptions[i][0])) {
				return line.substring(1);
			}
		}
		int pos = line.indexOf('=');
		if (pos > 0) {
			line = line.substring(pos + 1);
		}
		return line;
	}

	private boolean isUnknownWord(Collection<Term> terms) {
		return terms == null || terms.size() == 0;
	}

}",src/main/java/com/seaboat/text/analyzer/hownet/HownetGlossary.java
Term,"public class Term {

	private static String[][] Type = { { ""="", """" }, { ""aValue|"", """" }, { ""qValue|"", """" },
			{ ""attribute|"", """" }, { ""quantity|"", """" }, { ""unit|"", """" }, { ""%"", """" } };

	protected String word;
	protected String pos;
	protected String define;
	protected boolean substantive;
	protected String mainSememe;
	protected String[] secondSememes;
	protected String[] relationSememes;
	protected String[] symbolSememes;

	public Term(String word, String pos, String def) {
		this.word = word;
		this.pos = pos;
		this.define = (def == null) ? """" : def.trim();
		if (define.length() > 0 && define.charAt(0) == '{' && define.charAt(define.length() - 1) == '}') {
			this.substantive = false;
		} else {
			this.substantive = true;
		}

		parseDefine(def);
	}

	private void parseDefine(String tokenString) {
		List<String> secondList = new ArrayList<String>();
		List<String> relationList = new ArrayList<String>();
		List<String> symbolList = new ArrayList<String>();
		if (!this.substantive) {
			tokenString = define.substring(1, define.length() - 1);
		}
		StringTokenizer token = new StringTokenizer(tokenString, "","", false);
		if (token.hasMoreTokens()) {
			this.mainSememe = token.nextToken();
		}

		main: while (token.hasMoreTokens()) {
			String item = token.nextToken();
			if (item.equals(""""))
				continue;
			String symbol = item.substring(0, 1);
			for (int i = 0; i < Constants.Symbol_Descriptions.length; i++) {
				if (symbol.equals(Constants.Symbol_Descriptions[i][0])) {
					symbolList.add(item);
					continue main;
				}
			}
			if (item.indexOf('=') > 0) {
				relationList.add(item);
			} else {
				secondList.add(item);
			}
		}
		this.secondSememes = secondList.toArray(new String[secondList.size()]);
		this.relationSememes = relationList.toArray(new String[relationList.size()]);
		this.symbolSememes = symbolList.toArray(new String[symbolList.size()]);

	}

	public String getType() {
		for (int i = 0; i < Type.length; i++) {
			if (define.toUpperCase().indexOf(Type[i][0].toUpperCase()) >= 0) {
				return Type[i][1];
			}
		}
		return """";
	}

	public String getWord() {
		return word;
	}

	public void setWord(String word) {
		this.word = word;
	}

	public String getPos() {
		return pos;
	}

	public void setPos(String pos) {
		this.pos = pos;
	}

	public boolean isSubstantive() {
		return substantive;
	}

	public String getMainSememe() {
		return mainSememe;
	}

	public String[] getSecondSememes() {
		return secondSememes;
	}

	public String[] getRelationSememes() {
		return relationSememes;
	}

	public String[] getSymbolSememes() {
		return symbolSememes;
	}

	public String getDefine() {
		return define;
	}

	@Override
	public boolean equals(Object anObject) {
		if (anObject instanceof Term) {
			Term c = (Term) anObject;
			return word.equals(c.word) && define.equals(c.define);
		} else {
			return false;
		}
	}

	@Override
	public int hashCode() {
		return define == null ? word.hashCode() : define.hashCode();
	}

	public Set<String> getAllSememeNames() {
		Set<String> names = new HashSet<String>();
		names.add(getMainSememe());
		for (String item : getRelationSememes()) {
			names.add(item.substring(item.indexOf(""="") + 1));
		}
		for (String item : getSymbolSememes()) {
			names.add(item.substring(1));
		}
		for (String item : getSecondSememes()) {
			names.add(item);
		}
		return names;
	}

	@Override
	public String toString() {
		StringBuilder sb = new StringBuilder();
		sb.append(""name="");
		sb.append(this.word);
		sb.append(""; pos="");
		sb.append(this.pos);
		sb.append(""; define="");
		sb.append(this.define);
		sb.append(""; :["" + mainSememe);

		sb.append(""]; :["");
		for (String sem : secondSememes) {
			sb.append(sem);
			sb.append("";"");
		}

		sb.append(""]; [:"");
		for (String sem : relationSememes) {
			sb.append(sem);
			sb.append("";"");
		}

		sb.append(""]; [:"");
		for (String sem : symbolSememes) {
			sb.append(sem);
			sb.append("";"");
		}
		sb.append(""]"");
		return sb.toString();
	}
}",src/main/java/com/seaboat/text/analyzer/hownet/Term.java
Constants,"public class Constants {
	public static final String Symbol_Descriptions[][] = { { ""#"", """" }, { ""%"", """" },
			{ ""$"", ""VV"" }, { ""*"", """" }, { ""+"", """" }, { ""&"", """" },
			{ ""~"", """" }, { ""@"", ""V"" }, { ""?"", ""N"" }, { ""("", """" },
			{ ""^"", """" }, { ""!"", """" }, { ""["", """" } };
	public static final double delta = 0.2;
	public static final double beta1 = 0.5;
	public static final double beta2 = 0.2;
	public static final double beta3 = 0.17;
	public static final double beta4 = 0.13;
	public static final double PARAM_THETA = 0.5;
	public static final double PARAM_XI = 0.6;
	public static final double PARAM_OMEGA = 0.8;
	public static final int MAX_COMBINED_COUNT = 12;
}",src/main/java/com/seaboat/text/analyzer/hownet/Constants.java
HMMPredictor,"public class HMMPredictor {

	private static String path = ""./model/hmm/hmm.model"";

	private HMMModel hmm;

	private static HMMPredictor instance = new HMMPredictor();

	private ViterbiDecoder decoder;

	private HMMPredictor() {
		synchronized (HMMPredictor.class) {
			if (hmm == null)
				try {
					ObjectInputStream in = new ObjectInputStream(new FileInputStream(path));
					hmm = (HMMModel) in.readObject();
					decoder = new ViterbiDecoder(hmm);
					in.close();
				} catch (IOException | ClassNotFoundException e) {
					e.printStackTrace();
					throw new RuntimeException(""HMM fails to init... "");
				}
		}

	}

	public static HMMPredictor getIntance() {
		return instance;
	}

	public String[] predict(String s) {
		List<Term> termList = ToAnalysis.parse(s).getTerms();
		List<String> wordList = new ArrayList<String>();
		for (Term wordTerm : termList) {
			wordList.add(wordTerm.getName());
		}
		List<String> listF = new ArrayList<String>();
		for (int i = 0; i < wordList.size(); i++) {
			// if (StringUtil.hasDigit(wordList.get(i)))
			// continue;
			if (wordList.get(i).equals(""br""))
				continue;
			listF.add(wordList.get(i));
		}
		String[] words = new String[listF.size()];
		listF.toArray(words);
		decoder.decode(words);
		return words;
	}

	public String[] predict(List<String> wordList) {
		List<String> listF = new ArrayList<String>();
		for (int i = 0; i < wordList.size(); i++) {
			if (wordList.get(i).equals(""br""))
				continue;
			listF.add(wordList.get(i));
		}
		String[] words = new String[listF.size()];
		listF.toArray(words);
		decoder.decode(words);
		return words;
	}

}",src/main/java/com/seaboat/text/analyzer/tagging/HMMPredictor.java
HMMTrainer,"public class HMMTrainer {
	private static String path=""./model/hmm/hmm.model"";

	public static void main(String[] args) {
		HMMModel model = new HMMModel();
		List<String[]>[] lists = CorpusUtil.readPeopleDailyCorpus(""./data/corpus_data/train.txt"");
		try {
			model.train(lists[0], lists[1]);
		} catch (SizeException e) {
			e.printStackTrace();
		}
		try {
			ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(path));
			out.writeObject(model);
			out.close();
		} catch (IOException e) {
			e.printStackTrace();
		}
		
	}
}",src/main/java/com/seaboat/text/analyzer/tagging/HMMTrainer.java
ViterbiDecoder,"public class ViterbiDecoder implements Decode {

	private HMMModel model;

	public ViterbiDecoder(HMMModel model) {
		this.model = model;
	}

	@Override
	public String[] decode(String[] text) {
		int len = model.statusType.length;
		double[][] value = new double[text.length][len];
		int[][] previous = new int[text.length][len];
		int position;
		if (model.observationTypeIndex.get(text[0]) != null) {
			position = model.observationTypeIndex.get(text[0]);
			for (int j = 0; j < len; j++) {
				value[0][j] = model.statusPrioriProbability[j] * model.observationProbability[position][j];
			}
		} else {
			for (int j = 0; j < len; j++) {
				value[0][j] = 1;
			}
		}

		for (int i = 1; i < text.length; i++) {
			if (model.observationTypeIndex.get(text[i]) == null) {
				for (int j = 0; j < len; j++) {
					value[i][j] = 1;
				}
				continue;
			}
			position = model.observationTypeIndex.get(text[i]);
			for (int j = 0; j < len; j++) {
				double max = value[i - 1][0] * model.statusTransitionProbability[0][j]
						* model.observationProbability[position][j];
				int index = 0;
				for (int k = 1; k < len; k++) {
					value[i][j] = value[i - 1][k] * model.statusTransitionProbability[k][j]
							* model.observationProbability[position][j];
					if (value[i][j] > max) {
						index = k;
						max = value[i][j];
					}
				}
				previous[i][j] = index;
				value[i][j] = max;
			}
		}

		double max = -1;
		int index = 0;
		for (int i = 0; i < len; i++) {
			if (max < value[text.length - 1][i]) {
				index = i;
				max = value[text.length - 1][i];
			}
		}

		for (int i = text.length - 1; i >= 0; i--) {
			text[i] = text[i].concat(""/"" + model.statusType[index]);
			index = previous[i][index];
		}
		return text;
	}

}",src/main/java/com/seaboat/text/analyzer/ml/hmm/ViterbiDecoder.java
HMMModel,"public class HMMModel implements Serializable {
	protected static Logger logger = Logger.getLogger(HMMModel.class);
	private static final long serialVersionUID = -8307826997830463783L;
	String[] statusType;
	String[] observationType;
	double[] statusPrioriProbability;
	double[][] statusTransitionProbability;
	double[][] observationProbability;
	Hashtable<String, Integer> observationTypeIndex = new Hashtable<String, Integer>();

	/*
	 * //////
	 */
	public void train(List<String[]> observationList, List<String[]> statusList) throws SizeException {
		checkSize(observationList, statusList);
		String[] status = getAllStatus(statusList);
		Hashtable<String, Integer> statusFreq = getStatusFreq(status);
		calcStatusType(statusFreq);
		String[] observations = getAllObservation(observationList);
		this.observationType = calcObservationFreq(observations);
		Hashtable<String, Integer> transitionFreq = calcStatusTransitionFreq(status);
		this.statusPrioriProbability = calcStatusPrioriProbability(statusFreq);
		this.statusTransitionProbability = calcStatusTransitionProbability(statusFreq, transitionFreq);
		Hashtable<String, Integer> blockFreq = calcBlockFreq(observations, status);
		this.observationProbability = calcObservationProbability(blockFreq);
	}

	private void checkSize(List<String[]> observationList, List<String[]> statusList) throws SizeException {
		if (observationList.size() != statusList.size())
			throw new SizeException(""observation list size must be the same with status list size."");
		for (int i = 0; i < observationList.size(); i++) {
			if (observationList.get(i).length != statusList.get(i).length)
				throw new SizeException(""observation list size must be the same with status list size."");
		}
	}

	private Hashtable<String, Integer> getStatusFreq(String[] status) {
		Hashtable<String, Integer> statusFreq = new Hashtable<String, Integer>();
		for (int i = 1; i < status.length; i++) {
			if (statusFreq.containsKey(status[i])) {
				statusFreq.put(status[i], statusFreq.get(status[i]) + 1);
			} else {
				statusFreq.put(status[i], 1);
			}
		}
		return statusFreq;
	}

	private double[][] calcObservationProbability(Hashtable<String, Integer> blockFreq) {
		double[][] observationProbability = new double[observationType.length][statusType.length];
		for (int i = 0; i < observationType.length; i++) {
			String o = observationType[i];
			int total = 0;
			for (int j = 0; j < statusType.length; j++) {
				String chars = statusType[j];
				if (blockFreq.containsKey(o + ""/"" + chars)) {
					int num = blockFreq.get(o + ""/"" + chars);
					total += num;
				}
			}
			for (int j = 0; j < statusType.length; j++) {
				String chars = statusType[j];
				if (blockFreq.containsKey(o + ""/"" + chars)) {
					int numerator = blockFreq.get(o + ""/"" + chars);
					observationProbability[i][j] = (double) numerator / total;
				} else {
					observationProbability[i][j] = 0.00001;// avoid zero multiply
				}
			}
		}
		return observationProbability;
	}

	private Hashtable<String, Integer> calcBlockFreq(String[] observations, String[] status) {
		Hashtable<String, Integer> blockFreq = new Hashtable<String, Integer>();
		for (int i = 0; i < observations.length; i++) {
			String s = observations[i] + ""/"" + status[i];
			if (blockFreq.containsKey(s)) {
				blockFreq.put(s, blockFreq.get(s) + 1);
			} else {
				blockFreq.put(s, 1);
			}
		}
		return blockFreq;
	}

	private double[][] calcStatusTransitionProbability(Hashtable<String, Integer> statusFreq,
			Hashtable<String, Integer> transitionFreq) {
		double[][] transitionProbability = new double[statusType.length][statusType.length];
		for (int i = 0; i < statusType.length; i++) {
			for (int j = 0; j < statusType.length; j++) {
				String front = statusType[i];
				String last = statusType[j];
				if (transitionFreq.containsKey(front + "","" + last)) {
					int numerator = transitionFreq.get(front + "","" + last);
					int denominator = statusFreq.get(front);
					transitionProbability[i][j] = (double) numerator / denominator;
				}
			}
		}
		return transitionProbability;
	}

	private double[] calcStatusPrioriProbability(Hashtable<String, Integer> statusFreq) {
		double[] p = new double[statusType.length];
		int allPOSCount = 0;
		for (int i = 0; i < statusType.length; i++) {
			allPOSCount += statusFreq.get(statusType[i]);
		}
		for (int i = 0; i < statusType.length; i++) {
			p[i] = statusFreq.get(statusType[i]) * 1.0 / allPOSCount;
		}
		return p;
	}

	private Hashtable<String, Integer> calcStatusTransitionFreq(String[] status) {
		Hashtable<String, Integer> transitionFreq = new Hashtable<String, Integer>();
		for (int i = 0; i < status.length - 1; i++) {
			String temp = status[i] + "","" + status[i + 1];
			if (transitionFreq.containsKey(temp)) {
				transitionFreq.put(temp, transitionFreq.get(temp) + 1);
			} else {
				transitionFreq.put(temp, 1);
			}
		}
		return transitionFreq;
	}

	private String[] calcObservationFreq(String[] observations) {
		Hashtable<String, Integer> observationFreq = new Hashtable<String, Integer>();
		for (int i = 0; i < observations.length; i++) {
			if (observationFreq.containsKey(observations[i])) {
				observationFreq.put(observations[i], observationFreq.get(observations[i]) + 1);
			} else {
				observationFreq.put(observations[i], 1);
			}
		}
		int len = observationFreq.size();
		String[] observationType = new String[len];
		Enumeration<String> key = observationFreq.keys();
		for (int i = 0; i < observationFreq.size(); i++) {
			String str = (String) key.nextElement();
			observationType[i] = str;
			observationTypeIndex.put(str, i);
		}
		return observationType;
	}

	private String[] getAllObservation(List<String[]> observationList) {
		List<String> tempList = new ArrayList<String>();
		for (String[] ws : observationList)
			for (String s : ws)
				tempList.add(s);
		String[] observations = new String[tempList.size()];
		tempList.toArray(observations);
		return observations;
	}

	private String[] getAllStatus(List<String[]> statusList) {
		List<String> tempList = new ArrayList<String>();
		for (String[] contents : statusList)
			for (String c : contents)
				tempList.add(c);
		String[] status = new String[tempList.size()];
		tempList.toArray(status);
		return status;
	}

	private void calcStatusType(Hashtable<String, Integer> statusFreq) {
		int statusNum = statusFreq.size();
		this.statusType = new String[statusNum];
		Enumeration<String> key = statusFreq.keys();
		for (int i = 0; i < statusFreq.size(); i++) {
			statusType[i] = (String) key.nextElement();
		}
	}

}",src/main/java/com/seaboat/text/analyzer/ml/hmm/HMMModel.java
SimpleMDAGNode,"public class SimpleMDAGNode {
	//The character labeling an incoming transition to this node
	private final char letter;

	//The boolean denoting the accept state status of this node
	private final boolean isAcceptNode;

	//The int denoting the size of this node's outgoing transition set
	private final int transitionSetSize;

	//The int denoting the index (in the array which contains this node) at which this node's transition set begins
	private int transitionSetBeginIndex;

	/**
	 * Constructs a SimpleMDAGNode.
	 
	 * @param letter                a char representing the transition label leading to this SimpleMDAGNode    
	 * @param isAcceptNode          a boolean representing the accept state status of this SimpleMDAGNode
	 * @param transitionSetSize     an int denoting the size of this transition set
	 */
	public SimpleMDAGNode(char letter, boolean isAcceptNode, int transitionSetSize) {
		this.letter = letter;
		this.isAcceptNode = isAcceptNode;
		this.transitionSetSize = transitionSetSize;
		this.transitionSetBeginIndex = 0; //will be changed for all objects of this type, necessary for dummy root node creation
	}

	/**
	 * Retrieves the character representing the transition laben leading up to this node.
	 
	 * @return      the char representing the transition label leading up to this node
	 */
	public char getLetter() {
		return letter;
	}

	/**
	 * Retrieves the accept state status of this node.
	 
	 * @return      true if this node is an accept state, false otherwise
	 */
	public boolean isAcceptNode() {
		return isAcceptNode;
	}

	/**
	 * Retrieves the index in this node's containing array that its transition set begins at.
	 
	 * @return      an int of the index in this node's containing array at which its transition set begins
	 */
	public int getTransitionSetBeginIndex() {
		return transitionSetBeginIndex;
	}

	/**
	 * Retrieves the size of this node's outgoing transition set.
	 
	 * @return      an int denoting the size of this node's outgoing transition set
	 */
	public int getOutgoingTransitionSetSize() {
		return transitionSetSize;
	}

	/**
	 * Records the index in this node's containing array that its transition set begins at.
	 
	 * @param transitionSetBeginIndex       an int denoting the index in this node's containing array that is transition set beings at
	 */
	public void setTransitionSetBeginIndex(int transitionSetBeginIndex) {
		this.transitionSetBeginIndex = transitionSetBeginIndex;
	}

	/**
	 * Follows an outgoing transition from this node.
	 
	 * @param mdagDataArray     the array of SimpleMDAGNodes containing this node
	 * @param letter            the char representation of the desired transition's label
	 * @return                  the SimpleMDAGNode that is the target of the transition labeled with {@code letter},
	 *                          or null if there is no such labeled transition from this node
	 */
	public SimpleMDAGNode transition(SimpleMDAGNode[] mdagDataArray, char letter) {
		int onePastTransitionSetEndIndex = transitionSetBeginIndex + transitionSetSize;
		SimpleMDAGNode targetNode = null;

		//Loop through the SimpleMDAGNodes in this node's transition set, searching for
		//the one with a letter equal to that which labels the desired transition
		for (int i = transitionSetBeginIndex; i < onePastTransitionSetEndIndex; i++) {
			if (mdagDataArray[i].getLetter() == letter) {
				targetNode = mdagDataArray[i];
				break;
			}
		}
		/////

		return targetNode;
	}

	/**
	 * Follows a transition path starting from this node.
	 
	 * @param mdagDataArray     the array of SimpleMDAGNodes containing this node
	 * @param str               a String corresponding a transition path in the MDAG
	 * @return                  the SimpleMDAGNode at the end of the transition path corresponding to 
	 *                          {@code str}, or null if such a transition path is not present in the MDAG
	 */
	public SimpleMDAGNode transition(SimpleMDAGNode[] mdagDataArray, String str) {
		SimpleMDAGNode currentNode = this;
		int numberOfChars = str.length();

		//Iteratively transition through the MDAG using the chars in str
		for (int i = 0; i < numberOfChars; i++) {
			currentNode = currentNode.transition(mdagDataArray, str.charAt(i));
			if (currentNode == null)
				break;
		}
		/////

		return currentNode;
	}

	/**
	 * Follows a transition path starting from the source node of a MDAG.
	 
	 * @param mdagDataArray     the array containing the data of the MDAG to be traversed
	 * @param sourceNode        the dummy SimpleMDAGNode which functions as the source of the MDAG data in {@code mdagDataArray}
	 * @param str               a String corresponding to a transition path in the to-be-traversed MDAG
	 * @return                  the SimpleMDAGNode at the end of the transition path corresponding to 
	 *                          {@code str}, or null if such a transition path is not present in the MDAG
	 */
	public static SimpleMDAGNode traverseMDAG(SimpleMDAGNode[] mdagDataArray, SimpleMDAGNode sourceNode, String str) {
		char firstLetter = str.charAt(0);

		//Loop through the SimpleMDAGNodes in the processing MDAG's source node's transition set, 
		//searching for the the one with a letter (char) equal to the first char of str.
		//We can use that target node to transition through the MDAG with the rest of the string
		for (int i = 0; i < sourceNode.transitionSetSize; i++) {
			if (mdagDataArray[i].getLetter() == firstLetter)
				return mdagDataArray[i].transition(mdagDataArray, str.substring(1));
		}
		/////

		return null;
	}
}",src/main/java/com/seaboat/text/analyzer/data/structure/SimpleMDAGNode.java
MDAG,"public class MDAG {
	//MDAGNode from which all others in the structure are reachable (all manipulation and non-simplified MDAG search operations begin from this).
	private MDAGNode sourceNode = new MDAGNode(false);

	//SimpleMDAGNode from which all others in the structure are reachable (will be defined if this MDAG is simplified)
	private SimpleMDAGNode simplifiedSourceNode;

	//HashMap which contains the MDAGNodes collectively representing the all unique equivalence classes in the MDAG. 
	//Uniqueness is defined by the types of transitions allowed from, and number and type of nodes reachable
	//from the node of interest. Since there are no duplicate nodes in an MDAG, # of equivalence classes == # of nodes. 
	private HashMap<MDAGNode, MDAGNode> equivalenceClassMDAGNodeHashMap = new HashMap<MDAGNode, MDAGNode>();

	//Array that will contain a space-saving version of the MDAG after a call to simplify().
	private SimpleMDAGNode[] mdagDataArray;

	//HashSet which will contain the set of unique characters used as transition labels in the MDAG
	private TreeSet<Character> charTreeSet = new TreeSet<Character>();

	//An int denoting the total number of transitions between the nodes of the MDAG
	private int transitionCount;

	//Enum containing fields collectively denoting the set of all conditions that can be applied to a search on the MDAG
	private static enum SearchCondition {
		NO_SEARCH_CONDITION, PREFIX_SEARCH_CONDITION, SUBSTRING_SEARCH_CONDITION, SUFFIX_SEARCH_CONDITION;

		/**
		* Determines whether two Strings have a given type of relationship.
		
		* @param processingString      a String       
		* @param conditionString       a String      
		* @param searchCondition       an int denoting the type of condition to be satisfied
		* @return                      true if {@code processingString} has a relationship with 
		*                              {@code conditionString} described by the condition 
		*                              represented by {@code searchCondition}
		*/
		public boolean satisfiesCondition(String str1, String str2) {
			boolean satisfiesSearchCondition;

			switch (this) {
			case PREFIX_SEARCH_CONDITION:
				satisfiesSearchCondition = (str1.startsWith(str2));
				break;
			case SUBSTRING_SEARCH_CONDITION:
				satisfiesSearchCondition = (str1.contains(str2));
				break;
			case SUFFIX_SEARCH_CONDITION:
				satisfiesSearchCondition = (str1.endsWith(str2));
				break;
			default:
				satisfiesSearchCondition = true;
				break;
			}

			return satisfiesSearchCondition;
		}
	};
	/////

	/**
	 * Creates an MDAG from a newline delimited file containing the data of interest.
	 
	 * @param dataFile          a {@link java.io.File} representation of a file
	 *                          containing the Strings that the MDAG will contain 
	 * @throws IOException      if {@code datafile} cannot be opened, or a read operation on it cannot be carried out
	 */
	public MDAG(File dataFile) throws IOException {
		BufferedReader dataFileBufferedReader = new BufferedReader(new FileReader(dataFile));
		String currentString = """";
		String previousString = """";

		//Read all the lines in dataFile and add the String contained in each to the MDAG.
		while ((currentString = dataFileBufferedReader.readLine()) != null) {
			int mpsIndex = calculateMinimizationProcessingStartIndex(previousString, currentString);

			//If the transition path of the previousString needs to be examined for minimization or 
			//equivalence class representation after a certain point, call replaceOrRegister to do so.
			if (mpsIndex != -1) {
				String transitionSubstring = previousString.substring(0, mpsIndex);
				String minimizationProcessingSubstring = previousString.substring(mpsIndex);
				replaceOrRegister(sourceNode.transition(transitionSubstring), minimizationProcessingSubstring);
			}
			/////

			addStringInternal(currentString);
			previousString = currentString;
		}
		/////

		//Since we delay the minimization of the previously-added String
		//until after we read the next one, we need to have a seperate
		//statement to minimize the absolute last String.
		replaceOrRegister(sourceNode, previousString);
		dataFileBufferedReader.close();
	}

	/**
	* Creates an MDAG from a collection of Strings.
	
	* @param strCollection     a {@link java.util.Collection} containing Strings that the MDAG will contain
	*/
	public MDAG(Collection<String> strCollection) {
		addStrings(strCollection);
	}

	/**
	 * Adds a Collection of Strings to the MDAG.
	 
	 * @param strCollection     a {@link java.util.Collection} containing Strings to be added to the MDAG
	 */
	public final void addStrings(Collection<String> strCollection) {
		if (sourceNode != null) {
			String previousString = """";

			//Add all the Strings in strCollection to the MDAG.
			for (String currentString : strCollection) {
				int mpsIndex = calculateMinimizationProcessingStartIndex(previousString, currentString);

				//If the transition path of the previousString needs to be examined for minimization or 
				//equivalence class representation after a certain point, call replaceOrRegister to do so.
				if (mpsIndex != -1) {

					String transitionSubstring = previousString.substring(0, mpsIndex);
					String minimizationProcessingSubString = previousString.substring(mpsIndex);
					replaceOrRegister(sourceNode.transition(transitionSubstring), minimizationProcessingSubString);
				}
				/////

				addStringInternal(currentString);
				previousString = currentString;
			}
			/////

			//Since we delay the minimization of the previously-added String
			//until after we read the next one, we need to have a seperate
			//statement to minimize the absolute last String.
			replaceOrRegister(sourceNode, previousString);
		} else
			throw new UnsupportedOperationException(""MDAG is simplified. Unable to add additional Strings."");
	}

	/**
	 * Adds a string to the MDAG.
	 
	 * @param str       the String to be added to the MDAG 
	 */
	public void addString(String str) {
		if (sourceNode != null) {
			addStringInternal(str);
			replaceOrRegister(sourceNode, str);
		} else
			throw new UnsupportedOperationException(""MDAG is simplified. Unable to add additional Strings."");
	}

	private void splitTransitionPath(MDAGNode originNode, String storedStringSubstr) {
		HashMap<String, Object> firstConfluenceNodeDataHashMap = getTransitionPathFirstConfluenceNodeData(originNode,
				storedStringSubstr);
		Integer toFirstConfluenceNodeTransitionCharIndex = (Integer) firstConfluenceNodeDataHashMap
				.get(""toConfluenceNodeTransitionCharIndex"");
		MDAGNode firstConfluenceNode = (MDAGNode) firstConfluenceNodeDataHashMap.get(""confluenceNode"");

		if (firstConfluenceNode != null) {
			MDAGNode firstConfluenceNodeParent = originNode
					.transition(storedStringSubstr.substring(0, toFirstConfluenceNodeTransitionCharIndex));

			MDAGNode firstConfluenceNodeClone = firstConfluenceNode.clone(firstConfluenceNodeParent,
					storedStringSubstr.charAt(toFirstConfluenceNodeTransitionCharIndex));

			transitionCount += firstConfluenceNodeClone.getOutgoingTransitionCount();

			String unprocessedSubString = storedStringSubstr.substring(toFirstConfluenceNodeTransitionCharIndex + 1);
			splitTransitionPath(firstConfluenceNodeClone, unprocessedSubString);
		}
	}

	/**
	 * Calculates the length of the the sub-path in a transition path, that is used only by a given string.
	 
	 * @param str       a String corresponding to a transition path from sourceNode
	 * @return          an int denoting the size of the sub-path in the transition path
	 *                  corresponding to {@code str} that is only used by {@code str}
	 */
	private int calculateSoleTransitionPathLength(String str) {
		Stack<MDAGNode> transitionPathNodeStack = sourceNode.getTransitionPathNodes(str);
		transitionPathNodeStack.pop(); //The MDAGNode at the top of the stack is not needed
										//(we are processing the outgoing transitions of nodes inside str's transition path,
										//the outgoing transitions of the MDAGNode at the top of the stack are outside this path)

		transitionPathNodeStack.trimToSize();

		//Process each node in transitionPathNodeStack, using each to determine whether the
		//transition path corresponding to str is only used by str.  This is true if and only if
		//each node in the transition path has a single outgoing transition and is not an accept state.
		while (!transitionPathNodeStack.isEmpty()) {
			MDAGNode currentNode = transitionPathNodeStack.peek();
			if (currentNode.getOutgoingTransitions().size() <= 1 && !currentNode.isAcceptNode())
				transitionPathNodeStack.pop();
			else
				break;
		}
		/////

		return (transitionPathNodeStack.capacity() - transitionPathNodeStack.size());
	}

	/**
	 * Removes a String from the MDAG.
	 
	 * @param str       the String to be removed from the MDAG 
	 */
	public void removeString(String str) {
		if (sourceNode != null) {
			//Split the transition path corresponding to str to ensure that
			//any other transition paths sharing nodes with it are not affected
			splitTransitionPath(sourceNode, str);

			//Remove from equivalenceClassMDAGNodeHashMap, the entries of all the nodes in the transition path corresponding to str.
			removeTransitionPathRegisterEntries(str);

			//Get the last node in the transition path corresponding to str
			MDAGNode strEndNode = sourceNode.transition(str);

			if (!strEndNode.hasOutgoingTransitions()) {
				int soleInternalTransitionPathLength = calculateSoleTransitionPathLength(str);
				int internalTransitionPathLength = str.length() - 1;

				if (soleInternalTransitionPathLength == internalTransitionPathLength) {
					sourceNode.removeOutgoingTransition(str.charAt(0));
					transitionCount -= str.length();
				} else {
					//Remove the sub-path in str's transition path that is only used by str
					int toBeRemovedTransitionLabelCharIndex = (internalTransitionPathLength
							- soleInternalTransitionPathLength);
					MDAGNode latestNonSoloTransitionPathNode = sourceNode
							.transition(str.substring(0, toBeRemovedTransitionLabelCharIndex));
					latestNonSoloTransitionPathNode
							.removeOutgoingTransition(str.charAt(toBeRemovedTransitionLabelCharIndex));
					transitionCount -= str.substring(toBeRemovedTransitionLabelCharIndex).length();
					/////

					replaceOrRegister(sourceNode, str.substring(0, toBeRemovedTransitionLabelCharIndex));
				}

			} else {
				strEndNode.setAcceptStateStatus(false);
				replaceOrRegister(sourceNode, str);
			}
		} else
			throw new UnsupportedOperationException(""MDAG is simplified. Unable to remove any Strings."");
	}

	/**
	 * Determines the start index of the substring in the String most recently added to the MDAG
	 * that corresponds to the transition path that will be next up for minimization processing.
	 *
	 * The ""minimization processing start index"" is defined as the index in {@code prevStr} which starts the substring
	 * corresponding to the transition path that doesn't have its right language extended by {@code currStr}. The transition path of
	 * the substring before this point is not considered for minimization in order to limit the amount of times the
	 * equivalence classes of its nodes will need to be reassigned during the processing of Strings which share prefixes.
	 
	 * @param prevStr       the String most recently added to the MDAG
	 * @param currStr       the String next to be added to the MDAG
	 * @return              an int of the index in {@code prevStr} that starts the substring corresponding
	 *                      to the transition path next up for minimization processing 
	 */
	public int calculateMinimizationProcessingStartIndex(String prevStr, String currStr) {
		int mpsIndex;

		if (!currStr.startsWith(prevStr)) {
			//Loop through the corresponding indices of both Strings in search of the first index containing differing characters.
			//The transition path of the substring of prevStr from this point will need to be submitted for minimization processing.
			//The substring before this point, however, does not, since currStr will simply be extending the right languages of the 
			//nodes on its transition path.
			int shortestStringLength = Math.min(prevStr.length(), currStr.length());
			for (mpsIndex = 0; mpsIndex < shortestStringLength
					&& prevStr.charAt(mpsIndex) == currStr.charAt(mpsIndex); mpsIndex++) {
			}
			;
			/////
		} else
			mpsIndex = -1; //If the prevStr is a prefix of currStr, then currStr simply extends the right language of the transition path of prevStr. 

		return mpsIndex;
	}

	/**
	 * Determines the longest prefix of a given String that is
	 * the prefix of another String previously added to the MDAG.
	 
	 * @param str       the String to be processed 
	 * @return          a String of the longest prefix of {@code str} 
	 *                  that is also a prefix of a String contained in the MDAG
	 */
	public String determineLongestPrefixInMDAG(String str) {
		MDAGNode currentNode = sourceNode;
		int numberOfChars = str.length();
		int onePastPrefixEndIndex = 0;

		//Loop through the characters in str, using them in sequence to transition
		//through the MDAG until the currently processing node doesn't have a transition
		//labeled with the current processing char, or there are no more characters to process. 
		for (int i = 0; i < numberOfChars; i++, onePastPrefixEndIndex++) {
			char currentChar = str.charAt(i);
			if (currentNode.hasOutgoingTransition(currentChar))
				currentNode = currentNode.transition(currentChar);
			else
				break;
		}
		/////

		return str.substring(0, onePastPrefixEndIndex);
	}

	/**
	 * Determines and retrieves data related to the first confluence node 
	 * (defined as a node with two or more incoming transitions) of a
	 * transition path corresponding to a given String from a given node.
	 
	 * @param originNode        the MDAGNode from which the transition path corresponding to str starts from
	 * @param str               a String corresponding to a transition path in the MDAG
	 * @return                  a HashMap of Strings to Objects containing:
	 *                              - an int denoting the length of the path to the first confluence node in the transition path of interest
	 *                              - the MDAGNode which is the first confluence node in the transition path of interest (or null if one does not exist)
	 */
	public HashMap<String, Object> getTransitionPathFirstConfluenceNodeData(MDAGNode originNode, String str) {
		int currentIndex = 0;
		int charCount = str.length();
		MDAGNode currentNode = originNode;

		//Loop thorugh the characters in str, sequentially using them to transition through the MDAG in search of
		//(and breaking upon reaching) the first node that is the target of two or more transitions. The loop is 
		//also broken from if the currently processing node doesn't have a transition labeled with the currently processing char.
		for (; currentIndex < charCount; currentIndex++) {
			char currentChar = str.charAt(currentIndex);
			currentNode = (currentNode.hasOutgoingTransition(currentChar) ? currentNode.transition(currentChar) : null);

			if (currentNode == null || currentNode.isConfluenceNode())
				break;
		}
		/////

		boolean noConfluenceNode = (currentNode == originNode || currentIndex == charCount);

		//Create a HashMap containing the index of the last char in the substring corresponding
		//to the transitoin path to the confluence node, as well as the actual confluence node
		HashMap<String, Object> confluenceNodeDataHashMap = new HashMap<String, Object>(2);
		confluenceNodeDataHashMap.put(""toConfluenceNodeTransitionCharIndex"", (noConfluenceNode ? null : currentIndex));
		confluenceNodeDataHashMap.put(""confluenceNode"", noConfluenceNode ? null : currentNode);
		/////

		return confluenceNodeDataHashMap;
	}

	/**
	 * Performs minimization processing on a transition path starting from a given node.
	 *
	 * This entails either replacing a node in the path with one that has an equivalent right language/equivalence class
	 * (defined as set of transition paths that can be traversed and nodes able to be reached from it), or making it
	 * a representative of a right language/equivalence class if a such a node does not already exist.
	 
	 * @param originNode        the MDAGNode that the transition path corresponding to str starts from
	 * @param str              a String related to a transition path
	 */
	private void replaceOrRegister(MDAGNode originNode, String str) {
		char transitionLabelChar = str.charAt(0);
		MDAGNode relevantTargetNode = originNode.transition(transitionLabelChar);

		//If relevantTargetNode has transitions and there is at least one char left to process, recursively call 
		//this on the next char in order to further processing down the transition path corresponding to str
		if (relevantTargetNode.hasOutgoingTransitions() && !str.substring(1).isEmpty())
			replaceOrRegister(relevantTargetNode, str.substring(1));
		/////

		//Get the node representing the equivalence class that relevantTargetNode belongs to. MDAGNodes hash on the
		//transitions paths that can be traversed from them and nodes able to be reached from them;
		//nodes with the same equivalence classes will hash to the same bucket.
		MDAGNode equivalentNode = equivalenceClassMDAGNodeHashMap.get(relevantTargetNode);

		if (equivalentNode == null) //if there is no node with the same right language as relevantTargetNode
			equivalenceClassMDAGNodeHashMap.put(relevantTargetNode, relevantTargetNode);
		else if (equivalentNode != relevantTargetNode) //if there is another node with the same right language as relevantTargetNode, reassign the  
		{ //transition between originNode and relevantTargetNode, to originNode and the node representing the equivalence class of interest
			relevantTargetNode.decrementTargetIncomingTransitionCounts();
			transitionCount -= relevantTargetNode.getOutgoingTransitionCount(); //Since this method is recursive, the outgoing transitions of all of relevantTargetNode's child nodes have already been reassigned, 
																				//so we only need to decrement the transition count by the relevantTargetNode's outgoing transition count
			originNode.reassignOutgoingTransition(transitionLabelChar, relevantTargetNode, equivalentNode);
		}
	}

	/**
	 * Adds a transition path starting from a specific node in the MDAG.
	 
	 * @param originNode    the MDAGNode which will serve as the start point of the to-be-created transition path
	 * @param str           the String to be used to create a new transition path from {@code originNode}
	 */
	private void addTransitionPath(MDAGNode originNode, String str) {
		if (!str.isEmpty()) {
			MDAGNode currentNode = originNode;
			int charCount = str.length();

			//Loop through the characters in str, iteratevely adding
			// a transition path corresponding to it from originNode
			for (int i = 0; i < charCount; i++, transitionCount++) {
				char currentChar = str.charAt(i);
				boolean isLastChar = (i == charCount - 1);
				currentNode = currentNode.addOutgoingTransition(currentChar, isLastChar);

				charTreeSet.add(currentChar);
			}
			/////
		} else
			originNode.setAcceptStateStatus(true);
	}

	/**
	 * Removes from equivalenceClassMDAGNodeHashmap the entries of all the nodes in a transition path.
	 
	 * @param str       a String corresponding to a transition path from sourceNode
	 */
	private void removeTransitionPathRegisterEntries(String str) {
		MDAGNode currentNode = sourceNode;

		int charCount = str.length();

		for (int i = 0; i < charCount; i++) {
			currentNode = currentNode.transition(str.charAt(i));
			if (equivalenceClassMDAGNodeHashMap.get(currentNode) == currentNode)
				equivalenceClassMDAGNodeHashMap.remove(currentNode);

			//The hashCode of an MDAGNode is cached the first time a hash is performed without a cache value present.
			//Since we just hashed currentNode, we must clear this regardless of its presence in equivalenceClassMDAGNodeHashMap
			//since we're not actually declaring equivalence class representatives here.
			currentNode.clearStoredHashCode();
		}
	}

	/**
	 * Clones a transition path from a given node.
	 
	 * @param pivotConfluenceNode               the MDAGNode that the cloning operation is to be based from
	 * @param transitionStringToPivotNode       a String which corresponds with a transition path from souceNode to {@code pivotConfluenceNode}
	 * @param str                               a String which corresponds to the transition path from {@code pivotConfluenceNode} that is to be cloned
	 */
	private void cloneTransitionPath(MDAGNode pivotConfluenceNode, String transitionStringToPivotNode, String str) {
		MDAGNode lastTargetNode = pivotConfluenceNode.transition(str); //Will store the last node which was used as the base of a cloning operation
		MDAGNode lastClonedNode = null; //Will store the last cloned node
		char lastTransitionLabelChar = '\0'; //Will store the char which labels the transition to lastTargetNode from its parent node in the prefixString's transition path 

		//Loop backwards through the indices of str, using each as a boundary to create substrings of str of decreasing length
		//which will be used to transition to, and duplicate the nodes in the transition path of str from pivotConfluenceNode.
		for (int i = str.length(); i >= 0; i--) {
			String currentTransitionString = (i > 0 ? str.substring(0, i) : null);
			MDAGNode currentTargetNode = (i > 0 ? pivotConfluenceNode.transition(currentTransitionString)
					: pivotConfluenceNode);
			MDAGNode clonedNode;

			if (i == 0) //if we have reached pivotConfluenceNode
			{
				//Clone pivotConfluenceNode in a way that reassigns the transition of its parent node (in transitionStringToConfluenceNode's path) to the clone.
				String transitionStringToPivotNodeParent = transitionStringToPivotNode.substring(0,
						transitionStringToPivotNode.length() - 1);
				char parentTransitionLabelChar = transitionStringToPivotNode
						.charAt(transitionStringToPivotNode.length() - 1);
				clonedNode = pivotConfluenceNode.clone(sourceNode.transition(transitionStringToPivotNodeParent),
						parentTransitionLabelChar);
				/////
			} else
				clonedNode = currentTargetNode.clone(); //simply clone curentTargetNode

			transitionCount += clonedNode.getOutgoingTransitionCount();

			//If this isn't the first node we've cloned, reassign clonedNode's transition labeled
			//with the lastTransitionChar (which points to the last targetNode) to the last clone.
			if (lastClonedNode != null) {
				clonedNode.reassignOutgoingTransition(lastTransitionLabelChar, lastTargetNode, lastClonedNode);
				lastTargetNode = currentTargetNode;
			}

			//Store clonedNode and the char which labels the transition between the node it was cloned from (currentTargetNode) and THAT node's parent.
			//These will be used to establish an equivalent transition to clonedNode from the next clone to be created (it's clone parent).
			lastClonedNode = clonedNode;
			lastTransitionLabelChar = (i > 0 ? str.charAt(i - 1) : '\0');
			/////
		}
		/////
	}

	/**
	 * Adds a String to the MDAG (called by addString to do actual MDAG manipulation).
	 
	 * @param str       the String to be added to the MDAG
	 */
	private void addStringInternal(String str) {
		String prefixString = determineLongestPrefixInMDAG(str);
		String suffixString = str.substring(prefixString.length());

		//Retrive the data related to the first confluence node (a node with two or more incoming transitions)
		//in the transition path from sourceNode corresponding to prefixString.
		HashMap<String, Object> firstConfluenceNodeDataHashMap = getTransitionPathFirstConfluenceNodeData(sourceNode,
				prefixString);
		MDAGNode firstConfluenceNodeInPrefix = (MDAGNode) firstConfluenceNodeDataHashMap.get(""confluenceNode"");
		Integer toFirstConfluenceNodeTransitionCharIndex = (Integer) firstConfluenceNodeDataHashMap
				.get(""toConfluenceNodeTransitionCharIndex"");
		/////

		//Remove the register entries of all the nodes in the prefixString transition path up to the first confluence node 
		//(those past the confluence node will not need to be removed since they will be cloned and unaffected by the 
		//addition of suffixString). If there is no confluence node in prefixString, then remove the register entries in prefixString's entire transition path
		removeTransitionPathRegisterEntries((toFirstConfluenceNodeTransitionCharIndex == null ? prefixString
				: prefixString.substring(0, toFirstConfluenceNodeTransitionCharIndex)));

		//If there is a confluence node in the prefix, we must duplicate the transition path
		//of the prefix starting from that node, before we add suffixString (to the duplicate path).
		//This ensures that we do not disturb the other transition paths containing this node.
		if (firstConfluenceNodeInPrefix != null) {
			String transitionStringOfPathToFirstConfluenceNode = prefixString.substring(0,
					toFirstConfluenceNodeTransitionCharIndex + 1);
			String transitionStringOfToBeDuplicatedPath = prefixString
					.substring(toFirstConfluenceNodeTransitionCharIndex + 1);
			cloneTransitionPath(firstConfluenceNodeInPrefix, transitionStringOfPathToFirstConfluenceNode,
					transitionStringOfToBeDuplicatedPath);
		}
		/////

		//Add the transition based on suffixString to the end of the (possibly duplicated) transition path corresponding to prefixString
		addTransitionPath(sourceNode.transition(prefixString), suffixString);
	}

	/**
	 * Creates a SimpleMDAGNode version of an MDAGNode's outgoing transition set in mdagDataArray.
	 
	 * @param node                                      the MDAGNode containing the transition set to be inserted in to {@code mdagDataArray}
	 * @param mdagDataArray                             an array of SimpleMDAGNodes containing a subset of the data of the MDAG
	 * @param onePastLastCreatedConnectionSetIndex      an int of the index in {@code mdagDataArray} that the outgoing transition set of {@code node} is to start from 
	 * @return                                          an int of one past the end of the transition set located farthest in {@code mdagDataArray}
	 */
	private int createSimpleMDAGTransitionSet(MDAGNode node, SimpleMDAGNode[] mdagDataArray,
			int onePastLastCreatedTransitionSetIndex) {
		int pivotIndex = onePastLastCreatedTransitionSetIndex;
		node.setTransitionSetBeginIndex(pivotIndex);

		onePastLastCreatedTransitionSetIndex += node.getOutgoingTransitionCount();

		//Create a SimpleMDAGNode representing each transition label/target combo in transitionTreeMap, recursively calling this method (if necessary)
		//to set indices in these SimpleMDAGNodes that the set of transitions emitting from their respective transition targets starts from.
		TreeMap<Character, MDAGNode> transitionTreeMap = node.getOutgoingTransitions();
		for (Entry<Character, MDAGNode> transitionKeyValuePair : transitionTreeMap.entrySet()) {
			//Use the current transition's label and target node to create a SimpleMDAGNode
			//(which is a space-saving representation of the transition), and insert it in to mdagDataArray
			char transitionLabelChar = transitionKeyValuePair.getKey();
			MDAGNode transitionTargetNode = transitionKeyValuePair.getValue();
			mdagDataArray[pivotIndex] = new SimpleMDAGNode(transitionLabelChar, transitionTargetNode.isAcceptNode(),
					transitionTargetNode.getOutgoingTransitionCount());
			/////

			//If targetTransitionNode's outgoing transition set hasn't been inserted in to mdagDataArray yet, call this method on it to do so. 
			//After this call returns, transitionTargetNode will contain the index in mdagDataArray that its transition set starts from
			if (transitionTargetNode.getTransitionSetBeginIndex() == -1)
				onePastLastCreatedTransitionSetIndex = createSimpleMDAGTransitionSet(transitionTargetNode,
						mdagDataArray, onePastLastCreatedTransitionSetIndex);

			mdagDataArray[pivotIndex++].setTransitionSetBeginIndex(transitionTargetNode.getTransitionSetBeginIndex());
		}
		/////

		return onePastLastCreatedTransitionSetIndex;
	}

	/**
	 * Creates a space-saving version of the MDAG in the form of an array. 
	 * Once the MDAG is simplified, Strings can no longer be added to or removed from it.
	 */
	public void simplify() {
		if (sourceNode != null) {
			mdagDataArray = new SimpleMDAGNode[transitionCount];
			createSimpleMDAGTransitionSet(sourceNode, mdagDataArray, 0);
			simplifiedSourceNode = new SimpleMDAGNode('\0', false, sourceNode.getOutgoingTransitionCount());

			//Mark the previous MDAG data structure and equivalenceClassMDAGNodeHashMap
			//for garbage collection since they are no longer needed.
			sourceNode = null;
			equivalenceClassMDAGNodeHashMap = null;
			/////
		}
	}

	/**
	 * Determines whether a String is present in the MDAG.
	 
	 * @param str       the String to be searched for
	 * @return          true if {@code str} is present in the MDAG, and false otherwise
	 */
	public boolean contains(String str) {
		if (sourceNode != null) //if the MDAG hasn't been simplified
		{
			MDAGNode targetNode = sourceNode.transition(str);
			return (targetNode != null && targetNode.isAcceptNode());
		} else {
			SimpleMDAGNode targetNode = SimpleMDAGNode.traverseMDAG(mdagDataArray, simplifiedSourceNode, str);
			return (targetNode != null && targetNode.isAcceptNode());
		}
	}

	/**
	 * Retrieves Strings corresponding to all valid transition paths from a given node that satisfy a given condition.
	 
	 * @param strHashSet                a HashSet of Strings to contain all those in the MDAG satisfying
	 *                                  {@code searchCondition} with {@code conditionString}
	 * @param searchCondition           the SearchCondition enum field describing the type of relationship that Strings contained in the MDAG 
	 *                                  must have with {@code conditionString} in order to be included in the result set
	 * @param searchConditionString     the String that all Strings in the MDAG must be related with in the fashion denoted 
	 *                                  by {@code searchCondition} in order to be included in the result set
	 * @param prefixString              the String corresponding to the currently traversed transition path
	 * @param transitionTreeMap         a TreeMap of Characters to MDAGNodes collectively representing an MDAGNode's transition set
	 */
	private void getStrings(HashSet<String> strHashSet, SearchCondition searchCondition, String searchConditionString,
			String prefixString, TreeMap<Character, MDAGNode> transitionTreeMap) {
		//Traverse all the valid transition paths beginning from each transition in transitionTreeMap, inserting the
		//corresponding Strings in to strHashSet that have the relationship with conditionString denoted by searchCondition
		for (Entry<Character, MDAGNode> transitionKeyValuePair : transitionTreeMap.entrySet()) {
			String newPrefixString = prefixString + transitionKeyValuePair.getKey();
			MDAGNode currentNode = transitionKeyValuePair.getValue();

			if (currentNode.isAcceptNode()
					&& searchCondition.satisfiesCondition(newPrefixString, searchConditionString))
				strHashSet.add(newPrefixString);

			//Recursively call this to traverse all the valid transition paths from currentNode
			getStrings(strHashSet, searchCondition, searchConditionString, newPrefixString,
					currentNode.getOutgoingTransitions());
		}
		/////
	}

	/**
	 * Retrieves Strings corresponding to all valid transition paths from a given node that satisfy a given condition.
	 
	 * @param strHashSet                    a HashSet of Strings to contain all those in the MDAG satisfying
	 *                                      {@code searchCondition} with {@code conditionString}
	 * @param searchCondition               the SearchCondition enum field describing the type of relationship that Strings contained in the MDAG 
	 *                                      must have with {@code conditionString} in order to be included in the result set
	 * @param searchConditionString         the String that all Strings in the MDAG must be related with in the fashion denoted 
	 *                                      by {@code searchCondition} in order to be included in the result set
	 * @param prefixString                  the String corresponding to the currently traversed transition path
	 * @param transitionSetBegin            an int denoting the starting index of a SimpleMDAGNode's transition set in mdagDataArray
	 * @param onePastTransitionSetEnd       an int denoting one past the last index of a simpleMDAGNode's transition set in mdagDataArray
	 */
	private void getStrings(HashSet<String> strHashSet, SearchCondition searchCondition, String searchConditionString,
			String prefixString, SimpleMDAGNode node) {
		int transitionSetBegin = node.getTransitionSetBeginIndex();
		int onePastTransitionSetEnd = transitionSetBegin + node.getOutgoingTransitionSetSize();

		//Traverse all the valid transition paths beginning from each transition in transitionTreeMap, inserting the
		//corresponding Strings in to strHashSet that have the relationship with conditionString denoted by searchCondition
		for (int i = transitionSetBegin; i < onePastTransitionSetEnd; i++) {
			SimpleMDAGNode currentNode = mdagDataArray[i];
			String newPrefixString = prefixString + currentNode.getLetter();

			if (currentNode.isAcceptNode()
					&& searchCondition.satisfiesCondition(newPrefixString, searchConditionString))
				strHashSet.add(newPrefixString);

			//Recursively call this to traverse all the valid transition paths from currentNode
			getStrings(strHashSet, searchCondition, searchConditionString, newPrefixString, currentNode);
		}
		/////
	}

	/**
	 * Retrieves all the valid Strings that have been inserted in to the MDAG.
	 
	 * @return      a HashSet containing all the Strings that have been inserted into the MDAG
	 */
	public HashSet<String> getAllStrings() {
		HashSet<String> strHashSet = new HashSet<String>();

		if (sourceNode != null)
			getStrings(strHashSet, SearchCondition.NO_SEARCH_CONDITION, null, """", sourceNode.getOutgoingTransitions());
		else
			getStrings(strHashSet, SearchCondition.NO_SEARCH_CONDITION, null, """", simplifiedSourceNode);

		return strHashSet;
	}

	/**
	 * Retrieves all the Strings in the MDAG that begin with a given String.
	 
	 * @param prefixStr     a String that is the prefix for all the desired Strings
	 * @return              a HashSet containing all the Strings present in the MDAG that begin with {@code prefixString}       
	 */
	public HashSet<String> getStringsStartingWith(String prefixStr) {
		HashSet<String> strHashSet = new HashSet<String>();

		if (sourceNode != null) //if the MDAG hasn't been simplified
		{
			MDAGNode originNode = sourceNode.transition(prefixStr); //attempt to transition down the path denoted by prefixStr

			if (originNode != null) //if there a transition path corresponding to prefixString (one or more stored Strings begin with prefixString)
			{
				if (originNode.isAcceptNode())
					strHashSet.add(prefixStr);
				getStrings(strHashSet, SearchCondition.PREFIX_SEARCH_CONDITION, prefixStr, prefixStr,
						originNode.getOutgoingTransitions()); //retrieve all Strings that extend the transition path denoted by prefixStr
			}
		} else {
			SimpleMDAGNode originNode = SimpleMDAGNode.traverseMDAG(mdagDataArray, simplifiedSourceNode, prefixStr); //attempt to transition down the path denoted by prefixStr

			if (originNode != null) //if there a transition path corresponding to prefixString (one or more stored Strings begin with prefixStr)
			{
				if (originNode.isAcceptNode())
					strHashSet.add(prefixStr);
				getStrings(strHashSet, SearchCondition.PREFIX_SEARCH_CONDITION, prefixStr, prefixStr, originNode); //retrieve all Strings that extend the transition path denoted by prefixString
			}
		}

		return strHashSet;
	}

	/**
	 * Retrieves all the Strings in the MDAG that contain a given String.
	 
	 * @param str       a String that is contained in all the desired Strings
	 * @return          a HashSet containing all the Strings present in the MDAG that begin with {@code prefixString}
	 */
	public HashSet<String> getStringsWithSubstring(String str) {
		HashSet<String> strHashSet = new HashSet<String>();

		if (sourceNode != null) //if the MDAG hasn't been simplified
			getStrings(strHashSet, SearchCondition.SUBSTRING_SEARCH_CONDITION, str, """",
					sourceNode.getOutgoingTransitions());
		else
			getStrings(strHashSet, SearchCondition.SUBSTRING_SEARCH_CONDITION, str, """", simplifiedSourceNode);

		return strHashSet;
	}

	/**
	* Retrieves all the Strings in the MDAG that begin with a given String.
	
	* @param suffixString      a String that is the suffix for all the desired Strings
	* @return                  a HashSet containing all the Strings present in the MDAG that end with {@code suffixStr}       
	*/
	public HashSet<String> getStringsEndingWith(String suffixStr) {
		HashSet<String> strHashSet = new HashSet<String>();

		if (sourceNode != null) //if the MDAG hasn't been simplified
			getStrings(strHashSet, SearchCondition.SUFFIX_SEARCH_CONDITION, suffixStr, """",
					sourceNode.getOutgoingTransitions());
		else
			getStrings(strHashSet, SearchCondition.SUFFIX_SEARCH_CONDITION, suffixStr, """", simplifiedSourceNode);

		return strHashSet;
	}

	/**
	 * Returns the MDAG's source node.
	
	 * @return      the MDAGNode or SimpleMDAGNode functioning as the MDAG's source node.
	 */
	public Object getSourceNode() {
		return (sourceNode != null ? sourceNode : simplifiedSourceNode);
	}

	/**
	 * Returns the array of SimpleMDAGNodes collectively containing the 
	 * data of this MDAG, or null if it hasn't been simplified yet.
	 
	 * @return      the array of SimpleMDAGNodes collectively containing the data of this MDAG
	 *              if this MDAG has been simplified, or null if it has not
	 */
	public SimpleMDAGNode[] getSimpleMDAGArray() {
		return mdagDataArray;
	}

	/**
	 * Procures the set of characters which collectively label the MDAG's transitions.
	 
	 * @return      a TreeSet of chars which collectively label all the transitions in the MDAG
	 */
	public TreeSet<Character> getTransitionLabelSet() {
		return charTreeSet;
	}

	/**
	 * Determines if a child node object is accepting.
	  
	 * @param nodeObj                       an Object
	 * @return                              if {@code nodeObj} is either an MDAGNode or a SimplifiedMDAGNode,
	 *                                      true if the node is accepting, false otherwise
	 * throws IllegalArgumentException      if {@code nodeObj} is not an MDAGNode or SimplifiedMDAGNode
	 */
	public static boolean isAcceptNode(Object nodeObj) {
		if (nodeObj != null) {
			Class<? extends Object> nodeObjClass = nodeObj.getClass();

			if (nodeObjClass.equals(MDAGNode.class))
				return ((MDAGNode) nodeObj).isAcceptNode();
			else if (nodeObjClass.equals(SimpleMDAGNode.class))
				return ((SimpleMDAGNode) nodeObj).isAcceptNode();

		}

		throw new IllegalArgumentException(""Argument is not an MDAGNode or SimpleMDAGNode"");
	}

	private int countNodes(MDAGNode originNode, HashSet<Integer> nodeIDHashSet) {
		if (originNode != sourceNode)
			nodeIDHashSet.add(originNode.id);

		TreeMap<Character, MDAGNode> transitionTreeMap = originNode.getOutgoingTransitions();

		for (Entry<Character, MDAGNode> transitionKeyValuePair : transitionTreeMap.entrySet())
			countNodes(transitionKeyValuePair.getValue(), nodeIDHashSet);

		return nodeIDHashSet.size();
	}

	public int getNodeCount() {
		return countNodes(sourceNode, new HashSet<Integer>());
	}

	public int getEquivalenceClassCount() {
		return equivalenceClassMDAGNodeHashMap.size();
	}

	public int getTransitionCount() {
		return transitionCount;
	}
}",src/main/java/com/seaboat/text/analyzer/data/structure/MDAG.java
MDAGNode,"public class MDAGNode {
	public int id;

	//The boolean denoting the accept state status of this node
	private boolean isAcceptNode;

	//The TreeMap to containing entries that represent a transition (label and target node)
	private final TreeMap<Character, MDAGNode> outgoingTransitionTreeMap;

	//The int representing this node's incoming transition node count
	private int incomingTransitionCount = 0;

	//The int denoting index in a simplified mdag data array that this node's transition set begins at
	private int transitionSetBeginIndex = -1;

	//The int which will store this node's hash code after its been calculated (necessary due to how expensive the hashing calculation is)
	private Integer storedHashCode = null;

	/**
	 * Constructs an MDAGNode.
	 
	 * @param isAcceptNode     a boolean denoting the accept state status of this node 
	 */
	public MDAGNode(boolean isAcceptNode) {
		this.isAcceptNode = isAcceptNode;
		outgoingTransitionTreeMap = new TreeMap<Character, MDAGNode>();
	}

	/**
	 * Constructs an MDAGNode possessing the same accept state status and outgoing transitions as another.
	 
	 * @param node      the MDAGNode possessing the accept state status and 
	 *                  outgoing transitions that the to-be-created MDAGNode is to take on
	 */
	private MDAGNode(MDAGNode node) {
		isAcceptNode = node.isAcceptNode;
		outgoingTransitionTreeMap = new TreeMap<Character, MDAGNode>(node.outgoingTransitionTreeMap);

		//Loop through the nodes in this node's outgoing transition set, incrementing the number of
		//incoming transitions of each by 1 (to account for this newly created node's outgoing transitions)
		for (Entry<Character, MDAGNode> transitionKeyValuePair : outgoingTransitionTreeMap.entrySet())
			transitionKeyValuePair.getValue().incomingTransitionCount++;
		/////
	}

	public MDAGNode(boolean isAcceptNode, int id) {
		this.id = id;
		this.isAcceptNode = isAcceptNode;
		outgoingTransitionTreeMap = new TreeMap<Character, MDAGNode>();
	}

	@SuppressWarnings(""unused"")
	private MDAGNode(MDAGNode node, int id) {
		this.id = id;
		isAcceptNode = node.isAcceptNode;
		outgoingTransitionTreeMap = new TreeMap<Character, MDAGNode>(node.outgoingTransitionTreeMap);

		for (Map.Entry<Character, MDAGNode> transitionKeyValuePair : outgoingTransitionTreeMap.entrySet())
			transitionKeyValuePair.getValue().incomingTransitionCount++;
	}

	/**
	 * Creates an MDAGNode possessing the same accept state status and outgoing transitions as this node.
	 
	 * @return      an MDAGNode possessing the same accept state status and outgoing transitions as this node
	 */
	public MDAGNode clone() {
		return new MDAGNode(this);
	}

	/**
	 * Creates an MDAGNode possessing the same accept state status ant transition set 
	 * (incoming & outgoing) as this node. outgoing transitions as this node.
	 
	 * @param soleParentNode                        the MDAGNode possessing the only transition that targets this node
	 * @param parentToCloneTransitionLabelChar      the char which labels the transition from {@code soleParentNode} to this node
	 * @return                                      an MDAGNode possessing the same accept state status and transition set as this node. 
	 */
	public MDAGNode clone(MDAGNode soleParentNode, char parentToCloneTransitionLabelChar) {
		MDAGNode cloneNode = new MDAGNode(this);
		soleParentNode.reassignOutgoingTransition(parentToCloneTransitionLabelChar, this, cloneNode);

		return cloneNode;
	}

	/**
	 * Retrieves the index in a simplified mdag data array that the SimpleMDAGNode
	 * representation of this node's outgoing transition set begins at.
	 
	 * @return      the index in a simplified mdag data array that this node's transition set begins at,
	 *              or -1 if its transition set is not present in such an array
	 */
	public int getTransitionSetBeginIndex() {
		return transitionSetBeginIndex;
	}

	public Map.Entry<Character, MDAGNode> getLastTransition() {
		return outgoingTransitionTreeMap.lastEntry();
	}

	/**
	 * Retrieves this node's outgoing transition count.
	 
	 * @return      an int representing this node's number of outgoing transitions
	 */
	public int getOutgoingTransitionCount() {
		return outgoingTransitionTreeMap.size();
	}

	/**
	 * Retrieves this node's incoming transition count
	 
	 * @return      an int representing this node's number of incoming transitions
	 */
	public int getIncomingTransitionCount() {
		return incomingTransitionCount;
	}

	/**
	 * Determines if this node is a confluence node
	 * (defined as a node with two or more incoming transitions
	 
	 * @return      true if this node has two or more incoming transitions, false otherwise
	 */
	public boolean isConfluenceNode() {
		return (incomingTransitionCount > 1);
	}

	/**
	 * Retrieves the accept state status of this node.
	 
	 * @return      true if this node is an accept state, false otherwise
	 */
	public boolean isAcceptNode() {
		return isAcceptNode;
	}

	/**
	 * Sets this node's accept state status.
	 * 
	 * @param isAcceptNode     a boolean representing the desired accept state status 
	 */
	public void setAcceptStateStatus(boolean isAcceptNode) {
		this.isAcceptNode = isAcceptNode;
	}

	/**
	 * Records the index that this node's transition set starts at
	 * in an array containing this node's containing MDAG data (simplified MDAG).
	 
	 * @param transitionSetBeginIndex       a transition set
	 */
	public void setTransitionSetBeginIndex(int transitionSetBeginIndex) {
		this.transitionSetBeginIndex = transitionSetBeginIndex;
	}

	/**
	 * Determines whether this node has an outgoing transition with a given label.
	 
	 * @param letter        the char labeling the desired transition
	 * @return              true if this node possesses a transition labeled with
	 *                      {@code letter}, and false otherwise
	 */
	public boolean hasOutgoingTransition(char letter) {
		return outgoingTransitionTreeMap.containsKey(letter);
	}

	/**
	 * Determines whether this node has any outgoing transitions.
	 
	 * @return      true if this node has at least one outgoing transition, false otherwise
	 */
	public boolean hasOutgoingTransitions() {
		return !outgoingTransitionTreeMap.isEmpty();
	}

	/**
	 * Follows an outgoing transition of this node labeled with a given char.
	 
	 * @param letter        the char representation of the desired transition's label
	 * @return              the MDAGNode that is the target of the transition labeled with {@code letter},
	 *                      or null if there is no such labeled transition from this node
	 */
	public MDAGNode transition(char letter) {
		return outgoingTransitionTreeMap.get(letter);
	}

	/**
	 * Follows a transition path starting from this node.
	 
	 * @param str               a String corresponding a transition path in the MDAG
	 * @return                  the MDAGNode at the end of the transition path corresponding to 
	 *                          {@code str}, or null if such a transition path is not present in the MDAG
	 */
	public MDAGNode transition(String str) {
		int charCount = str.length();
		MDAGNode currentNode = this;

		//Iteratively transition through the MDAG using the chars in str
		for (int i = 0; i < charCount; i++) {
			currentNode = currentNode.transition(str.charAt(i));
			if (currentNode == null)
				break;
		}
		/////

		return currentNode;
	}

	/**
	 * Retrieves the nodes in the transition path starting
	 * from this node corresponding to a given String .
	 
	 * @param str       a String corresponding to a transition path starting from this node
	 * @return          a Stack of MDAGNodes containing the nodes in the transition path 
	 *                  denoted by {@code str}, in the order they are encountered in during transitioning
	 */
	public Stack<MDAGNode> getTransitionPathNodes(String str) {
		Stack<MDAGNode> nodeStack = new Stack<MDAGNode>();

		MDAGNode currentNode = this;
		int numberOfChars = str.length();

		//Iteratively transition through the MDAG using the chars in str,
		//putting each encountered node in nodeStack
		for (int i = 0; i < numberOfChars && currentNode != null; i++) {
			currentNode = currentNode.transition(str.charAt(i));
			nodeStack.add(currentNode);
		}
		/////

		return nodeStack;
	}

	/**
	 * Retrieves this node's outgoing transitions.
	 
	 * @return      a TreeMap containing entries collectively representing
	 *              all of this node's outgoing transitions
	 */
	public TreeMap<Character, MDAGNode> getOutgoingTransitions() {
		return outgoingTransitionTreeMap;
	}

	/**
	 * Decrements (by 1) the incoming transition counts of all of the nodes
	 * that are targets of outgoing transitions from this node.
	 */
	public void decrementTargetIncomingTransitionCounts() {
		for (Entry<Character, MDAGNode> transitionKeyValuePair : outgoingTransitionTreeMap.entrySet())
			transitionKeyValuePair.getValue().incomingTransitionCount--;
	}

	/**
	 * Reassigns the target node of one of this node's outgoing transitions.
	 
	 * @param letter            the char which labels the outgoing transition of interest
	 * @param oldTargetNode     the MDAGNode that is currently the target of the transition of interest
	 * @param newTargetNode     the MDAGNode that is to be the target of the transition of interest
	 */
	public void reassignOutgoingTransition(char letter, MDAGNode oldTargetNode, MDAGNode newTargetNode) {
		oldTargetNode.incomingTransitionCount--;
		newTargetNode.incomingTransitionCount++;

		outgoingTransitionTreeMap.put(letter, newTargetNode);
	}

	/**
	 * Creates an outgoing transition labeled with a 
	 * given char that has a new node as its target.
	 
	 * @param letter                        a char representing the desired label of the transition
	 * @param targetAcceptStateStatus       a boolean representing to-be-created transition target node's accept status
	 * @return                              the (newly created) MDAGNode that is the target of the created transition
	 */
	public MDAGNode addOutgoingTransition(char letter, boolean targetAcceptStateStatus) {
		MDAGNode newTargetNode = new MDAGNode(targetAcceptStateStatus);
		newTargetNode.incomingTransitionCount++;

		outgoingTransitionTreeMap.put(letter, newTargetNode);
		return newTargetNode;
	}

	public MDAGNode addOutgoingTransition(char letter, boolean isEndOfWord, int id) {
		MDAGNode newTargetNode = new MDAGNode(isEndOfWord, id);
		newTargetNode.incomingTransitionCount++;
		newTargetNode.id = id;

		outgoingTransitionTreeMap.put(letter, newTargetNode);
		return newTargetNode;
	}

	/**
	 * Removes a transition labeled with a given char. This only removes the connection
	 * between this node and the transition's target node; the target node is not deleted.
	 
	 * @param letter        the char labeling the transition of interest
	 */
	public void removeOutgoingTransition(char letter) {
		outgoingTransitionTreeMap.remove(letter);
	}

	/**
	 * Determines whether the sets of transition paths from two MDAGNodes are equivalent. This is an expensive operation.
	 
	 * @param outgoingTransitionTreeMap1        a TreeMap containing entries collectively representing
	 *                                          all of a node's outgoing transitions
	 * @param outgoingTransitionTreeMap2        a TreeMap containing entries collectively representing
	 *                                          all of a node's outgoing transitions
	 * @return                                  true if the set of transition paths from {@code node1}
	 *                                          and {@code node2} are equivalent
	 */
	public static boolean haveSameTransitions(MDAGNode node1, MDAGNode node2) {
		TreeMap<Character, MDAGNode> outgoingTransitionTreeMap1 = node1.outgoingTransitionTreeMap;
		TreeMap<Character, MDAGNode> outgoingTransitionTreeMap2 = node2.outgoingTransitionTreeMap;

		if (outgoingTransitionTreeMap1.size() == outgoingTransitionTreeMap2.size()) {
			//For each transition in outgoingTransitionTreeMap1, get the identically lableed transition
			//in outgoingTransitionTreeMap2 (if present), and test the equality of the transitions' target nodes
			for (Entry<Character, MDAGNode> transitionKeyValuePair : outgoingTransitionTreeMap1.entrySet()) {
				Character currentCharKey = transitionKeyValuePair.getKey();
				MDAGNode currentTargetNode = transitionKeyValuePair.getValue();

				if (!outgoingTransitionTreeMap2.containsKey(currentCharKey)
						|| !outgoingTransitionTreeMap2.get(currentCharKey).equals(currentTargetNode))
					return false;
			}
			/////
		} else
			return false;

		return true;
	}

	/**
	 * Clears this node's stored hash value
	 */
	public void clearStoredHashCode() {
		storedHashCode = null;
	}

	/**
	 * Evaluates the equality of this node with another object.
	 * This node is equal to obj if and only if obj is also an MDAGNode,
	 * and the set of transitions paths from this node and obj are equivalent.
	 
	 * @param obj       an object
	 * @return          true of {@code obj} is an MDAGNode and the set of 
	 *                  transition paths from this node and obj are equivalent
	 */
	@Override
	public boolean equals(Object obj) {
		boolean areEqual = (this == obj);

		if (!areEqual && obj != null && obj.getClass().equals(MDAGNode.class)) {
			MDAGNode node = (MDAGNode) obj;
			areEqual = (isAcceptNode == node.isAcceptNode && haveSameTransitions(this, node));
		}

		return areEqual;
	}

	/**
	 * Hashes this node using its accept state status and set of outgoing transition paths.
	 * This is an expensive operation, so the result is cached and only cleared when necessary.
	
	 * @return      an int of this node's hash code
	 */
	@Override
	public int hashCode() {

		if (storedHashCode == null) {
			int hash = 7;
			hash = 53 * hash + (this.isAcceptNode ? 1 : 0);
			hash = 53 * hash + (this.outgoingTransitionTreeMap != null ? this.outgoingTransitionTreeMap.hashCode() : 0); //recursively hashes the nodes in all the 
																															//transition paths stemming from this node
			storedHashCode = hash;
			return hash;
		} else
			return storedHashCode;
	}
}",src/main/java/com/seaboat/text/analyzer/data/structure/MDAGNode.java
DAGModel,"public class DAGModel {

	public Multimap<Integer, Range> calc(String s, Dict dict) {
		Multimap<Integer, Range> dag = ArrayListMultimap.create();
		int index = 0;
		int id = 1;
		String temp;
		while (true) {
			temp = s.substring(index, s.length());
			List<Integer> temp_list = dict.prefixSearch(temp);
			if (temp_list.size() == 0) {
				if (index == s.length())
					break;
				index++;
				continue;
			} else {
				for (int i : temp_list) {
					int len = dict.getStringByIndex(i).length();
					dag.put(id++, new Range(index, index + len - 1));
				}
				index++;
			}
			if (index >= s.length())
				break;
		}
		return dag;
	}

	class Range {
		int from;
		int to;

		public Range(int from, int to) {
			this.from = from;
			this.to = to;
		}

	}
}",src/main/java/com/seaboat/text/analyzer/data/structure/DAGModel.java
DoubleArrayTrie,"public class DoubleArrayTrie {
	private final static int BUF_SIZE = 16384;
	private final static int UNIT_SIZE = 8;

	private static class Node {
		int code;//
		int depth;//
		int left;//
		int right;//
	};

	private int check[];
	private int base[];

	private boolean used[];
	private int size;
	private int allocSize;
	private List<String> key;
	private int keySize;
	private int length[];
	private int value[];
	private int progress;
	private int nextCheckPos;
	int error_;

	private int resize(int newSize) {
		int[] base2 = new int[newSize];
		int[] check2 = new int[newSize];
		boolean used2[] = new boolean[newSize];
		if (allocSize > 0) {
			System.arraycopy(base, 0, base2, 0, allocSize);
			System.arraycopy(check, 0, check2, 0, allocSize);
			System.arraycopy(used, 0, used2, 0, allocSize);
		}

		base = base2;
		check = check2;
		used = used2;

		return allocSize = newSize;
	}

	private int fetch(Node parent, List<Node> siblings) {
		if (error_ < 0)
			return 0;

		int prev = 0;

		for (int i = parent.left; i < parent.right; i++) {
			if ((length != null ? length[i] : key.get(i).length()) < parent.depth)
				continue;

			String tmp = key.get(i);

			int cur = 0;
			if ((length != null ? length[i] : tmp.length()) != parent.depth)
				cur = (int) tmp.charAt(parent.depth) + 1;

			if (prev > cur) {
				error_ = -3;
				return 0;
			}

			if (cur != prev || siblings.size() == 0) {
				Node tmp_node = new Node();
				tmp_node.depth = parent.depth + 1;
				tmp_node.code = cur;
				tmp_node.left = i;
				if (siblings.size() != 0)
					siblings.get(siblings.size() - 1).right = i;

				siblings.add(tmp_node);
			}

			prev = cur;
		}

		if (siblings.size() != 0)
			siblings.get(siblings.size() - 1).right = parent.right;

		return siblings.size();
	}

	private int insert(List<Node> siblings) {
		if (error_ < 0)
			return 0;

		int begin = 0;
		int pos = ((siblings.get(0).code + 1 > nextCheckPos) ? siblings.get(0).code + 1 : nextCheckPos) - 1;
		int nonzero_num = 0;
		int first = 0;

		if (allocSize <= pos)
			resize(pos + 1);

		outer: while (true) {
			pos++;

			if (allocSize <= pos)
				resize(pos + 1);

			if (check[pos] != 0) {
				nonzero_num++;
				continue;
			} else if (first == 0) {
				nextCheckPos = pos;
				first = 1;
			}

			begin = pos - siblings.get(0).code;
			if (allocSize <= (begin + siblings.get(siblings.size() - 1).code)) {
				// progress can be zero
				double l = (1.05 > 1.0 * keySize / (progress + 1)) ? 1.05 : 1.0 * keySize / (progress + 1);
				resize((int) (allocSize * l));
			}

			if (used[begin])
				continue;

			for (int i = 1; i < siblings.size(); i++)
				if (check[begin + siblings.get(i).code] != 0)
					continue outer;

			break;
		}

		if (1.0 * nonzero_num / (pos - nextCheckPos + 1) >= 0.95)
			nextCheckPos = pos;

		used[begin] = true;
		size = (size > begin + siblings.get(siblings.size() - 1).code + 1) ? size
				: begin + siblings.get(siblings.size() - 1).code + 1;

		for (int i = 0; i < siblings.size(); i++)
			check[begin + siblings.get(i).code] = begin;

		for (int i = 0; i < siblings.size(); i++) {
			List<Node> new_siblings = new ArrayList<Node>();

			if (fetch(siblings.get(i), new_siblings) == 0) {
				base[begin + siblings.get(i).code] = (value != null) ? (-value[siblings.get(i).left] - 1)
						: (-siblings.get(i).left - 1);

				if (value != null && (-value[siblings.get(i).left] - 1) >= 0) {
					error_ = -2;
					return 0;
				}
				progress++;
			} else {
				int h = insert(new_siblings);
				base[begin + siblings.get(i).code] = h;
			}
		}
		return begin;
	}

	public DoubleArrayTrie() {
		check = null;
		base = null;
		used = null;
		size = 0;
		allocSize = 0;
		error_ = 0;
	}

	void clear() {
		check = null;
		base = null;
		used = null;
		allocSize = 0;
		size = 0;
	}

	public int getUnitSize() {
		return UNIT_SIZE;
	}

	public int getSize() {
		return size;
	}

	public int getTotalSize() {
		return size * UNIT_SIZE;
	}

	public int getNonzeroSize() {
		int result = 0;
		for (int i = 0; i < size; i++)
			if (check[i] != 0)
				result++;
		return result;
	}

	public int build(List<String> key) throws Exception {
		int flag = build(key, null, null, key.size());
		if (flag != 0)
			throw new Exception(""error occurs when building double array trie."");
		return flag;
	}

	public int build(List<String> _key, int _length[], int _value[], int _keySize) {
		if (_keySize > _key.size() || _key == null)
			return 0;
		key = _key;
		length = _length;
		keySize = _keySize;
		value = _value;
		progress = 0;

		resize(65536 * 32);

		base[0] = 1;
		nextCheckPos = 0;

		Node root_node = new Node();
		root_node.left = 0;
		root_node.right = keySize;
		root_node.depth = 0;

		List<Node> siblings = new ArrayList<Node>();
		fetch(root_node, siblings);
		insert(siblings);
		used = null;
		key = null;

		return error_;
	}

	public void open(String fileName) throws IOException {
		File file = new File(fileName);
		size = (int) file.length() / UNIT_SIZE;
		check = new int[size];
		base = new int[size];

		DataInputStream is = null;
		try {
			is = new DataInputStream(new BufferedInputStream(new FileInputStream(file), BUF_SIZE));
			for (int i = 0; i < size; i++) {
				base[i] = is.readInt();
				check[i] = is.readInt();
			}
		} finally {
			if (is != null)
				is.close();
		}
	}

	public void save(String fileName) throws IOException {
		DataOutputStream out = null;
		try {
			out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(fileName)));
			for (int i = 0; i < size; i++) {
				out.writeInt(base[i]);
				out.writeInt(check[i]);
			}
			out.close();
		} finally {
			if (out != null)
				out.close();
		}
	}

	public int exactMatchSearch(String key) {
		return exactMatchSearch(key, 0, 0, 0);
	}

	public int exactMatchSearch(String key, int pos, int len, int nodePos) {
		if (len <= 0)
			len = key.length();
		if (nodePos <= 0)
			nodePos = 0;

		int result = -1;

		char[] keyChars = key.toCharArray();

		int b = base[nodePos];
		int p;

		for (int i = pos; i < len; i++) {
			p = b + (int) (keyChars[i]) + 1;
			if (b == check[p])
				b = base[p];
			else
				return result;
		}

		p = b;
		int n = base[p];
		if (b == check[p] && n < 0) {
			result = -n - 1;
		}
		return result;
	}

	public List<Integer> commonPrefixSearch(String key) {
		return commonPrefixSearch(key, 0, 0, 0);
	}

	public List<Integer> commonPrefixSearch(String key, int pos, int len, int nodePos) {
		if (len <= 0)
			len = key.length();
		if (nodePos <= 0)
			nodePos = 0;

		List<Integer> result = new ArrayList<Integer>();

		char[] keyChars = key.toCharArray();

		int b = base[nodePos];
		int n;
		int p;

		for (int i = pos; i < len; i++) {
			p = b;
			n = base[p];

			if (b == check[p] && n < 0) {
				result.add(-n - 1);
			}

			p = b + (int) (keyChars[i]) + 1;
			if (b == check[p])
				b = base[p];
			else
				return result;
		}

		p = b;
		n = base[p];

		if (b == check[p] && n < 0) {
			result.add(-n - 1);
		}

		return result;
	}

	public void dump() {
		for (int i = 0; i < size; i++) {
			System.err.println(""i: "" + i + "" ["" + base[i] + "", "" + check[i] + ""]"");
		}
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/DoubleArrayTrie.java
Graph,"public class Graph {

	private Vertex[] vertexes = null;

	public Graph(Vertex[] vertexes, Edge[] edges) {
		this.vertexes = vertexes;
	}

	public Vertex[] getVertexes() {
		return this.vertexes;
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/Graph.java
Vertex,"public class Vertex {

	private boolean isVisited = false;

	private String vertexName = null;

	private List<Edge> toEdges = new ArrayList<Edge>();

	public Vertex(String vertexName) {
		this.vertexName = vertexName;
	}

	public void addEdge(Edge toEdge) {
		this.toEdges.add(toEdge);
	}

	public boolean isVisited() {
		return isVisited;
	}

	public void setVisited() {
		this.isVisited = true;
	}

	public List<Edge> getToEdges() {
		return this.toEdges;
	}

	public String getVertexName() {
		return this.vertexName;
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/Vertex.java
ACTrieNode,"public class ACTrieNode {

	private static Logger logger = Logger.getLogger(ACTrieNode.class);
	private ACTrieNode[] children;
	private byte[] value;
	private boolean deleted = false;
	private int status;
	private ACArray[] results = null;
	private ACTrieNode failureNode;
	private int frequency;
	private byte[] pos = null;
	private static String encoding = ""utf-8"";

	public ACTrieNode(String value) {
		try {
			this.value = value == null ? null : value.getBytes(encoding);
		} catch (UnsupportedEncodingException e) {
			logger.warn(""fail to create trie node. "", e);
		}
	}

	public boolean isEmpty() {
		return this.value == null && this.children == null;
	}

	public ACTrieNode[] getChildren() {
		return children;
	}

	public ACTrieNode getChild(String word) {
		try {
			byte[] w = word.getBytes(encoding);
			if (children == null)
				return null;
			for (ACTrieNode c : children) {
				if (isEqual(c.getValue(), w) && !c.deleted)
					return c;
			}
		} catch (UnsupportedEncodingException e) {
			logger.warn(""fail to get child node. "", e);
		}
		return null;
	}

	private boolean isEqual(byte[] b1, byte[] b2) {
		if (b1.length != b2.length)
			return false;
		for (int i = 0; i < b1.length; i++) {
			if (b1[i] != b2[i])
				return false;
		}
		return true;
	}

	public boolean allChildrenDeleted() {
		if (children == null)
			return true;
		for (ACTrieNode c : children) {
			if (!c.deleted)
				return false;
		}
		return true;
	}

	public void setChild(ACTrieNode child) {
		if (children == null) {
			children = new ACTrieNode[1];
			children[0] = child;
		} else {
			ACTrieNode[] temp = children;
			children = new ACTrieNode[temp.length + 1];
			System.arraycopy(temp, 0, children, 0, temp.length);
			children[children.length - 1] = child;
		}
	}

	public byte[] getValue() {
		return value;
	}

	public void setValue(String value) {
		try {
			this.value = value.getBytes(encoding);
		} catch (UnsupportedEncodingException e) {
			logger.warn(""fail to set value. "", e);
		}
	}

	public void setDeleted(boolean deleted) {
		this.deleted = deleted;
	}

	public boolean isDeleted() {
		return deleted;
	}

	public int getStatus() {
		return status;
	}

	public void setStatus(int status) {
		this.status = status;
	}

	public void addResult(String word) {
		byte[] b;
		try {
			b = word.getBytes(encoding);
		} catch (UnsupportedEncodingException e) {
			logger.warn(""fail to add result. "", e);
			return;
		}
		if (results == null) {
			results = new ACArray[] { new ACArray(b) };
			return;
		}
		ACArray[] temp = new ACArray[results.length + 1];
		System.arraycopy(results, 0, temp, 0, results.length);
		temp[results.length] = new ACArray(b);
		results = temp;
	}

	public ACArray[] getResults() {
		return results;
	}

	public ACTrieNode getFailureNode() {
		return failureNode;
	}

	public void setFailureNode(ACTrieNode failureNode) {
		this.failureNode = failureNode;
	}

	public int getFrequency() {
		return frequency;
	}

	public void setFrequency(int frequency) {
		this.frequency = frequency;
	}

	public byte[] getPos() {
		return pos;
	}

	public void setPos(String pos) {
		try {
			this.pos = pos.getBytes(encoding);
		} catch (UnsupportedEncodingException e) {
			logger.warn(""toString error. "", e);
		}
	}

	public String toString() {
		try {
			if (value != null)
				return ""id = "" + status + ""; value = "" + new String(value, ""utf-8"") + ""; failure node = ""
						+ (failureNode == null ? ""null"" : failureNode.getStatus());
		} catch (UnsupportedEncodingException e) {
			logger.warn(""toString error. "", e);
		}
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/ACTrieNode.java
ACTrieTree,"public class ACTrieTree {

	private static Logger logger = Logger.getLogger(ACTrieTree.class);
	protected ACTrieNode root;
	private static int status = 1;

	public ACTrieTree() {
		this.root = new ACTrieNode(null);
	}

	public void put(String word, String pos, int freq) throws IllegalArgumentException {
		if (word == null) {
			throw new IllegalArgumentException();
		}
		//build goto function 
		ACTrieNode current = this.root;
		String[] ss = word.split("""");
		for (int i = 0; i < ss.length; i++) {
			String s = ss[i];
			ACTrieNode child = current.getChild(s);
			if (child == null) {
				child = new ACTrieNode(s);
				current.setChild(child);
				child.setStatus(status++);
			}
			current = child;
			if (i == ss.length - 1) {
				current.setFrequency(freq);
				current.setPos(pos);
			}
			//build failure function
			if (i == 0) {
				current.setFailureNode(this.root);
			} else {
				ACTrieNode failureNode = null;
				label1: for (int j = i; j > 0; j--) {
					String temp = word.substring(i - j + 1, i + 1);
					failureNode = get(temp);
					if (failureNode != null)
						break label1;
				}
				if (failureNode == null)
					failureNode = this.root;
				current.setFailureNode(failureNode);
			}
		}
		//build output function
		current.addResult(word);
	}

	public void put(String word) throws IllegalArgumentException {
		if (word == null) {
			throw new IllegalArgumentException();
		}
		//build goto function 
		ACTrieNode current = this.root;
		String[] ss = word.split("""");
		for (int i = 0; i < ss.length; i++) {
			String s = ss[i];
			ACTrieNode child = current.getChild(s);
			if (child == null) {
				child = new ACTrieNode(s);
				current.setChild(child);
				child.setStatus(status++);
			}
			current = child;
			//build failure function
			if (i == 0) {
				current.setFailureNode(this.root);
			} else {
				ACTrieNode failureNode = null;
				label1: for (int j = i; j > 0; j--) {
					String temp = word.substring(i - j + 1, i + 1);
					failureNode = get(temp);
					if (failureNode != null)
						break label1;
				}
				if (failureNode == null)
					failureNode = this.root;
				current.setFailureNode(failureNode);
			}
		}
		//build output function
		current.addResult(word);
	}

	public ACTrieNode get(String word) throws IllegalArgumentException {
		if (word == null) {
			throw new IllegalArgumentException();
		}
		ACTrieNode current = this.root;
		for (String s : word.split("""")) {
			ACTrieNode child = current.getChild(s);
			if (child == null)
				return null;
			current = child;
		}
		return current;
	}

	public List<String> acSearch(String str) {
		List<String> list = new ArrayList<String>();
		ACTrieNode current = this.root;
		try {
			L1: for (String s : str.split("""")) {
				ACTrieNode child = current.getChild(s);
				//jump the prefix which is not exist in trie tree.
				if (child == null && current == root) {
					continue;
				}
				if (child == null) {
					ACTrieNode failure = current.getFailureNode();
					//jump the word which is not exist in trie tree.
					while (failure.getChild(s) == null) {
						if (failure == this.root) {
							current = failure;
							continue L1;
						}
						failure = failure.getFailureNode();
					}
					current = failure;
					child = current.getChild(s);
					if (child != null)
						current = child;
					else
						current = this.root;
				}
				if (System.getProperty(""Debug"") != null && System.getProperty(""Debug"").equalsIgnoreCase(""true""))
					if (child != null)
						System.out.println(child.toString());
				current = child;
				if (current.getResults() != null) {
					ACArray[] results = current.getResults();
					for (ACArray arr : results)
						list.add(new String(arr.getValue(), ""utf-8""));
				}
			}
		} catch (UnsupportedEncodingException e) {
			logger.warn(""fail to search trie tree. "", e);
		}
		return list;
	}

	public void remove(String word) {
		if (word == null || word.length() <= 0) {
			return;
		}
		for (int i = 0; i < word.length(); i++) {
			String sub_word = word.substring(0, word.length() - i);
			ACTrieNode current = this.root;
			for (String s : sub_word.split("""")) {
				ACTrieNode child = current.getChild(s);
				//not exist
				if (child == null)
					return;
				if (child != null && (child.getChildren() == null || child.allChildrenDeleted()))
					child.setDeleted(true);
				current = child;
			}
		}
	}

	public boolean isEmpty() {
		return this.root.isEmpty();
	}

	public void clear() {
		this.root = new ACTrieNode(null);
	}

	public void build(List<String> words) {
		for (String s : words)
			this.put(s);
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/ACTrieTree.java
ACArray,"public class ACArray {

	private byte[] value;

	public ACArray(byte[] value) {
		this.value = value;
	}

	public byte[] getValue() {
		return value;
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/ACArray.java
Dijkstra,"public class Dijkstra {

	private Map<Vertex, DistanceAndPrevVertex> calcGrid = null;

	public List<String> getShortestPath(Graph g, Vertex begin, Vertex end) {
		List<String> list = new ArrayList<String>();
		calcGrid = new HashMap<Vertex, DistanceAndPrevVertex>();
		for (Vertex v : g.getVertexes()) {
			calcGrid.put(v, new DistanceAndPrevVertex());
		}
		if (begin != null) {
			calcGrid.get(begin).setDis(0);
			begin.setVisited();
			List<Edge> edges = begin.getToEdges();
			this.loopPath(edges);
		}
		Stack<Vertex> path = new Stack<>();
		while (!end.equals(begin)) {
			path.push(end);
			end = calcGrid.get(end).getPrevV();
		}
		path.push(begin);
		while (!path.empty()) {
			list.add(path.pop().getVertexName());
		}
		return list;
	}

	private void loopPath(List<Edge> edges) {
		if (edges != null && edges.size() > 0) {
			for (Edge e : edges) {
				if (!e.getToVertex().isVisited()) {
					if (calcGrid.get(e.getToVertex()).getDis() == null || calcGrid.get(e.getToVertex())
							.getDis() > (e.getWeight() + calcGrid.get(e.getFromVertex()).getDis())) {
						calcGrid.get(e.getToVertex()).setDis(e.getWeight() + calcGrid.get(e.getFromVertex()).getDis());
						calcGrid.get(e.getToVertex()).setPrevV(e.getFromVertex());
					}
				}
			}
			Vertex minV = getMinVertex(calcGrid);
			if (minV != null) {
				minV.setVisited();
				loopPath(minV.getToEdges());
			}
		}
	}

	private Vertex getMinVertex(Map<Vertex, DistanceAndPrevVertex> calcGrid) {
		int minDis = Integer.MAX_VALUE;
		Vertex v = null;
		for (Map.Entry<Vertex, DistanceAndPrevVertex> e : calcGrid.entrySet()) {
			if (!e.getKey().isVisited()) {
				if (e.getValue().getDis() != null && e.getValue().getDis() < minDis) {
					minDis = e.getValue().getDis();
					v = e.getKey();
				}
			}
		}
		return v;
	}

	public Vertex getVertexByName(Graph g, String vName) {
		Vertex[] vertexes = g.getVertexes();
		if (null != vertexes) {
			for (Vertex v : vertexes) {
				if (v.getVertexName() != null && v.getVertexName().equals(vName)) {
					return v;
				}
			}
		}
		return null;
	}

	private class DistanceAndPrevVertex {
		private Integer dis = null;
		private Vertex prevV = null;

		public DistanceAndPrevVertex() {
		}

		public Integer getDis() {
			return dis;
		}

		public void setDis(Integer dis) {
			this.dis = dis;
		}

		public Vertex getPrevV() {
			return prevV;
		}

		public void setPrevV(Vertex prevV) {
			this.prevV = prevV;
		}
	}

	@SuppressWarnings(""unused"")
	public void printGraph(Graph g) {
		Vertex[] vertexes = g.getVertexes();
		if (null != vertexes) {
			for (Vertex v : vertexes) {
				printPath(v);
			}
		}
	}

	public void printPath(Vertex v) {
		if (v != null) {
			List<Edge> edges = v.getToEdges();
			if (null != edges && edges.size() > 0) {
				for (Edge e : edges) {
					if (!e.isVisited()) {
						System.out.println(e.getFromVertex().getVertexName() + "" -> "" + e.getToVertex().getVertexName()
								+ "" w:"" + e.getWeight());
						e.setVisited();
						printPath(e.getToVertex());
					}
				}
			}

		}
	}

}",src/main/java/com/seaboat/text/analyzer/data/structure/Dijkstra.java
Edge,"public class Edge {

	private boolean isVisited = false;

	private Integer weight = null;

	private Vertex fromVertex = null;

	private Vertex toVertex = null;

	public Edge(int weight, Vertex fromVertex, Vertex toVertex) {
		this.weight = weight;
		this.fromVertex = fromVertex;
		this.toVertex = toVertex;
	}

	public Integer getWeight() {
		return weight;
	}

	public Vertex getFromVertex() {
		return fromVertex;
	}

	public Vertex getToVertex() {
		return toVertex;
	}

	public boolean isVisited() {
		return isVisited;
	}

	public void setVisited() {
		isVisited = true;
	}
}",src/main/java/com/seaboat/text/analyzer/data/structure/Edge.java
OrganizationDict,"public class OrganizationDict implements Dict {

	private static Logger logger = Logger.getLogger(OrganizationDict.class);

	private static ACTrieTree tree = null;
	private static String DIC_FILE = ""/organizations.dic"";
	public static String[] dictionary = null;
	private static OrganizationDict instance;

	private OrganizationDict() {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading idiom dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public OrganizationDict(List<String> words) {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		initDict(words);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading idiom dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private void initDict(List<String> words) {
		Collections.sort(words);
		dictionary = new String[words.size()];
		for (int i = 0; i < dictionary.length; i++)
			dictionary[i] = words.get(i);
	}

	public static OrganizationDict get() {
		if (instance != null)
			return instance;
		synchronized (OrganizationDict.class) {
			if (instance == null)
				instance = new OrganizationDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			while ((line = bufferedReader.readLine()) != null) {
				words.add(line);
			}
			Collections.sort(words);
			dictionary = new String[words.size()];
			for (int i = 0; i < dictionary.length; i++)
				dictionary[i] = words.get(i);
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}

	public List<String> searchOrganization(String s) {
		return tree.acSearch(s);
	}

	public List<Integer> prefixSearch(String text) {
		return null;
	}

	public String getStringByIndex(Integer i) {
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/dict/OrganizationDict.java
PinyinDict,"public class PinyinDict implements Dict {

	private static Logger logger = Logger.getLogger(PinyinDict.class);

	private static DoubleArrayTrie tree = null;
	private static String DIC_FILE = ""/pinyin.dic"";
	public static String[] dictionary = null;
	private static PinyinDict instance;

	private PinyinDict() {
		tree = new DoubleArrayTrie();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public PinyinDict(List<String> words) {
		tree = new DoubleArrayTrie();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		initDict(words);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private void initDict(List<String> words) {
		Collections.sort(words);
		dictionary = new String[words.size()];
		for (int i = 0; i < dictionary.length; i++)
			dictionary[i] = words.get(i);
	}

	public static PinyinDict get() {
		if (instance != null)
			return instance;
		synchronized (PinyinDict.class) {
			if (instance == null)
				instance = new PinyinDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		List<KV> kvs = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			kvs = new ArrayList<KV>(Integer.parseInt(line));
			while ((line = bufferedReader.readLine()) != null) {
				String[] ss = line.split(""="");
				if (ss.length == 2) {
					kvs.add(new KV(ss[0], ss[1]));
				}
			}
			Collections.sort(kvs, new Comparator<KV>() {
				@Override
				public int compare(KV o1, KV o2) {
					return o1.k.compareTo(o2.k);
				}
			});
			dictionary = new String[kvs.size()];
			for (int i = 0; i < dictionary.length; i++)
				dictionary[i] = kvs.get(i).v;
			for (KV kv : kvs)
				words.add(kv.k);
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}

	public List<Integer> prefixSearch(String text) {
		return tree.commonPrefixSearch(text);
	}

	public int exactlySearch(String text) {
		return tree.exactMatchSearch(text);
	}

	public String getStringByIndex(Integer i) {
		return dictionary[i];
	}

	class KV {
		public KV(String k, String v) {
			this.k = k;
			this.v = v;
		}

		String k;
		String v;
	}
}",src/main/java/com/seaboat/text/analyzer/dict/PinyinDict.java
IdiomDict,"public class IdiomDict implements Dict {

	private static Logger logger = Logger.getLogger(IdiomDict.class);

	private static ACTrieTree tree = null;
	private static String DIC_FILE = ""/idiom.dic"";
	public static String[] dictionary = null;
	private static IdiomDict instance;

	private IdiomDict() {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading idiom dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public IdiomDict(List<String> words) {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		initDict(words);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading idiom dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private void initDict(List<String> words) {
		Collections.sort(words);
		dictionary = new String[words.size()];
		for (int i = 0; i < dictionary.length; i++)
			dictionary[i] = words.get(i);
	}

	public static IdiomDict get() {
		if (instance != null)
			return instance;
		synchronized (IdiomDict.class) {
			if (instance == null)
				instance = new IdiomDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			while ((line = bufferedReader.readLine()) != null) {
				words.add(line);
			}
			Collections.sort(words);
			dictionary = new String[words.size()];
			for (int i = 0; i < dictionary.length; i++)
				dictionary[i] = words.get(i);
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}
	
	public List<String> searchIdiom(String s) {
		return tree.acSearch(s);
	}

	public List<Integer> prefixSearch(String text) {
		return null;
	}

	public String getStringByIndex(Integer i) {
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/dict/IdiomDict.java
NameDict,"public class NameDict implements Dict {

	private static Logger logger = Logger.getLogger(NameDict.class);

	private static ACTrieTree tree = null;
	private static ACTrieTree etree = null;
	private static String DIC_FILE = ""/chinese-names.dic"";
	private static String E_DIC_FILE = ""/english-names.dic"";
	private static NameDict instance;

	private NameDict() {
		tree = new ACTrieTree();
		etree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		List<String> words2 = loadEnglishNamesDict(E_DIC_FILE);
		try {
			tree.build(words);
			etree.build(words2);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading name dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private List<String> loadEnglishNamesDict(String path) {
		return loadDict(path);
	}

	public NameDict(List<String> words) {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading name dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public static NameDict get() {
		if (instance != null)
			return instance;
		synchronized (NameDict.class) {
			if (instance == null)
				instance = new NameDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			while ((line = bufferedReader.readLine()) != null)
				words.add(line.trim());
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}

	public List<String> searchName(String s) {
		return tree.acSearch(s);
	}

	public List<String> searchEnglishName(String s) throws Exception {
		if (etree == null)
			throw new Exception(""english name dictionary is null"");
		List<String> list = new ArrayList<String>();
		String[] ss = s.split("" "");
		for (String st : ss) {
			ACTrieNode node = etree.get(st);
			if (node != null && node.getResults() != null)
				list.add(st);
		}
		return list;
	}

	@Override
	public List<Integer> prefixSearch(String text) {
		return null;
	}

	@Override
	public String getStringByIndex(Integer i) {
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/dict/NameDict.java
PlacenameDict,"public class PlacenameDict implements Dict {

	private static Logger logger = Logger.getLogger(PlacenameDict.class);

	private static ACTrieTree tree = null;
	private static String DIC_FILE = ""/placename.dic"";
	public static String[] dictionary = null;
	private static PlacenameDict instance;

	private PlacenameDict() {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading idiom dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public PlacenameDict(List<String> words) {
		tree = new ACTrieTree();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		initDict(words);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading idiom dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private void initDict(List<String> words) {
		Collections.sort(words);
		dictionary = new String[words.size()];
		for (int i = 0; i < dictionary.length; i++)
			dictionary[i] = words.get(i);
	}

	public static PlacenameDict get() {
		if (instance != null)
			return instance;
		synchronized (PlacenameDict.class) {
			if (instance == null)
				instance = new PlacenameDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			while ((line = bufferedReader.readLine()) != null) {
				words.add(line);
			}
			Collections.sort(words);
			dictionary = new String[words.size()];
			for (int i = 0; i < dictionary.length; i++)
				dictionary[i] = words.get(i);
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}

	public List<String> searchPlacename(String s) {
		return tree.acSearch(s);
	}

	public List<Integer> prefixSearch(String text) {
		return null;
	}

	public String getStringByIndex(Integer i) {
		return null;
	}

}",src/main/java/com/seaboat/text/analyzer/dict/PlacenameDict.java
CoreWordDict,"public class CoreWordDict implements Dict {

	private static Logger logger = Logger.getLogger(CoreWordDict.class);

	private static DoubleArrayTrie tree = null;
	private static String DIC_FILE = ""/core-words.dic"";
	public static String[] dictionary = null;
	private static int[] frequencies = null;
	private static byte[] pos = null;
	private static String[] POS_TYPE = null;
	private static CoreWordDict instance;

	private CoreWordDict() {
		tree = new DoubleArrayTrie();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public CoreWordDict(List<String> words) {
		tree = new DoubleArrayTrie();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		initDict(words);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private void initDict(List<String> words) {
		Collections.sort(words);
		dictionary = new String[words.size()];
		for (int i = 0; i < dictionary.length; i++)
			dictionary[i] = words.get(i);
	}

	public static CoreWordDict get() {
		if (instance != null)
			return instance;
		synchronized (CoreWordDict.class) {
			if (instance == null)
				instance = new CoreWordDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			Set<String> set = new HashSet<String>();
			Map<String, String> mapN = new HashMap<String, String>();
			Map<String, String> mapF = new HashMap<String, String>();
			while ((line = bufferedReader.readLine()) != null) {
				String[] ss = line.split(""	"");
				if (ss.length == 3) {
					words.add(ss[0]);
					set.add(ss[1]);
					mapN.put(ss[0], ss[1]);
					mapF.put(ss[0], ss[2]);
				}
			}
			Collections.sort(words);
			POS_TYPE = set.toArray(new String[set.size()]);
			dictionary = new String[words.size()];
			for (int i = 0; i < dictionary.length; i++)
				dictionary[i] = words.get(i);
			frequencies = new int[words.size()];
			for (int i = 0; i < frequencies.length; i++)
				frequencies[i] = Integer.parseInt(mapF.get(words.get(i)));
			pos = new byte[words.size()];
			for (int i = 0; i < pos.length; i++) {
				pos[i] = getPosIndex(mapN.get(words.get(i)));
			}
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}

	private byte getPosIndex(String pos) {
		for (int i = 0; i < POS_TYPE.length; i++) {
			if (POS_TYPE[i].equalsIgnoreCase(pos))
				return (byte) i;
		}
		return -1;
	}

	public List<Integer> prefixSearch(String text) {
		return tree.commonPrefixSearch(text);
	}
	
	public int exactlySearch(String text) {
		return tree.exactMatchSearch(text);
	}

	public String getStringByIndex(Integer i) {
		return dictionary[i];
	}

	public String getPostType(int i) {
		return POS_TYPE[pos[i]];
	}
}",src/main/java/com/seaboat/text/analyzer/dict/CoreWordDict.java
TraditionalDict,"public class TraditionalDict implements Dict {

	private static Logger logger = Logger.getLogger(TraditionalDict.class);

	private static DoubleArrayTrie tree = null;
	private static String DIC_FILE = ""/traditional2simplified.dic"";
	public static String[] dictionary = null;
	private static TraditionalDict instance;

	private TraditionalDict() {
		tree = new DoubleArrayTrie();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		List<String> words = loadDict(DIC_FILE);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	public TraditionalDict(List<String> words) {
		tree = new DoubleArrayTrie();
		long st = 0;
		if (logger.isDebugEnabled())
			st = System.nanoTime();
		initDict(words);
		try {
			tree.build(words);
		} catch (Exception e) {
			logger.error(e);
		}
		logger.debug(""loading dictionary elapsed time : "" + (System.nanoTime() - st) / (1000 * 1000) + ""ms"");
	}

	private void initDict(List<String> words) {
		Collections.sort(words);
		dictionary = new String[words.size()];
		for (int i = 0; i < dictionary.length; i++)
			dictionary[i] = words.get(i);
	}

	public static TraditionalDict get() {
		if (instance != null)
			return instance;
		synchronized (TraditionalDict.class) {
			if (instance == null)
				instance = new TraditionalDict();
			return instance;
		}
	}

	public List<String> loadDict(String path) {
		List<String> words = null;
		List<KV> kvs = null;
		InputStreamReader read;
		try {
			read = new InputStreamReader(DictSegment.class.getResourceAsStream(path), ""UTF-8"");
			BufferedReader bufferedReader = new BufferedReader(read);
			String line;
			line = bufferedReader.readLine();
			words = new ArrayList<String>(Integer.parseInt(line));
			kvs = new ArrayList<KV>(Integer.parseInt(line));
			while ((line = bufferedReader.readLine()) != null) {
				String[] ss = line.split(""="");
				if (ss.length == 2) {
					kvs.add(new KV(ss[0], ss[1]));
				}
			}
			Collections.sort(kvs, new Comparator<KV>() {
				@Override
				public int compare(KV o1, KV o2) {
					return o1.k.compareTo(o2.k);
				}
			});
			dictionary = new String[kvs.size()];
			for (int i = 0; i < dictionary.length; i++)
				dictionary[i] = kvs.get(i).v;
			for (KV kv : kvs)
				words.add(kv.k);
		} catch (FileNotFoundException e) {
			logger.error(""File not found"", e);
		} catch (IOException e) {
			logger.error(""IOException"", e);
		}
		return words;
	}

	public List<Integer> prefixSearch(String text) {
		return tree.commonPrefixSearch(text);
	}

	public String getStringByIndex(Integer i) {
		return dictionary[i];
	}

	class KV {
		public KV(String k, String v) {
			this.k = k;
			this.v = v;
		}

		String k;
		String v;
	}
}",src/main/java/com/seaboat/text/analyzer/dict/TraditionalDict.java
SizeException,"public class SizeException extends Exception {

	private static final long serialVersionUID = 9211163095362471733L;

	public SizeException(String s) {
		super(s);
	}
}",src/main/java/com/seaboat/text/analyzer/exception/SizeException.java
