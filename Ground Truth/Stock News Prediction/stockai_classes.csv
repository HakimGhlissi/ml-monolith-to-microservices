ClassName,Code,File
EmojiFilter,"class EmojiFilter(logging.Filter):
    def filter(self, record):
        # Only add emoji if it's not already in the message
        if not any(emoji in record.getMessage() for emoji in LOG_LEVELS.values()):
            record.msg = f""{LOG_LEVELS.get(record.levelname, '')} {record.msg}""
        return True",core/logging.py
DataConfig,"class DataConfig(BaseModel):
    """"""Data collection and storage configuration.""""""

    STOCK_DATA_DIR: Path = Path(""data/stock"")
    NEWS_DATA_DIR: Path = Path(""data/news"")
    LOGS_DIR: Path = Path(""logs"")
    LOOKBACK_PERIOD_DAYS: int = 365
    NEWS_HISTORY_DAYS: int = 7
    MAX_NEWS_ARTICLES: int = 100
    UPDATE_INTERVAL: int = 60  # minutes",core/config.py
PreprocessingConfig,"class PreprocessingConfig(BaseModel):
    """"""Preprocessing service configuration""""""

    SCALERS_DIR: Path = Path(""scalers"")
    TRAINING_SPLIT_RATIO: float = 0.8
    SEQUENCE_LENGTH: int = 60",core/config.py
ModelConfig,"class ModelConfig(BaseModel):
    """"""Model configuration.""""""

    MODELS_ROOT_DIR: Path = Path(""models"")
    PREDICTION_MODELS_DIR: Path = Path(""models/specific"")
    PROPHET_MODELS_DIR: Path = Path(""models/prophet"")
    NEWS_MODELS_DIR: Path = Path(""models/news"")
    SENTIMENT_MODEL_NAME: str = ""distilbert-base-uncased-finetuned-sst-2-english""
    FEATURES: list = [
        ""Open"",
        ""High"",
        ""Low"",
        ""Close"",
        ""Adj Close"",
        ""Volume"",
        ""Returns"",
        ""MA_5"",
        ""MA_20"",
        ""Volatility"",
        ""RSI"",
        ""MACD"",
        ""MACD_Signal"",
    ]
    SEQUENCE_LENGTH: int = 60
    BATCH_SIZE: int = 1024
    EPOCHS: int = 50
    VALIDATION_SPLIT: float = 0.2",core/config.py
APIConfig,"class APIConfig(BaseModel):
    """"""API configuration.""""""

    HOST: str = ""0.0.0.0""
    PORT: int = 8000
    DEBUG: bool = False
    CORS_ORIGINS: list = [""*""]
    API_VERSION: str = ""1.0""",core/config.py
RabbitMQConfig,"class RabbitMQConfig(BaseModel):
    """"""RabbitMQ configuration.""""""

    HOST: str = ""rabbitmq""  # Use the Docker container's hostname
    PORT: int = 5672
    USER: str = ""guest""
    PASSWORD: str = ""guest""
    VHOST: str = ""/""
    QUEUE_PREFIX: str = ""stock_ai""",core/config.py
Config,"class Config:
    """"""Main configuration class.""""""

    def __init__(self):
        self.data = DataConfig()
        self.preprocessing = PreprocessingConfig()
        self.model = ModelConfig()
        self.api = APIConfig()
        self.rabbitmq = RabbitMQConfig()

        # Create necessary directories
        self._create_directories()

    def _create_directories(self) -> None:
        """"""Create necessary directories.""""""
        directories = [
            self.data.STOCK_DATA_DIR,
            self.data.NEWS_DATA_DIR,
            self.data.LOGS_DIR,
            self.model.PREDICTION_MODELS_DIR,
            self.model.PROPHET_MODELS_DIR,
            self.model.NEWS_MODELS_DIR,
        ]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)",core/config.py
ProcessedData,"class ProcessedData(Generic[TX]):
    X: Optional[TX] = None
    y: Optional[np.ndarray] = None
    feature_index_map: Optional[dict[str, int]] = None
    start_date: Optional[datetime.date] = None
    end_date: Optional[datetime.date] = None",core/types.py
Metrics,"class Metrics:
    mae: float
    mse: float
    rmse: float
    r2: float",core/types.py
MetaInfo,"class MetaInfo(BaseModel):
    """"""API metadata information.""""""

    message: str
    version: str
    documentation: str
    endpoints: List[str]",api/schemas.py
HealthResponse,"class HealthResponse(BaseModel):
    """"""Health check response.""""""

    status: str
    timestamp: str
    components: Dict[str, bool]",api/schemas.py
ErrorResponse,"class ErrorResponse(BaseModel):
    """"""Error response model.""""""

    error: str
    detail: Optional[str] = None
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())",api/schemas.py
PredictionResponse,"class PredictionResponse(BaseModel):
    """"""Prediction response schema.""""""

    status: str
    symbol: str
    date: str
    predicted_price: float
    confidence: float
    model_type: str
    model_version: int
    timestamp: str",api/schemas.py
PredictionsResponse,"class PredictionsResponse(BaseModel):
    """"""Historical predictions response schema.""""""

    symbol: str
    predictions: List[PredictionResponse]
    timestamp: str",api/schemas.py
NewsAnalysisResponse,"class NewsAnalysisResponse(BaseModel):
    """"""News analysis response schema.""""""

    symbol: str
    period: Dict[str, str]
    total_articles: int
    sentiment_metrics: Dict[str, float]
    articles: List[Dict[str, Any]]
    model_version: str",api/schemas.py
TrainingTrainersResponse,"class TrainingTrainersResponse(BaseModel):
    """"""Trainers getter response schema.""""""

    status: str
    types: List[str]
    count: int
    timestamp: str",api/schemas.py
TrainingResponse,"class TrainingResponse(BaseModel):
    """"""Model training response.""""""

    status: str
    symbol: str
    model_type: str
    training_results: Dict[str, Any]
    metrics: Dict[str, float]
    deployment_results: Dict[str, Any] = None
    timestamp: str",api/schemas.py
TrainingStatusResponse,"class TrainingStatusResponse(BaseModel):
    """"""Training status response.""""""

    status: str
    symbol: str
    model_type: str
    timestamp: str
    result: Optional[TrainingResponse] = None
    error: Optional[str] = None",api/schemas.py
TrainingTask,"class TrainingTask(BaseModel):
    """"""Individual training task information.""""""

    symbol: str
    model_type: str
    status: str
    timestamp: str",api/schemas.py
TrainingTasksResponse,"class TrainingTasksResponse(BaseModel):
    """"""List of active training tasks.""""""

    tasks: List[TrainingTask]
    total_tasks: int
    timestamp: str",api/schemas.py
DataUpdateResponse,"class DataUpdateResponse(BaseModel):
    """"""Data update response.""""""

    symbol: str
    stock_data_updated: bool
    timestamp: str
    stock_records: int
    news_articles: int",api/schemas.py
StockDataResponse,"class StockDataResponse(BaseModel):
    """"""Stock data response.""""""

    symbol: str
    name: str
    data: List[Dict[str, Any]]
    meta: MetaInfo
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())",api/schemas.py
StockItem,"class StockItem(BaseModel):
    """"""Infos about a stock""""""

    symbol: str
    sector: str
    companyName: str
    marketCap: str
    lastSalePrice: str
    netChange: str
    percentageChange: str
    deltaIndicator: str",api/schemas.py
StocksListDataResponse,"class StocksListDataResponse(BaseModel):
    """"""Stocks data list data response.""""""

    count: int
    data: List[StockItem]
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())",api/schemas.py
NewsDataResponse,"class NewsDataResponse(BaseModel):
    """"""News data response.""""""

    symbol: str
    articles: List[Dict[str, Any]]
    total_articles: int
    sentiment_metrics: Dict[str, float]
    meta: MetaInfo",api/schemas.py
ModelMetadata,"class ModelMetadata(BaseModel):
    """"""Model metadata information.""""""

    version: str
    created_at: str
    last_used: str
    performance_metrics: Dict[str, float]
    training_params: Dict[str, Any]",api/schemas.py
ModelInfo,"class ModelInfo(BaseModel):
    """"""Model information.""""""

    symbol: str
    model_type: str
    metadata: ModelMetadata",api/schemas.py
ModelListResponse,"class ModelListResponse(BaseModel):
    """"""List of available models.""""""

    models: List[ModelInfo]
    total_models: int
    timestamp: str",api/schemas.py
ModelMetadataResponse,"class ModelMetadataResponse(BaseModel):
    """"""Detailed model metadata response.""""""

    symbol: str
    model_type: str
    version: str
    metadata: ModelMetadata
    timestamp: str",api/schemas.py
DirectDisplayResponse,"class DirectDisplayResponse(BaseModel):
    """"""Direct display response schema.""""""

    symbol: str
    predictions: List[PredictionResponse]
    next_day: PredictionResponse
    news_analysis: NewsAnalysisResponse
    timestamp: str",api/schemas.py
DataService,"class DataService(BaseService):
    """"""Service for managing data collection and processing.""""""

    def __init__(self):
        super().__init__()
        self.logger = logger[""data""]
        self.config = config
        self.stock_data_dir = self.config.data.STOCK_DATA_DIR
        self.news_data_dir = self.config.data.NEWS_DATA_DIR

    async def initialize(self) -> None:
        """"""Initialize the data service.""""""
        try:
            # Create necessary directories
            self.stock_data_dir.mkdir(parents=True, exist_ok=True)
            self.news_data_dir.mkdir(parents=True, exist_ok=True)
            self._initialized = True
            self.logger.info(""Data service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize data service: {str(e)}"")
            raise

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            self._initialized = False
            self.logger.info(""Data service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during data service cleanup: {str(e)}"")

    async def get_stock_name(self, symbol: str) -> str:
        """"""
        Get the name of a stock given its symbol

        Args:
            symbol (str): Stock symbol

        Returns:
            str: The stock name
        """"""

        symbol = symbol.upper()
        data_file = self.stock_data_dir / ""stock_names.csv""

        try:
            # Check if cache exist
            if data_file.exists():
                df = pd.read_csv(data_file, index_col=""symbol"")
                if symbol in df.index:
                    return df.loc[symbol, ""name""]
            else:
                # Empty DataFrame with correct structure
                df = pd.DataFrame(columns=[""name""])
                df.index.name = ""symbol""

            # Download data from Yahoo Finance
            stock = yf.Ticker(symbol)

            # Log the successful external request to Yahoo Finance (Prometheus)
            external_requests_total.labels(site=""yahoo_finance"", result=""success"").inc()

            # Get the stock name (company)
            name = stock.info.get(""shortName"")

            self.logger.info(f""Collected stock name for {symbol}"")

            # Add new entry
            df.loc[symbol] = name

            # Save updated csv cache
            df.to_csv(data_file)

            return name

        except Exception as e:
            self.logger.error(f""Error collecting stock name for {symbol}: {str(e)}"")

            # Log the unsuccessful external request to Yahoo Finance (Prometheuss)
            external_requests_total.labels(site=""yahoo_finance"", result=""error"").inc()
            raise

    async def get_nasdaq_stocks(self) -> dict:
        """"""
        Fetch the NASDAQ 100 stocks sorted by price percentage change (descending)

        Returns:
            dict: A dictionary containing the count of symbols and the list of symbols
            from the NASDAQ 100 index.
        """"""

        def parse_percentage_change(pct_str):
            """"""Parse the percentage change of a stock (absolute value)""""""
            try:
                return abs(float(pct_str.strip(""%"").replace("","", """")))
            except:
                return 0.0

        self.logger.info(""Starting NASDAQ 100 stocks data retrieval process"")

        try:
            # Fetch the data
            url = ""https://api.nasdaq.com/api/quote/list-type/nasdaq100""
            headers = {""User-Agent"": ""Mozilla/5.0""}
            response = requests.get(url, headers=headers)
            data = response.json()[""data""][""data""][""rows""]

            # Sort the list by absolute percentageChange (descending)
            sorted_stocks = sorted(
                data,
                key=lambda x: parse_percentage_change(x.get(""percentageChange"", ""0%"")),
                reverse=True,
            )
            stocks_data = {""count"": len(sorted_stocks), ""data"": sorted_stocks}

            self.logger.info(""Retrieved NASDAQ 100 stocks data"")

            # Return the stock data
            return stocks_data
        except Exception as e:
            self.logger.error(f""Error collecting NASDAQ 100 stocks data: {str(e)}"")
            raise

    async def collect_stock_data(
        self,
        symbol: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> pd.DataFrame:
        """"""
        Collect stock data from Yahoo Finance.

        Args:
            symbol: Stock symbol
            start_date: Start date for data collection
            end_date: End date for data collection

        Returns:
            DataFrame containing stock data
        """"""
        try:
            # Set default date range if not provided
            if not end_date:
                end_date = datetime.now(timezone.utc)
            if not start_date:
                start_date = get_start_date_from_trading_days(end_date)

            # Ensure dates are timezone-aware
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=timezone.utc)
            if end_date.tzinfo is None:
                end_date = end_date.replace(tzinfo=timezone.utc)

            # Adjust the end date to the last possible moment of the day (to have the full-day)
            end_date = datetime.combine(end_date, time.max)

            # Download data from Yahoo Finance
            stock = yf.Ticker(symbol)

            # Log the successful external request to Yahoo Finance (Prometheus)
            external_requests_total.labels(site=""yahoo_finance"", result=""success"").inc()

            df = stock.history(start=start_date, end=end_date)

            # TODO Save raw data ?
            # data_file = self.stock_data_dir / f""raw_{symbol}.csv""
            # df.to_csv(data_file, index=False)

            # Reset index to make Date a column
            df = df.reset_index()

            # Save data
            data_file = self.stock_data_dir / f""{symbol}_data.csv""
            df.to_csv(data_file, index=False)

            self.logger.info(f""Collected stock data for {symbol}"")
            return df

        except Exception as e:
            self.logger.error(f""Error collecting stock data for {symbol}: {str(e)}"")

            # Log the unsuccessful external request to Yahoo Finance (Prometheus)
            external_requests_total.labels(site=""yahoo_finance"", result=""error"").inc()
            raise

    async def get_current_price(self, symbol: str):
        """"""
        Retrieves the stock price for the given symbol on the latest trading day.

        Args:
            symbol (str): The stock symbol (e.g., ""AAPL"").

        Returns:
            pandas.DataFrame: A DataFrame containing the stock data for the latest trading day.
            str: Stock Company Name
        """"""
        try:
            # Get start and end dates (as today)
            start_date = get_latest_trading_day()
            end_date = start_date

            # Retrieve stock data prices
            df = await self._get_stock_data(symbol, start_date, end_date)

            # Retrieve the latest trading day
            current_price = df[df[""Date""].dt.date == end_date.date()]

            # Get the stock_name
            stock_name = await self.get_stock_name(symbol)

            self.logger.info(f""Retrieved current stock price for {symbol}"")

            return current_price, stock_name
        except Exception as e:
            self.logger.error(
                f""Error getting current stock price for {symbol}: {str(e)}""
            )
            raise

    async def get_recent_data(self, symbol: str, days_back: int = None):
        try:
            # If there is no number of days back, use the default config lookback period days
            if days_back is None:
                days_back = self.config.data.LOOKBACK_PERIOD_DAYS

            # Get start and end dates
            end_date = datetime.now()
            start_date = get_start_date_from_trading_days(end_date, days_back)

            # Retrieve stock data prices
            df = await self._get_stock_data(symbol, start_date, end_date)

            # Filter data for requested date range
            mask = (df[""Date""].dt.date >= start_date.date()) & (
                df[""Date""].dt.date <= end_date.date()
            )
            df = df[mask]

            # Get the stock_name
            stock_name = await self.get_stock_name(symbol)

            self.logger.info(
                f""Retrieved recent stock prices for {symbol} looking back for {days_back} trading days""
            )

            return df, stock_name
        except Exception as e:
            self.logger.error(
                f""Error getting recent stock prices for {symbol} looking back for {days_back} trading days : {str(e)}""
            )
            raise

    async def get_historical_stock_prices(
        self, symbol: str, start_date: datetime, end_date: datetime
    ) -> pd.DataFrame:
        """"""
        Get historical stock prices for a symbol. If data doesn't exist or is outdated, collect new data.

        Args:
            symbol: Stock symbol
            start_date: Start date for data retrieval
            end_date: End date for data retrieval

        Returns:
            pd.DataFrame: DataFrame containing stock data
            str: Stock Company Name
        """"""
        try:
            # Retrieve stock data prices
            df = await self._get_stock_data(symbol, start_date, end_date)

            # Ensure dates are timezone-aware
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=timezone.utc)
            if end_date.tzinfo is None:
                end_date = end_date.replace(tzinfo=timezone.utc)

            # Filter data for requested date range
            mask = (df[""Date""].dt.date >= start_date.date()) & (
                df[""Date""].dt.date <= end_date.date()
            )
            df = df[mask]

            # Sort by date
            df = df.sort_values(""Date"")

            # Get the stock_name
            stock_name = await self.get_stock_name(symbol)

            self.logger.info(f""Retrieved historical stock data for {symbol}"")

            return df, stock_name

        except Exception as e:
            self.logger.error(f""Error getting stock data for {symbol}: {str(e)}"")
            raise

    async def _get_stock_data(
        self, symbol: str, start_date: datetime, end_date: datetime
    ):
        """"""
        Retrieves stock data for a given symbol and date range, using a cached CSV file if available and valid.

        If a cached file exists and contains valid data for the requested date range, it is returned.
        Otherwise, new data is fetched via `collect_stock_data()`.

        Args:
            symbol (str): The stock symbol (e.g., ""AAPL"", ""GOOG"").
            start_date (datetime): The start date of the desired data range.
            end_date (datetime): The end date of the desired data range.

        Returns:
            pd.DataFrame: A DataFrame containing the stock data for the specified symbol and date range.
        """"""
        try:
            # Check if we have recent data
            data_file = self.stock_data_dir / f""{symbol}_data.csv""

            if data_file.exists():
                df = pd.read_csv(data_file)

                # Convert dates to timezone-aware UTC
                df[""Date""] = pd.to_datetime(df[""Date""], format=""mixed"", utc=True)

                # Check if the cache needs to be reloaded
                reload = self._is_cache_valid(df, start_date, end_date)

                if not reload:
                    self.logger.info(f""Load data from cache for {symbol}"")
                    # Return data from cache
                    return df

            # No data exists or need reload, collect new data
            return await self.collect_stock_data(symbol, start_date, end_date)
        except Exception as e:
            self.logger.error(f""Error getting stock data for {symbol}: {str(e)}"")
            raise

    def _is_cache_valid(self, df, start_date, end_date):
        """"""
        Checks whether the cached stock data is valid by ensuring that all expected
        NYSE trading days within the given date range are present in the DataFrame.

        Args:
            df (pd.DataFrame): The cached stock data.
            start_date (datetime): The start of the date range to validate.
            end_date (datetime): The end of the date range to validate.

        Returns:
            set: A set of missing trading dates. If empty, the cache is considered valid.
        """"""
        # Get the trading days within start_date and end_date
        nyse = mcal.get_calendar(""NYSE"")
        schedule = nyse.schedule(start_date=start_date, end_date=end_date)
        trading_dates = set(schedule[""market_open""].dt.date)

        # Extract the dates present in the data
        df_dates = set(df[""Date""].dt.date)

        # Check if all expected dates are present
        missing_dates = trading_dates - df_dates

        return missing_dates

    async def cleanup_data(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """"""
        Clean up and maintain data files.

        Args:
            symbol: Optional specific symbol to clean up. If None, cleans all data files.

        Returns:
            Dictionary containing cleanup results
        """"""
        try:
            cleaned_files = []
            failed_files = []

            # Get list of files to clean
            if symbol:
                files = [self.stock_data_dir / f""{symbol}_data.csv""]
            else:
                files = list(self.stock_data_dir.glob(""*_data.csv""))

            for data_file in files:
                try:
                    # Skip if file doesn't exist
                    if not data_file.exists():
                        continue

                    # Read and validate data
                    df = pd.read_csv(data_file)
                    df[""Date""] = pd.to_datetime(df[""Date""], format=""mixed"", utc=True)

                    # Check for issues
                    needs_cleanup = False
                    if len(df) < 80:
                        self.logger.warning(
                            f""Data file {data_file.name} has insufficient data points: {len(df)}""
                        )
                        needs_cleanup = True
                    elif (
                        get_latest_trading_day().astimezone(timezone.utc)
                        - df[""Date""].max()
                    ).days > 1:
                        self.logger.warning(
                            f""Data file {data_file.name} is outdated: {df['Date'].max()}""
                        )
                        needs_cleanup = True
                    elif df.isna().any().any():
                        self.logger.warning(
                            f""Data file {data_file.name} contains NaN values""
                        )
                        needs_cleanup = True

                    # Clean up if needed
                    if needs_cleanup:
                        # Backup the file
                        backup_file = data_file.with_suffix("".csv.bak"")
                        data_file.rename(backup_file)

                        # Collect fresh data
                        symbol = data_file.stem.split(""_"")[0]
                        await self.collect_stock_data(symbol)

                        cleaned_files.append(data_file.name)

                except Exception as e:
                    self.logger.error(f""Error cleaning up {data_file.name}: {str(e)}"")
                    failed_files.append(data_file.name)

            return {
                ""status"": ""success"",
                ""cleaned_files"": cleaned_files,
                ""failed_files"": failed_files,
                ""timestamp"": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f""Error during data cleanup: {str(e)}"")
            return {
                ""status"": ""error"",
                ""message"": str(e),
                ""timestamp"": datetime.now().isoformat(),
            }",services/data_service.py
VisualizationService,"class VisualizationService(BaseService):
    """"""Service for generating interactive stock visualizations.""""""

    def __init__(self, data_service: DataService):
        super().__init__()
        self.data_service = data_service
        self.logger = logger[""visualization""]

    async def get_stock_chart(
        self, symbol: str, days: int = 30, include_indicators: bool = True
    ) -> Dict[str, Any]:
        """"""
        Generate an interactive stock chart with technical indicators.

        Args:
            symbol: Stock symbol
            days: Number of days of historical data to include
            include_indicators: Whether to include technical indicators

        Returns:
            Dictionary containing the Plotly figure and metadata
        """"""
        try:
            # Get historical data
            data_result = await self.data_service.get_historical_data(symbol, days=days)
            if data_result[""status""] != ""success"":
                raise RuntimeError(f""Failed to get historical data for {symbol}"")

            df = data_result[""data""]

            # Create figure with secondary y-axis
            fig = make_subplots(
                rows=2 if include_indicators else 1,
                cols=1,
                shared_xaxes=True,
                vertical_spacing=0.05,
                row_heights=[0.7, 0.3] if include_indicators else [1.0],
            )

            # Add candlestick chart
            fig.add_trace(
                go.Candlestick(
                    x=df[""Date""],
                    open=df[""Open""],
                    high=df[""High""],
                    low=df[""Low""],
                    close=df[""Close""],
                    name=""OHLC"",
                ),
                row=1,
                col=1,
            )

            # Add volume
            fig.add_trace(
                go.Bar(x=df[""Date""], y=df[""Volume""], name=""Volume"", opacity=0.5),
                row=1,
                col=1,
            )

            if include_indicators:
                # Add technical indicators
                if ""RSI"" in df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=df[""Date""],
                            y=df[""RSI""],
                            name=""RSI"",
                            line=dict(color=""purple""),
                        ),
                        row=2,
                        col=1,
                    )
                    # Add RSI overbought/oversold lines
                    fig.add_hline(
                        y=70, line_dash=""dash"", line_color=""red"", row=2, col=1
                    )
                    fig.add_hline(
                        y=30, line_dash=""dash"", line_color=""green"", row=2, col=1
                    )

                if ""MACD"" in df.columns:
                    fig.add_trace(
                        go.Scatter(
                            x=df[""Date""],
                            y=df[""MACD""],
                            name=""MACD"",
                            line=dict(color=""blue""),
                        ),
                        row=2,
                        col=1,
                    )
                    if ""MACD_Signal"" in df.columns:
                        fig.add_trace(
                            go.Scatter(
                                x=df[""Date""],
                                y=df[""MACD_Signal""],
                                name=""MACD Signal"",
                                line=dict(color=""orange""),
                            ),
                            row=2,
                            col=1,
                        )

            # Update layout
            fig.update_layout(
                title=f""{symbol} Stock Price and Indicators"",
                yaxis_title=""Price"",
                xaxis_rangeslider_visible=False,
                height=800,
                template=""plotly_dark"",
            )

            if include_indicators:
                fig.update_yaxes(title_text=""Price"", row=1, col=1)
                fig.update_yaxes(title_text=""Indicators"", row=2, col=1)

            return {
                ""status"": ""success"",
                ""figure"": fig.to_json(),
                ""metadata"": {
                    ""symbol"": symbol,
                    ""last_updated"": datetime.now().isoformat(),
                    ""data_points"": len(df),
                    ""date_range"": {
                        ""start"": df[""Date""].min().isoformat(),
                        ""end"": df[""Date""].max().isoformat(),
                    },
                },
            }

        except Exception as e:
            self.logger.error(f""Error generating stock chart for {symbol}: {str(e)}"")
            return {""status"": ""error"", ""error"": str(e)}

    async def get_prediction_chart(
        self, symbol: str, days: int = 30, model_type: str = ""lstm""
    ) -> Dict[str, Any]:
        """"""
        Generate an interactive chart showing historical data and predictions.

        Args:
            symbol: Stock symbol
            days: Number of days of historical data to include
            model_type: Type of model to use for prediction

        Returns:
            Dictionary containing the Plotly figure and metadata
        """"""
        try:
            # Get historical data
            data_result = await self.data_service.get_historical_data(symbol, days=days)
            if data_result[""status""] != ""success"":
                raise RuntimeError(f""Failed to get historical data for {symbol}"")

            df = data_result[""data""]

            # Get predictions
            from services.prediction_service import PredictionService

            prediction_service = PredictionService(None, self.data_service)
            prediction_result = await prediction_service.get_historical_predictions(
                symbol, days=days
            )

            if prediction_result[""status""] != ""success"":
                raise RuntimeError(f""Failed to get predictions for {symbol}"")

            predictions = prediction_result[""historical_predictions""]
            pred_df = pd.DataFrame(predictions)

            # Create figure
            fig = go.Figure()

            # Add actual price
            fig.add_trace(
                go.Scatter(
                    x=df[""Date""],
                    y=df[""Close""],
                    name=""Actual Price"",
                    line=dict(color=""blue""),
                )
            )

            # Add predictions
            fig.add_trace(
                go.Scatter(
                    x=pd.to_datetime(pred_df[""date""]),
                    y=pred_df[""prediction""],
                    name=""Predicted Price"",
                    line=dict(color=""red"", dash=""dash""),
                )
            )

            # Add error bars
            fig.add_trace(
                go.Scatter(
                    x=pd.to_datetime(pred_df[""date""]),
                    y=pred_df[""error""],
                    name=""Prediction Error"",
                    line=dict(color=""gray""),
                    opacity=0.5,
                )
            )

            # Update layout
            fig.update_layout(
                title=f""{symbol} Price Predictions vs Actual"",
                yaxis_title=""Price"",
                xaxis_title=""Date"",
                height=600,
                template=""plotly_dark"",
            )

            return {
                ""status"": ""success"",
                ""figure"": fig.to_json(),
                ""metadata"": {
                    ""symbol"": symbol,
                    ""model_type"": model_type,
                    ""last_updated"": datetime.now().isoformat(),
                    ""data_points"": len(df),
                    ""prediction_points"": len(pred_df),
                    ""date_range"": {
                        ""start"": df[""Date""].min().isoformat(),
                        ""end"": df[""Date""].max().isoformat(),
                    },
                },
            }

        except Exception as e:
            self.logger.error(
                f""Error generating prediction chart for {symbol}: {str(e)}""
            )
            return {""status"": ""error"", ""error"": str(e)}

    async def get_correlation_matrix(
        self, symbols: List[str], days: int = 30
    ) -> Dict[str, Any]:
        """"""
        Generate a correlation matrix heatmap for multiple stocks.

        Args:
            symbols: List of stock symbols
            days: Number of days of historical data to include

        Returns:
            Dictionary containing the Plotly figure and metadata
        """"""
        try:
            # Get data for all symbols
            all_data = {}
            for symbol in symbols:
                data_result = await self.data_service.get_historical_data(
                    symbol, days=days
                )
                if data_result[""status""] == ""success"":
                    all_data[symbol] = data_result[""data""]

            if not all_data:
                raise RuntimeError(""No data available for any of the symbols"")

            # Calculate correlations
            correlations = pd.DataFrame()
            for symbol, df in all_data.items():
                correlations[symbol] = df[""Close""].pct_change()

            correlation_matrix = correlations.corr()

            # Create heatmap
            fig = go.Figure(
                data=go.Heatmap(
                    z=correlation_matrix,
                    x=correlation_matrix.columns,
                    y=correlation_matrix.columns,
                    colorscale=""RdBu"",
                    zmid=0,
                )
            )

            # Update layout
            fig.update_layout(
                title=""Stock Correlation Matrix"",
                xaxis_title=""Symbol"",
                yaxis_title=""Symbol"",
                height=600,
                template=""plotly_dark"",
            )

            return {
                ""status"": ""success"",
                ""figure"": fig.to_json(),
                ""metadata"": {
                    ""symbols"": symbols,
                    ""last_updated"": datetime.now().isoformat(),
                    ""days"": days,
                    ""correlation_range"": {
                        ""min"": correlation_matrix.min().min(),
                        ""max"": correlation_matrix.max().max(),
                    },
                },
            }

        except Exception as e:
            self.logger.error(f""Error generating correlation matrix: {str(e)}"")
            return {""status"": ""error"", ""error"": str(e)}",services/visualization_service.py
NewsService,"class NewsService(BaseService):
    """"""Service for news analysis and sentiment.""""""

    _instance = None
    _initialized = False
    _sentiment_analyzer = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(NewsService, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        super().__init__()
        self.sentiment_analyzer = None
        self.model_version = ""0.1.0""
        self.news_data = {}
        self.sentiment_cache = {}
        self.logger = logging.getLogger(__name__)
        self.layout = create_layout()

        logger.info(""Initializing NewsService..."")

        # Download required NLTK data for TextBlob
        try:
            import nltk

            nltk.download(""punkt"")
            nltk.download(""averaged_perceptron_tagger"")
            nltk.download(""wordnet"")
            logger.info(""NLTK data downloaded successfully"")
        except Exception as e:
            logger.warning(f""Failed to download NLTK data: {str(e)}"")

        # Download TextBlob corpora
        try:
            import subprocess

            subprocess.run([""python"", ""-m"", ""textblob.download_corpora""], check=True)
            logger.info(""TextBlob corpora downloaded successfully"")
        except Exception as e:
            logger.warning(f""Failed to download TextBlob corpora: {str(e)}"")

        # Initialize FinBERT model
        try:
            # Device selection for GPU/CPU
            if torch.cuda.is_available():
                self.device = torch.device(""cuda"")
                device_id = 0
                logger.info(""CUDA available, using GPU"")
            elif torch.backends.mps.is_available():
                self.device = torch.device(""mps"")
                device_id = -1
                logger.info(""MPS available, using Apple Silicon GPU"")
            else:
                self.device = torch.device(""cpu"")
                device_id = -1
                logger.info(""No GPU available, using CPU"")

            logger.info(""Loading FinBERT model..."")
            self.model = BertForSequenceClassification.from_pretrained(
                ""yiyanghkust/finbert-tone"", num_labels=3
            )
            self.tokenizer = BertTokenizer.from_pretrained(""yiyanghkust/finbert-tone"")
            self.model.to(self.device)

            logger.info(""FinBERT model loaded successfully"")
            self._sentiment_analyzer = self._analyze_with_finbert
        except Exception as e:
            logger.warning(
                f""Failed to initialize FinBERT, falling back to TextBlob: {str(e)}""
            )
            self._sentiment_analyzer = self._analyze_with_textblob

        self._initialized = True
        logger.info(""NewsService initialized successfully"")

    @retry(
        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _download_model(self) -> str:
        """"""Download the sentiment model with retry logic.""""""
        try:
            # First try to load from cache
            cache_dir = os.path.expanduser(""~/.cache/huggingface/hub"")
            model_path = os.path.join(cache_dir, self.config.model.SENTIMENT_MODEL_NAME)

            if os.path.exists(model_path):
                logger.info(f""Using cached model from {model_path}"")
                return model_path

            # If not in cache, download
            logger.info(
                f""Downloading model {self.config.model.SENTIMENT_MODEL_NAME}...""
            )
            return snapshot_download(
                self.config.model.SENTIMENT_MODEL_NAME,
                force_download=False,  # Don't force download if already cached
                local_files_only=False,
                cache_dir=cache_dir,
            )
        except Exception as e:
            logger.error(f""Model download attempt failed: {str(e)}"")
            raise

    async def initialize(self) -> None:
        """"""Initialize the news service.""""""
        try:
            # Create spinner
            spinner = create_spinner(""Initializing sentiment analyzer..."")

            # Start spinner
            spinner.start()

            try:
                # Try to download model with retry logic
                model_path = await self._download_model()

                # Initialize sentiment analyzer
                self.sentiment_analyzer = pipeline(
                    ""sentiment-analysis"",
                    model=model_path,
                    device=0 if self.config.model.USE_GPU else -1,
                )

                # Stop spinner
                spinner.stop()

                # Clear console and show success message
                time.sleep(0.5)  # Small delay to ensure spinner is cleared
                print_status(
                    ""Success"",
                    ""News service initialized successfully"",
                    ""success"",
                    clear_previous=True,
                )

                self.logger.info(""News service initialized successfully"")

            except Exception as e:
                spinner.stop()
                self.logger.warning(
                    f""Failed to download model, proceeding with TextBlob fallback: {str(e)}""
                )
                print_status(
                    ""Warning"",
                    ""Using TextBlob fallback for sentiment analysis"",
                    ""warning"",
                    clear_previous=True,
                )

                # Initialize TextBlob as fallback with improved confidence calculation
                def textblob_analyzer(text):
                    blob = TextBlob(text)
                    polarity = blob.sentiment.polarity
                    subjectivity = blob.sentiment.subjectivity

                    # Enhanced financial context words and phrases
                    positive_words = [
                        ""buy"",
                        ""bull"",
                        ""soar"",
                        ""growth"",
                        ""opportunity"",
                        ""magnificent"",
                        ""brilliant"",
                        ""steady"",
                        ""top"",
                        ""best"",
                        ""strong"",
                        ""outperform"",
                        ""upgrade"",
                        ""recommend"",
                        ""favorite"",
                        ""leading"",
                        ""dominant"",
                        ""innovative"",
                        ""breakthrough"",
                        ""revolutionary"",
                        ""transformative"",
                        ""promising"",
                        ""undervalued"",
                        ""bargain"",
                        ""attractive"",
                        ""compelling"",
                        ""conviction"",
                        ""long-term"",
                        ""sustainable"",
                    ]

                    negative_words = [
                        ""sell"",
                        ""bear"",
                        ""dip"",
                        ""turmoil"",
                        ""risk"",
                        ""beaten-down"",
                        ""down"",
                        ""collapse"",
                        ""crash"",
                        ""warning"",
                        ""concern"",
                        ""caution"",
                        ""trouble"",
                        ""struggle"",
                        ""challenge"",
                        ""headwind"",
                        ""pressure"",
                        ""decline"",
                        ""drop"",
                        ""fall"",
                        ""plunge"",
                        ""slump"",
                        ""weakness"",
                        ""vulnerable"",
                        ""exposed"",
                        ""threat"",
                        ""overvalued"",
                        ""expensive"",
                        ""premium"",
                        ""bubble"",
                        ""speculative"",
                        ""uncertain"",
                        ""volatile"",
                    ]

                    # Count occurrences of financial sentiment words
                    text_lower = text.lower()
                    positive_count = sum(
                        1 for word in positive_words if word.lower() in text_lower
                    )
                    negative_count = sum(
                        1 for word in negative_words if word.lower() in text_lower
                    )

                    # Analyze phrases for stronger sentiment signals
                    phrases = blob.noun_phrases
                    phrase_sentiment = 0
                    for phrase in phrases:
                        phrase_lower = phrase.lower()
                        if any(word in phrase_lower for word in positive_words):
                            phrase_sentiment += 0.2
                        if any(word in phrase_lower for word in negative_words):
                            phrase_sentiment -= 0.2

                    # Calculate context adjustment
                    word_adjustment = (positive_count - negative_count) * 0.1
                    context_adjustment = word_adjustment + phrase_sentiment

                    # Adjust polarity based on context
                    adjusted_polarity = polarity + context_adjustment

                    # Determine sentiment label with adjusted thresholds
                    if adjusted_polarity > 0.03:  # Even lower threshold for positive
                        label = ""POSITIVE""
                    elif adjusted_polarity < -0.03:  # Even lower threshold for negative
                        label = ""NEGATIVE""
                    else:
                        label = ""NEUTRAL""

                    # Calculate confidence score
                    # Base confidence on adjusted polarity and subjectivity
                    base_confidence = abs(adjusted_polarity)

                    # Increase confidence if there are clear financial sentiment words
                    word_confidence = min(1.0, (positive_count + negative_count) * 0.1)

                    # Add phrase confidence
                    phrase_confidence = min(1.0, abs(phrase_sentiment))

                    # Combine all confidence factors
                    confidence = (
                        base_confidence
                        + word_confidence
                        + phrase_confidence
                        + (1 - subjectivity)
                    ) / 4

                    # Ensure confidence is between 0 and 1
                    confidence = max(0.0, min(1.0, confidence))

                    # Log analysis details for debugging
                    self.logger.debug(
                        f""""""
                    Text: {text[:100]}...
                    Polarity: {polarity}
                    Subjectivity: {subjectivity}
                    Positive words: {positive_count}
                    Negative words: {negative_count}
                    Phrase sentiment: {phrase_sentiment}
                    Context adjustment: {context_adjustment}
                    Adjusted polarity: {adjusted_polarity}
                    Label: {label}
                    Confidence: {confidence}
                    """"""
                    )

                    return [{""label"": label, ""score"": confidence}]

                self.sentiment_analyzer = textblob_analyzer

        except Exception as e:
            self.logger.error(f""Failed to initialize news service: {str(e)}"")
            print_error(e)
            raise

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            # Clear sentiment analyzer
            self.sentiment_analyzer = None
            self._initialized = False
            self.logger.info(""News service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during news service cleanup: {str(e)}"")
            print_error(e)

    async def analyze_news(self, symbol: str, days: int = 7) -> Dict[str, Any]:
        """"""
        Analyze news articles for a given symbol.

        Args:
            symbol: Stock symbol
            days: Number of days of news to analyze

        Returns:
            Dictionary containing news analysis results
        """"""
        if not self._initialized:
            raise RuntimeError(""News service not initialized"")

        try:
            with create_spinner(f""Analyzing news for {symbol}..."") as spinner:
                # Get date range
                start_date, end_date = get_date_range(days)

                # Get news articles
                articles = await self._get_news_articles(symbol, start_date, end_date)

                # Analyze sentiment
                sentiment_results = await self._analyze_sentiment(articles)

                # Calculate aggregate metrics
                metrics = self._calculate_metrics(sentiment_results)

                return {
                    ""symbol"": symbol,
                    ""period"": {
                        ""start"": start_date.isoformat(),
                        ""end"": end_date.isoformat(),
                    },
                    ""total_articles"": len(articles),
                    ""sentiment_metrics"": metrics,
                    ""articles"": sentiment_results,
                    ""model_version"": self.model_version,
                }

        except Exception as e:
            self.logger.error(f""Error analyzing news for {symbol}: {str(e)}"")
            print_error(e)
            raise

    async def _get_news_articles(
        self, symbol: str, start_date: datetime, end_date: datetime
    ) -> List[Dict[str, Any]]:
        """"""Get news articles for a given symbol and date range.""""""
        try:
            # Ensure timezone-aware datetimes
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=timezone.utc)
            if end_date.tzinfo is None:
                end_date = end_date.replace(tzinfo=timezone.utc)

            # Ensure to include the entire end_date by setting its time to the end of the day
            end_date = end_date.replace(
                hour=23, minute=59, second=59, microsecond=999999
            )

            # Fetch news from Yahoo Finance
            ticker = yf.Ticker(symbol)
            news = ticker.news

            # Log the successful external request to Yahoo Finance (Prometheus)
            external_requests_total.labels(site=""yahoo_finance"", result=""success"").inc()

            articles = []
            for item in news:
                try:
                    # Extract article date
                    news_date = None
                    if ""pubDate"" in item:
                        news_date = datetime.fromisoformat(
                            item[""pubDate""].replace(""Z"", ""+00:00"")
                        )
                    elif ""content"" in item and ""pubDate"" in item[""content""]:
                        news_date = datetime.fromisoformat(
                            item[""content""][""pubDate""].replace(""Z"", ""+00:00"")
                        )

                    # Only include articles within the date range
                    if news_date and start_date <= news_date <= end_date:
                        article = {}
                        if ""content"" in item:
                            content = item[""content""]
                            article[""title""] = content.get(""title"", ""No Title"")
                            article[""content""] = content.get(""summary"", """")
                            article[""published_date""] = news_date

                            # Get URL
                            if (
                                ""clickThroughUrl"" in content
                                and content[""clickThroughUrl""]
                                and ""url"" in content[""clickThroughUrl""]
                            ):
                                article[""url""] = content[""clickThroughUrl""][""url""]
                            elif (
                                ""canonicalUrl"" in content
                                and content[""canonicalUrl""]
                                and ""url"" in content[""canonicalUrl""]
                            ):
                                article[""url""] = content[""canonicalUrl""][""url""]
                            else:
                                article[""url""] = """"

                            # Get source
                            if ""provider"" in content and content[""provider""]:
                                article[""source""] = content[""provider""].get(
                                    ""displayName"", ""Unknown""
                                )
                            else:
                                article[""source""] = ""Unknown""
                        else:
                            article[""title""] = item.get(""title"", ""No Title"")
                            article[""content""] = item.get(""summary"", """")
                            article[""published_date""] = news_date
                            article[""url""] = item.get(""link"", """")
                            article[""source""] = ""Unknown""

                        articles.append(article)
                except Exception as e:
                    self.logger.error(f""Error processing news item: {str(e)}"")

                    # Log the unsuccessful external request to Yahoo Finance (Prometheuss)
                    external_requests_total.labels(
                        site=""yahoo_finance"", result=""error""
                    ).inc()
                    continue

            return articles

        except Exception as e:
            self.logger.error(f""Error fetching news for {symbol}: {str(e)}"")
            return []

    async def _analyze_sentiment(
        self, articles: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """"""Analyze sentiment for a list of articles.""""""
        results = []
        for article in articles:
            try:
                # Truncate text if needed
                text = article[""title""] + "" "" + article[""content""]
                if len(text) > self.config.model.MAX_NEWS_LENGTH:
                    text = text[: self.config.model.MAX_NEWS_LENGTH]

                # Get sentiment analysis
                sentiment = self.sentiment_analyzer(text)[0]

                results.append(
                    {
                        ""title"": article[""title""],
                        ""url"": article[""url""],
                        ""published_date"": article[""published_date""],
                        ""sentiment"": sentiment[""label""],
                        ""confidence"": sentiment[""score""],
                    }
                )
            except Exception as e:
                self.logger.error(f""Error analyzing sentiment for article: {str(e)}"")
                continue

        return results

    def _calculate_metrics(
        self, sentiment_results: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """"""Calculate aggregate sentiment metrics.""""""
        if not sentiment_results:
            return {
                ""positive"": 0.0,
                ""negative"": 0.0,
                ""neutral"": 0.0,
                ""average_confidence"": 0.0,
            }

        total = len(sentiment_results)

        # positive = sum(1 for r in sentiment_results if r.get(""sentiment"") == ""POSITIVE"")
        positive = sum(
            result.get(""confidence"")
            for result in sentiment_results
            if result.get(""sentiment"") == ""POSITIVE""
            and result.get(""confidence"") is not None
        )

        # negative = sum(1 for r in sentiment_results if r.get(""sentiment"") == ""NEGATIVE"")
        negative = sum(
            result.get(""confidence"")
            for result in sentiment_results
            if result.get(""sentiment"") == ""NEGATIVE""
            and result.get(""confidence"") is not None
        )

        # neutral = total - positive - negative
        neutral = sum(
            result.get(""confidence"")
            for result in sentiment_results
            if result.get(""sentiment"") == ""NEUTRAL""
            and result.get(""confidence"") is not None
        )

        # Calculate average confidence only for articles with valid confidence scores
        valid_confidences = [
            r[""confidence""]
            for r in sentiment_results
            if r.get(""confidence"") is not None
        ]
        avg_confidence = (
            sum(valid_confidences) / len(valid_confidences)
            if valid_confidences
            else 0.0
        )

        return {
            ""positive"": positive / total if total > 0 else 0.0,
            ""negative"": negative / total if total > 0 else 0.0,
            ""neutral"": neutral / total if total > 0 else 0.0,
            ""average_confidence"": avg_confidence,
        }

    async def get_news_data(
        self, symbol: str, start_date: datetime, end_date: datetime
    ) -> Dict[str, Any]:
        """"""Get news data for a given symbol and date range.""""""
        try:
            self.logger.info(f""Starting news data retrieval for {symbol}"")

            # Fetch news articles
            articles = await self._get_news_articles(symbol, start_date, end_date)
            self.logger.info(f""Retrieved {len(articles)} articles for {symbol}"")

            if not articles:
                self.logger.info(f""No articles found for {symbol}"")
                return {
                    ""articles"": [],
                    ""total_articles"": 0,
                    ""sentiment_metrics"": {},
                    ""meta"": {
                        ""start_date"": start_date.isoformat(),
                        ""end_date"": end_date.isoformat(),
                        ""version"": self.model_version,
                        ""message"": ""No news articles found"",
                        ""documentation"": ""https://api.example.com/docs"",
                        ""endpoints"": [""/api/data/news/{symbol}""],
                    },
                }

            # Process and analyze articles
            start_time = time.perf_counter()  # Start timer

            processed_articles = []
            for article in articles:
                try:
                    self.logger.debug(
                        f""Processing article: {article.get('title', 'No title')}""
                    )

                    # Get sentiment analysis if available
                    sentiment = None
                    confidence = None
                    if self.sentiment_analyzer is not None:
                        try:
                            text = article[""title""] + "" "" + article[""content""]
                            self.logger.debug(f""Analyzing text: {text[:100]}..."")

                            sentiment_result = self.sentiment_analyzer(text)[0]
                            sentiment = sentiment_result[""label""]
                            confidence = sentiment_result[""score""]

                            self.logger.debug(
                                f""Sentiment analysis result: {sentiment} (confidence: {confidence})""
                            )
                        except Exception as e:
                            self.logger.warning(f""Sentiment analysis failed: {str(e)}"")

                    processed_article = {
                        ""title"": article[""title""],
                        ""url"": article[""url""],
                        ""published_date"": article[""published_date""].isoformat(),
                        ""source"": article[""source""],
                        ""sentiment"": sentiment,
                        ""confidence"": confidence,
                    }
                    processed_articles.append(processed_article)
                except Exception as e:
                    self.logger.error(f""Error processing article: {str(e)}"")
                    continue

            # Log the sentiment analysis time (Prometheus)
            sentiment_analysis_duration = time.perf_counter() - start_time
            sentiment_analysis_time_seconds.labels(
                number_articles=len(processed_articles)
            ).observe(sentiment_analysis_duration)

            self.logger.info(
                f""Successfully processed {len(processed_articles)} articles""
            )

            # Calculate sentiment metrics
            metrics = self._calculate_metrics(processed_articles)
            self.logger.debug(f""Calculated metrics: {metrics}"")

            result = {
                ""articles"": processed_articles,
                ""total_articles"": len(processed_articles),
                ""sentiment_metrics"": metrics,
                ""meta"": {
                    ""start_date"": start_date.isoformat(),
                    ""end_date"": end_date.isoformat(),
                    ""version"": self.model_version,
                    ""message"": ""News articles retrieved successfully"",
                    ""documentation"": ""https://api.example.com/docs"",
                    ""endpoints"": [""/api/data/news/{symbol}""],
                },
            }

            self.logger.info(f""Successfully completed news data retrieval for {symbol}"")
            return result

        except Exception as e:
            self.logger.error(f""Error getting news data: {str(e)}"")
            self.logger.error(f""Error type: {type(e)}"")
            self.logger.error(f""Error traceback: {e.__traceback__}"")
            raise

    def _analyze_with_finbert(self, text: str) -> List[Dict]:
        """"""Analyze sentiment using FinBERT model""""""
        try:
            # BERT models have a maximum token limit of 512 tokens
            max_length = 512

            # Tokenize with truncation
            encoded = self.tokenizer.encode_plus(
                text,
                max_length=max_length,
                truncation=True,
                padding=""max_length"",
                return_tensors=""pt"",
            )

            # Send to device
            input_ids = encoded[""input_ids""].to(self.device)
            attention_mask = encoded[""attention_mask""].to(self.device)

            # Get model outputs
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)

            # Get probabilities and sentiment
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]

            # Map indices to sentiments
            sentiment_map = {0: ""neutral"", 1: ""positive"", 2: ""negative""}
            sentiment_idx = torch.argmax(probs).item()
            sentiment = sentiment_map[sentiment_idx]
            confidence = probs[sentiment_idx].item()

            # Convert to opinion score
            opinion_map = {""positive"": 1, ""negative"": -1, ""neutral"": 0}
            opinion = opinion_map.get(sentiment, 0)

            # Generate summary
            confidence_percentage = round(confidence * 100)
            if confidence_percentage >= 80:
                strength = ""strongly""
            elif confidence_percentage >= 60:
                strength = ""moderately""
            else:
                strength = ""slightly""
            summary = f""{strength.capitalize()} {sentiment} sentiment ({confidence_percentage}% confidence)""

            return [
                {
                    ""label"": sentiment.upper(),
                    ""score"": confidence,
                    ""scores"": {
                        ""positive"": probs[1].item(),
                        ""negative"": probs[2].item(),
                        ""neutral"": probs[0].item(),
                    },
                    ""opinion"": opinion,
                    ""summary"": summary,
                }
            ]
        except Exception as e:
            logger.error(f""FinBERT analysis failed: {e}"")
            return self._analyze_with_textblob(text)

    def _analyze_with_textblob(self, text: str) -> List[Dict]:
        """"""Fallback sentiment analysis using TextBlob""""""
        try:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            subjectivity = blob.sentiment.subjectivity

            # Determine sentiment label
            if polarity > 0.1:
                sentiment = ""positive""
            elif polarity < -0.1:
                sentiment = ""negative""
            else:
                sentiment = ""neutral""

            # Calculate confidence
            confidence = (abs(polarity) + (1 - subjectivity)) / 2

            # Convert to opinion score
            opinion_map = {""positive"": 1, ""negative"": -1, ""neutral"": 0}
            opinion = opinion_map.get(sentiment, 0)

            # Generate summary
            confidence_percentage = round(confidence * 100)
            if confidence_percentage >= 80:
                strength = ""strongly""
            elif confidence_percentage >= 60:
                strength = ""moderately""
            else:
                strength = ""slightly""
            summary = f""{strength.capitalize()} {sentiment} sentiment ({confidence_percentage}% confidence)""

            return [
                {
                    ""label"": sentiment.upper(),
                    ""score"": confidence,
                    ""scores"": {
                        ""positive"": 1.0 if sentiment == ""positive"" else 0.0,
                        ""negative"": 1.0 if sentiment == ""negative"" else 0.0,
                        ""neutral"": 1.0 if sentiment == ""neutral"" else 0.0,
                    },
                    ""opinion"": opinion,
                    ""summary"": summary,
                }
            ]
        except Exception as e:
            logger.error(f""TextBlob analysis failed: {e}"")
            return [
                {
                    ""label"": ""NEUTRAL"",
                    ""score"": 1.0,
                    ""scores"": {""positive"": 0.0, ""negative"": 0.0, ""neutral"": 1.0},
                    ""opinion"": 0,
                    ""summary"": ""Unable to analyze sentiment"",
                }
            ]",services/news_service.py
BaseService,"class BaseService(ABC):
    """"""Base class for all services in the Stock AI system.""""""
    
    def __init__(self):
        self.config = config
        self.logger = logger
        self._initialized = False
    
    @abstractmethod
    async def initialize(self) -> None:
        """"""Initialize the service.""""""
        pass
    
    @abstractmethod
    async def cleanup(self) -> None:
        """"""Clean up resources used by the service.""""""
        pass
    
    def is_initialized(self) -> bool:
        """"""Check if the service is initialized.""""""
        return self._initialized
    
    def validate_input(self, data: Dict[str, Any]) -> bool:
        """"""Validate input data.""""""
        return True
    
    def format_error_response(self, error: Exception) -> Dict[str, Any]:
        """"""Format error response.""""""
        return {
            ""status"": ""error"",
            ""message"": str(error),
            ""timestamp"": datetime.utcnow().isoformat()
        }
    
    def format_success_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """"""Format success response.""""""
        return {
            ""status"": ""success"",
            ""data"": data,
            ""timestamp"": datetime.utcnow().isoformat()
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """"""Check service health.""""""
        return {
            ""status"": ""healthy"" if self._initialized else ""unhealthy"",
            ""initialized"": self._initialized,
            ""timestamp"": datetime.utcnow().isoformat()
        }",services/base_service.py
PredictionService,"class PredictionService(BaseService):
    """"""Service for stock price predictions.""""""

    def __init__(self, model_service: ModelService, data_service: DataService):
        super().__init__()
        self.models = {}
        self.scalers = {}
        self.model_version = ""0.1.0""
        self._initialized = False
        self.use_gpu = False  # Always False since we're using CPU-only version
        self.model_service = model_service
        self.data_service = data_service
        self.logger = logger[""prediction""]
        self.config = config
        self.rabbitmq_service = RabbitMQService()
        self._publish_task = None
        self._stop_publishing = False
        self.training_service = TrainingService(model_service, data_service)

    async def initialize(self) -> None:
        """"""Initialize the prediction service.""""""
        try:
            # Initialize services
            await self.data_service.initialize()
            await self.model_service.initialize()
            await self.training_service.initialize()

            # Train missing Prophet models
            await self._train_missing_prophet_models()

            # Load models
            await self._load_models()

            self._initialized = True
            self.logger.info(
                ""Prediction service initialized successfully with CPU configuration""
            )

            # Set up day-started callback
            self.rabbitmq_service.set_day_started_callback(self._on_day_started)

        except Exception as e:
            self.logger.error(f""Failed to initialize prediction service: {str(e)}"")
            raise

    async def _train_missing_prophet_models(self) -> None:
        """"""Train Prophet models for symbols that don't have them.""""""
        try:
            # List of symbols that should have Prophet models
            target_symbols = [
                ""AAPL"",
                ""ADBE"",
                ""AMZN"",
                ""CSCO"",
                ""GOOGL"",
                ""INTC"",
                ""META"",
                ""MSFT"",
                ""NVDA"",
                ""TSLA"",
            ]

            # Get existing Prophet models
            existing_models = [
                f.stem.split(""_"")[0]
                for f in Path(""models/prophet"").glob(""*_prophet.joblib"")
            ]

            # Find missing models
            missing_models = [
                symbol for symbol in target_symbols if symbol not in existing_models
            ]

            if not missing_models:
                self.logger.info(""All Prophet models are up to date"")
                return

            self.logger.info(
                f""Training Prophet models for: {', '.join(missing_models)}""
            )

            # Train missing models
            for symbol in missing_models:
                try:
                    self.logger.info(f""Training Prophet model for {symbol}"")

                    result = await self.training_service.train_model(
                        symbol=symbol,
                        model_type=""prophet"",
                        start_date=datetime.now()
                        - timedelta(days=365 * 2),  # 2 years of data
                        end_date=datetime.now(),
                        changepoint_prior_scale=0.05,
                        seasonality_prior_scale=10.0,
                        holidays_prior_scale=10.0,
                        seasonality_mode=""multiplicative"",
                    )

                    if result.get(""status"") == ""success"":
                        self.logger.info(
                            f""Successfully trained Prophet model for {symbol}""
                        )
                        self.logger.info(f""Model metrics: {result.get('metrics', {})}"")
                    else:
                        self.logger.error(
                            f""Failed to train Prophet model for {symbol}: {result.get('error', 'Unknown error')}""
                        )

                except Exception as e:
                    self.logger.error(
                        f""Error training Prophet model for {symbol}: {str(e)}""
                    )

        except Exception as e:
            self.logger.error(f""Error in _train_missing_prophet_models: {str(e)}"")
            raise

    def _on_day_started(self, message: Dict[str, Any]) -> None:
        """"""Handle day-started event.""""""
        try:
            self.logger.info(""Day started event received, publishing predictions"")
            # Create a task to run the coroutine in the background
            asyncio.create_task(self._publish_all_predictions())
        except Exception as e:
            self.logger.error(f""Error handling day-started event: {str(e)}"")

    async def _publish_all_predictions(self) -> None:
        """"""Publish predictions for all available models.""""""
        try:
            # Get all available models
            lstm_models = list(self.model_service._specific_models.keys())
            prophet_models = [
                f.stem.split(""_"")[0]
                for f in self.model_service.prophet_dir.glob(""*_prophet.joblib"")
            ]

            # Combine and deduplicate symbols
            all_symbols = list(set(lstm_models + prophet_models))

            self.logger.info(f""Publishing predictions for {len(all_symbols)} symbols"")

            # Create tasks for all predictions
            tasks = []
            for symbol in all_symbols:
                # Try LSTM first
                if symbol in lstm_models:
                    tasks.append(self.get_prediction(symbol, ""lstm""))
                # Then try Prophet if available
                if symbol in prophet_models:
                    tasks.append(self.get_prediction(symbol, ""prophet""))

            # Process predictions as they complete
            success_count = 0
            error_count = 0
            successful_predictions = []
            failed_predictions = []

            # Create a progress tracking task
            total_tasks = len(tasks)
            completed_tasks = 0

            # Process results as they complete
            for future in asyncio.as_completed(tasks):
                try:
                    result = await future
                    completed_tasks += 1

                    if isinstance(result, dict) and result.get(""status"") != ""error"":
                        success_count += 1
                        pred_info = {
                            ""symbol"": result.get(""symbol"", ""unknown""),
                            ""model_type"": result.get(""model_type"", ""unknown""),
                            ""prediction"": result.get(""prediction"", 0.0),
                            ""confidence"": result.get(""confidence_score"", 0.0),
                        }
                        successful_predictions.append(pred_info)
                        self.logger.info(
                            f""✅ [{completed_tasks}/{total_tasks}] {pred_info['symbol']} ({pred_info['model_type'].upper()}): ""
                            f""${pred_info['prediction']:.2f} (Confidence: {pred_info['confidence']:.1%})""
                        )
                    else:
                        error_count += 1
                        fail_info = {
                            ""symbol"": (
                                result.get(""symbol"", ""unknown"")
                                if isinstance(result, dict)
                                else ""unknown""
                            ),
                            ""model_type"": (
                                result.get(""model_type"", ""unknown"")
                                if isinstance(result, dict)
                                else ""unknown""
                            ),
                            ""error"": (
                                str(result)
                                if not isinstance(result, dict)
                                else result.get(""error"", ""unknown error"")
                            ),
                        }
                        failed_predictions.append(fail_info)
                        self.logger.warning(
                            f""❌ [{completed_tasks}/{total_tasks}] {fail_info['symbol']} ({fail_info['model_type'].upper()}): {fail_info['error']}""
                        )
                except Exception as e:
                    error_count += 1
                    self.logger.error(
                        f""❌ [{completed_tasks}/{total_tasks}] Error processing prediction: {str(e)}""
                    )

            # Log final summary
            self.logger.info(f""📊 Prediction Summary:"")
            self.logger.info(f""   Total predictions attempted: {total_tasks}"")
            self.logger.info(f""   Successful predictions: {success_count}"")
            self.logger.info(f""   Failed predictions: {error_count}"")

            # Log successful predictions summary
            if successful_predictions:
                self.logger.info(""✨ Successful Predictions Summary:"")
                for pred in successful_predictions:
                    self.logger.info(
                        f""   {pred['symbol']} ({pred['model_type'].upper()}): ""
                        f""${pred['prediction']:.2f} (Confidence: {pred['confidence']:.1%})""
                    )

            # Log failed predictions summary
            if failed_predictions:
                self.logger.warning(""❌ Failed Predictions Summary:"")
                for fail in failed_predictions:
                    self.logger.warning(
                        f""   {fail['symbol']} ({fail['model_type'].upper()}): {fail['error']}""
                    )

        except Exception as e:
            self.logger.error(f""Error publishing predictions: {str(e)}"")

    async def start_auto_publishing(self, interval_minutes: int = 5) -> None:
        """"""
        Start automatically publishing predictions for all available models.

        Args:
            interval_minutes: Time interval between publishing predictions
        """"""
        if not self._initialized:
            raise RuntimeError(""Prediction service not initialized"")

        self._stop_publishing = False
        self._publish_task = asyncio.create_task(self._publish_loop(interval_minutes))
        self.logger.info(
            f""Started auto-publishing predictions every {interval_minutes} minutes""
        )

    async def stop_auto_publishing(self) -> None:
        """"""Stop the auto-publishing task.""""""
        if self._publish_task:
            self._stop_publishing = True
            await self._publish_task
            self._publish_task = None
            self.logger.info(""Stopped auto-publishing predictions"")

    async def _publish_loop(self, interval_minutes):
        """"""Main loop for publishing predictions.""""""
        self.logger.info(""✨ Starting prediction publishing loop"")

        while not self._stop_publishing:
            try:
                # Get all symbols that need predictions
                symbols = self._get_symbols_for_prediction()
                if not symbols:
                    self.logger.info(""✨ No symbols need predictions, waiting..."")
                    await asyncio.sleep(60)  # Check every minute
                    continue

                self.logger.info(f""✨ Found {len(symbols)} symbols needing predictions"")

                # Create tasks for all predictions
                tasks = []
                for symbol in symbols:
                    # Create tasks for both LSTM and Prophet predictions
                    lstm_task = asyncio.create_task(self._get_lstm_prediction(symbol))
                    tasks.append(lstm_task)

                    # Only add Prophet task if model exists
                    prophet_models = [
                        f.stem.split(""_"")[0]
                        for f in self.model_service.prophet_dir.glob(""*_prophet.joblib"")
                    ]
                    if symbol in prophet_models:
                        prophet_task = asyncio.create_task(
                            self._get_prophet_prediction(symbol)
                        )
                        tasks.append(prophet_task)

                # Process predictions as they complete
                total_tasks = len(tasks)
                completed_tasks = 0
                successful_predictions = []
                failed_predictions = []

                for completed_task in asyncio.as_completed(tasks):
                    try:
                        result = await completed_task
                        completed_tasks += 1

                        if result:
                            symbol, prediction, model_type = result
                            # Publish the prediction
                            if await self._publish_prediction(
                                symbol, prediction, model_type
                            ):
                                successful_predictions.append(
                                    (symbol, prediction, model_type)
                                )
                                self.logger.info(
                                    f""✨ ✅ [{completed_tasks}/{total_tasks}] {symbol} ({model_type}): ""
                                    f""${prediction['prediction']:.2f} (Confidence: {prediction['confidence_score']:.1%})""
                                )
                            else:
                                failed_predictions.append((symbol, model_type))
                                self.logger.error(
                                    f""✨ ❌ [{completed_tasks}/{total_tasks}] Failed to publish prediction for {symbol} ({model_type})""
                                )
                    except Exception as e:
                        completed_tasks += 1
                        self.logger.error(
                            f""❌ Error processing prediction task: {str(e)}""
                        )
                        failed_predictions.append((""unknown"", ""unknown""))

                # Log final summary
                self.logger.info(""✨ 📊 Prediction Summary:"")
                self.logger.info(f""✨    Total predictions attempted: {total_tasks}"")
                self.logger.info(
                    f""✨    Successful predictions: {len(successful_predictions)}""
                )
                self.logger.info(f""✨    Failed predictions: {len(failed_predictions)}"")

                if successful_predictions:
                    self.logger.info(""✨ Successful Predictions Summary:"")
                    for symbol, prediction, model_type in successful_predictions:
                        self.logger.info(
                            f""✨    {symbol} ({model_type}): ""
                            f""${prediction['prediction']:.2f} (Confidence: {prediction['confidence_score']:.1%})""
                        )

                if failed_predictions:
                    self.logger.info(""✨ Failed Predictions Summary:"")
                    for symbol, model_type in failed_predictions:
                        self.logger.info(f""✨    {symbol} ({model_type})"")

                # Wait before next iteration
                await asyncio.sleep(interval_minutes * 60)  # Wait before next batch

            except Exception as e:
                self.logger.error(f""❌ Error in publish loop: {str(e)}"")
                await asyncio.sleep(interval_minutes * 60)  # Wait before retrying

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            await self.stop_auto_publishing()
            self.models.clear()
            self.scalers.clear()
            self._initialized = False
            self.rabbitmq_service.close()
            K.clear_session()  # Use Keras backend to clear session
            self.logger.info(""Prediction service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during prediction service cleanup: {str(e)}"")

    async def _load_models(self) -> None:
        """"""Load prediction models and scalers.""""""
        # Implementation will be added later
        pass

    async def get_prediction(
        self, symbol: str, model_type: str = ""lstm""
    ) -> Dict[str, Any]:
        """"""Get stock price prediction for a given symbol.""""""
        if not self._initialized:
            raise RuntimeError(""Prediction service not initialized"")

        if not validate_stock_symbol(symbol):
            raise ValueError(f""Invalid stock symbol: {symbol}"")

        if model_type not in [""lstm"", ""prophet""]:
            raise ValueError(f""Unsupported model type: {model_type}"")

        try:
            if model_type == ""prophet"":
                # Check if Prophet model exists for this symbol
                prophet_models = [
                    f.stem.split(""_"")[0]
                    for f in self.model_service.prophet_dir.glob(""*_prophet.joblib"")
                ]
                if symbol not in prophet_models:
                    available_models = "", "".join(sorted(prophet_models))
                    self.logger.warning(
                        f""No Prophet model available for {symbol}. Available Prophet models: {available_models}""
                    )
                    return {
                        ""status"": ""error"",
                        ""error"": f""No Prophet model available for {symbol}"",
                        ""symbol"": symbol,
                        ""model_type"": model_type,
                        ""timestamp"": datetime.now().isoformat(),
                    }

            if model_type == ""lstm"":
                _, prediction, _ = await self._get_lstm_prediction(symbol)
            else:
                prediction = await self._get_prophet_prediction(symbol)

            if prediction.get(""status"") == ""error"":
                return prediction

            # Format prediction for RabbitMQ
            rabbitmq_prediction = {
                ""prediction"": prediction[""prediction""],
                ""confidence_score"": prediction[""confidence_score""],
                ""model_type"": prediction[""model_type""],
                ""model_version"": prediction[""model_version""],
                ""timestamp"": (
                    prediction[""timestamp""].isoformat()
                    if isinstance(prediction[""timestamp""], datetime)
                    else prediction[""timestamp""]
                ),
            }

            # Publish prediction to RabbitMQ
            try:
                self.rabbitmq_service.publish_stock_quote(symbol, rabbitmq_prediction)
            except Exception as e:
                self.logger.error(f""Failed to publish prediction to RabbitMQ: {str(e)}"")

            return format_prediction_response(
                prediction=prediction[""prediction""],
                confidence=prediction[""confidence_score""],
                model_type=prediction[""model_type""],
                model_version=prediction[""model_version""],
                symbol=symbol,
            )

        except Exception as e:
            self.logger.error(f""Error getting prediction for {symbol}: {str(e)}"")
            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def _get_lstm_prediction(
        self, symbol: str
    ) -> Tuple[str, Dict[str, Any], str]:
        """"""Get prediction using LSTM model.""""""
        try:
            self.logger.info(f""Starting prediction for {symbol}"")

            model_result = await self.model_service.load_model(symbol, ""lstm"")
            if model_result[""status""] != ""success"":
                self.logger.error(f""Model loading failed for {symbol}"")
                raise RuntimeError(f""Failed to load LSTM model for {symbol}"")

            model = model_result[""model""]
            scaler = model_result[""scaler""]

            df, _ = await self.data_service.get_stock_data(symbol)
            self.logger.info(f""Data shape for {symbol}: {df.shape}"")
            self.logger.info(f""Data columns for {symbol}: {df.columns.tolist()}"")

            # Validate data columns
            missing_features = [
                f for f in self.config.model.FEATURES if f not in df.columns
            ]
            if missing_features:
                raise ValueError(
                    f""Missing required features: {', '.join(missing_features)}""
                )

            df = calculate_technical_indicators(df)
            self.logger.info(
                f""Data shape after technical indicators for {symbol}: {df.shape}""
            )
            self.logger.info(
                f""Data columns after technical indicators for {symbol}: {df.columns.tolist()}""
            )

            features_df = df[self.config.model.FEATURES].copy()
            self.logger.info(f""Features shape for {symbol}: {features_df.shape}"")

            # Validate for NaN values
            if features_df.isna().any().any():
                nan_cols = features_df.columns[features_df.isna().any()].tolist()
                raise ValueError(f""Found NaN values in features: {', '.join(nan_cols)}"")

            scaled_features = scaler.transform(features_df)
            self.logger.info(
                f""Scaled features shape for {symbol}: {scaled_features.shape}""
            )

            # Ensure we have enough data for the sequence
            if len(scaled_features) < self.config.model.SEQUENCE_LENGTH:
                raise ValueError(
                    f""Not enough data for prediction. Need {self.config.model.SEQUENCE_LENGTH} days, ""
                    f""but only have {len(scaled_features)} days of data.""
                )

            # Take the last SEQUENCE_LENGTH days of data
            sequence = scaled_features[-self.config.model.SEQUENCE_LENGTH :].reshape(
                1, self.config.model.SEQUENCE_LENGTH, len(self.config.model.FEATURES)
            )
            self.logger.info(f""Final sequence shape for {symbol}: {sequence.shape}"")

            scaled_prediction = model.predict(sequence)

            last_close = df[""Close""].iloc[-1]
            last_scaled = scaled_features[-1, self.config.model.FEATURES.index(""Close"")]

            start_time = time.perf_counter()  # Start timer

            prediction = self._inverse_scale_lstm_prediction(
                scaled_prediction[0, 0],
                last_close,
                last_scaled,
                scaler,
                self.config.model.FEATURES.index(""Close""),
            )

            confidence = self._calculate_lstm_confidence(
                sequence,
                scaled_prediction,
                model,
                scaler,
                self.config.model.FEATURES.index(""Close""),
            )

            # Log the prediction confidence (Prometheus)
            prediction_confidence.labels(symbol=symbol, model_type=""lstm"").set(
                confidence
            )

            change = prediction - last_close
            change_percent = (change / last_close) * 100

            self.logger.info(f""Prediction ready for {symbol}:"")
            self.logger.info(
                f""Current: ${last_close:.2f} → Predicted: ${prediction:.2f}""
            )
            self.logger.info(f""Change: ${change:+.2f} ({change_percent:+.2f}%)"")
            self.logger.info(f""Confidence: {confidence:.1%}"")

            # Log the prediction time (Prometheus)
            prediction_duration = time.perf_counter() - start_time
            prediction_time_seconds.labels(model_type=""lstm"", symbol=symbol).observe(
                prediction_duration
            )

            # Log the sucessful prediction count (Prometheus)
            predictions_total.labels(
                model_type=""lstm"", symbol=symbol, result=""sucess""
            ).inc()

            prediction_data = {
                ""prediction"": float(prediction),
                ""timestamp"": datetime.now() + timedelta(days=1),
                ""confidence_score"": float(confidence),
                ""model_version"": model_result.get(""version"", ""1.0.0""),
                ""model_type"": ""lstm"",
                ""prediction_details"": {
                    ""last_close"": float(last_close),
                    ""predicted_change"": float(change),
                    ""predicted_change_percent"": float(change_percent),
                    ""sequence_length"": self.config.model.SEQUENCE_LENGTH,
                    ""features_used"": self.config.model.FEATURES,
                },
            }

            return symbol, prediction_data, ""lstm""

        except Exception as e:
            self.logger.error(f""Prediction failed for {symbol}: {str(e)}"")

            # Log the unsucessful prediction count (prometheus)
            predictions_total.labels(
                model_type=""lstm"", symbol=symbol, result=""error""
            ).inc()
            raise

    def _inverse_scale_lstm_prediction(
        self,
        scaled_prediction: float,
        last_close: float,
        last_scaled: float,
        scaler: Any,
        close_idx: int,
    ) -> float:
        """"""Inverse scale the LSTM prediction.""""""
        try:
            scaled_change = scaled_prediction - last_scaled
            dummy = np.zeros((1, len(self.config.model.FEATURES)))
            dummy[0, close_idx] = scaled_change
            real_change = scaler.inverse_transform(dummy)[0, close_idx]
            prediction = last_close + real_change

            if np.isnan(prediction) or np.isinf(prediction):
                self.logger.warning(
                    f""Invalid prediction value detected: {prediction}. Using last close price.""
                )
                return float(last_close)

            relative_change = (prediction - last_close) / last_close
            if abs(relative_change) > 0.2:
                conservative_price = last_close * (
                    1 + (prediction - last_close) / last_close * 0.1
                )
                self.logger.debug(
                    f""Large change detected: {relative_change:.2%}. Using conservative estimate: {conservative_price:.2f}""
                )
                return float(conservative_price)

            return float(prediction)

        except Exception as e:
            self.logger.error(f""Error in inverse scaling: {str(e)}"")
            return float(last_close)

    def _calculate_lstm_confidence(
        self,
        sequence: np.ndarray,
        scaled_prediction: np.ndarray,
        model: Any,
        scaler: Any,
        close_idx: int,
    ) -> float:
        """"""Calculate confidence score for LSTM prediction.""""""
        try:
            # Calculate historical volatility from the sequence
            historical_volatility = np.std(sequence[:, :, close_idx]) / np.mean(
                sequence[:, :, close_idx]
            )

            # Calculate prediction magnitude (relative to last price)
            last_price = sequence[-1, -1, close_idx]
            prediction_magnitude = (
                abs(scaled_prediction[0, 0] - last_price) / abs(last_price)
                if last_price != 0
                else float(""inf"")
            )

            # Calculate confidence based on volatility and prediction magnitude
            # Lower volatility and smaller prediction changes result in higher confidence
            volatility_factor = np.exp(-2 * historical_volatility)
            magnitude_factor = np.exp(-3 * prediction_magnitude)

            # Combine factors with weights
            confidence = 0.7 * volatility_factor + 0.3 * magnitude_factor
            confidence = np.clip(confidence, 0, 1)

            self.logger.debug(f""Confidence calculation details:"")
            self.logger.debug(f""Historical volatility: {historical_volatility:.4f}"")
            self.logger.debug(f""Prediction magnitude: {prediction_magnitude:.4f}"")
            self.logger.debug(f""Volatility factor: {volatility_factor:.4f}"")
            self.logger.debug(f""Magnitude factor: {magnitude_factor:.4f}"")
            self.logger.debug(f""Final confidence: {confidence:.4f}"")

            return float(confidence)

        except Exception as e:
            self.logger.error(f""Error calculating LSTM confidence: {str(e)}"")
            return 0.5

    async def _get_prophet_prediction(
        self, symbol: str
    ) -> Tuple[str, Dict[str, Any], str]:
        """"""Get prediction using Prophet model.""""""
        try:
            # Load model from joblib file
            prophet_dir = Path(""models/prophet"")
            model_file = prophet_dir / f""{symbol}_prophet.joblib""
            metadata_file = prophet_dir / f""{symbol}_prophet_metadata.json""

            if not model_file.exists():
                raise RuntimeError(
                    f""Failed to load Prophet model for {symbol}: Model file not found""
                )

            # Load the model and metadata
            model = joblib.load(model_file)
            with open(metadata_file, ""r"") as f:
                model_metadata = json.load(f)

            # Get latest data
            data_result = await self.data_service.get_latest_data(symbol)
            if data_result[""status""] != ""success"":
                raise RuntimeError(f""Failed to get latest data for {symbol}"")

            df = data_result[""data""]
            if ""RSI"" not in df.columns:
                df = calculate_technical_indicators(df)

            # Store original timezone information
            original_tz = df[""Date""].iloc[0].tzinfo

            # Convert to timezone-naive for Prophet
            dates = pd.to_datetime(df[""Date""])
            if dates.dt.tz is not None:
                dates = dates.dt.tz_localize(None)

            prophet_df = pd.DataFrame({""ds"": dates, ""y"": df[""Close""]})

            # Add regressors to the dataframe
            if hasattr(model, ""extra_regressors""):
                for regressor in model.extra_regressors.keys():
                    if regressor in df.columns:
                        prophet_df[regressor] = df[regressor]
                    else:
                        matching_cols = [
                            col
                            for col in df.columns
                            if col.lower() == regressor.lower()
                        ]
                        if matching_cols:
                            prophet_df[regressor] = df[matching_cols[0]]
                        else:
                            raise ValueError(
                                f""Regressor '{regressor}' missing from dataframe""
                            )

            # Create future dataframe with timezone-naive dates
            future = model.make_future_dataframe(periods=1)
            future_dates = pd.to_datetime(future[""ds""])
            if future_dates.dt.tz is not None:
                future_dates = future_dates.dt.tz_localize(None)
            future[""ds""] = future_dates

            # Add regressors to future dataframe
            if hasattr(model, ""extra_regressors""):
                for regressor in model.extra_regressors.keys():
                    if regressor in df.columns:
                        future[regressor] = df[regressor].iloc[-1]
                    else:
                        matching_cols = [
                            col
                            for col in df.columns
                            if col.lower() == regressor.lower()
                        ]
                        if matching_cols:
                            future[regressor] = df[matching_cols[0]].iloc[-1]
                        else:
                            raise ValueError(
                                f""Regressor '{regressor}' missing from dataframe""
                            )

            start_time = time.perf_counter()  # Start timer

            # Make prediction
            forecast = model.predict(future)

            # Log the prediction time (Prometheus)
            prediction_duration = time.perf_counter() - start_time
            prediction_time_seconds.labels(model_type=""prophet"", symbol=symbol).observe(
                prediction_duration
            )

            # Get the last prediction
            last_prediction = forecast.iloc[-1]

            # Calculate confidence based on prediction interval width
            interval_width = (
                last_prediction[""yhat_upper""] - last_prediction[""yhat_lower""]
            )
            relative_width = interval_width / abs(last_prediction[""yhat""])

            self.logger.debug(f""Prophet confidence calculation for {symbol}:"")
            self.logger.debug(f""  Prediction: {last_prediction['yhat']:.2f}"")
            self.logger.debug(
                f""  Interval: [{last_prediction['yhat_lower']:.2f}, {last_prediction['yhat_upper']:.2f}]""
            )
            self.logger.debug(f""  Interval width: {interval_width:.2f}"")
            self.logger.debug(f""  Relative width: {relative_width:.4f}"")

            # Convert to confidence score (0-1)
            # Use a more conservative sigmoid function that accounts for stock market uncertainty
            # The parameters are tuned to give reasonable confidence scores for typical stock price movements
            raw_confidence = 1 / (1 + np.exp(10 * relative_width - 1))
            self.logger.debug(f""  Raw confidence: {raw_confidence:.4f}"")

            # Cap confidence at 0.85 to maintain uncertainty
            confidence = min(raw_confidence, 0.85)

            # Ensure minimum confidence of 0.50
            confidence = max(confidence, 0.50)

            # Round to 3 decimal places for cleaner output
            confidence = round(confidence, 3)
            self.logger.debug(f""  Final confidence: {confidence:.3f}"")

            # Log the prediction confidence (Prometheus)
            prediction_confidence.labels(symbol=symbol, model_type=""prophet"").set(
                confidence
            )

            # Convert prediction dates back to original timezone
            prediction_date = pd.to_datetime(forecast[""ds""].iloc[-1])
            if original_tz is not None:
                prediction_date = prediction_date.tz_localize(original_tz)

            # Log the sucessful prediction count (Prometheus)
            predictions_total.labels(
                model_type=""prophet"", symbol=symbol, result=""sucess""
            ).inc()

            prediction_data = {
                ""prediction"": float(last_prediction[""yhat""]),
                ""timestamp"": prediction_date.isoformat(),
                ""confidence_score"": confidence,  # Store as decimal between 0 and 1
                ""model_version"": model_metadata.get(""model_version"", ""1.0.0""),
                ""model_type"": ""prophet"",
                ""prediction_details"": {
                    ""yhat_lower"": float(last_prediction[""yhat_lower""]),
                    ""yhat_upper"": float(last_prediction[""yhat_upper""]),
                    ""interval_width"": float(
                        last_prediction[""yhat_upper""] - last_prediction[""yhat_lower""]
                    ),
                    ""timezone"": str(original_tz) if original_tz else None,
                },
            }

            return symbol, prediction_data, ""prophet""

        except Exception as e:
            self.logger.error(
                f""Error getting Prophet prediction for {symbol}: {str(e)}""
            )

            # Log the unsucessful prediction count (Prometheus)
            predictions_total.labels(
                model_type=""prophet"", symbol=symbol, result=""error""
            ).inc()
            raise

    def _get_symbols_for_prediction(self) -> List[str]:
        """"""Get list of symbols that need predictions.""""""
        try:
            # Get all available models
            lstm_models = list(self.model_service._specific_models.keys())
            prophet_models = [
                f.stem.split(""_"")[0]
                for f in self.model_service.prophet_dir.glob(""*_prophet.joblib"")
            ]

            # Combine and deduplicate symbols
            all_symbols = list(set(lstm_models + prophet_models))

            if not all_symbols:
                self.logger.warning(""⚠️ No models available for prediction"")
                return []

            self.logger.info(
                f""✨ Found {len(all_symbols)} symbols with available models""
            )
            return all_symbols

        except Exception as e:
            self.logger.error(f""❌ Error getting symbols for prediction: {str(e)}"")
            return []

    async def _publish_prediction(
        self, symbol: str, prediction: Dict[str, Any], model_type: str
    ) -> bool:
        """"""
        Publish a prediction to RabbitMQ.

        Args:
            symbol: Stock symbol
            prediction: Prediction data
            model_type: Type of model used (lstm or prophet)

        Returns:
            bool: True if published successfully, False otherwise
        """"""
        try:
            # Format prediction for RabbitMQ
            rabbitmq_prediction = {
                ""prediction"": prediction[""prediction""],
                ""confidence_score"": prediction[""confidence_score""],
                ""model_type"": model_type,
                ""model_version"": prediction.get(""model_version"", ""1.0.0""),
                ""timestamp"": prediction.get(""timestamp"", datetime.now().isoformat()),
            }

            # Ensure RabbitMQ connection is ready
            if not self.rabbitmq_service._event.is_set():
                self.logger.info(""✨ Waiting for RabbitMQ connection to be ready..."")
                if not self.rabbitmq_service._event.wait(timeout=30):
                    self.logger.error(
                        ""❌ Timeout waiting for RabbitMQ connection to be ready""
                    )
                    return False

            # Publish to RabbitMQ
            success = self.rabbitmq_service.publish_stock_quote(
                symbol, rabbitmq_prediction
            )

            if success:
                self.logger.info(
                    f""✅ Published prediction for {symbol} ({model_type}): ""
                    f""${prediction['prediction']:.2f} (Confidence: {prediction['confidence_score']:.1%})""
                )
            else:
                self.logger.error(
                    f""❌ Failed to publish prediction for {symbol} ({model_type})""
                )

            return success

        except Exception as e:
            self.logger.error(f""❌ Error publishing prediction for {symbol}: {str(e)}"")
            return False",services/prediction_service.py
ModelService,"class ModelService(BaseService):
    """"""Service for managing model storage and versioning.""""""

    def __init__(self):
        super().__init__()
        self.model_dir = Path(""models/specific"").resolve()
        self.prophet_dir = Path(""models/prophet"").resolve()
        self._model_metadata = {}
        self.model_version = ""1.0.0""
        self.logger = logger[""model""]
        self.logger.info(f""Model service initialized with model_dir: {self.model_dir}"")
        self.logger.info(
            f""Model service initialized with prophet_dir: {self.prophet_dir}""
        )
        self._specific_models = {}
        self._specific_scalers = {}
        self._general_model = None
        self._general_scalers = None

    async def initialize(self) -> None:
        """"""Initialize the model service.""""""
        try:
            await self._load_model_metadata()
            await self._load_models()
            self._initialized = True
            self.logger.info(""Model service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize model service: {str(e)}"")
            raise

    async def cleanup(self) -> None:
        """"""Clean up resources used by the model service.""""""
        try:
            await self._save_model_metadata()
            self._initialized = False
            self.logger.info(""Model service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Failed to clean up model service: {str(e)}"")
            raise

    async def save_model(
        self,
        symbol: str,
        model_type: str,
        model: Any,
        metrics: Dict[str, float],
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """"""Save a trained model with its metrics and metadata.""""""
        try:
            if not self._initialized:
                raise RuntimeError(""Model service not initialized"")

            # Create model directory
            model_dir = self._get_version_dir(symbol, model_type)
            model_dir.mkdir(parents=True, exist_ok=True)

            # Save model
            model_path = model_dir / f""{symbol}_model.keras""
            if hasattr(model, ""save""):  # Check if it's a Keras model
                models.save_model(model, str(model_path), save_format=""keras_v3"")
            else:
                model_path = model_dir / f""{symbol}_{model_type}_prophet.joblib""
                joblib.dump(model, model_path)

            # Save metrics
            metrics_path = model_dir / f""{symbol}_metrics.json""
            with open(metrics_path, ""w"") as f:
                json.dump(metrics, f)

            # Save metadata
            if metadata:
                metadata_path = model_dir / f""{symbol}_metadata.json""
                with open(metadata_path, ""w"") as f:
                    json.dump(metadata, f)

            # Update model metadata
            model_key = f""{symbol}_{model_type}""
            if model_key not in self._model_metadata:
                self._model_metadata[model_key] = []

            self._model_metadata[model_key].append(
                {
                    ""version"": self.model_version,
                    ""timestamp"": datetime.utcnow().isoformat(),
                    ""metrics"": metrics,
                    ""metadata"": metadata or {},
                }
            )

            await self._save_model_metadata()

            return {
                ""status"": ""success"",
                ""message"": f""Model saved successfully for {symbol} ({model_type})"",
                ""version"": self.model_version,
                ""timestamp"": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            self.logger.error(f""Failed to save model: {str(e)}"")
            return self.format_error_response(e)

    async def _load_models(self) -> None:
        """"""Load all models from disk.""""""
        try:
            loaded_lstm = []
            loaded_prophet = []
            total_processed = 0

            # Get total number of items to process (both directories and files)
            total_items = 0
            if self.model_dir.exists():
                # Count LSTM model directories
                total_items += len(
                    [
                        d
                        for d in self.model_dir.iterdir()
                        if d.is_dir() and not d.name.lower() in [""lstm"", ""prophet""]
                    ]
                )
            if self.prophet_dir.exists():
                # Count Prophet model files
                total_items += len(list(self.prophet_dir.glob(""*_prophet.joblib"")))

            # Start loading indicator
            self.logger.info(""🤖 Loading models..."")

            # Load LSTM models from symbol-specific directories
            if self.model_dir.exists():
                for symbol_dir in self.model_dir.iterdir():
                    if symbol_dir.is_dir() and not symbol_dir.name.lower() in [
                        ""lstm"",
                        ""prophet"",
                    ]:
                        symbol = symbol_dir.name
                        try:
                            # Load scaler
                            scaler_path = symbol_dir / f""{symbol}_scaler.gz""
                            model_path = symbol_dir / f""{symbol}_model.keras""

                            if scaler_path.exists() and model_path.exists():
                                self._specific_scalers[symbol] = joblib.load(
                                    scaler_path
                                )
                                self._specific_models[symbol] = models.load_model(
                                    str(model_path), compile=False
                                )
                                loaded_lstm.append(symbol)

                            total_processed += 1
                            progress = (total_processed / total_items) * 100
                            print(
                                f""\r🔄 Loading models... {progress:.1f}% ({total_processed}/{total_items})"",
                                end="""",
                                flush=True,
                            )

                        except Exception as e:
                            self.logger.error(
                                f""❌ Error loading model for {symbol}: {str(e)}""
                            )

            # Load Prophet models from symbol-specific directories
            prophet_base_dir = self.prophet_dir
            if prophet_base_dir.exists():
                for model_file in prophet_base_dir.glob(""*_prophet.joblib""):
                    symbol = model_file.stem.split(""_"")[0]
                    try:
                        self._specific_models[symbol] = joblib.load(model_file)
                        loaded_prophet.append(symbol)

                        total_processed += 1
                        progress = (total_processed / total_items) * 100
                        print(
                            f""\r🔄 Loading models... {progress:.1f}% ({total_processed}/{total_items})"",
                            end="""",
                            flush=True,
                        )

                    except Exception as e:
                        self.logger.error(
                            f""❌ Error loading Prophet model for {symbol}: {str(e)}""
                        )

            print()

            self.logger.info(f""✨ Model loading summary:"")
            self.logger.info(f""   LSTM models loaded: {len(loaded_lstm)} tickers"")
            self.logger.info(f""   Prophet models loaded: {len(loaded_prophet)} tickers"")
            if loaded_lstm:
                self.logger.info(f""   LSTM tickers: {', '.join(sorted(loaded_lstm))}"")
            if loaded_prophet:
                self.logger.info(
                    f""   Prophet tickers: {', '.join(sorted(loaded_prophet))}""
                )

        except Exception as e:
            self.logger.error(f""❌ Error loading models: {str(e)}"")
            raise

    async def load_model(self, symbol: str, model_type: str) -> Dict[str, Any]:
        """"""Load a trained model and its scaler.""""""
        try:
            self.logger.info(f""Loading {model_type.upper()} model for {symbol}"")

            if model_type == ""lstm"":
                model_dir = self.model_dir / symbol
                model_path = model_dir / f""{symbol}_model.keras""
                scaler_path = model_dir / f""{symbol}_scaler.gz""
                scaler_metadata_path = model_dir / f""{symbol}_scaler_metadata.json""
            elif model_type == ""prophet"":
                model_dir = self.prophet_dir
                model_path = model_dir / f""{symbol}_prophet.joblib""
                metadata_path = model_dir / f""{symbol}_prophet_metadata.json""
            else:
                raise ValueError(f""Unsupported model type: {model_type}"")

            if not model_path.exists():
                self.logger.warning(f""Model file not found: {model_path}"")
                return {
                    ""status"": ""error"",
                    ""error"": f""Model file not found for {symbol} ({model_type})"",
                    ""symbol"": symbol,
                    ""model_type"": model_type,
                    ""timestamp"": datetime.utcnow().isoformat(),
                }

            try:
                if model_type == ""lstm"":
                    model = models.load_model(str(model_path), compile=False)
                    scaler = joblib.load(scaler_path) if scaler_path.exists() else None
                    scaler_metadata = {}
                    if scaler_metadata_path.exists():
                        with open(scaler_metadata_path, ""r"") as f:
                            scaler_metadata = json.load(f)

                    return {
                        ""status"": ""success"",
                        ""model"": model,
                        ""scaler"": scaler,
                        ""scaler_metadata"": scaler_metadata,
                        ""version"": self.model_version,
                        ""timestamp"": datetime.utcnow().isoformat(),
                    }
                else:  # prophet
                    model = joblib.load(model_path)
                    metadata = {}
                    if metadata_path.exists():
                        with open(metadata_path, ""r"") as f:
                            metadata = json.load(f)

                    return {
                        ""status"": ""success"",
                        ""model"": model,
                        ""metadata"": metadata,
                        ""version"": self.model_version,
                        ""timestamp"": datetime.utcnow().isoformat(),
                    }
            except Exception as e:
                self.logger.error(
                    f""Error loading {model_type} model for {symbol}: {str(e)}""
                )
                return {
                    ""status"": ""error"",
                    ""error"": f""Error loading model: {str(e)}"",
                    ""symbol"": symbol,
                    ""model_type"": model_type,
                    ""timestamp"": datetime.utcnow().isoformat(),
                }

        except Exception as e:
            self.logger.error(f""Error in load_model: {str(e)}"")
            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.utcnow().isoformat(),
            }

    # ... (rest of the methods remain unchanged: list_models, delete_model, _get_version_dir, etc.)

    async def list_models(
        self, symbol: Optional[str] = None, model_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """"""List all available models with their metadata.""""""
        try:
            if not self._initialized:
                raise RuntimeError(""Model service not initialized"")

            models = []
            for model_key, versions in self._model_metadata.items():
                model_symbol, model_type_str = model_key.split(""_"")

                if symbol and model_symbol != symbol:
                    continue
                if model_type and model_type_str != model_type:
                    continue

                for version_data in versions:
                    models.append(
                        {
                            ""symbol"": model_symbol,
                            ""model_type"": model_type_str,
                            ""version"": version_data[""version""],
                            ""timestamp"": version_data[""timestamp""],
                            ""metrics"": version_data[""metrics""],
                            ""metadata"": version_data[""metadata""],
                        }
                    )

            return models

        except Exception as e:
            self.logger.error(f""Failed to list models: {str(e)}"")
            return []

    async def delete_model(self, symbol: str, model_type: str, version: str) -> bool:
        """"""Delete a specific version of a model.""""""
        try:
            if not self._initialized:
                raise RuntimeError(""Model service not initialized"")

            # Get version directory
            version_dir = self._get_version_dir(symbol, model_type)
            if not version_dir.exists():
                raise FileNotFoundError(f""No model found for {symbol} ({model_type})"")

            # Delete model files
            model_path = version_dir / f""{symbol}_model.keras""
            metrics_path = version_dir / f""{symbol}_metrics.json""
            metadata_path = version_dir / f""{symbol}_metadata.json""

            if model_path.exists():
                model_path.unlink()
            if metrics_path.exists():
                metrics_path.unlink()
            if metadata_path.exists():
                metadata_path.unlink()

            # Update model metadata
            model_key = f""{symbol}_{model_type}""
            if model_key in self._model_metadata:
                self._model_metadata[model_key] = [
                    v
                    for v in self._model_metadata[model_key]
                    if v[""version""] != version
                ]

                if not self._model_metadata[model_key]:
                    del self._model_metadata[model_key]

                await self._save_model_metadata()

            return True

        except Exception as e:
            self.logger.error(f""Failed to delete model: {str(e)}"")
            return False

    def _get_version_dir(self, symbol: str, model_type: str) -> Path:
        """"""Get the directory path for a specific model version.""""""
        if model_type == ""lstm"":
            return self.model_dir / symbol
        elif model_type == ""prophet"":
            return self.prophet_dir / symbol
        else:
            raise ValueError(f""Unsupported model type: {model_type}"")

    def _get_latest_version(self, symbol: str, model_type: str) -> str:
        """"""Get the latest version of a model.""""""
        model_key = f""{symbol}_{model_type}""
        if model_key in self._model_metadata and self._model_metadata[model_key]:
            return self._model_metadata[model_key][-1][""version""]
        return self.model_version

    async def _load_model_metadata(self) -> None:
        """"""Load model metadata from disk.""""""
        metadata_path = self.model_dir / ""model_metadata.json""
        if metadata_path.exists():
            with open(metadata_path, ""r"") as f:
                self._model_metadata = json.load(f)

    async def _save_model_metadata(self) -> None:
        """"""Save model metadata to disk.""""""
        metadata_path = self.model_dir / ""model_metadata.json""
        with open(metadata_path, ""w"") as f:
            json.dump(self._model_metadata, f)",services/model_service.py
RabbitMQService,"class RabbitMQService:
    """"""Service for publishing messages to RabbitMQ.""""""
    
    _instance = None
    _lock = threading.Lock()
    _initialized = False
    
    def __new__(cls):
        """"""Implement singleton pattern.""""""
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(RabbitMQService, cls).__new__(cls)
                cls._instance.logger = logger['rabbitmq']
                cls._instance.logger.debug(""✨ Creating new RabbitMQ service instance"")
            else:
                cls._instance.logger.debug(""✨ Returning existing RabbitMQ service instance"")
            return cls._instance
    
    def __init__(self):
        """"""Initialize RabbitMQ connection and channel.""""""
        # Skip initialization if already initialized
        if self._initialized:
            self.logger.debug(""✨ Skipping initialization - already initialized"")
            return
            
        self._initialized = True
        self.logger.debug(""✨ Starting RabbitMQ service initialization"")
        
        self.exchange_name = 'quote-exchange'  # This matches the C# configuration
        self.day_started_exchange = 'day-started-exchange'
        self.day_started_queue = 'stock-ai-day-started'
        
        # Get host from environment variable if available, otherwise use config
        self.host = os.environ.get('RABBITMQ_HOST', config.rabbitmq.HOST)
        self.port = int(os.environ.get('RABBITMQ_PORT', config.rabbitmq.PORT))
        
        self.connection_params = pika.ConnectionParameters(
            host=self.host,
            port=self.port,
            credentials=pika.PlainCredentials(
                config.rabbitmq.USER,
                config.rabbitmq.PASSWORD
            ),
            heartbeat=30,
            blocked_connection_timeout=30,
            connection_attempts=5,
            retry_delay=5,
            socket_timeout=30,
            stack_timeout=30,
            frame_max=131072,
            tcp_options={
                'TCP_KEEPIDLE': 60,
                'TCP_KEEPINTVL': 10,
                'TCP_KEEPCNT': 3
            }
        )
        self.connection = None
        self.channel = None
        self._is_shutting_down = False
        self._lock = threading.Lock()
        self._event = threading.Event()
        self._day_started_callback = None
        self._io_thread = None
        self._connection_state = 'disconnected'
        atexit.register(self._cleanup_on_exit)
        
        # Log configuration (without sensitive data)
        self.logger.info(f""✨ Initializing RabbitMQ service with host={self.host}, port={self.port}"")
        self.logger.info(f""✨ Environment RABBITMQ_HOST: {os.environ.get('RABBITMQ_HOST', 'not set')}"")
        self.logger.info(f""✨ Environment RABBITMQ_PORT: {os.environ.get('RABBITMQ_PORT', 'not set')}"")
        
        # Try to connect, but don't fail initialization if connection fails
        try:
            self.connect()
        except Exception as e:
            self.logger.error(f""❌ Initial connection attempt failed: {str(e)}"")
            self.logger.warning(""⚠️ Service will continue in disconnected state. Connection will be retried on first publish attempt."")
    
    def set_day_started_callback(self, callback: Callable[[Dict[str, Any]], None]) -> None:
        """"""Set the callback function for day-started events.""""""
        self._day_started_callback = callback
        self.logger.info(""✅ Day-started callback set"")
        
        # If we're already connected, ensure the callback is properly set up
        if self.connection and not self.connection.is_closed and self.channel and not self.channel.is_closed:
            self._setup_day_started_consumer()
    
    def _setup_day_started_consumer(self) -> None:
        """"""Set up the day-started event consumer.""""""
        try:
            if not self._day_started_callback:
                self.logger.warning(""No day-started callback set, skipping consumer setup"")
                return
            
            # Declare queue for day-started events
            self.channel.queue_declare(
                queue=self.day_started_queue,
                durable=True,
                exclusive=False,
                auto_delete=False,
                callback=self._on_day_started_queue_declared
            )
        except Exception as e:
            self.logger.error(f""Error setting up day-started consumer: {str(e)}"")
    
    def _on_connection_open(self, connection):
        """"""Called when the connection is opened.""""""
        self.logger.info(""✨ Connection opened"")
        self.connection = connection
        self._connection_state = 'connected'
        self.logger.info(f""✨ Connection state updated to: {self._connection_state}"")
        self.connection.channel(on_open_callback=self._on_channel_open)
    
    def _on_connection_closed(self, connection, reason):
        """"""Called when the connection is closed.""""""
        self.logger.warning(f""⚠️ Connection closed: {reason}"")
        self._connection_state = 'disconnected'
        self._cleanup(force=True)
        if not self._is_shutting_down:
            self.logger.info(""✨ Attempting to reconnect..."")
            self.connect()
    
    def _on_channel_open(self, channel):
        """"""Called when the channel is opened.""""""
        self.logger.info(""✨ Channel opened"")
        self.channel = channel
        self._connection_state = 'channel_open'
        self.logger.info(f""✨ Connection state updated to: {self._connection_state}"")
        
        # Declare quote exchange
        self.logger.info(f""✨ Declaring quote exchange '{self.exchange_name}'..."")
        self.channel.exchange_declare(
            exchange=self.exchange_name,
            exchange_type='fanout',
            durable=True,
            callback=self._on_quote_exchange_declared
        )
    
    def _on_quote_exchange_declared(self, frame):
        """"""Called when the quote exchange is declared.""""""
        self.logger.info(f""✨ Quote exchange {self.exchange_name} declared successfully"")
        self._connection_state = 'quote_exchange_declared'
        self.logger.info(f""✨ Connection state updated to: {self._connection_state}"")
        
        # Create a queue for publishing
        self.logger.info(f""✨ Creating queue for quote exchange..."")
        self.channel.queue_declare(
            queue='stock-ai-quotes',
            durable=True,
            exclusive=False,
            auto_delete=False,
            callback=self._on_quote_queue_declared
        )
        
        # Declare day-started exchange
        self.logger.info(f""✨ Declaring day-started exchange '{self.day_started_exchange}'..."")
        self.channel.exchange_declare(
            exchange=self.day_started_exchange,
            exchange_type='fanout',
            durable=True,
            callback=self._on_day_started_exchange_declared
        )
    
    def _on_quote_queue_declared(self, frame):
        """"""Called when the quote queue is declared.""""""
        self.logger.info(f""✨ Quote queue 'stock-ai-quotes' declared successfully"")
        
        # Bind queue to exchange
        self.logger.info(f""✨ Binding queue to quote exchange..."")
        self.channel.queue_bind(
            exchange=self.exchange_name,
            queue='stock-ai-quotes',
            routing_key='',
            callback=self._on_quote_queue_bound
        )
    
    def _on_quote_queue_bound(self, frame):
        """"""Called when the quote queue is bound.""""""
        self.logger.info(""✨ Quote queue bound to exchange successfully"")
        self._connection_state = 'quote_queue_bound'
        self.logger.info(f""✨ Connection state updated to: {self._connection_state}"")
        self._event.set()  # Signal that setup is complete
        self.logger.info(""✨ Setup complete, connection is ready for publishing"")
        
        # If we have a day-started callback, ensure it's set up
        if self._day_started_callback:
            self._setup_day_started_consumer()
    
    def _on_day_started_exchange_declared(self, frame):
        """"""Called when the day-started exchange is declared.""""""
        self.logger.info(f""✨ Day-started exchange {self.day_started_exchange} declared successfully"")
        self._connection_state = 'day_started_exchange_declared'
        self.logger.info(f""✨ Connection state updated to: {self._connection_state}"")
        
        # Set up day-started consumer if callback is set
        if self._day_started_callback:
            self._setup_day_started_consumer()
        else:
            self.logger.warning(""⚠️ No day-started callback set, skipping consumer setup for now (it will be set on first publish)"")
            # Don't set the event here, wait for quote queue to be bound
            self.logger.info(""✨ Waiting for quote queue to be bound..."")
    
    def _on_day_started_queue_declared(self, frame):
        """"""Called when the day-started queue is declared.""""""
        self.logger.info(f""✨ Day-started queue {self.day_started_queue} declared"")
        
        # Bind queue to exchange
        self.channel.queue_bind(
            exchange=self.day_started_exchange,
            queue=self.day_started_queue,
            routing_key='',
            callback=self._on_day_started_queue_bound
        )
    
    def _on_day_started_queue_bound(self, frame):
        """"""Called when the day-started queue is bound.""""""
        self.logger.info(""✨ Day-started queue bound to exchange"")
        
        # Start consuming
        self.channel.basic_consume(
            queue=self.day_started_queue,
            on_message_callback=self._on_day_started_message,
            auto_ack=True
        )
        
        self.logger.info(""✅ Day-started event consumer is ready"")
    
    def _on_day_started_message(self, channel, method, properties, body):
        """"""Called when a day-started message is received.""""""
        try:
            message = json.loads(body)
            self.logger.info(f""✨ Received day-started event: {message}"")
            
            if self._day_started_callback:
                # Create a new event loop for this thread if needed
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                # Schedule the callback to run in the current event loop
                loop.call_soon_threadsafe(
                    lambda: asyncio.create_task(self._day_started_callback(message))
                )
            else:
                self.logger.warning(""⚠️ No callback set for day-started events"")
                
        except Exception as e:
            self.logger.error(f""❌ Error processing day-started message: {str(e)}"")
    
    def connect(self):
        """"""Establish connection to RabbitMQ with retries.""""""
        if self._is_shutting_down:
            return
        
        with self._lock:
            for attempt in range(5):  # Max 5 retries
                try:
                    self.logger.info(f""✨ Attempting to connect to RabbitMQ (attempt {attempt + 1}/5)"")
                    self.logger.debug(f""Connection parameters: host={self.host}, port={self.port}"")
                    
                    # Test TCP connection first
                    try:
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(10)  # Increased timeout
                        sock.connect((self.host, self.port))
                        sock.close()
                        self.logger.info(""✨ TCP connection test successful"")
                    except socket.error as e:
                        self.logger.error(f""❌ TCP connection failed: {str(e)}"")
                        self.logger.error(f""Host: {self.host}, Port: {self.port}"")
                        self.logger.error(""Please check if RabbitMQ is running and accessible"")
                        if attempt == 4:  # Last attempt
                            raise
                        time.sleep(5)  # Wait before retrying
                        continue
                    
                    self._event.clear()
                    self._connection_state = 'connecting'
                    self.connection = pika.SelectConnection(
                        parameters=self.connection_params,
                        on_open_callback=self._on_connection_open,
                        on_open_error_callback=lambda conn, err: self.logger.error(f""❌ Connection open failed: {str(err)}""),
                        on_close_callback=self._on_connection_closed
                    )
                    
                    # Start the IO loop in a separate thread
                    if self._io_thread is None or not self._io_thread.is_alive():
                        self._io_thread = threading.Thread(
                            target=self._run_io_loop,
                            name=""RabbitMQ-IOLoop"",
                            daemon=True
                        )
                        self._io_thread.start()
                    
                    # Wait for connection setup with increased timeout
                    if self._event.wait(timeout=30):  # Increased from 10
                        self.logger.info(""✅ Successfully connected to RabbitMQ"")
                        return
                    else:
                        self.logger.warning(f""⚠️ Timeout waiting for connection setup. Current state: {self._connection_state}"")
                        self._cleanup(force=True)
                        
                except Exception as e:
                    if attempt == 4:  # Last attempt
                        self.logger.error(f""❌ Failed to connect to RabbitMQ after 5 attempts: {str(e)}"")
                        self.logger.error(""Please check the following:"")
                        self.logger.error(""1. Is RabbitMQ running?"")
                        self.logger.error(""2. Are the host and port correct?"")
                        self.logger.error(""3. Is the RabbitMQ server accessible from this container?"")
                        self.logger.error(""4. Are the credentials correct?"")
                        raise
                    self.logger.warning(f""⚠️ Connection attempt {attempt + 1} failed: {str(e)}"")
                    time.sleep(5)  # Wait 5 seconds before retrying
    
    def _run_io_loop(self):
        """"""Run the IO loop in a separate thread.""""""
        try:
            self.logger.info(""✨ Starting RabbitMQ IO loop"")
            self.connection.ioloop.start()
        except Exception as e:
            self.logger.error(f""❌ IO loop terminated with error: {str(e)}"")
        finally:
            self.logger.info(""IO loop stopped"")
    
    def publish_stock_quote(self, symbol: str, prediction: Dict[str, Any]) -> bool:
        """"""
        Publish a stock quote to RabbitMQ.
        
        Args:
            symbol: Stock symbol
            prediction: Prediction data dictionary
            
        Returns:
            bool: True if message was published successfully, False otherwise
        """"""
        if self._is_shutting_down:
            self.logger.warning(""⚠️ Cannot publish during shutdown"")
            return False
        
        with self._lock:
            for attempt in range(3):  # Max 3 retries
                try:
                    # Log current connection state
                    self.logger.info(f""✨ Publishing attempt {attempt + 1}/3 for {symbol}"")
                    self.logger.info(f""✨ Current connection state: {self._connection_state}"")
                    
                    # Ensure connection and channel are open
                    if not self.connection or self.connection.is_closed or not self.channel or self.channel.is_closed:
                        self.logger.warning(""⚠️ Connection or channel is closed, attempting to reconnect..."")
                        try:
                            self.connect()
                            if not self._event.wait(timeout=10):
                                raise pika.exceptions.AMQPConnectionError(""Failed to establish connection"")
                            self.logger.info(""✅ Successfully reconnected to RabbitMQ"")
                        except Exception as e:
                            self.logger.error(f""❌ Failed to establish connection for publishing: {str(e)}"")
                            return False
                    
                    # Ensure exchange and queue are set up
                    if self._connection_state != 'quote_queue_bound':
                        self.logger.warning(""⚠️ Quote exchange or queue not properly set up, attempting to set up..."")
                        try:
                            # Declare exchange
                            self.channel.exchange_declare(
                                exchange=self.exchange_name,
                                exchange_type='fanout',
                                durable=True
                            )
                            self._connection_state = 'quote_exchange_declared'
                            
                            # Declare queue
                            self.channel.queue_declare(
                                queue='stock-ai-quotes',
                                durable=True,
                                exclusive=False,
                                auto_delete=False
                            )
                            
                            # Bind queue
                            self.channel.queue_bind(
                                exchange=self.exchange_name,
                                queue='stock-ai-quotes',
                                routing_key=''
                            )
                            self._connection_state = 'quote_queue_bound'
                            self.logger.info(""✅ Successfully set up quote exchange and queue"")
                        except Exception as e:
                            self.logger.error(f""❌ Failed to set up quote exchange and queue: {str(e)}"")
                            return False
                    
                    # Generate a unique message ID
                    message_id = str(uuid.uuid4())
                    timestamp = datetime.utcnow()
                    
                    # Structure message to match C# StockQuote type
                    message = {
                        ""Symbol"": symbol,
                        ""Price"": float(prediction['prediction']),  # Convert to float to match C# decimal
                        ""Date"": timestamp.isoformat(),  # ISO format for DateTime
                        ""CorrelationId"": message_id,  # Add correlation ID for tracking
                        ""MessageType"": ""StockQuote"",  # Add message type for MassTransit
                        ""ModelType"": prediction.get('model_type', 'unknown'),  # Add model type
                        ""Confidence"": float(prediction.get('confidence_score', 0.0)),  # Add confidence score
                        ""ModelVersion"": prediction.get('model_version', 'unknown')  # Add model version
                    }
                    
                    # Log message details
                    self.logger.info(f""✨ Preparing to publish message for {symbol}:"")
                    self.logger.info(f""   Price: ${message['Price']:.2f}"")
                    self.logger.info(f""   Model Type: {message['ModelType']}"")
                    self.logger.info(f""   Confidence: {message['Confidence']:.1%}"")
                    self.logger.info(f""   Model Version: {message['ModelVersion']}"")
                    
                    # Serialize message
                    message_body = json.dumps(message).encode('utf-8')
                    
                    # Check message size
                    if len(message_body) > self.connection_params.frame_max:
                        self.logger.error(f""❌ Message size ({len(message_body)} bytes) exceeds maximum frame size"")
                        return False
                    
                    # Create properties
                    properties = pika.BasicProperties(
                        delivery_mode=2,  # Persistent message
                        content_type='application/json',
                        content_encoding='utf-8',
                        message_id=message_id,
                        correlation_id=message_id,
                        timestamp=int(time.time()),
                        type='StockQuote',  # Match C# message type
                        headers={
                            'MT-Activity-Id': message_id,
                            'MT-Message-Type': 'StockQuote',
                            'MT-Model-Type': prediction.get('model_type', 'unknown')  # Add model type to headers
                        }
                    )
                    
                    # Log publishing attempt
                    self.logger.info(f""✨ Publishing to exchange '{self.exchange_name}'..."")
                    
                    # Publish message
                    self.channel.basic_publish(
                        exchange=self.exchange_name,
                        routing_key='',
                        body=message_body,
                        properties=properties
                    )
                    
                    self.logger.info(
                        f""✅ Successfully published prediction for {symbol} ({prediction.get('model_type', 'unknown')}): ""
                        f""${prediction.get('prediction', 0.0):.2f} (Confidence: {prediction.get('confidence_score', 0.0):.1%})""
                    )
                    return True
                    
                except Exception as e:
                    if attempt == 2:  # Last attempt
                        self.logger.error(f""❌ Failed to publish message for {symbol} after 3 attempts: {str(e)}"")
                        self.logger.error(f""❌ Connection state: {self._connection_state}"")
                        self.logger.error(f""❌ Connection open: {self.connection and not self.connection.is_closed if self.connection else False}"")
                        self.logger.error(f""❌ Channel open: {self.channel and not self.channel.is_closed if self.channel else False}"")
                        return False
                    self.logger.warning(f""⚠️ Publish attempt {attempt + 1} failed: {str(e)}"")
                    time.sleep(2)  # Wait 2 seconds before retrying
    
    def _cleanup(self, force: bool = False):
        """"""Clean up RabbitMQ resources.""""""
        try:
            if self.channel and not self.channel.is_closed:
                self.channel.close()
            if self.connection and not self.connection.is_closed:
                self.connection.close()
        except Exception as e:
            if force:
                self.logger.error(f""❌ Error during forced cleanup: {str(e)}"")
            else:
                self.logger.warning(f""⚠️ Error during cleanup: {str(e)}"")
    
    def _cleanup_on_exit(self):
        """"""Clean up on application exit.""""""
        self._is_shutting_down = True
        self._cleanup(force=True)
    
    def close(self):
        """"""Close the RabbitMQ connection.""""""
        self._is_shutting_down = True
        self._cleanup(force=True)",services/rabbitmq_service.py
EvaluationService,"class EvaluationService(BaseService):

    def __init__(self):
        self.logger = logger[""evaluation""]

    async def initialize(self) -> None:
        """"""Initialize the evaluation service.""""""
        try:
            self._initialized = True
            self.logger.info(""Evaluation service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize evaluation service: {str(e)}"")
            raise

    async def evaluate(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
    ) -> dict:
        """"""
        Evaluate the performance of a trained model using predicted and true target values.

        Args:
            y_true (np.ndarray): Ground truth target values.
            y_pred (np.ndarray): Predicted target values by the model.

        Returns:
            Metrics: Dictionary of evaluation metrics (e.g., {'mae': ..., 'rmse': ...}).
        """"""

        try:
            self.logger.info(f""Starting evaluation"")

            metrics = self._calculate_metrics(y_true, y_pred)

            # Returns the metrics as a dict
            return metrics.__dict__
        except Exception as e:
            self.logger.error(f""Evaluation failed: {str(e)}"")
            raise

    async def is_ready_for_deployment(
        self, candidate_metrics: dict, live_metrics: dict
    ) -> bool:
        """"""
        Determines whether the candidate model should be deployed by comparing it
        to the currently deployed (live) model.

        Args:
            candidate_metrics (Metrics): Metrics for the candidate model.
            live_metrics (Metrics): Metrics for the current live model.

        Returns:
            bool: True if the candidate should be deployed, False otherwise.
        """"""
        try:
            self.logger.info(f""Evaluating deployment decision based on metrics"")

            # Check if deployment is needed
            deploy = self._has_better_metrics(candidate_metrics, live_metrics)

            # Log the result
            if deploy:
                self.logger.info(
                    ""Candidate model performs better. Ready for deployment.""
                )
            else:
                self.logger.info(""Candidate model does not outperform live model."")

            return deploy
        except Exception as e:
            self.logger.error(f""Error during deployment evaluation: {e}"")
            raise

    def _has_better_metrics(self, candidate_metrics: dict, live_metrics: dict) -> bool:
        """"""
        Returns True if the candidate model has a lower MAE than the live model.
        """"""
        try:
            return candidate_metrics[""mae""] < live_metrics[""mae""]
        except (KeyError, TypeError) as e:
            self.logger.warning(f""Missing or invalid 'mae' in metrics: {e}"")
            return False

    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Metrics:
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        return Metrics(mae=float(mae), mse=float(mse), rmse=float(rmse), r2=float(r2))

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            self._initialized = False
            self.logger.info(""Evaluation service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during evaluation service cleanup: {str(e)}"")",services/evaluation_service.py
TrainingService,"class TrainingService(BaseService):
    """"""Service for model training and evaluation.""""""

    def __init__(self):
        super().__init__()
        self._initialized = False
        self.training_tasks = {}
        self.logger = logger[""training""]

    async def initialize(self) -> None:
        """"""Initialize the training service.""""""
        try:
            self._enable_full_reproducibility()
            self._initialized = True
            self.logger.info(""Training service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize training service: {str(e)}"")
            raise

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            # Cancel any running training tasks
            for task in self.training_tasks.values():
                if not task.done():
                    task.cancel()

            self.training_tasks.clear()
            self._initialized = False
            self.logger.info(""Training service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during training service cleanup: {str(e)}"")

    async def get_trainers(self) -> Dict[str, Any]:
        """"""Retrieve the list of available training trainers (from the TrainerFactory)""""""
        try:
            self.logger.info(""Starting to retrieve the list of available trainers."")
            trainers = ModelRegistry.list_models()
            self.logger.info(
                f""Successfully retrieved {len(trainers)} trainers from ModelRegistry.""
            )
            return {
                ""status"": ""success"",
                ""types"": trainers,
                ""count"": len(trainers),
                ""timestamp"": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f""Error getting the trainers : {str(e)}"")
            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""timestamp"": datetime.now().isoformat(),
            }

    async def train_model(
        self,
        symbol: str,
        model_type: str,
        data: ProcessedData,
        **kwargs,
    ) -> Dict[str, Any]:
        """"""
        Train a model for a given symbol.

        Args:
            symbol: Stock symbol
            model_type: Type of model to train (lstm or prophet)
            start_date: Start date for training data
            end_date: End date for training data
            **kwargs: Additional training parameters

        Returns:
            Dictionary containing training results
        """"""
        if not self._initialized:
            self.logger.error(""Training service not initialized."")
            return {
                ""status"": ""error"",
                ""error"": ""Training service not initialized"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

        if not validate_stock_symbol(symbol):
            self.logger.error(f""Invalid stock symbol: {symbol}"")
            return {
                ""status"": ""error"",
                ""error"": f""Invalid stock symbol: {symbol}"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

        if model_type not in ModelRegistry.list_models():
            self.logger.error(f""Unsupported model type: {model_type}"")
            return {
                ""status"": ""error"",
                ""error"": f""Unsupported model type: {model_type}"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

        if not data or data.X is None:
            self.logger.error(f""No valid data for model training for {symbol}."")
            return {
                ""status"": ""error"",
                ""error"": f""No valid data for model training for {symbol}"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

        try:
            # Begin to monitor cpu usage
            monitor_task = asyncio.create_task(
                monitor_training_cpu_usage(model_type, symbol)
            )

            # Begin to monitor memory usage
            monitor_task = asyncio.create_task(
                monitor_training_memory_usage(model_type, symbol)
            )

            # Create the model
            model = ModelRegistry.create(name=model_type, symbol=symbol)

            # Create training task
            task = asyncio.create_task(model.train_and_save(data))
            self.logger.debug(
                f""Training task for {model_type} model for {symbol} has started.""
            )

            # Store task
            task_key = f""{symbol}_{model_type}""
            self.training_tasks[task_key] = task

            # Wait for training completion
            result = await task
            self.logger.info(f""Training completed for {model_type} model for {symbol}."")

            # Stop the monitor after training completes
            monitor_task.cancel()

            # Remove completed task
            del self.training_tasks[task_key]

            # Log the sucessful training count (Prometheus)
            training_total.labels(
                model_type=model_type, symbol=symbol, result=""sucess""
            ).inc()

            self.logger.info(
                f""Training successful for {model_type} model for {symbol}.""
            )

            return {
                ""status"": ""success"",
                ""timestamp"": datetime.now().isoformat(),
                **result,
            }

        except Exception as e:
            self.logger.error(
                f""Error training {model_type} model for {symbol}: {str(e)}""
            )

            # Log the unsucessful training count (Prometheus)
            training_total.labels(
                model_type=model_type, symbol=symbol, result=""error""
            ).inc()

            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def get_training_status(self, symbol: str, model_type: str) -> Dict[str, Any]:
        """"""
        Get training status for a model.

        Args:
            symbol: Stock symbol
            model_type: Type of model

        Returns:
            Dictionary containing training status
        """"""
        task_key = f""{symbol}_{model_type}""
        task = self.training_tasks.get(task_key)

        if not task:
            return {
                ""status"": ""not_found"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

        if task.done():
            try:
                result = task.result()
                return {
                    ""status"": ""completed"",
                    ""symbol"": symbol,
                    ""model_type"": model_type,
                    ""result"": result,
                    ""timestamp"": datetime.now().isoformat(),
                }
            except Exception as e:
                return {
                    ""status"": ""failed"",
                    ""symbol"": symbol,
                    ""model_type"": model_type,
                    ""error"": str(e),
                    ""timestamp"": datetime.now().isoformat(),
                }
        else:
            return {
                ""status"": ""in_progress"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def cancel_training(self, symbol: str, model_type: str) -> Dict[str, Any]:
        """"""
        Cancel training for a model.

        Args:
            symbol: Stock symbol
            model_type: Type of model

        Returns:
            Dictionary containing cancellation status
        """"""
        task_key = f""{symbol}_{model_type}""
        task = self.training_tasks.get(task_key)

        if not task:
            return {
                ""status"": ""not_found"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

        if not task.done():
            task.cancel()
            del self.training_tasks[task_key]
            return {
                ""status"": ""cancelled"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }
        else:
            return {
                ""status"": ""already_completed"",
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def get_active_training_tasks(self) -> List[Dict[str, Any]]:
        """"""
        Get list of active training tasks.

        Returns:
            List of dictionaries containing task information
        """"""
        return [
            {
                ""symbol"": task_key.split(""_"")[0],
                ""model_type"": task_key.split(""_"")[1],
                ""status"": ""in_progress"" if not task.done() else ""completed"",
                ""timestamp"": datetime.now().isoformat(),
            }
            for task_key, task in self.training_tasks.items()
        ]

    def _enable_full_reproducibility(self, seed: int = 42):
        """"""
        Ensures reproducibility by setting random seeds and configuring TensorFlow.
        """"""
        os.environ[""PYTHONHASHSEED""] = str(seed)
        os.environ[""TF_DETERMINISTIC_OPS""] = ""1""
        os.environ[""TF_ENABLE_ONEDNN_OPTS""] = ""0""

        random.seed(seed)
        np.random.seed(seed)
        tf.random.set_seed(seed)

        tf.config.threading.set_inter_op_parallelism_threads(1)
        tf.config.threading.set_intra_op_parallelism_threads(1)",services/training/training_service.py
ModelRegistry,"class ModelRegistry:
    _registry = {}

    @classmethod
    def register(cls, name):
        def inner_wrapper(wrapped_class):
            cls._registry[name] = wrapped_class
            return wrapped_class

        return inner_wrapper

    @classmethod
    def create(cls, name, **kwargs) -> ""BaseModel"":
        if name not in cls._registry:
            available = cls.list_models()
            raise ValueError(
                f""Model '{name}' not found in registry. Available models: {available}""
            )
        return cls._registry[name](**kwargs)

    @classmethod
    def list_models(cls):
        return list(cls._registry.keys())

    @classmethod
    def get_registry(cls):
        """"""Get the current registry for debugging""""""
        return cls._registry.copy()",services/training/model_registry.py
ProphetPredictor,"class ProphetPredictor(PythonModel):
    def load_context(self, context):
        import joblib

        model_path = context.artifacts.get(""model"")
        if not model_path:
            raise ValueError(
                f""Model path for {self.model_name} is missing from MLflow artifacts.""
            )
        self.model = joblib.load(model_path)

    def predict(self, context, model_input, params=None):
        return self.model.predict(model_input)",services/training/predictors/prophet_predictor.py
LSTMPredictor,"class LSTMPredictor(PythonModel):
    def load_context(self, context):
        from keras import models  # Replace tensorflow import

        model_path = context.artifacts.get(""model"")
        if not model_path:
            raise ValueError(
                f""Model path for {self.model_name} is missing from MLflow artifacts.""
            )

        self.model = models.load_model(model_path, compile=False)

    def predict(self, context, model_input, params=None):
        return self.model.predict(model_input)",services/training/predictors/lstm_predictor.py
BaseSaver,"class BaseSaver(ABC):
    @abstractmethod
    def save_model(self, model, base_path) -> Path:
        """"""Save the trained model to disk.""""""
        pass

    async def save(self, model, base_path) -> Path:
        """"""Save the trained model to disk.""""""
        try:
            base_path.mkdir(parents=True, exist_ok=True)
            model_path = self.save_model(model, base_path)
            return model_path
        except Exception as e:
            raise RuntimeError(f""Failed to save model to '{base_path}': {e}"") from e",services/training/models/saving_strategies.py
JoblibSaver,"class JoblibSaver(BaseSaver):
    def save_model(self, model, base_path):
        import joblib

        model_path = base_path / ""model.joblib""
        joblib.dump(model, model_path)

        return model_path",services/training/models/saving_strategies.py
KerasSaver,"class KerasSaver(BaseSaver):
    def save_model(self, model, base_path):
        model_path = base_path / ""model.keras""
        model.save(str(model_path))
        return model_path",services/training/models/saving_strategies.py
ProphetModel,"class ProphetModel(BaseModel):
    def __init__(
        self,
        symbol,
        saver=JoblibSaver(),
        trainer=ProphetTrainer(),
        predictor=ProphetPredictor(),
    ):
        super().__init__(MODEL_NAME, symbol, saver, trainer, predictor)",services/training/models/prophet_model.py
BaseModel,"class BaseModel(ABC, PythonModel):
    """"""Base class for all model trainers.""""""

    def __init__(
        self,
        model_type: str,
        symbol: str,
        saver: BaseSaver,
        trainer: BaseTrainer,
        predictor: PythonModel,
    ):
        self.model_type = model_type
        self.symbol = symbol

        self.saver = saver
        self.trainer = trainer
        self.predictor = predictor

        self.config = config
        self.logger = logger[""training""]
        self.model_root_dir = self.config.model.MODELS_ROOT_DIR

    async def train_and_save(self, data: ProcessedData) -> str:
        """"""
        Trains the model, saves it locally, and logs it to MLflow.

        Args:
            data (FormattedInput): Preprocessed training data.

        Returns:
            str: Name of the registered MLflow model.
        """"""

        try:
            mlflow.set_experiment(""training_experiments"")
            with mlflow.start_run() as run:

                # Set custom tags
                mlflow.set_tag(""stage"", ""training"")
                mlflow.set_tag(""model_type"", self.model_type)
                mlflow.set_tag(""symbol"", self.symbol)
                mlflow.set_tag(""training_data_start_date"", data.start_date)
                mlflow.set_tag(""training_data_end_date"", data.end_date)

                # Train the model
                model, training_history = await self.trainer.train(data)

                # Save the model (locally)
                saved_training_model_path = await self.saver.save(
                    model, base_path=self._get_training_model_dir()
                )

                # Register the model to MLFlow
                mlflow.pyfunc.log_model(
                    python_model=self.predictor,
                    artifact_path=""model"",
                    input_example=data.X,  # TODO To long to log. We will need to get one sample
                    artifacts={""model"": str(saved_training_model_path)},
                )

            return {
                ""run_id"": run.info.run_id,
                ""run_info"": run.info.__dict__,
                ""training_history"": training_history,
            }

        except Exception as e:
            raise RuntimeError(
                f""Failed to train and save model '{self.model_name}': {e}""
            ) from e

    def _get_training_model_dir(self) -> Path:
        """"""Return the path to the model training directory""""""
        return self.model_root_dir / self.model_type / self.symbol",services/training/models/base_model.py
LSTMModel,"class LSTMModel(BaseModel):
    def __init__(
        self,
        symbol,
        saver=KerasSaver(),
        trainer=LSTMTrainer(),
        predictor=LSTMPredictor(),
    ):
        super().__init__(MODEL_NAME, symbol, saver, trainer, predictor)",services/training/models/lstm_model.py
ProphetTrainer,"class ProphetTrainer(BaseTrainer):
    async def train(
        self, data: ProcessedData[pd.DataFrame], **kwargs
    ) -> Tuple[Prophet, Dict[str, Any]]:
        try:
            # Extract the features (only X, since the Prophet model is trained using a DataFrame)
            data = data.X

            # Initialize Prophet model
            model = Prophet(
                changepoint_prior_scale=kwargs.get(""changepoint_prior_scale"", 0.05),
                seasonality_prior_scale=kwargs.get(""seasonality_prior_scale"", 10.0),
                holidays_prior_scale=kwargs.get(""holidays_prior_scale"", 10.0),
                seasonality_mode=kwargs.get(""seasonality_mode"", ""multiplicative""),
            )

            # Add additional regressors
            for feature in data.columns:
                if feature not in [""Close"", ""Date"", ""ds"", ""y""]:
                    model.add_regressor(feature)

            # Fit model
            model.fit(data)

            # Get training history
            history = {
                ""changepoints"": (
                    model.changepoints.tolist()
                    if hasattr(model, ""changepoints"")
                    else []
                ),
                ""trend"": model.params[""k""].tolist() if ""k"" in model.params else [],
                ""seasonality"": (
                    model.params[""beta""].tolist() if ""beta"" in model.params else []
                ),
            }

            return model, history

        except Exception as e:
            raise RuntimeError(
                f""Error occurred while training the Prophet model: {str(e)}""
            ) from e",services/training/trainers/prophet_trainer.py
BaseTrainer,"class BaseTrainer(ABC):
    """"""Base class for all model trainers.""""""

    @abstractmethod
    async def train(self, data: ProcessedData, **kwargs) -> Tuple[Any, Dict[str, Any]]:
        """"""Train the model.""""""
        pass",services/training/trainers/base_trainer.py
LSTMTrainer,"class LSTMTrainer(BaseTrainer):
    """"""Trainer for LSTM models.""""""

    async def train(
        self, data: ProcessedData[np.ndarray], **kwargs
    ) -> Tuple[Any, Dict[str, Any]]:
        try:
            X_train, y_train = data.X, data.y

            model = self._build_model(X_train.shape[1:])

            model.compile(optimizer=""adam"", loss=""mse"", metrics=[""mae""])

            history = model.fit(
                X_train,
                y_train,
                epochs=kwargs.get(""epochs"", 50),
                batch_size=kwargs.get(""batch_size"", 32),
                validation_split=0.2,
                callbacks=[
                    callbacks.EarlyStopping(
                        monitor=""val_loss"", patience=5, restore_best_weights=True
                    )
                ],
            )

            return model, history.history

        except Exception as e:
            raise RuntimeError(
                f""Error occurred while training the LSTM model: {str(e)}""
            ) from e

    def _build_model(self, input_shape: Tuple[int, ...]) -> Any:
        """"""Build LSTM model architecture.""""""
        model = Sequential(
            [
                layers.LSTM(
                    100,
                    return_sequences=True,
                    input_shape=input_shape,
                    kernel_initializer=""glorot_uniform"",
                    recurrent_initializer=""orthogonal"",
                ),
                layers.Dropout(0.2),
                layers.LSTM(
                    50,
                    kernel_initializer=""glorot_uniform"",
                    recurrent_initializer=""orthogonal"",
                ),
                layers.Dropout(0.2),
                layers.Dense(
                    50, activation=""relu"", kernel_initializer=""glorot_uniform""
                ),
                layers.Dense(
                    25, activation=""relu"", kernel_initializer=""glorot_uniform""
                ),
                layers.Dense(1),
            ]
        )

        return model",services/training/trainers/lstm_trainer.py
BaseDataProcessor,"class BaseDataProcessor(ABC):
    """"""
    Abstract base class for data processing steps in preprocessing.
    """"""

    @abstractmethod
    def process(
        self, data: Union[pd.DataFrame, np.ndarray]
    ) -> Union[pd.DataFrame, np.ndarray]:
        """"""
        Process the data.

        Args:
            data (pd.DataFrame): Input stock data.

        Returns:
            pd.DataFrame|np.ndarray: Processed stock data.
        """"""
        pass",services/data_processing/abstract.py
DataProcessingService,"class DataProcessingService(BaseService):

    def __init__(self):
        super().__init__()
        self.logger = logger[""data_processing""]

    async def initialize(self) -> None:
        """"""Initialize the data processing service.""""""
        try:
            self._initialized = True

            # Map the phase to the correction function
            self.phase_function_map = {
                ""training"": self._preprocess_training_phase,
                ""prediction"": self._preprocess_prediction_phase,
                ""evaluation"": self._preprocess_evaluation_phase,
            }

            self.logger.info(""Data processing service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize data processing service: {str(e)}"")
            raise

    async def promote_scaler(self, symbol: str, model_type: str):
        """"""
        Promote the training scalers to the prediction phase for a specific model and symbol.

        This copies both the features and targets scalers from the 'training' directory
        to the 'prediction' directory, if the model requires scaling.

        Args:
            symbol (str): The stock symbol
            model_type (str): The type of model
        """"""
        try:
            # Promote the scaler
            scaler_manager = ScalerManager(model_type=model_type, symbol=symbol)
            promoted = scaler_manager.promote_scaler()

            if promoted:
                self.logger.info(
                    f""Successfully promoted scalers for model '{model_type}' and symbol '{symbol}'.""
                )
            else:
                self.logger.info(
                    f""No scaler promotion needed for model '{model_type}' and symbol '{symbol}'.""
                )
        except Exception as e:
            self.logger.error(
                f""Failed to promote the scalers for {model_type} model for {symbol} : {str(e)}""
            )
            raise

    async def preprocess_data(
        self,
        symbol: str,
        data: pd.DataFrame,
        model_type: str,
        phase: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Union[ProcessedData, Tuple[ProcessedData, ProcessedData]]:
        """"""
        Preprocess the stock data for models input.

        This method first applies common preprocessing steps that are independent of the phase.
        Then, it delegates to the appropriate phase-specific preprocessing function.

        Args:
            data (pd.DataFrame): Raw stock data

        Returns:
            FormattedInput: Processed data formatted specifically for input into a model.
        """"""

        # TODO Check existing preproccessed Data (MinIO + Redis ? Or Cache ?)
        """"""
        Exemple:
        if preprocessing_is_cached(symbol, start_date, end_date)
            return preprocessed data
        else
            generate preprocessed data
        """"""

        try:

            self.logger.info(
                f""Starting preprocessing for symbol={symbol} for model {model_type} during {phase} phase""
            )

            if phase not in self.phase_function_map:
                raise ValueError(f""Unknown phase '{phase}'"")

            # Clean the data
            clean_data = DataCleaner().process(data)
            self.logger.debug(
                f""Data cleaned for symbol={symbol}, model_type={model_type}""
            )

            # Retrieve dates
            dates = clean_data[""Date""].dt.date

            # Build features
            features = FeatureBuilder().process(clean_data)
            self.logger.debug(
                f""Features built for symbol={symbol}, model_type={model_type}""
            )

            # Select features
            features = FeatureSelector(model_type).process(features)
            self.logger.debug(
                f""Features selected for symbol={symbol}, model_type={model_type}""
            )

            # Capture the column-to-index map right after feature selection
            feature_index_map = {col: idx for idx, col in enumerate(features.columns)}

            # Format the data
            input_data = InputFormatter(model_type, phase).process(features)
            self.logger.debug(
                f""Data formatted for symbol={symbol}, model_type={model_type}, phase={phase}""
            )

            # Add the column-to-index map to the input data
            input_data.feature_index_map = feature_index_map

            # Execute the correction function based on the phase
            return self.phase_function_map[phase](input_data, dates, symbol, model_type)

        except Exception as e:
            self.logger.error(
                f""Error preprocessing the stock data for model {model_type} for {symbol} during {phase} phase: {str(e)}""
            )
            raise

    async def postprocess_data(
        self,
        symbol: str,
        prediction: pd.DataFrame,
        model_type: str,
        phase: str,
    ) -> ProcessedData:
        """"""
        Postprocess the predictions by reversing the scaling applied during preprocessing.

        Args:
            symbol (str): Stock symbol
            targets (pd.DataFrame): Scaled predictions to be unnormalized.
            model_type: Type of model
            phase (str): The phase (e.g., ""train"", ""test"", or ""inference"").

        Returns:
            FormattedInput: Processed data including the unscaled targets
        """"""
        try:
            self.logger.info(
                f""Starting postprocessing for {symbol} symbol for {model_type} model during {phase} phase""
            )

            data = ProcessedData(y=prediction)

            data = DataNormalizer(
                symbol=symbol, model_type=model_type, phase=phase
            ).unprocess(data)
            self.logger.debug(
                f""Targets unnormalized for symbol={symbol}, model_type={model_type}""
            )

            data = OutputFormatter(model_type=model_type).process(data)
            self.logger.debug(
                f""Targets formatted for symbol={symbol}, model_type={model_type}""
            )

            self.logger.info(
                f""Postprocessing completed for {symbol} symbol for {model_type} model during {phase} phase""
            )

            return data
        except Exception as e:
            self.logger.error(
                f""Error postprocessing the stock data for {model_type} model for {symbol} during {phase} phase: {str(e)}""
            )
            raise

    def _preprocess_training_phase(
        self, features: ProcessedData, dates: pd.Series, symbol: str, model_type: str
    ):
        """"""Handle the preprocessing steps for training.""""""

        # Split the data
        train_data, test_data = DataSplitter().process(features)
        self.logger.debug(
            f""Data split completed for symbol={symbol}, model_type={model_type} — ""
            f""train size: {len(train_data.X) if train_data.X is not None else 0}, ""
            f""test size: {len(test_data.X) if test_data.X is not None else 0}""
        )

        # Normalize the training data
        norm_train_data = DataNormalizer(symbol, model_type, ""training"").process(
            train_data, fit=True
        )
        self.logger.debug(
            f""Training data normalized for symbol={symbol}, model_type={model_type}""
        )

        # Normalize the test data
        norm_test_data = DataNormalizer(symbol, model_type, ""training"").process(
            test_data, fit=False
        )
        self.logger.debug(
            f""Test data normalized for symbol={symbol}, model_type={model_type}""
        )

        # Retrieve training dataset start and end dates
        train_start_date = dates[0]
        train_end_date = dates[len(norm_train_data.X) - 1]

        # Retrieve test dataset start and end dates
        test_start_date = dates[len(norm_train_data.X)]
        test_end_date = dates[len(dates) - 1]

        # Add start and end dates to preprocessed data (Train and test)
        norm_train_data.start_date = train_start_date
        norm_train_data.end_date = train_end_date

        norm_test_data.start_date = test_start_date
        norm_test_data.end_date = test_end_date

        self.logger.info(
            f""Completed training phase preprocessing for {symbol} symbol for {model_type} model""
        )

        return norm_train_data, norm_test_data

    def _preprocess_evaluation_phase(
        self, features: ProcessedData, dates: pd.Series, symbol: str, model_type: str
    ):
        """"""Handle the preprocessing steps for evaluation""""""

        # Split the data
        _, eval_data = DataSplitter().process(features)
        self.logger.debug(
            f""Data split completed for symbol={symbol}, model_type={model_type} — ""
            f""Eval size: {len(eval_data.X) if eval_data.X is not None else 0}""
        )

        # Normalize the test data (use prediction-phase scaler)
        norm_eval_data = DataNormalizer(symbol, model_type, ""prediction"").process(
            eval_data, fit=False
        )
        self.logger.debug(
            f""Evaluation data normalized for symbol={symbol}, model_type={model_type}""
        )

        # Retrieve the start and end dates
        eval_start_date = dates[len(dates) - len(norm_eval_data.X)]
        eval_end_date = dates[len(dates) - 1]

        # Add start and end dates to preprocessed data
        norm_eval_data.start_date = eval_start_date
        norm_eval_data.end_date = eval_end_date

        self.logger.info(
            f""Completed evaluation phase preprocessing for {symbol} symbol for model {model_type}""
        )

        return norm_eval_data

    def _preprocess_prediction_phase(
        self, features: ProcessedData, dates: pd.Series, symbol: str, model_type: str
    ):
        """"""Handle the preprocessing steps for prediction.""""""

        # Normalize the data
        norm_features = DataNormalizer(symbol, model_type, ""prediction"").process(
            features, fit=False
        )

        self.logger.debug(
            f""Prediction data normalized for symbol={symbol}, model_type={model_type}""
        )

        # Retrieve the start and end dates
        pred_start_date = dates[len(dates) - len(norm_features.X)]
        pred_end_date = dates[len(dates) - 1]

        # Add start and end dates to preprocessed data
        norm_features.start_date = pred_start_date
        norm_features.end_date = pred_end_date

        self.logger.info(
            f""Completed prediction phase preprocessing for {symbol} symbol for {model_type} model""
        )

        return norm_features

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            self._initialized = False
            self.logger.info(""Preprocessing service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during preprocessing service cleanup: {str(e)}"")",services/data_processing/data_processing_service.py
ScalerFactory,"class ScalerFactory:
    _SCALABLE_MODELS = {""lstm""}

    @staticmethod
    def create_scaler(model_type: str) -> MinMaxScaler:
        if model_type in ScalerFactory._SCALABLE_MODELS:
            return MinMaxScaler()
        return None

    @staticmethod
    def model_requires_scaling(model_type: str) -> bool:
        return model_type in ScalerFactory._SCALABLE_MODELS",services/data_processing/scaler_manager.py
ScalerManager,"class ScalerManager:
    FEATURES_SCALER_TYPE = ""features""
    TARGETS_SCALER_TYPE = ""targets""
    VALID_SCALER_TYPES = {FEATURES_SCALER_TYPE, TARGETS_SCALER_TYPE}

    def __init__(self, model_type: str, symbol: str):
        self.model_type = model_type
        self.symbol = symbol

    def model_requires_scaling(self) -> bool:
        return ScalerFactory.model_requires_scaling(self.model_type)

    def create_scaler(self) -> MinMaxScaler:
        return ScalerFactory.create_scaler(self.model_type)

    def save_scaler(self, scaler, phase, scaler_type: str):
        """"""
        Save the given scaler to disk.

        Args:
            scaler: A fitted scikit-learn scaler to be saved.
            scaler_type: The type of scaler. Must be one of 'features' or 'targets'.
        """"""
        try:
            joblib.dump(scaler, self.get_scaler_path(phase, scaler_type))
        except Exception as e:
            raise e

    def load_scaler(
        self, phase: str, scaler_type: str = FEATURES_SCALER_TYPE
    ) -> MinMaxScaler:
        """"""
        Load the saved scaler from disk given the model_type and the symbol

        Args:
            scaler_type: The type of scaler. Must be one of 'features' or 'targets'.
                This determines whether the scaler is for input features or the targets variable.

        Returns:
            Any (sklearn scalers): The loaded scaler instance.
        """"""
        scaler_path = self.get_scaler_path(phase, scaler_type)

        if not scaler_path.exists:
            raise FileNotFoundError(
                f""Scaler not found for model {self.model_type} for symbol {self.symbol} for {phase} phase""
            )

        return joblib.load(scaler_path)

    def promote_scaler(self) -> bool:
        """"""
        Promote scalers from the 'training' phase to the 'prediction' phase (if needed).

        Returns:
            bool: True if the scalers were successfully promoted, False if no promotion was needed.
        """"""
        try:
            if self.model_requires_scaling():

                # Get the training scalers
                src_features_scaler_path = self.get_scaler_path(
                    ""training"", self.FEATURES_SCALER_TYPE
                )
                src_targets_scaler_path = self.get_scaler_path(
                    ""training"", self.TARGETS_SCALER_TYPE
                )

                dst_features_scaler_path = self.get_scaler_path(
                    ""prediction"", self.FEATURES_SCALER_TYPE
                )
                dst_targets_scaler_path = self.get_scaler_path(
                    ""prediction"", self.TARGETS_SCALER_TYPE
                )

                if (
                    not src_features_scaler_path.exists()
                    or not src_targets_scaler_path.exists()
                ):
                    raise FileNotFoundError(f""Missing scaler to promote"")

                # Copy the training scalers to prediction scalers
                shutil.copyfile(src_features_scaler_path, dst_features_scaler_path)
                shutil.copyfile(src_targets_scaler_path, dst_targets_scaler_path)

                return True
            else:
                return False
        except Exception as e:
            raise RuntimeError(""Error while promoting the scalers"") from e

    def get_scaler_path(
        self, phase: str, scaler_type: str = FEATURES_SCALER_TYPE
    ) -> Path:
        """"""
        Returns the full path to the scaler file based on model type, symbol, and phase.
        Creates the directory if it doesn't exist.

        Args:
            scaler_type: The type of scaler. Must be one of 'features' or 'targets'.
                This determines whether the scaler is for input features or the targets variable.

        Returns:
            Path: Path to the scaler file.
        """"""

        self._validate_scaler_type(scaler_type)

        # Directory of the scaler
        scaler_dir = (
            config.preprocessing.SCALERS_DIR / self.model_type / phase / self.symbol
        )
        scaler_dir.mkdir(parents=True, exist_ok=True)

        # Full scaler path (with the corresponding phase)
        scaler_path = scaler_dir / f""{scaler_type}_scaler.gz""

        return scaler_path

    def _validate_scaler_type(self, scaler_type: str):
        """"""
        Validates the scaler type to ensure it is either 'features' or 'targets'.

        Args:
            scaler_type: The type of scaler. Must be one of 'features' or 'targets'.
                This determines whether the scaler is for input features or the targets variable.
        """"""
        if scaler_type not in self.VALID_SCALER_TYPES:
            raise ValueError(
                f""Invalid scaler_type '{scaler_type}'. Must be one of {self.VALID_SCALER_TYPES}""
            )",services/data_processing/scaler_manager.py
DataSplitter,"class DataSplitter(BaseDataProcessor):

    def process(self, data: ProcessedData) -> Tuple[ProcessedData, ProcessedData]:
        """"""
        Split the preprocessed data into training and test datasets.

        Args:
            data (PreprocessedData): Preprocessed features and targets.

        Returns:
            Tuple[PreprocessedData, PreprocessedData]: Train and test datasets.
        """"""

        try:
            # Test ratio
            test_ratio = 1 - config.preprocessing.TRAINING_SPLIT_RATIO

            # Get the datasets sizes
            X_train, X_test, y_train, y_test = train_test_split(
                data.X, data.y, test_size=test_ratio, shuffle=False
            )

            return (
                ProcessedData(
                    X=X_train, y=y_train, feature_index_map=data.feature_index_map
                ),
                ProcessedData(
                    X=X_test, y=y_test, feature_index_map=data.feature_index_map
                ),
            )
        except Exception as e:
            raise RuntimeError(""Error while splitting the data"") from e",services/data_processing/steps/splitter.py
FeatureBuilder,"class FeatureBuilder(BaseDataProcessor):
    """"""
    Class that processes stock data to generate additional features, including:

    - Returns (percentage and log returns)
    - Technical indicators (Moving Averages, Volatility, RSI, MACD)
    - Temporal features (Day of week, Month, Quarter)
    """"""

    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        try:
            # Add returns features
            data = self._calculate_and_add_returns(data)

            # Add techical indicators features
            data = self._calculate_and_add_technical_indicators(data)

            # Add temporal features
            data = self._add_temporal_features(data)

            return data
        except Exception as e:
            raise RuntimeError(f""Error building new features from stock data"") from e

    def _calculate_and_add_returns(self, data: pd.DataFrame) -> pd.DataFrame:
        """"""
        Calculate the returns and the log returns of the stock data (Close price)

        Args:
            data (pd.DataFrame): Dataframe containing the stock data

        Returns:
            pd.DataFrame: Dataframe containing the stock data with the retuns calculated
        """"""
        try:
            # Create a copy of the DataFrame to avoid modifying the original
            data = data.copy()

            # Calculate Returns (percentage change)
            data[""Returns""] = data[""Close""].pct_change()

            # Calculate Log Returns
            data[""Log_Returns""] = np.log(1 + data[""Close""].pct_change())

            # Replace NaN values with forward fill then backward fill
            data = data.ffill().bfill()

            return data

        except Exception as e:
            raise RuntimeError(f""Error calculating log returns."") from e

    def _calculate_and_add_technical_indicators(
        self, data: pd.DataFrame
    ) -> pd.DataFrame:
        """"""
        Calculate technical indicators for stock data.

        Args:
            data: DataFrame with OHLCV data

        Returns:
            DataFrame with additional technical indicators
        """"""
        try:
            # Create a copy of the DataFrame to avoid modifying the original
            data = data.copy()

            # Calculate Moving Averages
            data[""MA_5""] = data[""Close""].rolling(window=5).mean()
            data[""MA_20""] = data[""Close""].rolling(window=20).mean()

            # Calculate Volatility (20-day standard deviation of returns)
            data[""Volatility""] = data[""Returns""].rolling(window=20).std()

            # Calculate RSI
            delta = data[""Close""].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            data[""RSI""] = 100 - (100 / (1 + rs))

            # Calculate MACD
            exp1 = data[""Close""].ewm(span=12, adjust=False).mean()
            exp2 = data[""Close""].ewm(span=26, adjust=False).mean()
            data[""MACD""] = exp1 - exp2
            data[""MACD_Signal""] = data[""MACD""].ewm(span=9, adjust=False).mean()

            # Ensure Adj Close exists (if not, use Close)
            if ""Adj Close"" not in data.columns:
                data[""Adj Close""] = data[""Close""]

            # Replace NaN values with forward fill then backward fill
            data = data.ffill().bfill()

            return data

        except Exception as e:
            raise RuntimeError(f""Error calculating technical indicators."") from e

    def _add_temporal_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """"""
        Add temporal features of the stock data. This features include :
        - Day of the week as a number
        - Month as a number
        - Quarter as a number (Q1, Q2, Q3, Q4)

        Args:
            data (pd.DataFrame): Dataframe containing the stock data

        Returns:
            pd.DataFrame: Dataframe containing the temporal features
        """"""
        try:
            # Create a copy of the DataFrame to avoid modifying the original
            data = data.copy()

            # Add day of week column (the day week as a number)
            data[""Day_of_week""] = data[""Date""].dt.dayofweek

            # Add month column (month as a number)
            data[""Month""] = data[""Date""].dt.month

            # Add quarter column
            data[""Quarter""] = data[""Date""].dt.quarter

            return data

        except Exception as e:
            raise RuntimeError(f""Error adding the temporal features."") from e",services/data_processing/steps/feature_builder.py
DataCleaner,"class DataCleaner(BaseDataProcessor):
    """"""
    Cleans stock data by resetting index, parsing dates, and filling missing values.
    """"""

    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """"""
        Cleans the data (pd.DataFrame).

        Args:
            data (pd.DataFrame): Raw stock data.

        Returns:
            pd.DataFrame: Cleaned data.

        """"""
        try:
            data[""Date""] = pd.to_datetime(data[""Date""], format=""mixed"", utc=True)
            data = data.ffill().bfill()

            return data
        except Exception as e:
            raise RuntimeError(f""Error cleaning stock data"") from e",services/data_processing/steps/cleaner.py
OutputFormatter,"class OutputFormatter(BaseDataProcessor):
    """"""
    Class that formats model output data for different
    model types using the appropriate strategy.
    """"""

    def __init__(self, model_type: str):
        self.model_type = model_type

    def process(self, data) -> np.ndarray:
        """"""
        Formats model output data

        Args:
            data (Any): Model output data

        Returns:
            np.ndarray: Formatted model output data
        """"""
        try:

            # Extract the targets
            y = data.y

            if isinstance(y, list) or isinstance(y, np.ndarray):
                y = np.array(y)  # Convert to numpy array (for list instance)
                data_formatter = NumpyOutputFormatter()
            else:
                data_formatter = self._get_data_formatter()

            formatted_y = data_formatter.format(y)
            return ProcessedData(y=formatted_y)
        except Exception as e:
            raise RuntimeError(""Error while formatting (output) the data"") from e

    def _get_data_formatter(self) -> OutputFormatterStrategy:
        if self.model_type == ""prophet"":
            return ProphetOutputFormatter()
        else:
            raise NotImplemented(f""No output formatter for model {self.model_type}"")",services/data_processing/steps/formatters/output_formatter.py
InputFormatter,"class InputFormatter(BaseDataProcessor):
    """"""
    Class that formats the data for different model types using
    appropriate formatting strategies.
    """"""

    def __init__(self, model_type: str, phase: str):
        self.model_type = model_type
        self.phase = phase

    def process(self, data) -> ProcessedData:
        """"""
        Formats the data for the different model types input.

        Args:
            data (pd.DataFrame): Input stock data.

        Returns:
            FormattedData: Either X and Y (for training phase) or just X (for prediction phase)
        """"""
        try:
            data_formatter = self._get_data_formatter()
            formatted_data = data_formatter.format(data, self.phase)
            return formatted_data
        except Exception as e:
            raise RuntimeError(""Error while formatting (input) the data"") from e

    def _get_data_formatter(self) -> InputFormatterStrategy:
        if self.model_type == ""lstm"":
            return SequenceInputFormatter()
        elif self.model_type == ""prophet"":
            return ProphetInputFormatter()
        else:
            raise NotImplemented(f""No input formatter for model {self.model_type}"")",services/data_processing/steps/formatters/input_formatter.py
InputFormatterStrategy,"class InputFormatterStrategy(ABC):
    @abstractmethod
    def format(self, data: pd.DataFrame, phase: str) -> ProcessedData:
        pass",services/data_processing/steps/formatters/input_strategies/base_strategy.py
ProphetInputFormatter,"class ProphetInputFormatter(InputFormatterStrategy):
    def format(self, data, phase):
        if phase == ""prediction"":

            # Get the latest data
            latest_data = data.tail(1).reset_index(drop=True)

            # Get the next available business date and update it (in the ds column)
            latest_data.at[0, ""ds""] = self._get_next_business_day()

            return ProcessedData(X=latest_data)

        elif phase == ""training"" or ""evaluation"":
            # Return the training data as-is (already in Prophet format)
            y = data[""y""].values
            return ProcessedData(X=data, y=y)
        else:
            raise ValueError(
                f""Invalid phase '{phase}'. Expected 'training' or 'prediction'.""
            )

    def _get_next_business_day(self) -> pd.Timestamp:
        return (pd.Timestamp.today().normalize() + BDay(1)).strftime(""%Y-%m-%d"")",services/data_processing/steps/formatters/input_strategies/prophet_formatter.py
SequenceInputFormatter,"class SequenceInputFormatter(InputFormatterStrategy):

    def __init__(self):
        super().__init__()
        self.sequence_length = config.preprocessing.SEQUENCE_LENGTH

    def format(self, data, phase):

        # Target index values
        target_index = data.columns.get_loc(""Close"")

        # Convert the data to a numpy array
        data_np = data.to_numpy()

        if phase == ""training"" or phase == ""evaluation"":
            X, y = self._create_sequences(data_np, target_index)
            return ProcessedData(X=X, y=y)
        elif phase == ""prediction"":
            # Returns the last sequence (for next day prediction)
            return ProcessedData(X=self._get_last_sequence(data))
        else:
            raise ValueError(
                f""Invalid phase '{phase}'. Expected 'training' or 'prediction'.""
            )

    def _create_sequences(
        self, data: np.ndarray, target_index: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """"""Create sequences for time series data.""""""

        if self.sequence_length > len(data):
            raise ValueError(
                f""Sequence length ({self.sequence_length}) is greater than the length of the data ({len(data)}).""
            )

        X, y = [], []
        for i in range(len(data) - self.sequence_length):
            X.append(data[i : (i + self.sequence_length)])
            y.append(data[i + self.sequence_length, target_index])
        return np.array(X), np.array(y)

    def _get_last_sequence(self, data: np.ndarray) -> np.ndarray:
        """"""Get the last sequence (for prediction)""""""
        sequence = data[-self.sequence_length :]
        # To match model input (1, sequence_length, num_features)
        return np.expand_dims(sequence, axis=0)",services/data_processing/steps/formatters/input_strategies/sequence_formatter.py
NumpyOutputFormatter,"class NumpyOutputFormatter(OutputFormatterStrategy):
    def format(self, targets: np.ndarray):
        if targets.ndim > 1:
            # Flatten to 1 dimension
            targets = targets.ravel()
        return targets",services/data_processing/steps/formatters/output_strategies/numpy_formatter.py
OutputFormatterStrategy,"class OutputFormatterStrategy(ABC):
    @abstractmethod
    def format(self, targets) -> np.ndarray:
        pass",services/data_processing/steps/formatters/output_strategies/base_strategy.py
ProphetOutputFormatter,"class ProphetOutputFormatter(OutputFormatterStrategy):
    def format(self, targets: pd.DataFrame):
        return targets[""yhat""].values",services/data_processing/steps/formatters/output_strategies/prophet_formatter.py
FeatureSelector,"class FeatureSelector(BaseDataProcessor):
    """"""
    Class that selects features for different model types using
    appropriate feature selection strategies.
    """"""

    def __init__(self, model_type: str):
        self.model_type = model_type

    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        try:
            feature_selector = self._get_feature_selector()
            data_selected = feature_selector.select(data)
            return data_selected
        except Exception as e:
            raise RuntimeError(
                f""Error selecting features for {self.model_type} model""
            ) from e

    def _get_feature_selector(self) -> FeatureSelectionStrategy:
        """"""
        Returns the appropriate feature selection strategy based on the model type.
        """"""

        if self.model_type == ""lstm"":
            return TechnicalAnalysisFeatureSelector()
        elif self.model_type == ""prophet"":
            return ProphetFeatureSelector()
        elif self.model_type == ""xgboost"":
            raise NotImplementedError(
                f""No feature selection stratagy implemented for the model {self.model_type}""
            )
        else:
            raise ValueError(f""Unknown model_type: {self.model_type}"")",services/data_processing/steps/feature_selection/feature_selector.py
ProphetFeatureSelector,"class ProphetFeatureSelector(FeatureSelectionStrategy):
    """"""
    Selector strategy for meta Prophet model
    """"""

    def select(self, data):

        # Rename the features Date and Close (to match Prophet Input)
        data.rename(columns={""Date"": ""ds""}, inplace=True)
        data[""ds""] = data[""ds""].dt.date
        data.rename(columns={""Close"": ""y""}, inplace=True)

        features = [
            ""y"",
            ""ds"",
            ""Open"",
            ""High"",
            ""Low"",
            ""Volume"",
            ""Returns"",
            ""MA_5"",
            ""MA_20"",
            ""Volatility"",
            ""RSI"",
            ""MACD"",
            ""MACD_Signal"",
        ]

        return data[features]",services/data_processing/steps/feature_selection/strategies/prophet_feature_selector.py
OHLCVFeatureSelector,"class OHLCVFeatureSelector(FeatureSelectionStrategy):
    """"""
    Selector strategy that returns the OHLCV (Open, High, Low, Close, Volume)
    features of the data frame.
    """"""

    def select(self, data):
        features = [""Open"", ""High"", ""Low"", ""Close"", ""Volume""]
        return data[features]",services/data_processing/steps/feature_selection/strategies/ohlcv_feature_selector.py
FeatureSelectionStrategy,"class FeatureSelectionStrategy(ABC):
    @abstractmethod
    def select(self, data: pd.DataFrame) -> pd.DataFrame:
        pass",services/data_processing/steps/feature_selection/strategies/base_strategy.py
AllFeatureSelector,"class AllFeatureSelector(FeatureSelectionStrategy):
    """"""
    Strategy that returns all the features of the data frame.
    """"""

    def select(self, data):
        return data",services/data_processing/steps/feature_selection/strategies/all_features_selector.py
SeasonalFeatureSelector,"class SeasonalFeatureSelector(FeatureSelectionStrategy):
    """"""
    Selector strategy that returns features corresponding to
    seasonal analysis along with a set of technical indicators.
    """"""

    def select(self, data):
        features = [
            ""Close"",
            ""Day_of_week"",
            ""Month"",
            ""Quarter"",
            ""MA_5"",
            ""MA_20"",
            ""MACD"",
            ""RSI"",
        ]
        return data[features]",services/data_processing/steps/feature_selection/strategies/seasonal_features_selector.py
TechnicalAnalysisFeatureSelector,"class TechnicalAnalysisFeatureSelector(FeatureSelectionStrategy):
    """"""
    Selector strategy that returns the Core Technical Analysis features
    (Open, High, Low, Close, Volume, Returns, MA_5, MA_20, RSI, MACD, MACD_Signal, Volatility).
    """"""

    def select(self, data):
        features = [
            ""Open"",
            ""High"",
            ""Low"",
            ""Close"",
            ""Volume"",
            ""Returns"",
            ""MA_5"",
            ""MA_20"",
            ""Volatility"",
            ""RSI"",
            ""MACD"",
            ""MACD_Signal"",
        ]
        return data[features]",services/data_processing/steps/feature_selection/strategies/technical_analysis_feature_selector.py
ScalerFactory,"class ScalerFactory:
    @staticmethod
    def create_scaler(model_type: str):
        if model_type == ""lstm"":
            return MinMaxScaler()
        else:
            return None",services/data_processing/steps/normalizer/scaler_factory.py
DataNormalizer,"class DataNormalizer(BaseDataProcessor):
    """"""
    Class that normalizes data for different model types using
    model-specific scalers during training and prediction.
    """"""

    def __init__(self, symbol, model_type: str, phase: str):
        self.symbol = symbol
        self.model_type = model_type
        self.phase = phase
        self.scaler_manager = ScalerManager(model_type, symbol)

    def process(self, data: ProcessedData, fit=False) -> ProcessedData:
        """"""
        Process the data by applying normalization if required.

        Args:
            data (PreprocessedData): Features and targets to normalize.
            fit (bool): If True, fit a new scaler.

        Returns:
            PreprocessedData: Normalized features and targets.
        """"""
        try:

            if self.scaler_manager.model_requires_scaling():

                # Extract features and targets
                X = data.X
                y = data.y

                results = []

                for scaler_type, inputs in {""features"": X, ""targets"": y}.items():

                    if inputs is not None:
                        # Keep the origianl shape of the data
                        original_shape = inputs.shape

                        # Reshape for scaler
                        if inputs.ndim == 3:
                            inputs = inputs.reshape(-1, inputs.shape[-1])
                        if inputs.ndim == 1:
                            inputs = inputs.reshape(-1, 1)

                        # Load (or create) the scaler
                        scaler = self._load_or_fit_scaler(
                            scaler_type, fit=fit, data=inputs
                        )

                        # Scale the data
                        scaled_data = scaler.transform(inputs)

                        # RESHAPE BACK AFTER SCALING
                        if len(original_shape) == 3:
                            scaled_data = scaled_data.reshape(original_shape)

                        elif len(original_shape) == 1:
                            scaled_data = scaled_data.reshape(
                                -1
                            )  # Flatten y back to 1D if it was 1D

                        results.append(scaled_data)
                    else:
                        results.append(None)

                return ProcessedData(
                    X=results[0], y=results[1], feature_index_map=data.feature_index_map
                )

            else:
                # No Normalization needed
                return data

        except Exception as e:
            raise RuntimeError(f""Error while scaling data."") from e

    def unprocess(self, data: ProcessedData):
        """"""
        Reverse the normalization applied to the targets included in the data.

        This method uses the stored scaler to apply inverse transformation
        to the given target data, restoring it to its original scale.

        Args:
            data (PreprocessedData): Features and targets.

        Returns:
            PreprocessedData: Unnormalized target values in the PreprocessedData format.
        """"""

        if self.scaler_manager.model_requires_scaling():

            # Extract the targets
            y = data.y

            original_y_shape = y.shape

            # Reshape for scaler
            if isinstance(y, np.ndarray):
                if y.ndim == 3:
                    y = y.reshape(-1, y.shape[-1])
                if y.ndim == 1:
                    y = y.reshape(-1, 1)

            # Load the targets scaler
            y_scaler = self.scaler_manager.load_scaler(
                self.phase, ScalerManager.TARGETS_SCALER_TYPE
            )

            # Unscale the targets
            unscaled_y = y_scaler.inverse_transform(y)

            if len(original_y_shape) == 3:
                unscaled_y = unscaled_y.reshape(original_y_shape)
            elif len(original_y_shape) == 1:
                unscaled_y = unscaled_y.reshape(-1)  # Flatten y back to 1D if it was 1D

            return ProcessedData(y=unscaled_y)
        else:
            # No Unnormalize needed
            return data

    def _load_or_fit_scaler(self, scaler_type: str, fit: bool, data):
        """"""
        Returns the appropriate scaler based on the model type.
        """"""
        try:
            if fit:
                # Create it
                scaler = self.scaler_manager.create_scaler()
                # Fit and save the scaler
                scaler.fit(data)
                self.scaler_manager.save_scaler(scaler, self.phase, scaler_type)
            else:
                scaler = self.scaler_manager.load_scaler(self.phase, scaler_type)

            return scaler

        except Exception as e:
            raise RuntimeError(f""Error preparing the scaler."") from e",services/data_processing/steps/normalizer/normalizer.py
MLflowModelManager,"class MLflowModelManager:

    # Production alias (used to identify the production model)
    PRODUCTION_ALIAS = ""production""

    def __init__(self):
        self.client = MlflowClient()

    async def list_models(self):
        """"""
        Lists all registered MLflow models
        """"""
        try:
            models = self.client.search_registered_models()
            return [model.name for model in models]
        except Exception as e:
            raise RuntimeError(f""Error listing the models: {str(e)}"") from e

    async def load_model(self, model_identifier: str):
        """"""
        Load the latest MLflow model

        Args:
            model_identifier (str): Identifier for the model (run ID of a
                logged model (training model) or name of a registered model (live model)).

        Returns:
            mlflow.pyfunc.PythonModel: The loaded MLflow model
            int: Version of the loaded MLflow model
        """"""
        try:
            if self._run_exists(model_identifier):
                # Path to the logged trained model
                model_uri = f""runs:/{model_identifier}/model""

                # There is no model version for a logged model
                version = None
            else:
                # Path to the production model (live model)
                model_uri = f""models:/{model_identifier}@{self.PRODUCTION_ALIAS}""

                # Get the version of the registred model
                versions = self.client.search_model_versions(
                    f""name='{model_identifier}'""
                )
                for v in versions:
                    if self.PRODUCTION_ALIAS in v.aliases:
                        version = v.version
                        break

            # Load the model
            model = mlflow.pyfunc.load_model(model_uri)

            return model, version

        except Exception as e:
            raise RuntimeError(
                f""Error loading the model {model_identifier}: {str(e)}""
            ) from e

    def log_metrics(self, model_identifier: str, metrics: dict):
        """"""
        Log evaluation metrics to MLflow for a given model.

        Args:
            model_identifier (str): Identifier for the model (run ID of a
                logged model (training model) or name of a registered model (live model)).
            metrics (dict): Metrics (evaluation)
        """"""
        try:
            # Get the last run_id of the model
            run_id = self._get_run_id_model(model_identifier)

            # Get the current step
            current_step = self._get_current_step(run_id=run_id)

            # New step
            new_step = current_step + 1

            # Log the metrics
            with mlflow.start_run(run_id=run_id):
                for key in metrics:
                    mlflow.log_metric(key, value=metrics[key], step=new_step)

            # Update the step
            self._update_step_tag(run_id=run_id, step=new_step)

        except Exception as e:
            raise RuntimeError(
                f""Failed to log metrics for model '{model_identifier}': {str(e)}""
            ) from e

    def promote(self, train_run_id: str, prod_model_name: str):
        """"""
        Promote a trained MLflow model (logged in a run) to a registered production model.

        Args:
            train_run_id (str): The MLflow run ID where the trained model is logged.
            prod_model_name (str): The name to register the model under in the MLflow model registry.

        Returns:
            ModelVersion: The registered MLflow ModelVersion object.
        """"""
        try:
            # Generate the training model uri
            train_model_uri = f""runs:/{train_run_id}/model""

            # Promote the model (training to prediction (prod))
            mv = mlflow.register_model(train_model_uri, prod_model_name)

            # Add the alias production to the model
            self.client.set_registered_model_alias(
                name=prod_model_name, alias=self.PRODUCTION_ALIAS, version=mv.version
            )

            # Initialize the step counter (for logging)
            self._update_step_tag(train_run_id, 0)

            return mv
        except Exception as e:
            raise RuntimeError(f""Error promoting training model : {str(e)}"") from e

    def model_exists(self, model_name: str) -> bool:
        """"""
        Check if a model exists in the MLflow registry.

        Args:
            model_name (str): The name of the model to check in the registry.

        Returns:
            bool: True if the model exists, False otherwise.
        """"""

        versions = self.client.get_latest_versions(model_name)
        return len(versions) > 0

    def get_metrics(self, model_name: str):
        """"""
        Retrieve metrics for a model based on its run ID.

        Args:
            model_name (str): The name of the model to retrieve metrics for.

        Returns:
            dict: A dictionary containing the metrics for the specified model.
        """"""
        try:
            # Get the last run of the model
            run_id = self._get_run_id_model(model_name)
            run = self.client.get_run(run_id)

            # Retrieve the metrics from the run
            metrics = run.data.metrics

            return metrics
        except Exception as e:
            raise RuntimeError(
                f""Error retrieving metrics for model {model_name}: {str(e)}""
            ) from e

    def _get_current_step(self, run_id: str, key: str = ""step"") -> int:
        """"""
        Retrieve the current step value stored as a tag in an MLflow run.

        Args:
            run_id (str): The MLflow run ID to retrieve the tag from.
            key (str): The tag key used to store the step value. Defaults to ""step"".

        Returns:
            int: The current step value. Returns 0 if the tag is not set.
        """"""
        run = self.client.get_run(run_id)
        step_str = run.data.tags.get(key, ""0"")
        return int(step_str)

    def _update_step_tag(self, run_id: str, step: int, key: str = ""step""):
        """"""
        Update or create a tag in an MLflow run to store the current step value.

        Args:
            run_id (str): The MLflow run ID where the tag should be set.
            step (int): The step value to store.
            key (str): The tag key used to store the step value. Defaults to ""step"".
        """"""
        self.client.set_tag(run_id, key, str(step))

    def _get_run_id_model(self, model_identifier: str) -> str:
        """"""
        Retrieve the run ID of a MLflow model (live model) based on the model indentifier

        Args:
            model_identifier (str): Identifier for the model (run ID of a
                logged model (training model) or name of aregistered model (live model)).

        Returns:
            str: The run ID associated with the production MLflow model
        """"""
        try:
            if self._run_exists(model_identifier):
                return model_identifier

            versions = self.client.search_model_versions(f""name='{model_identifier}'"")

            if not versions:
                raise RuntimeError(f""No versions found for model '{model_identifier}'."")

            for v in versions:
                if self.PRODUCTION_ALIAS in v.aliases:
                    return v.run_id

            raise RuntimeError(
                f""No model '{model_identifier}' has alias '{self.PRODUCTION_ALIAS}'.""
            )

        except Exception as e:
            raise RuntimeError(
                f""Failed to retrieve run ID for model '{model_identifier}': {str(e)}""
            ) from e

    def _run_exists(self, run_id: str) -> bool:
        """"""
        Check whether a run with the given run ID exists in the MLflow tracking server.

        Args:
            run_id (str): The unique identifier of the MLflow run.

        Returns:
            bool: True if the run exists and is accessible; False if it does not exist
                or has been deleted.
        """"""
        try:
            mlflow.get_run(run_id)
            return True
        except Exception:
            return False",services/deployment/mlflow_model_manager.py
DeploymentService,"class DeploymentService(BaseService):
    def __init__(self):
        super().__init__()
        self.logger = logger[""deployment""]
        self.mlflow_model_manager = None
        self.evaluator = None
        self._initialized = False
        self._prediction_cache = {}

    async def initialize(self) -> None:
        """"""Initialize the Deployment service.""""""
        try:
            await self._load_models()
            self.mlflow_model_manager = MLflowModelManager()
            self._initialized = True
            self.logger.info(""Deployment service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize deployment service: {str(e)}"")
            raise

    async def _load_models(self) -> None:
        """"""Load all models from disk.""""""
        pass

    async def production_model_exists(self, prod_model_name: str) -> bool:
        """"""
        Checks whether a production model (live model) with the given name exists
        in the MLflow registry.

        Args:
            prod_model_name (str): The name of the production model to check.

        Returns:
            bool: True if the production model exists, False otherwise.
        """"""
        try:
            models = await self.list_models()
            return prod_model_name in models
        except Exception as e:
            self.logger.error(
                f""Failed to check if {prod_model_name} production model exists: {str(e)}""
            )
            raise

    async def list_models(self):
        """"""
        Retrieves and returns a list of all available production model (live model) names
        from the MLflow model registry.

        Returns:
            List[str]: A list of model names
        """"""
        try:
            self.logger.info(""Listing all avalaible models (in MLFlow)."")
            available_models_names = await self.mlflow_model_manager.list_models()

            return available_models_names
        except Exception as e:
            self.logger.error(f""Failed to list the models: {str(e)}"")
            raise

    async def predict(
        self, model_identifier: str, X: Union[pd.DataFrame, np.ndarray, list]
    ) -> dict[Any, int]:
        """"""
        Run prediction on input data using either a logged model or a registered production model.

        Args:
            model_identifier (str): Identifier for the model (run ID of a
                logged model (training model) or name of a registered model (live model)).
            X: Input features.

        Returns:
            dict[str,Any]: A dictionary containing:
                - ""predictions"": The prediction output from the model (e.g., list, ndarray, or DataFrame).
                - ""model_version"": The version of the model used for prediction.
        """"""
        try:
            self.logger.info(f""Starting prediction using model {model_identifier}."")

            # Generate input hash for caching
            input_hash = self._hash_input(X)
            if not input_hash:
                self.logger.warning(
                    ""Could not generate a valid input hash. Caching will be skipped.""
                )
                cache_key = None
            else:
                cache_key = (model_identifier, input_hash)
                self.logger.debug(f""Cache key generated: {cache_key}"")

            # Load model version
            model, current_version = await self.mlflow_model_manager.load_model(
                model_identifier
            )

            # Log the successful loading of the model
            self.logger.debug(f""Model {model_identifier} successfully loaded."")

            # Use cache if available and version matches
            if cache_key and cache_key in self._prediction_cache:
                cached_pred, cached_ver = self._prediction_cache[cache_key]
                if cached_ver == current_version:
                    self.logger.info(
                        f""Using cached prediction for model {model_identifier} with input hash {input_hash}""
                    )
                    return {""predictions"": cached_pred, ""model_version"": cached_ver}

            # Perform prediction
            predictions = model.predict(X)

            # Log the completion of the prediction
            self.logger.info(f""Prediction completed for model {model_identifier}."")

            # Cache the result
            if cache_key:
                self._prediction_cache[cache_key] = (predictions, current_version)
                self.logger.debug(f""Prediction cached for key {cache_key}."")

            return {""predictions"": predictions, ""model_version"": current_version}

        except Exception as e:
            self.logger.error(
                f""Failed to predict with model {model_identifier} : {str(e)}""
            )
            raise

    async def calculate_prediction_confidence(
        self, model_type: str, prediction_input, y_pred
    ) -> list[float]:
        """"""
        Calculates the prediction confidence score for the specified model type.

        Args:
            model_type (str): The type of the model (e.g., ""lstm"", ""prophet"").
            prediction_input: The input data used for the prediction.
            y_pred: The model's predicted output.

        Returns:
            list[float] | None: The confidence(s) score(s) between 0 and 1, or None if unsupported.
        """"""
        try:
            self.logger.info(
                f""Starting prediction confidence calculation with {model_type} model.""
            )
            # Confidences calculation
            confidences = ConfidenceCalculator(model_type).calculate_confidence(
                y_pred, prediction_input
            )

            self.logger.info(
                f""Prediction confidence calculation doned with {model_type} model.""
            )
            return confidences

        except Exception as e:
            self.logger.error(
                f""Failed to calculate prediction confidence score with model {model_type} : {str(e)}""
            )
            raise

    async def log_metrics(self, model_identifier: str, metrics: dict) -> bool:
        """"""
        Logs evaluation metrics to MLflow for the specified model.

        Args:
            model_identifier (str): Identifier for the model (run ID of a
                logged model (training model) or name of a registered model (live model)).
            metrics: An object containing evaluation metrics to log.

        Returns:
            bool: True if the metrics were logged successfully.
        """"""
        try:
            self.mlflow_model_manager.log_metrics(model_identifier, metrics)
            self.logger.info(
                f""Metrics successfully logged to MLflow for model {model_identifier}""
            )

            return True
        except Exception as e:
            self.logger.error(
                f""Failed to log metrics to MLFlow with model {model_identifier} : {str(e)}""
            )
            return False

    async def promote_model(self, run_id: str, prod_model_name: str) -> dict[str, Any]:
        """"""
        Promote a logged training model to the production model registry.

        Parameters:
            run_id (str): The run id of the logged trained model
            prod_model_name (str): Name of the production model

        Returns:
            dict[str,Any]: Deployment results
        """"""
        try:
            # Promotion
            mv = self.mlflow_model_manager.promote(run_id, prod_model_name)
            self.logger.info(f""Successfully promoted {mv.name} model to MLflow"")

            # Invalidate all cache entries for the production model name
            to_remove = [k for k in self._prediction_cache if k[0] == prod_model_name]
            for key in to_remove:
                del self._prediction_cache[key]
                self.logger.info(
                    f""Invalidated prediction cache for model {key[0]} after promotion""
                )

            return {
                ""deployed"": True,
                ""model_name"": mv.name,
                ""version"": mv.version,
                ""run_id"": mv.run_id,
            }

        except Exception as e:
            self.logger.error(
                f""Failed to promote the training model for {prod_model_name} model : {str(e)}""
            )
            raise

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            self._initialized = False
            self.logger.info(""Deployment service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during deployment service cleanup: {str(e)}"")

    def _hash_input(self, X) -> str:
        """"""
        Returns an MD5 hash of the prediction input `X`, used for caching.

        Parameters:
            X: Prediction input data to be hashed.

        Returns:
            str: MD5 hash string or None on error.
        """"""
        try:

            def convert(obj):
                if isinstance(obj, pd.DataFrame):
                    return obj.to_dict(orient=""records"")
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                if isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):
                    return obj.item()
                raise TypeError(f""Object of type {type(obj)} is not JSON serializable"")

            input_str = json.dumps(X, sort_keys=True, default=convert)
            return hashlib.md5(input_str.encode()).hexdigest()
        except Exception as e:
            self.logger.error(f""Failed to hash input for caching: {str(e)}"")
            return None",services/deployment/deployment_service.py
ConfidenceCalculator,"class ConfidenceCalculator:
    def __init__(self, model_type: str):
        self.model_type = model_type

    def calculate_confidence(self, y_pred, prediction_input) -> list[float]:
        try:
            confidence_calculator = self._get_confidence_calculator()
            confidence = confidence_calculator.calculate(y_pred, prediction_input)
            return confidence
        except Exception as e:
            raise RuntimeError(
                f""Error while calculating the confidence score for {self.model_type} model""
            ) from e

    def _get_confidence_calculator(self) -> ConfidenceCalculatorStrategy:
        """"""
        Returns the appropriate confidence calculator strategy based on the model type.
        """"""

        if self.model_type == ""lstm"":
            return LSTMConfidenceCalculator()
        elif self.model_type == ""prophet"":
            return ProphetConfidenceCalculator()
        else:
            # There is no confidence calculator for other models
            return None",services/deployment/confidence/confidence_calculator.py
LSTMConfidenceCalculator,"class LSTMConfidenceCalculator(ConfidenceCalculatorStrategy):
    def calculate(self, y_pred, prediction_input):

        try:
            # Get the ""Close"" index
            close_index = prediction_input.feature_index_map[""Close""]

            # Get the sequence from the prediction input
            sequences = prediction_input.X

            confidences = []

            # Loop through each sequence
            for i, sequence in enumerate(sequences):
                close_series = sequence[:, close_index]

                # Calculate historical volatility for this sequence
                historical_volatility = np.std(close_series) / np.mean(close_series)

                # Prediction magnitude relative to last close price in the sequence
                last_price = close_series[-1]
                predicted_value = y_pred[i, 0]

                prediction_magnitude = (
                    abs(predicted_value - last_price) / abs(last_price)
                    if last_price != 0
                    else float(""inf"")
                )

                # Confidence logic
                volatility_factor = np.exp(-2 * historical_volatility)
                magnitude_factor = np.exp(-3 * prediction_magnitude)

                confidence = 0.5 * volatility_factor + 0.5 * magnitude_factor
                confidence = np.clip(confidence, 0, 1)

                confidences.append(round(confidence, 3))

            return confidences
        except Exception as e:
            raise e",services/deployment/confidence/strategies/lstm_confidence.py
ConfidenceCalculatorStrategy,"class ConfidenceCalculatorStrategy(ABC):
    """"""Base strategy for calculating the confidence score""""""

    @abstractmethod
    def calculate(
        self,
        y_pred,
        prediction_input: ProcessedData,
    ) -> list[float]:
        pass",services/deployment/confidence/strategies/base_strategy.py
ProphetConfidenceCalculator,"class ProphetConfidenceCalculator(ConfidenceCalculatorStrategy):
    def calculate(self, y_pred: pd.DataFrame, _):
        try:
            confidences = []
            for index, prediction in y_pred.iterrows():

                # Calculate confidence based on prediction interval width
                interval_width = prediction[""yhat_upper""] - prediction[""yhat_lower""]
                relative_width = interval_width / abs(prediction[""yhat""])

                # Convert to confidence score (0-1)
                # Use a more conservative sigmoid function that accounts for stock market uncertainty
                # The parameters are tuned to give reasonable confidence scores for typical stock price movements
                raw_confidence = 1 / (1 + np.exp(10 * relative_width - 1))

                # Round to 3 decimal places for cleaner output
                confidence = round(raw_confidence, 3)

                confidences.append(confidence)

            return confidences
        except Exception as e:
            raise e",services/deployment/confidence/strategies/prophet_confidence.py
OrchestrationService,"class OrchestrationService(BaseService):

    def __init__(
        self,
        data_service: DataService,
        preprocessing_service: DataProcessingService,
        training_service: TrainingService,
        deployment_service: DeploymentService,
        evaluation_service: EvaluationService,
    ):
        super().__init__()
        self.data_service = data_service
        self.preprocessing_service = preprocessing_service
        self.training_service = training_service
        self.deployment_service = deployment_service
        self.evaluation_service = evaluation_service
        self.logger = logger[""orchestration""]

        # Scheduler (use to schdule all predictions)
        self.scheduler = AsyncIOScheduler()

    async def initialize(self) -> None:
        """"""Initialize the orchestration service.""""""
        try:
            # Configure the prediction scheduler
            self._set_prediction_scheduler()

            self._initialized = True
            self.logger.info(""Orchestration service initialized successfully"")
        except Exception as e:
            self.logger.error(f""Failed to initialize orchestration service: {str(e)}"")
            raise

    async def run_training_pipeline(self, model_type: str, symbol: str):
        """"""
        Run the full training pipeline for the specified model and symbol.

        Args:
            model_type (str): The type of model to be used (e.g., 'LSTM', 'Prophet').
            symbol (str): The stock symbol for which the model is being trained.

        Returns:
            result: The result of the training pipeline execution.
        """"""

        try:
            # Enforce upper case for the symbol
            symbol = symbol.upper()

            self.logger.info(
                f""Starting training pipeline for {model_type} model for {symbol}.""
            )

            # Run the training pipeline
            result = run_training_flow(
                model_type,
                symbol,
                self.data_service,
                self.preprocessing_service,
                self.training_service,
                self.deployment_service,
                self.evaluation_service,
            )

            self.logger.info(
                f""Training pipeline completed successfully for {model_type} model for {symbol}.""
            )

            # Add a success status to the result
            result[""status""] = ""success""

            return result
        except Exception as e:
            self.logger.error(
                f""Error running the training pipeline for model {model_type} for {symbol}: {str(e)}""
            )
            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def run_prediction_pipeline(self, model_type: str, symbol: str):
        """"""
        Run the full prediction pipeline for the specified model and symbol.

        Args:
            model_type (str): The type of model to be used (e.g., 'LSTM', 'Prophet').
            symbol (str): The stock symbol for which the model is being used for prediction.

        Returns:
            result: The result of the prediction pipeline execution.
        """"""
        try:
            # Enforce upper case for the symbol
            symbol = symbol.upper()

            self.logger.info(
                f""Starting prediction pipeline for {model_type} model for {symbol}.""
            )

            # Run the prediction pipeline
            prediction_result = run_prediction_flow(
                model_type,
                symbol,
                self.data_service,
                self.preprocessing_service,
                self.deployment_service,
            )

            if prediction_result:

                # Extract the prediction results
                prediction = prediction_result[""y_pred""][0]
                confidence = prediction_result[""confidence""][0]
                model_version = prediction_result[""model_version""]

                self.logger.info(
                    f""Prediction pipeline completed successfully for {model_type} model for {symbol}.""
                )

                return format_prediction_response(
                    prediction=prediction,
                    confidence=confidence,
                    model_type=model_type,
                    symbol=symbol,
                    model_version=model_version,
                )

            else:
                self.logger.info(
                    f""No live model available to make prediction with {model_type} model for {symbol}.""
                )
                return {
                    ""status"": ""error"",
                    ""error"": f""No live model available to make prediction with {model_type} model for {symbol}."",
                    ""symbol"": symbol,
                    ""model_type"": model_type,
                    ""timestamp"": datetime.now().isoformat(),
                }
        except Exception as e:
            self.logger.error(
                f""Error running the prediction pipeline for model {model_type} for {symbol}: {str(e)}""
            )
            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def run_evaluation_pipeline(self, model_type: str, symbol: str):
        """"""
        Run the full evaluation pipeline for the specified model and symbol.

        Args:
            model_type (str): The type of model to be used (e.g., 'LSTM', 'Prophet').
            symbol (str): The stock symbol for which the model is being evaluated.

        Returns:
            result: The result of the evaluation pipeline execution.
        """"""

        try:
            # Enforce upper case for the symbol
            symbol = symbol.upper()

            self.logger.info(
                f""Starting evaluation pipeline for {model_type} model for {symbol}.""
            )

            # Run the evaluation pipeline
            result = run_evaluation_flow(
                model_type,
                symbol,
                self.data_service,
                self.preprocessing_service,
                self.deployment_service,
                self.evaluation_service,
            )

            # Log the successful completion of the pipeline
            self.logger.info(
                f""Evaluation pipeline completed successfully for {model_type} model for {symbol}.""
            )

            return result
        except Exception as e:
            self.logger.error(
                f""Error running the evaluation pipeline for model {model_type} for {symbol}: {str(e)}""
            )
            return {
                ""status"": ""error"",
                ""error"": str(e),
                ""symbol"": symbol,
                ""model_type"": model_type,
                ""timestamp"": datetime.now().isoformat(),
            }

    async def cleanup(self) -> None:
        """"""Clean up resources.""""""
        try:
            self._initialized = False
            self.logger.info(""Orchestration service cleaned up successfully"")
        except Exception as e:
            self.logger.error(f""Error during orchestration service cleanup: {str(e)}"")

    def _set_prediction_scheduler(self):
        """"""Set the prediction scheduler""""""
        # Start the scheduler
        self.scheduler.start()

        # Configure a trigger for the scheduler (15 mins after the market close on weekdays)
        eastern = timezone(""US/Eastern"")
        trigger = CronTrigger(
            day_of_week=""mon-fri"", hour=16, minute=45, timezone=eastern
        )

        # Add job to the scheduler
        self.scheduler.add_job(self._predict_all, trigger=trigger)

    async def _predict_all(self):
        """"""Run batch prediction for all configured model types and symbols.""""""

        # Retrieve the model types
        trainers = await self.training_service.get_trainers()
        model_types = trainers[""types""]

        # Retrieve the symbols to predict
        stocks_data = await self.data_service.get_nasdaq_stocks()
        symbols = [item[""symbol""] for item in stocks_data[""data""]]

        run_batch_prediction(
            model_types,
            symbols,
            self.data_service,
            self.preprocessing_service,
            self.deployment_service,
        )",services/orchestration/orchestration_service.py
